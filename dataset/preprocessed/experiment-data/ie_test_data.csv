sample_dir_path,rps,sentence
dataset/preprocessed/test-data/hypernym_discovery/7,['Hypernym Discovery'],"This paper describes 300 - sparsans ' participation in SemEval - 2018 Task 9 : Hypernym Discovery , with a system based on sparse coding and a formal concept hierarchy obtained from word embeddings ."
dataset/preprocessed/test-data/hypernym_discovery/3,"['Hypernym Discovery', 'Domain Adaptation']",Supervised Distributional Hypernym Discovery via Domain Adaptation
dataset/preprocessed/test-data/hypernym_discovery/3,['taxonomy learning'],"In addition , while not being taxonomy learning systems per se , semi-supervised systems for Information Extraction such as NELL rely crucially on taxonomized concepts and their relations within their learning process ."
dataset/preprocessed/test-data/hypernym_discovery/1,['Neural Hypernym Discovery'],SJTU- NLP at SemEval-2018 Task 9 : Neural Hypernym Discovery with Term Embeddings
dataset/preprocessed/test-data/hypernym_discovery/1,['hypernym discovery'],"This paper describes a hypernym discovery system for our participation in the SemEval - 2018 Task 9 , which aims to discover the best ( set of ) candidate hypernyms for input concepts or entities , given the search space of a pre-defined vocabulary ."
dataset/preprocessed/test-data/hypernym_discovery/1,['hypernym detection'],"A relevant well - known scenario is hypernym detection , which is a binary task to decide whether a hypernymic relationship holds between a pair of words or not ."
dataset/preprocessed/test-data/hypernym_discovery/5,['Hypernym Discovery'],CRIM at SemEval-2018 Task 9 : A Hybrid Approach to Hypernym Discovery
dataset/preprocessed/test-data/hypernym_discovery/8,['Detecting Hypernymy Relations'],Apollo at SemEval-2018 Task 9 : Detecting Hypernymy Relations Using Syntactic Dependencies
dataset/preprocessed/test-data/hypernym_discovery/6,['Hypernym Discovery'],EXPR at SemEval- 2018 Task 9 : A Combined Approach for Hypernym Discovery
dataset/preprocessed/test-data/hypernym_discovery/0,['Unsupervised Hypernym Discovery'],ADAPT at SemEval- 2018 Task 9 : Skip - Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora
dataset/preprocessed/test-data/hypernym_discovery/0,['hypernym discovery'],This paper describes a simple but competitive unsupervised system for hypernym discovery .
dataset/preprocessed/test-data/hypernym_discovery/2,['Hypernymy Detection'],Hypernyms under Siege : Linguistically - motivated Artillery for Hypernymy Detection
dataset/preprocessed/test-data/hypernym_discovery/2,['recognize hypernymy'],"In the last two decades , the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy ."
dataset/preprocessed/test-data/hypernym_discovery/4,['Hypernym discovery'],Hypernym discovery aims to discover the hypernym word sets given a hyponym word and proper corpus .
dataset/preprocessed/test-data/hypernym_discovery/4,['Hypernym Detection'],"In the past SemEval contest ( Sem Eval - 2015 task 17 1 , SemEval - 2016 task 13 2 ) , the "" Hypernym Detection "" task was treated as a classfication task , i.e. , given a ( hyponym , hypernym ) pair , deciding whether the pair is a true hypernymic relation or not ."
dataset/preprocessed/test-data/constituency_parsing/7,['Recurrent Neural Network Grammars'],What Do Recurrent Neural Network Grammars Learn About Syntax ?
dataset/preprocessed/test-data/constituency_parsing/7,['Recurrent neural network grammars ( RNNG )'],Recurrent neural network grammars ( RNNG ) area recently proposed probabilistic generative modeling family for natural language .
dataset/preprocessed/test-data/constituency_parsing/7,['RNNGs'],"We focus on RNNGs as generative probabilistic models over trees , as summarized in 2 ."
dataset/preprocessed/test-data/constituency_parsing/3,['Syntactic constituency parsing'],Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades .
dataset/preprocessed/test-data/constituency_parsing/1,['Pretraining of Self - attention Networks'],Cloze - driven Pretraining of Self - attention Networks
dataset/preprocessed/test-data/constituency_parsing/1,['pretraining a bi-directional transformer model'],We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems .
dataset/preprocessed/test-data/constituency_parsing/1,['Language model pretraining'],Language model pretraining has recently been shown to provide significant performance gains for a range of challenging language understanding problems .
dataset/preprocessed/test-data/constituency_parsing/5,['Transition - based Constituent Parsing'],In- Order Transition - based Constituent Parsing
dataset/preprocessed/test-data/constituency_parsing/5,['neural transition - based constituent parsing'],Both bottom - up and top - down strategies have been used for neural transition - based constituent parsing .
dataset/preprocessed/test-data/constituency_parsing/8,['Constituency Parsing'],Constituency Parsing with a Self - Attentive Encoder
dataset/preprocessed/test-data/constituency_parsing/6,['Parsing'],Parsing as Language Modeling
dataset/preprocessed/test-data/constituency_parsing/6,['syntactic parsing'],"We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing - 93.8 F 1 on section 23 , using 2 - 21 as training , 24 as development , plus tri-training ."
dataset/preprocessed/test-data/constituency_parsing/6,['deep learning syntactic parsing'],"Recent work on deep learning syntactic parsing models has achieved notably good results , e.g. , with 92.4 F 1 on Penn Treebank constituency parsing and with 92.8 F 1 ."
dataset/preprocessed/test-data/constituency_parsing/0,['Grammars'],Recurrent Neural Network Grammars
dataset/preprocessed/test-data/constituency_parsing/2,['Constituency Parsing'],An Empirical Study of Building a Strong Baseline for Constituency Parsing
dataset/preprocessed/test-data/constituency_parsing/4,['constituency parsing'],Recent work has proposed several generative neural models for constituency parsing that achieve state - of - the - art results .
dataset/preprocessed/test-data/constituency_parsing/4,['neural constituency parsing'],Recent work on neural constituency parsing has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler .
dataset/preprocessed/test-data/document_classification/7,['Multilingual Document Classification'],A Corpus for Multilingual Document Classification in Eight Languages
dataset/preprocessed/test-data/document_classification/7,['Cross - lingual document classification'],Cross - lingual document classification aims at training a document classifier on resources in one language and transferring it to a different language without any additional resources .
dataset/preprocessed/test-data/document_classification/3,['Text Classification'],Squeezed Very Deep Convolutional Neural Networks for Text Classification
dataset/preprocessed/test-data/document_classification/16,['Text Classification'],A C - LSTM Neural Network for Text Classification
dataset/preprocessed/test-data/document_classification/10,['Text Classification'],Neural Attentive Bag - of - Entities Model for Text Classification
dataset/preprocessed/test-data/document_classification/9,['Text Classification'],Investigating Capsule Networks with Dynamic Routing for Text Classification
dataset/preprocessed/test-data/document_classification/19,['Text Classification'],Text Classification Improved by Integrating Bidirectional LSTM with Two - dimensional Max Pooling
dataset/preprocessed/test-data/document_classification/1,['CROSS - LINGUAL DOCUMENT CLASSIFICATION'],BRIDGING THE DOMAIN GAP IN CROSS - LINGUAL DOCUMENT CLASSIFICATION
dataset/preprocessed/test-data/document_classification/1,['cross - lingual understanding ( XLU )'],"Recent developments in cross - lingual understanding ( XLU ) has made progress in this area , trying to bridge the language barrier using language universal representations ."
dataset/preprocessed/test-data/document_classification/1,['XLU'],We combine state - of - the - art cross - lingual methods with recently proposed methods for weakly supervised learning such as unsupervised pre-training and unsupervised data augmentation to simultaneously close both the language gap and the domain gap in XLU .
dataset/preprocessed/test-data/document_classification/5,['Text Classification'],HDLTex : Hierarchical Deep Learning for Text Classification
dataset/preprocessed/test-data/document_classification/5,['document classification'],"Central to these information processing methods is document classification , which has become an important application for supervised learning ."
dataset/preprocessed/test-data/document_classification/5,['hierarchical classification'],Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification ( HDLTex ) .
dataset/preprocessed/test-data/document_classification/18,['Text Classification'],Character - level Convolutional Networks for Text Classification
dataset/preprocessed/test-data/document_classification/15,['SEMI - SUPERVISED TEXT CLASSIFICATION'],ADVERSARIAL TRAINING METHODS FOR SEMI - SUPERVISED TEXT CLASSIFICATION
dataset/preprocessed/test-data/document_classification/8,['Text Categorization'],Disconnected Recurrent Neural Networks for Text Categorization
dataset/preprocessed/test-data/document_classification/6,['Text Classification'],Explicit Interaction Model towards Text Classification
dataset/preprocessed/test-data/document_classification/11,['Text Classification'],Task - oriented Word Embedding for Text Classification
dataset/preprocessed/test-data/document_classification/14,['Supervised and Semi- Supervised Text Categorization'],Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings
dataset/preprocessed/test-data/document_classification/14,['text categorization'],"One - hot CNN ( convolutional neural network ) has been shown to be effective for text categorization ( Johnson & Zhang , 2015a ; b ) ."
dataset/preprocessed/test-data/document_classification/17,['Text Classification'],Very Deep Convolutional Networks for Text Classification
dataset/preprocessed/test-data/document_classification/0,['Text Classification'],Bag of Tricks for Efficient Text Classification
dataset/preprocessed/test-data/document_classification/12,['Text Classification'],Graph Convolutional Networks for Text Classification
dataset/preprocessed/test-data/document_classification/2,['Document Classification'],Rethinking Complex Neural Network Architectures for Document Classification
dataset/preprocessed/test-data/document_classification/4,['Text Classification'],Joint Embedding of Words and Labels for Text Classification
dataset/preprocessed/test-data/document_classification/13,['Text Categorization'],Deep Pyramid Convolutional Neural Networks for Text Categorization
dataset/preprocessed/test-data/document_classification/20,['Practical Text Classification'],Practical Text Classification With Large Pre-Trained Language Models
dataset/preprocessed/test-data/face_alignment/7,['Joint 3D Face Reconstruction and Dense Alignment'],Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network
dataset/preprocessed/test-data/face_alignment/7,"['3 D face reconstruction', 'face alignment']",3 D face reconstruction and face alignment are two fundamental and highly related topics in computer vision .
dataset/preprocessed/test-data/face_alignment/3,['Robust Face Alignment'],Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression
dataset/preprocessed/test-data/face_alignment/3,"['Face alignment', 'facial landmark localization']","Face alignment , also known as facial landmark localization , seeks to localize pre-defined landmarks on human faces ."
dataset/preprocessed/test-data/face_alignment/16,['Face Alignment'],Deep Multi- Center Learning for Face Alignment
dataset/preprocessed/test-data/face_alignment/10,['Robust Facial Landmark Localisation'],Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks
dataset/preprocessed/test-data/face_alignment/10,"['Facial landmark localisation', 'face alignment']","Facial landmark localisation , or face alignment , aims at finding the coordinates of a set of pre-defined key points for 2 D face images ."
dataset/preprocessed/test-data/face_alignment/9,['Facial Landmark Detection'],Semantic Alignment : Finding Semantically Consistent Ground - truth for Facial Landmark Detection
dataset/preprocessed/test-data/face_alignment/1,['Large Pose 3D Face Reconstruction from a Single Image'],Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression
dataset/preprocessed/test-data/face_alignment/1,['3 D face reconstruction'],Abstract 3 D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty .
dataset/preprocessed/test-data/face_alignment/5,['Face Alignment'],Look at Boundary : A Boundary - Aware Face Alignment Algorithm
dataset/preprocessed/test-data/face_alignment/5,['facial landmark detection'],"Face alignment , which refers to facial landmark detection in this work , serves as a key step for many face applications , e.g. , face recognition , face verification and face frontalisation ."
dataset/preprocessed/test-data/face_alignment/18,['robust face alignment'],Deep Alignment Network : A convolutional neural network for robust face alignment
dataset/preprocessed/test-data/face_alignment/18,['face alignment'],"The goal of face alignment is to localize a set of predefined facial landmarks ( eye corners , mouth corners etc. ) in an image of a face ."
dataset/preprocessed/test-data/face_alignment/15,['Face Alignment Across Large Poses'],Face Alignment Across Large Poses : A 3D Solution
dataset/preprocessed/test-data/face_alignment/15,['Face alignment'],"Face alignment , which fits a face model to an image and extracts the semantic meanings of facial pixels , has been an important topic in CV community ."
dataset/preprocessed/test-data/face_alignment/8,['Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image'],Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D - Assisted Self - Supervised Learning
dataset/preprocessed/test-data/face_alignment/8,['3 D face reconstruction from a single 2D image'],3 D face reconstruction from a single 2D image is a challenging problem with broad applications .
dataset/preprocessed/test-data/face_alignment/8,['3 D face reconstruction'],3 D face reconstruction is an important task in the field of computer vision and graphics .
dataset/preprocessed/test-data/face_alignment/6,['Face Alignment'],Face Alignment using a 3D Deeply - initialized Ensemble of Regression Trees
dataset/preprocessed/test-data/face_alignment/11,['3D Morphable Model Regression'],Unsupervised Training for 3D Morphable Model Regression
dataset/preprocessed/test-data/face_alignment/14,['Facial Alignment'],Faster Than Real - time Facial Alignment : A 3D Spatial Transformer Network Approach in Unconstrained Poses
dataset/preprocessed/test-data/face_alignment/14,"['Robust face recognition and analysis', 'localization of facial features']",Robust face recognition and analysis are contingent upon accurate localization of facial features .
dataset/preprocessed/test-data/face_alignment/17,"['Facial landmark detection', 'face alignment']","Facial landmark detection , or face alignment , is a fundamental task that has been extensively studied ."
dataset/preprocessed/test-data/face_alignment/0,['reconstructing facial shape from single images'],"3D Morphable Models ( 3 DMMs ) are powerful statistical models of 3D facial shape and texture , and among the stateof - the - art methods for reconstructing facial shape from single images ."
dataset/preprocessed/test-data/face_alignment/0,['3 D facial shape estimation from single images'],3 D facial shape estimation from single images has attracted the attention of many researchers the past twenty years .
dataset/preprocessed/test-data/face_alignment/12,['Dense Face Alignment'],Dense Face Alignment
dataset/preprocessed/test-data/face_alignment/12,['Face alignment'],Face alignment is a classic problem in the computer vision field .
dataset/preprocessed/test-data/face_alignment/2,['Face Alignment In The Wild'],DeCaFA : Deep Convolutional Cascade for Face Alignment In The Wild
dataset/preprocessed/test-data/face_alignment/2,['Face Alignment'],"Face Alignment is an active computer vision domain , that consists in localizing a number of facial landmarks that vary across datasets ."
dataset/preprocessed/test-data/face_alignment/4,['Facial Landmarks Detection'],Facial Landmarks Detection by Self - Iterative Regression based Landmarks - Attention Network
dataset/preprocessed/test-data/face_alignment/13,"['face alignment', '3D reconstruction']","We demonstrate the superior representation power of our nonlinear 3 DMM over its linear counterpart , and its contribution to face alignment and 3D reconstruction ."
dataset/preprocessed/test-data/data-to-text_generation/3,['Text Generation'],Pragmatically Informative Text Generation
dataset/preprocessed/test-data/data-to-text_generation/3,['language generation and interpretation'],Computational approaches to pragmatics cast language generation and interpretation as gametheoretic or Bayesian inference procedures .
dataset/preprocessed/test-data/data-to-text_generation/1,['Sequence - to - Sequence Natural Language Generation'],A Deep Ensemble Model with Slot Alignment for Sequence - to - Sequence Natural Language Generation
dataset/preprocessed/test-data/data-to-text_generation/1,['Natural language generation'],Natural language generation lies at the core of generative dialogue systems and conversational agents .
dataset/preprocessed/test-data/data-to-text_generation/5,['Neural Data - to - Text Generation'],Step - by - Step : Separating Planning from Realization in Neural Data - to - Text Generation
dataset/preprocessed/test-data/data-to-text_generation/5,['Data - to - text generation'],"Data - to - text generation can be conceptually divided into two parts : ordering and structuring the information ( planning ) , and generating fluent language describing the information ( realization ) ."
dataset/preprocessed/test-data/data-to-text_generation/6,['Character - based Data - to - text Generation'],Copy Mechanism and Tailored Training for Character - based Data - to - text Generation
dataset/preprocessed/test-data/data-to-text_generation/6,['natural language generation'],"In the last few years , many different methods have been focusing on using deep recurrent neural networks for natural language generation ."
dataset/preprocessed/test-data/data-to-text_generation/6,['text generation'],"Moreover , since characters constitute the common "" building blocks "" of every text , it also allows a more general approach to text generation , enabling the possibility to exploit transfer learning for training ."
dataset/preprocessed/test-data/data-to-text_generation/0,['Data - to - Text Generation'],A Hierarchical Model for Data - to - Text Generation
dataset/preprocessed/test-data/data-to-text_generation/0,"['Transcribing structured data into natural language descriptions', 'data - to - text']","Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as "" data - to - text "" ."
dataset/preprocessed/test-data/data-to-text_generation/2,['Structured Data to Text Generation'],Deep Graph Convolutional Encoders for Structured Data to Text Generation
dataset/preprocessed/test-data/data-to-text_generation/2,['neural text generation from graph - structured data'],Most previous work on neural text generation from graph - structured data relies on standard sequence - to - sequence methods .
dataset/preprocessed/test-data/data-to-text_generation/2,['graph structured data to text generation'],Most previous work casts the graph structured data to text generation task as a sequenceto - sequence problem .
dataset/preprocessed/test-data/data-to-text_generation/4,['Data - to - Text Generation'],Data - to - Text Generation with Content Selection and Planning
dataset/preprocessed/test-data/dependency_parsing/7,['parsing'],"This form of training , which accounts for model predictions at training time , improves parsing accuracies ."
dataset/preprocessed/test-data/dependency_parsing/7,['Natural language parsing'],"Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures ; this formalization is known as transitionbased parsing , and is often coupled with a greedy search procedure ."
dataset/preprocessed/test-data/dependency_parsing/7,['transition - based parsing'],"The literature on transition - based parsing is vast , but all works share in common a classification component that takes into account features of the current parser state 1 and predicts the next action to take conditioned on the state ."
dataset/preprocessed/test-data/dependency_parsing/3,"['POS tagging', 'dependency parsing']",From POS tagging to dependency parsing for biomedical event extraction
dataset/preprocessed/test-data/dependency_parsing/1,['Dependency Parsing'],Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations
dataset/preprocessed/test-data/dependency_parsing/5,['Neural Network Transition - Based Parsing'],Structured Training for Neural Network Transition - Based Parsing
dataset/preprocessed/test-data/dependency_parsing/5,['neural network transition - based dependency parsing'],We present structured perceptron training for neural network transition - based dependency parsing .
dataset/preprocessed/test-data/dependency_parsing/5,['dependency parsing'],"Lately , dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages and the efficiency of dependency parsers ."
dataset/preprocessed/test-data/dependency_parsing/5,['transition - based parsing'],"In transition - based parsing , sentences are processed in a linear left to right pass ; at each position , the parser needs to choose from a set of possible actions defined by the transition strategy ."
dataset/preprocessed/test-data/dependency_parsing/8,['Globally Normalized Transition - Based Neural Networks'],Globally Normalized Transition - Based Neural Networks
dataset/preprocessed/test-data/dependency_parsing/8,['globally normalized transition - based neural network'],"We introduce a globally normalized transition - based neural network model that achieves state - of - the - art part - ofspeech tagging , dependency parsing and sentence compression results ."
dataset/preprocessed/test-data/dependency_parsing/6,['NEURAL DEPENDENCY PARSING'],DEEP BIAFFINE ATTENTION FOR NEURAL DEPENDENCY PARSING
dataset/preprocessed/test-data/dependency_parsing/0,['joint POS tagging and dependency parsing'],An improved neural network model for joint POS tagging and dependency parsing
dataset/preprocessed/test-data/dependency_parsing/0,['joint part - of - speech ( POS ) tagging and dependency parsing'],We propose a novel neural network model for joint part - of - speech ( POS ) tagging and dependency parsing .
dataset/preprocessed/test-data/dependency_parsing/0,['Dependency parsing'],"Dependency parsing - a key research topic in natural language processing ( NLP ) in the last decade ) - has also been demonstrated to be extremely useful in many applications such as relation extraction , semantic parsing and machine translation ) ."
dataset/preprocessed/test-data/dependency_parsing/2,['graphbased dependency parsing'],"It represents a new state of the art for graphbased dependency parsing for English , Chinese , and German ."
dataset/preprocessed/test-data/dependency_parsing/4,['Dependency Parsing'],Stack - Pointer Networks for Dependency Parsing
dataset/preprocessed/test-data/entity_linking/7,['Semi-supervised Word Sense Disambiguation'],Semi-supervised Word Sense Disambiguation with Neural Models
dataset/preprocessed/test-data/entity_linking/7,['word sense disambiguation ( WSD )'],Determining the intended sense of words in text - word sense disambiguation ( WSD ) - is a longstanding problem in natural language processing .
dataset/preprocessed/test-data/entity_linking/7,['WSD'],"Recently , researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms ."
dataset/preprocessed/test-data/entity_linking/3,['Word Sense Disambiguation'],Does BERT Make Any Sense ? Interpretable Word Sense Disambiguation with Contextualized Embeddings
dataset/preprocessed/test-data/entity_linking/3,['word sense disambiguation ( WSD )'],"Since vectors of the same word type can vary depending on the respective context , they implicitly provide a model for word sense disambiguation ( WSD ) ."
dataset/preprocessed/test-data/entity_linking/3,['WSD'],We introduce a simple but effective approach to WSD using a nearest neighbor classification on CWEs .
dataset/preprocessed/test-data/entity_linking/16,['Word Sense Disambiguation'],One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data
dataset/preprocessed/test-data/entity_linking/16,['Word Sense Disambiguation ( WSD )'],"To mine these data properly , attributable to their innate ambiguity , a Word Sense Disambiguation ( WSD ) algorithm can avoid numbers of difficulties in Natural Language Processing ( NLP ) pipeline ."
dataset/preprocessed/test-data/entity_linking/16,['WSD'],"However , considering a large number of ambiguous words in one language or technical domain , we may encounter limiting constraints for proper deployment of existing WSD models ."
dataset/preprocessed/test-data/entity_linking/10,['Deep contextualized word representations'],Deep contextualized word representations
dataset/preprocessed/test-data/entity_linking/10,['deep contextualized word representation'],"We introduce a new type of deep contextualized word representation that models both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) ."
dataset/preprocessed/test-data/entity_linking/10,['Pre-trained word representations'],Pre-trained word representations are a key component in many neural language understanding models .
dataset/preprocessed/test-data/entity_linking/9,['Named Entity Disambiguation'],Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation
dataset/preprocessed/test-data/entity_linking/9,['Named Entity Disambiguation ( NED )'],"Named Entity Disambiguation ( NED ) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base ( KB ) ( e.g. , Wikipedia ) ."
dataset/preprocessed/test-data/entity_linking/9,['NED'],"In this paper , we propose a novel embedding method specifically designed for NED ."
dataset/preprocessed/test-data/entity_linking/1,['Named Entity Disambiguation'],Pre-training of Deep Contextualized Embeddings of Words and Entities for Named Entity Disambiguation
dataset/preprocessed/test-data/entity_linking/1,['named entity disambiguation ( NED )'],"In this paper , we propose a new contextualized embedding model of words and entities for named entity disambiguation ( NED ) ."
dataset/preprocessed/test-data/entity_linking/1,['NED'],We evaluated our model by addressing NED using a simple NED model based on the trained contextualized embeddings .
dataset/preprocessed/test-data/entity_linking/5,['Neural Word Sense Disambiguation'],Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships
dataset/preprocessed/test-data/entity_linking/5,['Word Sense Disambiguation ( WSD )'],"In Word Sense Disambiguation ( WSD ) , the predominant approach generally involves a supervised system trained on sense annotated corpora ."
dataset/preprocessed/test-data/entity_linking/5,['WSD'],"Our method leads to state of the art results on most WSD evaluation tasks , while improving the coverage of supervised systems , reducing the training time and the size of the models , without additional training data ."
dataset/preprocessed/test-data/entity_linking/15,['Entity Linking'],Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories
dataset/preprocessed/test-data/entity_linking/15,['entity linking ( EL )'],"The first stage for every QA approach is entity linking ( EL ) , that is the identification of entity mentions in the question and linking them to entities in KB ."
dataset/preprocessed/test-data/entity_linking/15,['EL'],The state - of - the - art QA systems usually rely on off - the - shelf EL systems to extract entities from the question .
dataset/preprocessed/test-data/entity_linking/8,"['learning word embeddings', 'context dependent representations of phrases']","Language modeling tasks , in which words , or word - pieces , are predicted on the basis of a local context , have been very effective for learning word embeddings and context dependent representations of phrases ."
dataset/preprocessed/test-data/entity_linking/6,['Neural Word Sense Disambiguation'],Incorporating Glosses into Neural Word Sense Disambiguation
dataset/preprocessed/test-data/entity_linking/6,['Word Sense Disambiguation ( WSD )'],Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .
dataset/preprocessed/test-data/entity_linking/6,['WSD'],Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .
dataset/preprocessed/test-data/entity_linking/11,['Neural Word Sense Disambiguation'],Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation
dataset/preprocessed/test-data/entity_linking/11,['neural WSD'],"We propose two different methods that greatly reduce the size of neural WSD models , with the benefit of improving their coverage without additional training data , and without impacting their precision ."
dataset/preprocessed/test-data/entity_linking/11,['WSD'],"In addition to our methods , we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks ."
dataset/preprocessed/test-data/entity_linking/11,['Word Sense Disambiguation ( WSD )'],"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory ."
dataset/preprocessed/test-data/entity_linking/14,['Knowledge - based Word Sense Disambiguation'],Knowledge - based Word Sense Disambiguation using Topic Models
dataset/preprocessed/test-data/entity_linking/14,['Disambiguation'],Disambiguation is an open problem in Natural Language Processing which is particularly challenging and useful in the unsupervised setting where all the words in any given text need to be disambiguated without using any labeled data .
dataset/preprocessed/test-data/entity_linking/14,['WSD'],Typically WSD systems use the sentence or a small window of words around the target word as the context for disambiguation because their computational complexity scales exponentially with the size of the context .
dataset/preprocessed/test-data/entity_linking/14,['Word Sense Disambiguation ( WSD )'],Word Sense Disambiguation ( WSD ) is the task of mapping an ambiguous word in a given context to its correct meaning .
dataset/preprocessed/test-data/entity_linking/0,['Joint Entity Disambiguation'],Deep Joint Entity Disambiguation with Local Neural Attention
dataset/preprocessed/test-data/entity_linking/0,['Entity disambiguation ( ED )'],Entity disambiguation ( ED ) is an important stage in text understanding which automatically resolves references to entities in a given knowledge base ( KB ) .
dataset/preprocessed/test-data/entity_linking/0,['ED'],"ED research has largely focused on two types of contextual information for disambiguation : local information based on words that occur in a context window around an entity mention , and , global information , exploiting document - level coherence of the referenced entities ."
dataset/preprocessed/test-data/entity_linking/12,['Neural Word Sense Disambiguation'],Incorporating Glosses into Neural Word Sense Disambiguation
dataset/preprocessed/test-data/entity_linking/12,['Word Sense Disambiguation ( WSD )'],Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .
dataset/preprocessed/test-data/entity_linking/12,['WSD'],Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .
dataset/preprocessed/test-data/entity_linking/2,['Word Sense Disambiguation'],Neural Sequence Learning Models for Word Sense Disambiguation
dataset/preprocessed/test-data/entity_linking/2,['WSD'],"As one of the long - standing challenges in Natural Language Processing ( NLP ) , Word Sense Disambiguation , WSD ) has received considerable attention over recent years ."
dataset/preprocessed/test-data/entity_linking/4,['Learning Distributed Representations of Texts and Entities from Knowledge Base'],Learning Distributed Representations of Texts and Entities from Knowledge Base
dataset/preprocessed/test-data/entity_linking/4,"['jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities']","In particular , we propose Neural Text - Entity Encoder ( NTEE ) , a neural network model to jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities ."
dataset/preprocessed/test-data/entity_linking/13,['Word Sense Disambiguation'],Word Sense Disambiguation using a Bidirectional LSTM
dataset/preprocessed/test-data/entity_linking/13,['word sense disambiguation ( WSD )'],"The task of assigning a word token in a text , e.g. rock , to a well defined word sense in a lexicon is called word sense disambiguation ( WSD ) ."
dataset/preprocessed/test-data/entity_linking/13,['WSD'],"Improved WSD would be beneficial to many natural language processing ( NLP ) problems , e.g. machine translation , information Retrieval , information Extraction , and sense aware word representations ."
dataset/preprocessed/test-data/coreference_resolution/7,['Coreference Resolution'],Coreference Resolution with Entity Equalization
dataset/preprocessed/test-data/coreference_resolution/3,['Coreference Resolution'],Higher - order Coreference Resolution with Coarse - to - fine Inference
dataset/preprocessed/test-data/coreference_resolution/9,['Coreference Resolution'],BERT for Coreference Resolution : Baselines and Analysis
dataset/preprocessed/test-data/coreference_resolution/1,['Coreference Resolution'],End - to - end Deep Reinforcement Learning Based Coreference Resolution
dataset/preprocessed/test-data/coreference_resolution/5,['Coreference Resolution'],Learning Global Features for Coreference Resolution
dataset/preprocessed/test-data/coreference_resolution/5,['coreference prediction'],There is compelling evidence that coreference prediction would benefit from modeling global information about entity - clusters .
dataset/preprocessed/test-data/coreference_resolution/8,['End - to - end Neural Coreference Resolution'],End - to - end Neural Coreference Resolution
dataset/preprocessed/test-data/coreference_resolution/8,['end - to - end coreference resolution'],We introduce the first end - to - end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector .
dataset/preprocessed/test-data/coreference_resolution/6,['End - to - End Co -reference Resolution'],Learning Word Representations with Cross - Sentence Dependency for End - to - End Co -reference Resolution
dataset/preprocessed/test-data/coreference_resolution/6,['end - to - end co-reference resolution ( E2E - CR )'],"In this work , we present a word embedding model that learns cross - sentence dependency for improving end - to - end co-reference resolution ( E2E - CR ) ."
dataset/preprocessed/test-data/coreference_resolution/6,['E2E - CR'],"While the traditional E2E - CR model generates word representations by running long short - term memory ( LSTM ) recurrent neural networks on each sentence of an input article or conversation separately , we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency ."
dataset/preprocessed/test-data/coreference_resolution/6,['Co-reference resolution'],Co-reference resolution requires models to cluster mentions that refer to the same physical entities .
dataset/preprocessed/test-data/coreference_resolution/0,['Coreference Resolution'],Improving Coreference Resolution by Learning Entity - Level Distributed Representations
dataset/preprocessed/test-data/coreference_resolution/2,['Coreference resolution'],Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning .
dataset/preprocessed/test-data/coreference_resolution/4,['Abstract Anaphora Resolution'],A Mention - Ranking Model for Abstract Anaphora Resolution
dataset/preprocessed/test-data/coreference_resolution/4,['anaphora ( or coreference ) resolution'],"Current research in anaphora ( or coreference ) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real Leo Born , Juri Opitz and Anette Frank contributed equally to this work ."
dataset/preprocessed/test-data/face_detection/7,['Object Detection'],Recurrent Scale Approximation for Object Detection in CNN
dataset/preprocessed/test-data/face_detection/7,['multiscale object detection'],"Since convolutional neural network ( CNN ) lacks an inherent mechanism to handle large scale variations , we always need to compute feature maps multiple times for multiscale object detection , which has the bottleneck of computational cost in practice ."
dataset/preprocessed/test-data/face_detection/3,['face detection'],"We propose a method to address challenges in unconstrained face detection , such as arbitrary pose variations and occlusions ."
dataset/preprocessed/test-data/face_detection/3,['automatic face recognition'],It is the first step in automatic face recognition applications .
dataset/preprocessed/test-data/face_detection/3,['unconstrained face detection problem'],"In this paper , we refer to face detection with arbitrary facial variations as the unconstrained face detection problem ."
dataset/preprocessed/test-data/face_detection/16,"['Face Detection', 'Landmark Localization', 'Pose Estimation', 'Gender Recognition']","HyperFace : A Deep Multi-task Learning Framework for Face Detection , Landmark Localization , Pose Estimation , and Gender Recognition"
dataset/preprocessed/test-data/face_detection/10,['face detection'],"In this paper , we propose a novel face detection network with three novel contributions that address three key aspects of face detection , including better feature learning , progressive loss design and anchor assign based data augmentation , respectively ."
dataset/preprocessed/test-data/face_detection/21,['Efficient Face Detection'],Supervised Transformer Network for Efficient Face Detection
dataset/preprocessed/test-data/face_detection/21,['real - word face detection'],Large pose variations remain to be a challenge that confronts real - word face detection .
dataset/preprocessed/test-data/face_detection/9,['Finding Tiny Faces'],Finding Tiny Faces
dataset/preprocessed/test-data/face_detection/9,"['object recognition', 'detecting small objects']","Though tremendous strides have been made in object recognition , one of the remaining open challenges is detecting small objects ."
dataset/preprocessed/test-data/face_detection/9,['finding small faces'],"We explore three aspects of the problem in the context of finding small faces : the role of scale invariance , image resolution , and contextual reasoning ."
dataset/preprocessed/test-data/face_detection/9,['face detection'],"lem in the context of face detection : the role of scale invariance , image resolution and contextual reasoning ."
dataset/preprocessed/test-data/face_detection/19,['Face Detection'],Face Detection Using Improved Faster RCNN
dataset/preprocessed/test-data/face_detection/1,['Detecting faces in an image'],"Detecting faces in an image is considered to be one of the most practical tasks in computer vision applications , and many studies are proposed from the beginning of the computer vision research ."
dataset/preprocessed/test-data/face_detection/1,['face detection'],"After the advent of deep neural networks , many face detection algorithms applying the deep network have reported significant performance improvement to the conventional face detectors ."
dataset/preprocessed/test-data/face_detection/5,['Face detection and alignment in unconstrained environment'],"Abstract - Face detection and alignment in unconstrained environment are challenging due to various poses , illuminations and occlusions ."
dataset/preprocessed/test-data/face_detection/5,['face detection and face alignment'],"However , most of the available face detection and face alignment methods ignore the inherent correlation between these two tasks ."
dataset/preprocessed/test-data/face_detection/18,['Unconstrained Face Detection'],CMS- RCNN : Contextual Multi- Scale Region - based CNN for Unconstrained Face Detection
dataset/preprocessed/test-data/face_detection/18,['Robust face detection in the wild'],"Robust face detection in the wild is one of the ultimate components to support various facial related problems , i.e. unconstrained face recognition , facial periocular recognition , facial landmarking and pose estimation , facial expression recognition , 3 D facial model construction , etc ."
dataset/preprocessed/test-data/face_detection/18,['face detection'],"Although the face detection problem has been intensely studied for decades with various commercial applications , it still meets problems in some real - world scenarios due to numerous challenges , e.g. heavy facial occlusions , extremely low resolutions , strong illumination , exceptionally pose variations , image or video compression artifacts , etc ."
dataset/preprocessed/test-data/face_detection/15,['face detection'],"Although tremendous strides have been made in face detection , one of the remaining open challenges is to achieve real - time speed on the CPU as well as maintain high performance , since effective models for face detection tend to be computationally prohibitive ."
dataset/preprocessed/test-data/face_detection/8,['Detecting Faces'],Detecting Faces Using Region - based Fully Convolutional Networks
dataset/preprocessed/test-data/face_detection/8,['Face detection'],Face detection has achieved great success using the region - based methods .
dataset/preprocessed/test-data/face_detection/6,['Robust Face Detection'],Robust Face Detection via Learning Small Faces on Hard Images
dataset/preprocessed/test-data/face_detection/6,['Face detection'],"Face detection is a fundamental and important computer vision problem , which is critical for many face - related tasks , such as face alignment , tracking and recognition ."
dataset/preprocessed/test-data/face_detection/11,['Fast Object Detection'],A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection
dataset/preprocessed/test-data/face_detection/11,['fast multi-scale object detection'],"A unified deep neural network , denoted the multi -scale CNN ( MS - CNN ) , is proposed for fast multi-scale object detection ."
dataset/preprocessed/test-data/face_detection/11,['object detection'],"State - of - the - art object detection performance , at up to 15 fps , is reported on datasets , such as KITTI and Caltech , containing a substantial number of small objects ."
dataset/preprocessed/test-data/face_detection/14,['Face Detection'],WIDER FACE : A Face Detection Benchmark
dataset/preprocessed/test-data/face_detection/17,['Face detection'],"Face detection has been well studied for many years and one of remaining challenges is to detect small , blurred and partially occluded faces in uncontrolled environment ."
dataset/preprocessed/test-data/face_detection/0,['Face Detection'],Accurate Face Detection for High Performance
dataset/preprocessed/test-data/face_detection/12,['object detection in images'],Region proposal mechanisms are essential for existing deep learning approaches to object detection in images .
dataset/preprocessed/test-data/face_detection/12,['object detection'],"Existing deep learning approaches to solve this task ( e.g. , R - CNN and its variants ) mainly rely on region proposal mechanisms ( e.g. , region proposal networks ( RPN s ) ) to generate potential bounding boxes in an image and then classify these bounding boxes to achieve object detection ."
dataset/preprocessed/test-data/face_detection/2,['High Performance Face Detection'],Selective Refinement Network for High Performance Face Detection
dataset/preprocessed/test-data/face_detection/2,['face detection'],"High performance face detection remains a very challenging problem , especially when there exists many tiny faces ."
dataset/preprocessed/test-data/face_detection/4,['Face detection'],"Face detection , as a fundamental technology for various applications , is always deployed on edge devices which have limited memory storage and low computing power ."
dataset/preprocessed/test-data/face_detection/13,['Face Localisation in the Wild'],RetinaFace : Single - stage Dense Face Localisation in the Wild
dataset/preprocessed/test-data/face_detection/13,['face localisation'],"Though tremendous strides have been made in uncontrolled face detection , accurate and efficient face localisation in the wild remains an open challenge ."
dataset/preprocessed/test-data/face_detection/20,['Multi-view Face Detection'],Aggregate Channel Features for Multi-view Face Detection
dataset/preprocessed/test-data/face_detection/20,['Face detection'],Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones .
dataset/preprocessed/test-data/face_detection/20,['Human face detection'],Human face detection have long been one of the most fundamental problems in computer vision and humancomputer interaction .
dataset/preprocessed/test-data/natural_language_inference/28,['TRACKING THE WORLD STATE'],TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS
dataset/preprocessed/test-data/natural_language_inference/28,['simple story understanding scenario'],"In this paper , we investigate this problem through a simple story understanding scenario , in which the agent is given a sequence of textual statements and events , and then given another series of statements about the final state of the world ."
dataset/preprocessed/test-data/natural_language_inference/7,['Machine reading'],"Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen , but until now large scale training and test datasets have been missing for this type of evaluation ."
dataset/preprocessed/test-data/natural_language_inference/7,['machine reading and comprehension'],"Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars , or information extraction methods of detecting predicate argument triples that can later be queried as a relational database ."
dataset/preprocessed/test-data/natural_language_inference/3,['Neural - Network - Based Question Answering'],Exploring Question Understanding and Adaptation in Neural - Network - Based Question Answering
dataset/preprocessed/test-data/natural_language_inference/3,"['machine comprehension ( MC )', 'question answering ( QA )']",The last several years have seen intensive interest in exploring neural - networkbased models for machine comprehension ( MC ) and question answering ( QA ) .
dataset/preprocessed/test-data/natural_language_inference/25,['Multi - Document Reading Comprehension'],A Deep Cascade Model for Multi - Document Reading Comprehension
dataset/preprocessed/test-data/natural_language_inference/25,['Machine reading comprehension ( MRC )'],"Machine reading comprehension ( MRC ) , which empowers computers with the ability to read and comprehend knowledge and then answer questions from textual data , has made rapid progress in recent years ."
dataset/preprocessed/test-data/natural_language_inference/25,['MRC'],"From the early cloze - style test to answer extraction from a single paragraph , and to the more complex open - domain question answering from web data , great efforts have been made to push the MRC technique to more practical applications ."
dataset/preprocessed/test-data/natural_language_inference/16,['Text Understanding'],Text Understanding with the Attention Sum Reader Network
dataset/preprocessed/test-data/natural_language_inference/16,['text comprehension'],"Thanks to the size of these datasets , the associated text comprehension task is well suited for deep - learning techniques that currently seem to outperform all alternative approaches ."
dataset/preprocessed/test-data/natural_language_inference/10,['Story Cloze Test'],A Simple and Effective Approach to the Story Cloze Test
dataset/preprocessed/test-data/natural_language_inference/26,['Machine Reading Comprehension with Unanswerable Questions'],U - Net : Machine Reading Comprehension with Unanswerable Questions
dataset/preprocessed/test-data/natural_language_inference/26,['Machine reading comprehension ( MRC )'],"Machine reading comprehension ( MRC ) is a challenging task in natural language processing , which requires that machine can read , understand , and answer questions about a text ."
dataset/preprocessed/test-data/natural_language_inference/26,['MRC'],"Benefiting from the rapid development of deep learning techniques and large - scale benchmarks , the end - to - end neural methods have achieved promising results on MRC task ."
dataset/preprocessed/test-data/natural_language_inference/21,['Reading Comprehension'],Attention - over - Attention Neural Networks for Reading Comprehension
dataset/preprocessed/test-data/natural_language_inference/21,['Cloze - style reading comprehension'],Cloze - style reading comprehension is a representative problem in mining relationship between document and query .
dataset/preprocessed/test-data/natural_language_inference/21,['read and comprehend the human languages'],"To read and comprehend the human languages are challenging tasks for the machines , which requires that the understanding of natural languages and the ability to do reasoning over various clues ."
dataset/preprocessed/test-data/natural_language_inference/30,['Matching Natural Language Sentences'],Convolutional Neural Network Architectures for Matching Natural Language Sentences
dataset/preprocessed/test-data/natural_language_inference/30,['Semantic matching'],"Semantic matching is of central importance to many natural language tasks [ 2,28 ] ."
dataset/preprocessed/test-data/natural_language_inference/30,['Matching two potentially heterogenous language objects'],Matching two potentially heterogenous language objects is central to many natural language applications .
dataset/preprocessed/test-data/natural_language_inference/23,['Text Semantic Matching'],Deep Fusion LSTMs for Text Semantic Matching
dataset/preprocessed/test-data/natural_language_inference/23,['modelling the relevance / similarity of a pair of texts'],"Among many natural language processing ( NLP ) tasks , such as text classification , question answering and machine translation , a common problem is modelling the relevance / similarity of a pair of texts , which is also called text semantic matching ."
dataset/preprocessed/test-data/natural_language_inference/9,['Baseline for the Natural Questions'],A BERT Baseline for the Natural Questions
dataset/preprocessed/test-data/natural_language_inference/19,['Natural Language Inference'],Natural Language Inference by Tree - Based Convolution and Heuristic Matching
dataset/preprocessed/test-data/natural_language_inference/19,['recognize entailment and contradiction'],"In this paper , we propose the TBCNNpair model to recognize entailment and contradiction between two sentences ."
dataset/preprocessed/test-data/natural_language_inference/19,"['Recognizing entailment and contradiction', 'natural language inference ( NLI )']",Recognizing entailment and contradiction between two sentences ( called a premise and a hypothesis ) is known as natural language inference ( NLI ) in .
dataset/preprocessed/test-data/natural_language_inference/19,['NLI'],"Several examples are illustrated in NLI is in the core of natural language understanding and has wide applications in NLP , e.g. , question answering and automatic summarization ."
dataset/preprocessed/test-data/natural_language_inference/22,['Question Answering'],Multi- task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems
dataset/preprocessed/test-data/natural_language_inference/22,['Question Answering ( QA )'],Question Answering ( QA ) systems are used to provide proper responses to users ' questions automatically .
dataset/preprocessed/test-data/natural_language_inference/22,['QA'],Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification ( PI ) problem .
dataset/preprocessed/test-data/natural_language_inference/1,['Reading Comprehension'],Cut to the Chase : A Context Zoom - in Network for Reading Comprehension
dataset/preprocessed/test-data/natural_language_inference/1,['Reading Comprehension ( RC )'],In recent years many deep neural networks have been proposed to solve Reading Comprehension ( RC ) tasks .
dataset/preprocessed/test-data/natural_language_inference/1,['RC'],"To show the effectiveness of our architecture , we conducted several experiments on the recently proposed and challenging RC dataset ' Nar - rative QA ' ."
dataset/preprocessed/test-data/natural_language_inference/5,['Sentence Similarity Learning'],Sentence Similarity Learning by Lexical Decomposition and Composition
dataset/preprocessed/test-data/natural_language_inference/5,['sentence similarity'],"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences ."
dataset/preprocessed/test-data/natural_language_inference/27,['CONVERSATIONAL QUESTION AN - SWERING'],SDNET : CONTEXTUALIZED ATTENTION - BASED DEEP NETWORK FOR CONVERSATIONAL QUESTION AN - SWERING
dataset/preprocessed/test-data/natural_language_inference/27,['Conversational question answering ( CQA )'],Conversational question answering ( CQA ) is a novel QA task that requires understanding of dialogue context .
dataset/preprocessed/test-data/natural_language_inference/27,"['machine reading comprehension ( MRC )', 'CQA']","Different from traditional single - turn machine reading comprehension ( MRC ) tasks , CQA includes passage comprehension , coreference resolution , and contextual understanding ."
dataset/preprocessed/test-data/natural_language_inference/18,['Parameter Re-Initialization'],Parameter Re-Initialization through Cyclical Batch Size Schedules
dataset/preprocessed/test-data/natural_language_inference/18,['Optimal parameter initialization'],Optimal parameter initialization remains a crucial problem for neural network training .
dataset/preprocessed/test-data/natural_language_inference/15,['Neural Natural Language Inference'],Neural Natural Language Inference Models Enhanced with External Knowledge
dataset/preprocessed/test-data/natural_language_inference/15,['natural language inference'],Modeling natural language inference is a very challenging task .
dataset/preprocessed/test-data/natural_language_inference/15,['neural - network - based inference'],"With the availability of large annotated data , it has recently become feasible to train complex models such as neural - network - based inference models , which have shown to achieve the state - of - the - art performance ."
dataset/preprocessed/test-data/natural_language_inference/15,['natural language inference ( NLI )'],"Although there exist relatively large annotated data , can machines learn all knowledge needed to perform natural language inference ( NLI ) from these data ?"
dataset/preprocessed/test-data/natural_language_inference/15,['NLI'],"If not , how can neural - network - based NLI models benefit from external knowledge and how to build NLI models to leverage it ?"
dataset/preprocessed/test-data/natural_language_inference/15,['recognizing textual entailment ( RTE )'],"Natural language inference ( NLI ) , also known as recognizing textual entailment ( RTE ) , is an important NLP problem concerned with determining inferential relationship ( e.g. , entailment , contradiction , or neutral ) between a premise p and a hypothesis h."
dataset/preprocessed/test-data/natural_language_inference/8,['Natural Language Inference'],Learning Natural Language Inference using Bidirectional LSTM model and Inner- Attention
dataset/preprocessed/test-data/natural_language_inference/8,['recognizing text entailment'],"In this paper , we proposed a sentence encoding - based model for recognizing text entailment ."
dataset/preprocessed/test-data/natural_language_inference/8,['recognizing text entailment ( RTE )'],"Given a pair of sentences , the goal of recognizing text entailment ( RTE ) is to determine whether the hypothesis can reasonably be inferred from the premises ."
dataset/preprocessed/test-data/natural_language_inference/8,['RTE'],"There were three types of relation in RTE , Entailment ( inferred to be true ) , Contradiction ( inferred to be false ) and Neutral ( truth unknown ) ."
dataset/preprocessed/test-data/natural_language_inference/6,['Dynamic Self - Attention'],Dynamic Self - Attention : Computing Attention over Words Dynamically for Sentence Embedding
dataset/preprocessed/test-data/natural_language_inference/6,['Dynamic Self - Attention ( DSA )'],"In this paper , we propose Dynamic Self - Attention ( DSA ) , a new self - attention mechanism for sentence embedding ."
dataset/preprocessed/test-data/natural_language_inference/6,['DSA'],"We design DSA by modifying dynamic routing in capsule network ( Sabour et al. , 2017 ) for natural language processing ."
dataset/preprocessed/test-data/natural_language_inference/11,['QUESTION ANSWERING'],Published as a conference paper at ICLR 2017 DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING
dataset/preprocessed/test-data/natural_language_inference/11,['Question answering ( QA )'],Question answering ( QA ) is a crucial task in natural language processing that requires both natural language understanding and world knowledge .
dataset/preprocessed/test-data/natural_language_inference/11,['QA'],"Previous QA datasets tend to be high in quality due to human annotation , but small in size ."
dataset/preprocessed/test-data/natural_language_inference/14,['Memory Networks'],End - To - End Memory Networks
dataset/preprocessed/test-data/natural_language_inference/17,['NATURAL LANGUAGE UNDERSTAND - ING'],GLUE : A MULTI - TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTAND - ING
dataset/preprocessed/test-data/natural_language_inference/17,['natural language understanding ( NLU )'],"For natural language understanding ( NLU ) technology to be maximally useful , it must be able to process language in away that is not exclusive to a single task , genre , or dataset ."
dataset/preprocessed/test-data/natural_language_inference/17,['NLU'],"However , the low absolute performance of our best model indicates the need for improved general NLU systems ."
dataset/preprocessed/test-data/natural_language_inference/29,['MACHINE COMPREHENSION'],PHASE CONDUCTOR ON MULTI - LAYERED ATTEN - TIONS FOR MACHINE COMPREHENSION
dataset/preprocessed/test-data/natural_language_inference/29,['question answering'],"Benefiting from the availability of large - scale benchmark datasets such as SQuAD , the attention - based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors ."
dataset/preprocessed/test-data/natural_language_inference/24,['Answer Open-Domain Questions'],Reading Wikipedia to Answer Open-Domain Questions
dataset/preprocessed/test-data/natural_language_inference/24,['answering factoid questions in an open - domain setting'],"This paper considers the problem of answering factoid questions in an open - domain setting using Wikipedia as the unique knowledge source , such as one does when looking for answers in an encyclopedia ."
dataset/preprocessed/test-data/natural_language_inference/24,['open - domain question answering'],"Unlike knowledge bases ( KBs ) such as Freebase or DB - Pedia , which are easier for computers to process but too sparsely populated for open - domain question answering , Wikipedia contains up - to - date knowledge that humans are interested in ."
dataset/preprocessed/test-data/natural_language_inference/31,['Scaling Memory - Augmented Neural Networks'],Scaling Memory - Augmented Neural Networks with Sparse Reads and Writes
dataset/preprocessed/test-data/natural_language_inference/31,['memory augmented neural networks'],We refer to this class of models as memory augmented neural networks ( MANNs ) .
dataset/preprocessed/test-data/natural_language_inference/0,['Reasoning over Multiple Mentions using Coreference'],Neural Models for Reasoning over Multiple Mentions using Coreference
dataset/preprocessed/test-data/natural_language_inference/0,['coreference - based reasoning'],"We call this coreference - based reasoning since multiple pieces of information , which may lie across sentence , paragraph or document boundaries , are tied together with the help of referring expressions which denote the same real - world entity ."
dataset/preprocessed/test-data/natural_language_inference/12,['TEXT BASED REASONING'],FINDING REMO ( RELATED MEMORY OBJECT ) : A SIMPLE NEURAL ARCHITECTURE FOR TEXT BASED REASONING
dataset/preprocessed/test-data/natural_language_inference/12,['text - based question and answering'],"To solve the text - based question and answering task that requires relational reasoning , it is necessary to memorize a large amount of information and find out the question relevant information from the memory ."
dataset/preprocessed/test-data/natural_language_inference/2,['Natural Language Inference'],Stochastic Answer Networks for Natural Language Inference
dataset/preprocessed/test-data/natural_language_inference/2,['recognizing textual entailment ( RTE )'],"The natural language inference task , also known as recognizing textual entailment ( RTE ) , is to infer the relation between a pair of sentences ( e.g. , premise and hypothesis ) ."
dataset/preprocessed/test-data/natural_language_inference/4,['Large - Scale Reading Comprehension'],MemoReader : Large - Scale Reading Comprehension through Neural Memory Controller
dataset/preprocessed/test-data/natural_language_inference/4,['Machine reading comprehension'],Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text .
dataset/preprocessed/test-data/natural_language_inference/4,['Reading comprehension ( RC )'],Reading comprehension ( RC ) to understand this knowledge is a major challenge that can vastly increase the range of knowledge available to the machines .
dataset/preprocessed/test-data/natural_language_inference/4,['RC'],"In this paper , we propose a novel deep neural network architecture to handle a long - range dependency in RC tasks ."
dataset/preprocessed/test-data/natural_language_inference/13,['Natural Language Comprehension'],Natural Language Comprehension with the EpiReader
dataset/preprocessed/test-data/natural_language_inference/13,['machine comprehension of text'],"We present the EpiReader , a novel model for machine comprehension of text ."
dataset/preprocessed/test-data/natural_language_inference/13,"['Machine comprehension of unstructured , real - world text']","Machine comprehension of unstructured , real - world text is a major research goal for natural language processing ."
dataset/preprocessed/test-data/natural_language_inference/13,['machine comprehension'],"Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text , and evaluate a model 's response to the questions ."
dataset/preprocessed/test-data/natural_language_inference/20,['Text Understanding'],Neural Tree Indexers for Text Understanding
