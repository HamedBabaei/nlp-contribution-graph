sample_dir_path,text,index,label,text-len
dataset/preprocessed/test-data/hypernym_discovery/7,"This paper describes 300 - sparsans ' participation in SemEval - 2018 Task 9 : Hypernym Discovery , with a system based on sparse coding and a formal concept hierarchy obtained from word embeddings .",3,1,35
dataset/preprocessed/test-data/hypernym_discovery/7,"Our system took first place in subtasks ( 1B ) Italian ( all and entities ) , ( 1C ) Spanish entities , and ( 2B ) music entities .",4,0,30
dataset/preprocessed/test-data/hypernym_discovery/7,"Natural language phenomena are extremely sparse by their nature , whereas continuous word embeddings employ dense representations of words .",6,0,20
dataset/preprocessed/test-data/hypernym_discovery/7,Turning these dense representations into a much sparser form can help in focusing on most salient parts of word representations .,7,0,21
dataset/preprocessed/test-data/hypernym_discovery/7,Sparsity - based techniques often involve the coding of a large number of signals over the same dictionary .,8,0,19
dataset/preprocessed/test-data/hypernym_discovery/7,"Sparse , overcomplete representations have been motivated in various domains as away to increase separability and interpretability and stability in the presence of noise .",9,0,25
dataset/preprocessed/test-data/hypernym_discovery/7,Non-negativity has also been argued to be advantageous for interpretability .,10,0,11
dataset/preprocessed/test-data/hypernym_discovery/7,"As illustrates this in the language domain , where sparse features are interpreted as lexical attributes , "" to describe the city of Pittsburgh , one might talk about phenomena typical of the city , like erratic weather and large bridges .",11,0,42
dataset/preprocessed/test-data/hypernym_discovery/7,"It is redundant and inefficient to list negative properties , like the absence of the Statue of Liberty "" .",12,0,20
dataset/preprocessed/test-data/hypernym_discovery/7,utilizes non-negative sparse coding for word translation by training sparse word vectors for the two languages such that coding bases correspond to each other .,13,0,25
dataset/preprocessed/test-data/hypernym_discovery/7,Here we apply sparse feature pairs to hypernym extraction .,14,0,10
dataset/preprocessed/test-data/hypernym_discovery/7,"The role of an attribute pair i , j ? ? ( q ) ? ( h ) ( where q is the query word , h is the hypernym candidate , and ?( w ) is the index of a non -zero component in the sparse representations of w ) is similar to interaction terms in regression , see section 2 for details .",15,0,65
dataset/preprocessed/test-data/hypernym_discovery/7,Sparse representation is related to hypernymy in various natural ways .,16,0,11
dataset/preprocessed/test-data/hypernym_discovery/7,One of them is through Formal concept Analysis ( FCA ) .,17,0,12
dataset/preprocessed/test-data/hypernym_discovery/7,The idea of acquiring concept hierarchies from a text corpus with the tools of Formal concept Analysis ( FCA ) is relatively new .,18,0,24
dataset/preprocessed/test-data/hypernym_discovery/7,Our submissions experiment with formal concept analysis tool by .,19,0,10
dataset/preprocessed/test-data/hypernym_discovery/7,"See the next section for a description of formal concept lattices , and how hypernyms can be found in them .",20,0,21
dataset/preprocessed/test-data/hypernym_discovery/7,"Another natural formulation is related to hierarchical sparse coding , where trees describe the order in which variables "" enter the model "" ( i.e. , take non - zero values ) .",21,0,33
dataset/preprocessed/test-data/hypernym_discovery/7,"A node may take a non-zero value only if it s ancestors also do : the dimensions that correspond to top level nodes should focus on "" general "" meaning components thatare present in most words .",22,0,37
dataset/preprocessed/test-data/hypernym_discovery/7,offer an implementation that is efficient for gigaword corpora .,23,0,10
dataset/preprocessed/test-data/hypernym_discovery/7,Exploiting the correspondence between the variable tree and the hypernym hierarchy offers itself as a natural choice .,24,0,18
dataset/preprocessed/test-data/hypernym_discovery/7,"The task evaluated systems on their ability to extract hypernyms for query words in five subtasks ( three languages , English , Italian , and Spanish , and two domains , medical and music ) .",25,0,36
dataset/preprocessed/test-data/hypernym_discovery/7,Queries have been categorized as concepts or entities .,26,0,9
dataset/preprocessed/test-data/hypernym_discovery/7,"Results were reported for each category separately as well as in combined form , thus resulting in 5 3 combinations .",27,0,21
dataset/preprocessed/test-data/hypernym_discovery/7,"Our system took first place in subtasks ( 1B ) Italian ( all and entities ) , ( 1C ) Spanish entities , and ( 2B ) music entities .",28,0,30
dataset/preprocessed/test-data/hypernym_discovery/7,Detailed results for our system appear in section 3 .,29,0,10
dataset/preprocessed/test-data/hypernym_discovery/7,Our source code is available online 1 .,30,0,8
dataset/preprocessed/test-data/hypernym_discovery/7,Formal concept analysis,31,0,3
dataset/preprocessed/test-data/hypernym_discovery/7,Formal concept Analysis ( FCA ) is the mathematization of concept and conceptual hierarchy .,32,0,15
dataset/preprocessed/test-data/hypernym_discovery/7,"In FCA terminology , a context is a set of objects O , a set of attributes A , and a binary incidence relation I ?",33,0,26
dataset/preprocessed/test-data/hypernym_discovery/7,O A between members of O and A .,34,0,9
dataset/preprocessed/test-data/hypernym_discovery/7,"In our application , I associates a word w ?",35,0,10
dataset/preprocessed/test-data/hypernym_discovery/7,O to the indices of its non -zero sparse coding coordinates i ?,36,0,13
dataset/preprocessed/test-data/hypernym_discovery/7,"A. FCA finds formal concepts , pairs O , A of object sets and attribute sets ( O ? O , A ? A ) such that A consists of the shared attributes of objects in O ( and no more ) , and O consists of the objects in O that have all the attributes in A ( and no more ) .",37,0,64
dataset/preprocessed/test-data/hypernym_discovery/7,"( There is a closure - operator related to each FCA context , for which O and A are closed sets iff O , A is a concept . )",38,0,30
dataset/preprocessed/test-data/hypernym_discovery/7,O is called the extent and A is the intent of the concept .,39,0,14
dataset/preprocessed/test-data/hypernym_discovery/7,"There is an order defined in the context : if A 1 , B 1 and A 2 , B 2 are concepts in C ,",40,0,26
dataset/preprocessed/test-data/hypernym_discovery/7,The concept order forms a lattice .,41,0,7
dataset/preprocessed/test-data/hypernym_discovery/7,The smallest concept whose extent contains a word is said to introduce the object .,42,0,15
dataset/preprocessed/test-data/hypernym_discovery/7,We expect that h will be a hypernym of q iff n ( q ) ? n ( h ) where n ( w ) denotes the node in the concept lattice that introduces w .,43,0,36
dataset/preprocessed/test-data/hypernym_discovery/7,The closedness of extents and intents has an important structural consequence .,44,0,12
dataset/preprocessed/test-data/hypernym_discovery/7,Adding attributes to A ( e.g. responses of additional neurons ) will very probably grow the model .,45,0,18
dataset/preprocessed/test-data/hypernym_discovery/7,"However , the original concepts will be embedded as a substructure in the larger lattice , with their ordering relationships preserved .",46,0,22
dataset/preprocessed/test-data/hypernym_discovery/7,We use the popular skip - gram ( SG ) approach to train d = 100 dimensional dense distributed word representations for each sub-corpus .,47,0,25
dataset/preprocessed/test-data/hypernym_discovery/7,"The word embeddings are trained over the text corpora provided by the shared task organizers with the default training parameters of word2vec ( w2 v ) , i.e. a window size of 10 and 25 negative samples for each positive context .",48,0,42
dataset/preprocessed/test-data/hypernym_discovery/7,We derived multi-token units by relying on the word2 phrase software accompanying the w2 v toolkit .,49,0,17
dataset/preprocessed/test-data/hypernym_discovery/3,Supervised Distributional Hypernym Discovery via Domain Adaptation,2,1,7
dataset/preprocessed/test-data/hypernym_discovery/3,Lexical taxonomies are graph - like hierarchical structures that provide a formal representation of knowledge .,4,0,16
dataset/preprocessed/test-data/hypernym_discovery/3,Most knowledge graphs to date rely on is -a ( hypernymic ) relations as the backbone of their semantic structure .,5,0,21
dataset/preprocessed/test-data/hypernym_discovery/3,"In this paper , we propose a supervised distributional framework for hypernym discovery which operates at the sense level , enabling large - scale automatic acquisition of dis ambiguated taxonomies .",6,0,31
dataset/preprocessed/test-data/hypernym_discovery/3,"By exploiting semantic regularities between hyponyms and hypernyms in embeddings spaces , and integrating a domain clustering algorithm , our model becomes sensitive to the target data .",7,0,28
dataset/preprocessed/test-data/hypernym_discovery/3,"We evaluate several configurations of our approach , training with information derived from a manually created knowledge base , along with hypernymic relations obtained from Open Information Extraction systems .",8,0,30
dataset/preprocessed/test-data/hypernym_discovery/3,The integration of both sources of knowledge yields the best over all results according to both automatic and manual evaluation on ten different domains .,9,0,25
dataset/preprocessed/test-data/hypernym_discovery/3,"Lexical taxonomies ( taxonomies henceforth ) are graph - like hierarchical structures where terms are nodes , and are typically organized over a predefined merging or splitting criterion .",11,0,29
dataset/preprocessed/test-data/hypernym_discovery/3,"By embedding cues about how we perceive concepts , and how these concepts generalize in a domain of knowledge , these resources bear a capacity for generalization that lies at the core of human cognition and have become key in Natural Language Processing ( NLP ) tasks where inference and reasoning have proved to be essential .",12,0,57
dataset/preprocessed/test-data/hypernym_discovery/3,"In fact , taxonomies have enabled a remarkable number of novel NLP techniques , e.g. the contribution of WordNet to lexical semantics as well as various tasks , from word sense dis ambiguation to information retrieval , question answering and textual entailment .",13,0,43
dataset/preprocessed/test-data/hypernym_discovery/3,"To date , the application of taxonomies in NLP has consisted mainly of , on one hand , formally representing a domain of knowledge ( e.g. Food ) , and , on the other hand , constituting the semantic backbone of large - scale knowledge repositories such as ontologies or Knowledge Bases ( KBs ) .",14,0,56
dataset/preprocessed/test-data/hypernym_discovery/3,"In domain knowledge formalization , prominent work has made use of the web ( Kozareva and Hovy , 2010 ) , lexico - syntactic patterns , syntactic evidence , graph - based algorithms or popularity of web sources .",15,0,39
dataset/preprocessed/test-data/hypernym_discovery/3,"As for enabling large - scale knowledge repositories , this task often tackles the additional problem of dis ambiguating word senses and entity mentions .",16,0,25
dataset/preprocessed/test-data/hypernym_discovery/3,"Notable approaches of this kind include Yago , WikiTaxonomy , and the Wikipedia Bitaxonomy .",17,0,15
dataset/preprocessed/test-data/hypernym_discovery/3,"In addition , while not being taxonomy learning systems per se , semi-supervised systems for Information Extraction such as NELL rely crucially on taxonomized concepts and their relations within their learning process .",18,1,33
dataset/preprocessed/test-data/hypernym_discovery/3,"Taxonomy learning is roughly based on a twostep process , namely is -a ( hypernymic ) relation de-tection , and graph induction .",19,0,23
dataset/preprocessed/test-data/hypernym_discovery/3,The hypernym detection phase has gathered much interest not only for taxonomy learning but also for lexical semantics .,20,0,19
dataset/preprocessed/test-data/hypernym_discovery/3,"It has been addressed by means of pattern - based methods 1 , clustering ) and graph - based approaches .",21,0,21
dataset/preprocessed/test-data/hypernym_discovery/3,"Moreover , work stemming from distributional semantics introduced notions of linguistic regularities found in vector representations such as word embeddings .",22,0,21
dataset/preprocessed/test-data/hypernym_discovery/3,"In this area , supervised approaches , arguably the most popular nowadays , learn a feature vector between term - hypernym vector pairs and train classifiers to predict hypernymic relations .",23,0,31
dataset/preprocessed/test-data/hypernym_discovery/3,"These pairs maybe represented either as a concatenation of both vectors , difference , dot -product , or including additional linguistic information for LSTMbased learning .",24,0,26
dataset/preprocessed/test-data/hypernym_discovery/3,"In this paper we propose TAXOEMBED 2 , a hypernym detection algorithm based on sense embeddings , which can be easily applied to the construction of lexical taxonomies .",25,0,29
dataset/preprocessed/test-data/hypernym_discovery/3,"It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces and , unlike previous approaches , leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge .",26,0,36
dataset/preprocessed/test-data/hypernym_discovery/3,Our best configuration ( ranking first in two thirds of the experiments conducted ) considers two training sources :,27,0,19
dataset/preprocessed/test-data/hypernym_discovery/3,( 1 ) Manually curated pairs from Wikidata ; and ( 2 ) Hypernymy relations from a KB which integrates several Open Information Extraction ( OIE ) systems .,28,0,29
dataset/preprocessed/test-data/hypernym_discovery/3,"Since our method uses a very large semantic network as reference sense inventory , we are able to perform jointly hypernym extraction and dis ambiguation , from which 1 The terminology is not entirely unified in this respect .",29,0,39
dataset/preprocessed/test-data/hypernym_discovery/3,"In addition to pattern - based , other terms like path - based or rule - based are also used .",30,0,21
dataset/preprocessed/test-data/hypernym_discovery/3,2 Data and source code available from the following link : www.taln.upf.edu/taxoembed . expanding existing ontologies becomes a trivial task .,31,0,21
dataset/preprocessed/test-data/hypernym_discovery/3,"Compared to word - level taxonomy learning , TAXO - EMBED results in more refined and unambiguous hypernymic relations at the sense level , with a direct application in tasks such as semantic search .",32,0,35
dataset/preprocessed/test-data/hypernym_discovery/3,"Evaluation ( both manual and automatic ) shows that we can effectively replicate the Wikidata is - a branch , and capture previously unseen relations in other reference taxonomies ( YAGO or WIBI ) .",33,0,35
dataset/preprocessed/test-data/hypernym_discovery/3,Pattern - based methods for hypernym identification exploit the joint co-ocurrence of term and hypernym in text corpora .,35,0,19
dataset/preprocessed/test-data/hypernym_discovery/3,"Building upon Hearst 's patterns , these approaches have focused on , for instance , exploiting templates for harvesting candidate instances which are ranked via mutual information , training a classifier with WordNet hypernymic relations combined with syntactic dependencies , or applying a doubly - anchored method , which queries the web with two semantically related terms for collecting domainspecific corpora .",36,0,62
dataset/preprocessed/test-data/hypernym_discovery/3,"Syntactic information is also used for supervised definition and hypernym extraction , or together with Wikipedia - specific heuristics .",37,0,20
dataset/preprocessed/test-data/hypernym_discovery/3,"One of the main drawbacks of these methods is that they require both term and hypernym to co-occur in text within a certain window , which strongly hinders their recall .",38,0,31
dataset/preprocessed/test-data/hypernym_discovery/3,"Higher recall can be achieved thanks to distributional methods , as they do not have co-occurrence requirements .",39,0,18
dataset/preprocessed/test-data/hypernym_discovery/3,"In addition , they can be tailored to cover any number of predefined semantic relations such as cohyponymy or meronymy , but also cause - effect or entity - origin ) .",40,0,32
dataset/preprocessed/test-data/hypernym_discovery/3,"However , they are often more imprecise and seem to perform best in discovering broader semantic relations .",41,0,18
dataset/preprocessed/test-data/hypernym_discovery/3,"One way to surmount the issue of generality was proposed by , who explored the possibility to learn a hypernymic transformation matrix over a word embeddings space .",42,0,28
dataset/preprocessed/test-data/hypernym_discovery/3,"As shown empirically in Fu et al. 's original work , the hypernymic relation that holds for the pair ( dragonfly , insect ) differs from the one of e.g. ( carpenter , man ) .",43,0,36
dataset/preprocessed/test-data/hypernym_discovery/3,"Prior to training , their system addresses this discrepancy via k-means clustering using a held - out development set for tuning .",44,0,22
dataset/preprocessed/test-data/hypernym_discovery/3,The previously described methods for hypernym and taxonomy learning operate inherently at the surface level .,45,0,16
dataset/preprocessed/test-data/hypernym_discovery/3,"This is partly due to the way evaluation is conducted , which is often limited to very specific domains with no integrative potential ( e.g. taxonomies in food , science or equipment from ) , or restricted to lists of word pairs .",46,0,43
dataset/preprocessed/test-data/hypernym_discovery/3,"Hence , a drawback of surface - level taxonomy learning , apart from ambiguity issues , is that they require additional and error - prone steps to identify semantic clusters .",47,0,31
dataset/preprocessed/test-data/hypernym_discovery/3,"Alternatively , recent advances in OIE based on dis ambiguation and deeper semantic analysis have shown their potential to construct taxonomized dis ambiguated resources both at node and at relation level .",48,0,32
dataset/preprocessed/test-data/hypernym_discovery/3,"However , in addition to their inherently broader scope , OIE approaches are designed to achieve high coverage , and hence they tend to produce noisier data compared to taxonomy learning systems .",49,0,33
dataset/preprocessed/test-data/hypernym_discovery/1,SJTU- NLP at SemEval-2018 Task 9 : Neural Hypernym Discovery with Term Embeddings,2,1,13
dataset/preprocessed/test-data/hypernym_discovery/1,"This paper describes a hypernym discovery system for our participation in the SemEval - 2018 Task 9 , which aims to discover the best ( set of ) candidate hypernyms for input concepts or entities , given the search space of a pre-defined vocabulary .",4,1,45
dataset/preprocessed/test-data/hypernym_discovery/1,We introduce a neural network architecture for the concerned task and empirically study various neural network models to build the representations in latent space for words and phrases .,5,0,29
dataset/preprocessed/test-data/hypernym_discovery/1,"The evaluated models include convolutional neural network , long - short term memory network , gated recurrent unit and recurrent convolutional neural network .",6,0,24
dataset/preprocessed/test-data/hypernym_discovery/1,"We also explore different embedding methods , including word embedding and sense embedding for better performance .",7,0,17
dataset/preprocessed/test-data/hypernym_discovery/1,"Recently , neural network ( NN ) models have shown competitive or even better results than traditional linear models with handcrafted sparse fea -",8,0,24
dataset/preprocessed/test-data/hypernym_discovery/1,Hypernym - hyponym relationship is an is - a semantic relation between terms as shown in Table,10,0,17
dataset/preprocessed/test-data/hypernym_discovery/1,"Various natural language processing ( NLP ) tasks , especially those semantically intensive ones aiming for inference and reasoning with generalization capability , such as question answering and textual entailment , can benefit from identifying semantic relations between words beyond synonymy .",12,0,42
dataset/preprocessed/test-data/hypernym_discovery/1,The hypernym discovery task aims to discover the most appropriate hypernym ( s ) for input concepts or entities from a pre-defined corpus .,13,0,24
dataset/preprocessed/test-data/hypernym_discovery/1,"A relevant well - known scenario is hypernym detection , which is a binary task to decide whether a hypernymic relationship holds between a pair of words or not .",14,1,30
dataset/preprocessed/test-data/hypernym_discovery/1,"A hypernym detection system should be capable of learning taxonomy and lexical semantics , including pattern - based methods and graph - based approaches .",15,0,25
dataset/preprocessed/test-data/hypernym_discovery/1,"However , our concerned task , hypernym discovery , is rather more challenging since it requires the systems to explore the semantic connection with all the exhausted candidates in the latent space and rank a candidate set instead of a binary classification in previous work .",16,0,46
dataset/preprocessed/test-data/hypernym_discovery/1,"The other challenge is representation for terms , including words and phrases , where the phrase embedding could not be obtained byword embeddings directly .",17,0,25
dataset/preprocessed/test-data/hypernym_discovery/1,A simple method is to average the inner word embeddings to form the phrase embedding .,18,0,16
dataset/preprocessed/test-data/hypernym_discovery/1,"However , this is too coarse since each word might share different weights .",19,0,14
dataset/preprocessed/test-data/hypernym_discovery/1,"Current systems like ( Espinosa - Anke et al. , 2016 a ) commonly discover hypernymic relations by exploiting linear transformation matrix in embedding space , where the embedding should contain words and phrases , resulting to be parameter - exploded and hard to train .",20,0,46
dataset/preprocessed/test-data/hypernym_discovery/1,"Besides , these systems might be insufficient to obtain the deep relationships between terms .",21,0,15
dataset/preprocessed/test-data/hypernym_discovery/1,"In this work , we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases .",23,0,30
dataset/preprocessed/test-data/hypernym_discovery/1,"In our system , we leverage an unambiguous vector representation via term embedding , and we take advantage of deep neural networks to discover the hypernym relationships between terms .",24,0,30
dataset/preprocessed/test-data/hypernym_discovery/1,The rest of the paper is organized as follows :,25,0,10
dataset/preprocessed/test-data/hypernym_discovery/1,"Section 2 briefly describes our system , Section 3 shows our experiments on the hyperym discovery task including the general - purpose and domainspecific one .",26,0,26
dataset/preprocessed/test-data/hypernym_discovery/1,Section 4 concludes this paper .,27,0,6
dataset/preprocessed/test-data/hypernym_discovery/1,"Our hypernym discovery system can be roughly split into two parts , Term Embedding and Hypernym Relationship Learning .",29,0,19
dataset/preprocessed/test-data/hypernym_discovery/1,"We first train term embeddings , either using word embedding or sense embedding to represent each word .",30,0,18
dataset/preprocessed/test-data/hypernym_discovery/1,"Then , neural networks are used to discover and rank the hypernym candidates forgiven terms .",31,0,16
dataset/preprocessed/test-data/hypernym_discovery/1,"To use deep neural networks , symbolic data needs to be transformed into distributed representations .",33,0,16
dataset/preprocessed/test-data/hypernym_discovery/1,We use Glove toolkit to train the word embeddings using UMBC corpus .,34,0,13
dataset/preprocessed/test-data/hypernym_discovery/1,"Moreover , in order to perform word sense induction and dis ambiguation , the word embedding could be transformed to sense embedding , which is induced from exhisting word embeddings via clustering of ego - networks of related words .",35,0,40
dataset/preprocessed/test-data/hypernym_discovery/1,"Thus , each input word or phrase is embedded into vector sequence , w = {x 1 , x 2 , . . . , x l } where l denotes the sequence length .",36,0,35
dataset/preprocessed/test-data/hypernym_discovery/1,"If the input term is a word , then l = 1 while for phrases , l means the number of words .",37,0,23
dataset/preprocessed/test-data/hypernym_discovery/1,"Previous work like TAXOEMBED ( Espinosa - Anke et al. , 2016 a ) uses transformation matrix for hypernm relationship learning , which might be not optimal due to the lack of deeper nonlinear fea - ture extraction .",39,0,39
dataset/preprocessed/test-data/hypernym_discovery/1,"Thus , we empirically survey various neural networks to represent terms in latent space .",40,0,15
dataset/preprocessed/test-data/hypernym_discovery/1,"After obtaining the representation for input term and all the candidate hypernyms , to give the ranked hypernym list , the cosine similarity between the term and the candidate hypernym is computed by ,",41,0,34
dataset/preprocessed/test-data/hypernym_discovery/1,where x i and y i denote the two concerned vectors .,42,0,12
dataset/preprocessed/test-data/hypernym_discovery/1,"Our candidate neural networks include Convolutional Neural Network ( CNN ) , Long - short Term Memory network ( LSTM ) , Gated Recurrent Unit ( GRU ) and Recurrent Convolutional Neural Network ( RCNN ) .",43,0,37
dataset/preprocessed/test-data/hypernym_discovery/1,The structure of GRU used in this paper are described as follows .,45,0,13
dataset/preprocessed/test-data/hypernym_discovery/1,where denotes the element - wise multiplication .,46,0,8
dataset/preprocessed/test-data/hypernym_discovery/1,"rt and z tare the reset and update gates respectively , andh t the hidden states .",47,0,17
dataset/preprocessed/test-data/hypernym_discovery/1,LSTM LSTM unit is defined as follows .,48,0,8
dataset/preprocessed/test-data/hypernym_discovery/5,CRIM at SemEval-2018 Task 9 : A Hybrid Approach to Hypernym Discovery,2,1,12
dataset/preprocessed/test-data/hypernym_discovery/5,This report describes the system developed by the CRIM team for the hypernym discovery task at SemEval 2018 .,4,0,19
dataset/preprocessed/test-data/hypernym_discovery/5,This system exploits a combination of supervised projection learning and unsupervised pattern - based hypernym discovery .,5,0,17
dataset/preprocessed/test-data/hypernym_discovery/5,It was ranked first on the 3 sub - tasks for which we submitted results .,6,0,16
dataset/preprocessed/test-data/hypernym_discovery/5,The goal of the hypernym discovery task at Sem - Eval 2018 is to predict the hypernyms of a query given a large vocabulary of candidate hypernyms .,8,0,28
dataset/preprocessed/test-data/hypernym_discovery/5,A query can be either a concept ( e.g. cocktail or epistemology ) or a named entity ( e.g. Craig Anderson or City of Whitehorse ) .,9,0,27
dataset/preprocessed/test-data/hypernym_discovery/5,Two types of data were provided to train the systems : a large unlabeled text corpus and a small training set of examples comprising a query and its hypernyms .,10,0,30
dataset/preprocessed/test-data/hypernym_discovery/5,More details on this task maybe found in the task description paper .,11,0,13
dataset/preprocessed/test-data/hypernym_discovery/5,"The system developed by the CRIM team for the task of hypernym discovery exploits a combination of two approaches : an unsupervised , pattern - based approach and a supervised , projection learning approach .",12,0,35
dataset/preprocessed/test-data/hypernym_discovery/5,"These two approaches are described in Sections 2 and 3 , then Section 4 describes our hybrid system and Section 5 presents our results .",13,0,25
dataset/preprocessed/test-data/hypernym_discovery/5,Pattern - Based Hypernym Discovery,14,0,5
dataset/preprocessed/test-data/hypernym_discovery/5,Pattern - based approaches to relation extraction have been discussed in the literature for quite sometime ( see surveys by and ) .,15,0,23
dataset/preprocessed/test-data/hypernym_discovery/5,"They can be used to discover various relations , including domain - specific ones and more general ones , such as hypernymy .",16,0,23
dataset/preprocessed/test-data/hypernym_discovery/5,"The patternbased approach to hypernym discovery was pioneered by , who defined specific textual patterns ( e.g. Y such as X ) to mine hyponym / hypernym pairs from corpora .",17,0,31
dataset/preprocessed/test-data/hypernym_discovery/5,"This approach is known to suffer from low recall because it assumes that hyponym / hypernym pairs will occur together in one of these patterns , which is often not the case .",18,0,33
dataset/preprocessed/test-data/hypernym_discovery/5,"For instance , using the training data of sub - task 1 A , we found that the majority of training pairs never co-occur within the same paragraph in corpus 1 A , let alone within a pattern that suggests hypernymy .",19,0,42
dataset/preprocessed/test-data/hypernym_discovery/5,"To increase recall , we extend the basic patternbased approach to hypernym discovery in two ways .",20,0,17
dataset/preprocessed/test-data/hypernym_discovery/5,"First , we identify co-hyponyms for each query and add the hypernyms discovered for these terms to those found for the query .",21,0,23
dataset/preprocessed/test-data/hypernym_discovery/5,"These cohyponyms are identified using patterns , and filtered based on distributional similarity using the embeddings described in Section 3.3 .",22,0,21
dataset/preprocessed/test-data/hypernym_discovery/5,"Furthermore , we discover additional hypernyms using a method based on the following assumptions : most multi-word expressions are compositional , and the prevailing head - modifier relation is hypernymy .",23,0,31
dataset/preprocessed/test-data/hypernym_discovery/5,"The co-hyponym patterns we use are limited to enumeration patterns ( e.g. X 1 , X 2 and X 3 ) .",24,0,22
dataset/preprocessed/test-data/hypernym_discovery/5,"For hypernyms , we use an extended set of Hearst - like patterns which we selected empirically ( e.g. Y such as X , Y other than X , not all Y are X , Y including X , Y especially X , Y like X , Y for example X , Y which includes X , X are also Y , X are all Y , not Y so much as X ) .",25,0,75
dataset/preprocessed/test-data/hypernym_discovery/5,"Our pattern - based hypernym discovery algorithm can be defined as follows : given a query q , 1 . Create the empty set Q , which will contain an extended set of queries .",26,0,35
dataset/preprocessed/test-data/hypernym_discovery/5,Search for the co-hyponym patterns in the corpus to discover co-hyponyms of q .,28,0,14
dataset/preprocessed/test-data/hypernym_discovery/5,Add these to Q and store their frequency ( number of times a given co -hyponym was found using these patterns ) .,29,0,23
dataset/preprocessed/test-data/hypernym_discovery/5,Score each co-hyponym q ?,31,0,5
dataset/preprocessed/test-data/hypernym_discovery/5,Q by multiplying the frequency of q by the cosine similarity of the embeddings of q and q .,32,0,19
dataset/preprocessed/test-data/hypernym_discovery/5,"Rank the co-hyponyms in Q according to this score , keep the top n , 1 and discard the rest .",33,0,21
dataset/preprocessed/test-data/hypernym_discovery/5,Add the original query q to Q.,35,0,7
dataset/preprocessed/test-data/hypernym_discovery/5,Create the empty set of hypernyms H q .,37,0,9
dataset/preprocessed/test-data/hypernym_discovery/5,For each query q ?,39,0,5
dataset/preprocessed/test-data/hypernym_discovery/5,"Q , search for the hypernym patterns in the corpus to discover hypernyms of q .",40,0,16
dataset/preprocessed/test-data/hypernym_discovery/5,Add these to H q .,41,0,6
dataset/preprocessed/test-data/hypernym_discovery/5,"Add the head of each term in H q to this set , as well as the head of the original query q.",43,0,23
dataset/preprocessed/test-data/hypernym_discovery/5,Score each candidate c ?,45,0,5
dataset/preprocessed/test-data/hypernym_discovery/5,"H q by multiplying its normalized frequency 2 by the cosine similarity between the embeddings of c and q , and rank the candidates according to this score .",46,0,29
dataset/preprocessed/test-data/hypernym_discovery/5,"Although the pattern - based search for both cohyponyms and hypernyms can find terms not included in the provided vocabulary ( which could also be useful ) , we discarded out - of - vocabulary terms because we had not learned embeddings for them .",47,0,45
dataset/preprocessed/test-data/hypernym_discovery/5,Learning Projections for Hypernym Discovery,48,0,5
dataset/preprocessed/test-data/hypernym_discovery/5,Several supervised learning approaches based on word embeddings have recently been developed for the task of hypernym detection and the related task of hypernym discovery .,49,0,26
dataset/preprocessed/test-data/hypernym_discovery/8,Apollo at SemEval-2018 Task 9 : Detecting Hypernymy Relations Using Syntactic Dependencies,2,1,12
dataset/preprocessed/test-data/hypernym_discovery/8,"This paper presents the participation of Apollo 's team in the SemEval - 2018 Task 9 "" Hypernym Discovery "" , Subtask 1 : "" General - Purpose Hypernym Discovery "" , which tries to produce a ranked list of hypernyms for a specific term .",4,0,46
dataset/preprocessed/test-data/hypernym_discovery/8,We propose a novel approach for automatic extraction of hypernymy relations from a corpus by using dependency patterns .,5,0,19
dataset/preprocessed/test-data/hypernym_discovery/8,The results show that the application of these patterns leads to a higher score than using the traditional lexical patterns .,6,0,21
dataset/preprocessed/test-data/hypernym_discovery/8,This paper presents the Apollo team 's system for hypernym discovery which participated in task 9 of Semeval 2018 based on unsupervised machine learning .,8,0,25
dataset/preprocessed/test-data/hypernym_discovery/8,It is a rule - based system that exploits syntactic dependency paths that generalize Hearst - style lexical patterns .,9,0,20
dataset/preprocessed/test-data/hypernym_discovery/8,"The paper is structured in 4 sections : this section presents existing approaches for automatic extraction of hypernymy relations , Section 2 contains the current system architecture .",10,0,28
dataset/preprocessed/test-data/hypernym_discovery/8,"The next section presents the web interface of the project , and , finally , Section 4 briefly analyses the results and drafts some conclusions .",11,0,26
dataset/preprocessed/test-data/hypernym_discovery/8,"Since language is a "" vital organ "" , constantly evolving and changing overtime , there are many words which lose one of their meanings or attach a new meaning .",12,0,31
dataset/preprocessed/test-data/hypernym_discovery/8,"For instance , when searching the word "" apple "" in WordNet , it appears defined as "" fruit with red or yellow or green skin and sweet to tart crisp whitish flesh "" and "" native Eurasian tree widely cultivated in many varieties for it s firm rounded edible fruits "" but searching in British National Corpus 1 , we will remark that the term is used more frequently as a named entity ( referring to a "" company "" ) .",13,0,83
dataset/preprocessed/test-data/hypernym_discovery/8,"From this point of view , we consider that developing a system for hypernym discovery that uses linguistic features from a corpus could be more useful for this task than using a manuallycrafted taxonomy .",14,0,35
dataset/preprocessed/test-data/hypernym_discovery/8,"It is well known that in natural language processing ( NLP ) , one of the biggest challenges is to understand the meaning of words .",15,0,26
dataset/preprocessed/test-data/hypernym_discovery/8,"Also , detecting hypernymy relations is an important task in NLP , which has been pursued for over two decades , and it is addressed in the literature using two complementary approaches : rule - based and distributional methods .",16,0,40
dataset/preprocessed/test-data/hypernym_discovery/8,Rule - based methods base the decision on the lexicosyntactic paths connecting the joint occurrences of two or more terms in a corpus .,17,0,24
dataset/preprocessed/test-data/hypernym_discovery/8,"In the case of supervised distributional methods , term - pair is represented using some combination of the terms ' embedding vectors .",18,0,23
dataset/preprocessed/test-data/hypernym_discovery/8,"This challenge has been shown to directly help in downstream applications such automatic hypernymy detection is useful for NLP tasks such as : taxonomy creation , recognizing textual entailment , text generation , Question Answering systems , semantic search , Natural Language Inference , Coreference Resolution and many others .",19,0,50
dataset/preprocessed/test-data/hypernym_discovery/8,"Traditional procedures to evaluate taxonomies have focused on measuring the quality of the edges , i.e. , assessing the quality of the is - a relations .",20,0,27
dataset/preprocessed/test-data/hypernym_discovery/8,This process typically consists of extracting a random sample of edges and manually labeling them by human judges .,21,0,19
dataset/preprocessed/test-data/hypernym_discovery/8,"In addition to the manual effort required to perform this evaluation , this procedure is not easily replicable from taxonomy to taxonomy ( which would most likely include different sets of concepts ) , and do not reflect the over all quality of a taxonomy .",22,0,46
dataset/preprocessed/test-data/hypernym_discovery/8,"Moreover , some taxonomy learning approaches link their concepts to existing resources such as Wikipedia .",23,0,16
dataset/preprocessed/test-data/hypernym_discovery/8,A new Approach to Detect Hypernymy Relation,24,0,7
dataset/preprocessed/test-data/hypernym_discovery/8,The main purpose of this project was to identify the best ( set of ) candidate hypernyms for a certain term from the given corpus 2 .,25,0,27
dataset/preprocessed/test-data/hypernym_discovery/8,"In our system , we considered the rule - based approach and , in order to extract the corresponding patterns , we used syntactic dependencies relations ( Universal Dependencies Parser 3 ) .",26,0,33
dataset/preprocessed/test-data/hypernym_discovery/8,"Below , we present our method of extracting hypernyms from text :",27,0,12
dataset/preprocessed/test-data/hypernym_discovery/8,"2 For this subtask , we used the 3 - billion - word UMBC corpus , which consists of paragraphs extracted from the web as part of the Stanford WebBase Project .",28,0,32
dataset/preprocessed/test-data/hypernym_discovery/8,This is a very large corpus containing information from different domains .,29,0,12
dataset/preprocessed/test-data/hypernym_discovery/8,3 Universal Dependencies ( UD ) is a framework for crosslinguistically consistent grammatical annotation and an open community effort with over 200 contributors producing more than 100 treebanks in over 60 languages .,30,0,33
dataset/preprocessed/test-data/hypernym_discovery/8,Tokenization : sentence boundaries are detected and punctuation signs are separated from words ;,31,0,14
dataset/preprocessed/test-data/hypernym_discovery/8,Part - of - speech tagging : the process of assigning a part - of - speech or lexical class marker to each word in a corpus .,32,0,28
dataset/preprocessed/test-data/hypernym_discovery/8,"Words in natural languages usually encode many pieces of information , such as : what the word "" means "" in the real world , what categories , if any , the word belongs to , what is the function of the word in the sentence ?",33,0,47
dataset/preprocessed/test-data/hypernym_discovery/8,Many language processing applications need to extract the information encoded in the words .,34,0,14
dataset/preprocessed/test-data/hypernym_discovery/8,"Parsers which analyze sentence structure need to know / check agreement between : subjects and verbs , adjectives and nouns , determiners and nouns , etc .",35,0,27
dataset/preprocessed/test-data/hypernym_discovery/8,Information retrieval systems benefit from know what the stem of a word is .,36,0,14
dataset/preprocessed/test-data/hypernym_discovery/8,Machine translation systems need to analyze words to their components and generate words with specific features in the target language .,37,0,21
dataset/preprocessed/test-data/hypernym_discovery/8,Dependency parsing : the syntactic parsing of a sentence consists of finding the correct syntactic structure of that sentence in a given formalism / grammar .,38,0,26
dataset/preprocessed/test-data/hypernym_discovery/8,"Dependency parsing structure consists of lexical items , linked by binary asymmetric relations called dependencies .",39,0,16
dataset/preprocessed/test-data/hypernym_discovery/8,"It is interested in grammatical relations between individual words ( governing & dependent words ) , it does not propose a recursive structure , rather a network of relations .",40,0,30
dataset/preprocessed/test-data/hypernym_discovery/8,"These relations can also have labels and the phrasal nodes are missing in the dependency structure , when compared to constituency structure .",41,0,23
dataset/preprocessed/test-data/hypernym_discovery/8,One of the boosts for this approach was to develop new dependency patterns for identifying hypernymy relations from text thatare based on dependency relations .,42,0,25
dataset/preprocessed/test-data/hypernym_discovery/8,The increased popularity and the universal inventory of categories and guidelines ( which facilitate annotation across languages ) of Universal Dependencies determined us to use this resource in order to automatically extract the hypernyms from the corpus .,43,0,38
dataset/preprocessed/test-data/hypernym_discovery/8,Figure1 : Project 's architecture,44,0,5
dataset/preprocessed/test-data/hypernym_discovery/8,"In this manner , we managed to compress a list of 44 lexico- syntactic patterns used for the hypernyms extraction 4 in only 8 dependencies patterns .",45,0,27
dataset/preprocessed/test-data/hypernym_discovery/8,"In the next lines , we present few examples of lexico - syntactic patterns that were replaced by dependencies patterns :",46,0,21
dataset/preprocessed/test-data/hypernym_discovery/8,"{X and other Y ; X or other Y ; X and any other Y ; X and some other Y ; Y other than X ; X like other Y ; Y other than X } replaced by X "" amod "" Y;",47,0,44
dataset/preprocessed/test-data/hypernym_discovery/8,{ X is a Y ; X was a Y ; X area Y ; X are Y ; X will be a Y ; X is an adj Y ; X was a adj .,48,0,36
dataset/preprocessed/test-data/hypernym_discovery/8,Y ; X area adj .,49,0,6
dataset/preprocessed/test-data/hypernym_discovery/6,EXPR at SemEval- 2018 Task 9 : A Combined Approach for Hypernym Discovery,2,1,13
dataset/preprocessed/test-data/hypernym_discovery/6,"In this paper , we present our proposed system ( EXPR ) to participate in the hypernym discovery task of SemEval 2018 .",4,0,23
dataset/preprocessed/test-data/hypernym_discovery/6,The task addresses the challenge of discovering hypernym relations from a text corpus .,5,0,14
dataset/preprocessed/test-data/hypernym_discovery/6,Our proposal is a combined approach of path - based technique and distributional technique .,6,0,15
dataset/preprocessed/test-data/hypernym_discovery/6,We use dependency parser on a corpus to extract candidate hypernyms and represent their dependency paths as a feature vector .,7,0,21
dataset/preprocessed/test-data/hypernym_discovery/6,The feature vector is concatenated with a feature vector obtained using Wikipedia pre-trained term embedding model .,8,0,17
dataset/preprocessed/test-data/hypernym_discovery/6,The concatenated feature vector fits a supervised machine learning method to learn a classifier model .,9,0,16
dataset/preprocessed/test-data/hypernym_discovery/6,This model is able to classify new candidate hypernyms as hypernym or not .,10,0,14
dataset/preprocessed/test-data/hypernym_discovery/6,Our system performs well to discover new hypernyms not defined in gold hypernyms .,11,0,14
dataset/preprocessed/test-data/hypernym_discovery/6,"Hypernymy is an important lexical - semantic relation that is useful for many applications such as question answering , machine translation , information retrieval , and soon .",13,0,28
dataset/preprocessed/test-data/hypernym_discovery/6,"In addition , hypernym relations are the backbone for building ontologies .",14,0,12
dataset/preprocessed/test-data/hypernym_discovery/6,Various methods have been proposed to detect hypernym relation from text corpora .,15,0,13
dataset/preprocessed/test-data/hypernym_discovery/6,Most of these techniques are either path - based techniques or distributional techniques .,16,0,14
dataset/preprocessed/test-data/hypernym_discovery/6,"In path - based methods , the detection of hypernym relations is based on the lexico - syntactic paths connecting a pair of terms in a corpus .",17,0,28
dataset/preprocessed/test-data/hypernym_discovery/6,"Conversely , distributional methods are based on the distribution of term pair contexts .",18,0,14
dataset/preprocessed/test-data/hypernym_discovery/6,Most of these methods were unsupervised .,19,0,7
dataset/preprocessed/test-data/hypernym_discovery/6,"Recently , focus shifted towards supervised methods .",20,0,8
dataset/preprocessed/test-data/hypernym_discovery/6,This task inherits complexity and is far from being solved .,21,0,11
dataset/preprocessed/test-data/hypernym_discovery/6,The SemEval organizers address the same task but with a novel formulation .,22,0,13
dataset/preprocessed/test-data/hypernym_discovery/6,They reformulate the task from hypernym detection into hypernym discovery .,23,0,11
dataset/preprocessed/test-data/hypernym_discovery/6,"This novel formulation makes the task more realistic in terms of actual downstream application , while also enabling the benefits of information retrieval evaluation metrics .",24,0,26
dataset/preprocessed/test-data/hypernym_discovery/6,Hypernym detection focuses on deciding whether a hypernymic relation holds between a given pair of terms or not .,25,0,19
dataset/preprocessed/test-data/hypernym_discovery/6,Hypernym discovery focuses on discovering a set containing the best hypernyms for a given term from a given vocabulary search space .,26,0,22
dataset/preprocessed/test-data/hypernym_discovery/6,The task is divided into two subtasks : General - Purpose Hypernym Discovery and Domain - Specific Hypernym Discovery .,27,0,20
dataset/preprocessed/test-data/hypernym_discovery/6,"The first consists of discovering hypernym in a general - purpose corpus , thus the SemEval organizers provide the participants with data for three languages : English , Italian , and Spanish .",28,0,33
dataset/preprocessed/test-data/hypernym_discovery/6,"The second consists of discovering hypernym in a domain - specific corpus , thus they provide the participants with data for two specific domains : Medical and Music .",29,0,29
dataset/preprocessed/test-data/hypernym_discovery/6,"The data contains a list of training terms along with gold hypernyms , a list of testing terms , and a vocabulary search space .",30,0,25
dataset/preprocessed/test-data/hypernym_discovery/6,The term is either a concept or an entity .,31,0,10
dataset/preprocessed/test-data/hypernym_discovery/6,"To tackle this task , we propose an approach that combines a path - based technique and distributional technique via concatenating two feature vectors : a feature vector constructed using dependency parser output and a feature vector obtained using term embeddings .",32,0,42
dataset/preprocessed/test-data/hypernym_discovery/6,"Then , by using the concatenated vector we create a binary supervised classifier model based on support vector machine ( SVM ) algorithm .",33,0,24
dataset/preprocessed/test-data/hypernym_discovery/6,The model predicts if a term and its candidate hypernym are hypernym related or not .,34,0,16
dataset/preprocessed/test-data/hypernym_discovery/6,Most of the previous approaches for hypernymy detection are either path - based ( patterns ) or distributional based .,36,0,20
dataset/preprocessed/test-data/hypernym_discovery/6,"Recently , some approaches are taking advantages of the combination of pathbased and distributional techniques .",37,0,16
dataset/preprocessed/test-data/hypernym_discovery/6,Path - Based,38,0,3
dataset/preprocessed/test-data/hypernym_discovery/6,Path - based approaches are heuristic methods that predict hypernymy between a pair of terms if they match a particular pattern in a sentence of the corpus .,39,0,28
dataset/preprocessed/test-data/hypernym_discovery/6,These patterns are either manually identified or automatically extracted .,40,0,10
dataset/preprocessed/test-data/hypernym_discovery/6,"Approaches based on handcrafted patterns yield a good precision , but their recall is very low .",41,0,17
dataset/preprocessed/test-data/hypernym_discovery/6,"Approaches based on automatic learning of patterns achieve better performance by a small improvement in terms of precision and a considerable improvement in terms of recall , but the main limitation of these approaches is the sparsity of the feature space .",42,0,42
dataset/preprocessed/test-data/hypernym_discovery/6,"Distributional approaches predict hypernym relations between terms based on their distributional representation , by either unsupervised or supervised models .",44,0,20
dataset/preprocessed/test-data/hypernym_discovery/6,The early unsupervised distributional models are based on symmetric measures .,45,0,11
dataset/preprocessed/test-data/hypernym_discovery/6,"Later , asymmetric measures are introduced based on the Distributional Inclusion Hypothesis ( DIH ) .",46,0,16
dataset/preprocessed/test-data/hypernym_discovery/6,"More recent , ; introduce new measures based on assumption that DIH is not correct for all cases .",47,0,19
dataset/preprocessed/test-data/hypernym_discovery/6,"While , most of the supervised models rely on term embedding to represent the feature vector between the terms x and y .",48,0,23
dataset/preprocessed/test-data/hypernym_discovery/6,Various vector representations have been used such as concatenation x ?,49,0,11
dataset/preprocessed/test-data/hypernym_discovery/0,ADAPT at SemEval- 2018 Task 9 : Skip - Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora,2,1,19
dataset/preprocessed/test-data/hypernym_discovery/0,This paper describes a simple but competitive unsupervised system for hypernym discovery .,4,1,13
dataset/preprocessed/test-data/hypernym_discovery/0,"The system uses skip - gram word embeddings with negative sampling , trained on specialised corpora .",5,0,17
dataset/preprocessed/test-data/hypernym_discovery/0,Candidate hypernyms for an input word are predicted based on cosine similarity scores .,6,0,14
dataset/preprocessed/test-data/hypernym_discovery/0,Two sets of word embedding models were trained separately on two specialised corpora : a medical corpus and a music industry corpus .,7,0,23
dataset/preprocessed/test-data/hypernym_discovery/0,Our system scored highest in the medical domain among the competing unsupervised systems but performed poorly on the music industry domain .,8,0,22
dataset/preprocessed/test-data/hypernym_discovery/0,Our approach does not depend on any external data other than raw specialised corpora .,9,0,15
dataset/preprocessed/test-data/hypernym_discovery/0,The SemEval-2018 shared task on Hypernymy Discovery sought to study approaches for identifying words that hold a hypernymic relation .,11,0,20
dataset/preprocessed/test-data/hypernym_discovery/0,Two words have a hypernymic relation if one of the words belongs to a taxonomical class that is more general than that of the other word .,12,0,27
dataset/preprocessed/test-data/hypernym_discovery/0,"For example , the word vehicle belongs to a more general taxonomical class than car does , as car is a type of vehicle .",13,0,25
dataset/preprocessed/test-data/hypernym_discovery/0,Hypernymy can be seen as an is - a relationship .,14,0,11
dataset/preprocessed/test-data/hypernym_discovery/0,Hypernymy has been studied from different angles in the natural language processing literature as it is related to the human cognitive ability of generalis ation .,15,0,26
dataset/preprocessed/test-data/hypernym_discovery/0,"This shared task differs from recent taxonomy evaluation tasks by concentrating on Hypernym Discovery : the task of predicting ( discovering ) n hypernym candidates for a given input word , within the vocabulary of a specific domain .",16,0,39
dataset/preprocessed/test-data/hypernym_discovery/0,This shared task provided a general language domain vocabulary and two specialised domain vocabularies in English : medical and music indus - try .,17,0,24
dataset/preprocessed/test-data/hypernym_discovery/0,"For each vocabulary , a reference corpus was also supplied .",18,0,11
dataset/preprocessed/test-data/hypernym_discovery/0,"In addition to these English vocabularies , general language domain vocabularies for Spanish and Italian were also provided .",19,0,19
dataset/preprocessed/test-data/hypernym_discovery/0,The ADAPT team focused on the two specialised domain English subtasks by developing an unsupervised system that builds word embeddings from the supplied reference corpora for these domains .,20,0,29
dataset/preprocessed/test-data/hypernym_discovery/0,"Word embeddings trained on large corpora have been shown to capture semantic relations between words , including hypernym -hyponym relations .",21,0,21
dataset/preprocessed/test-data/hypernym_discovery/0,The word embeddings built and used by the system presented here exploit this property .,22,0,15
dataset/preprocessed/test-data/hypernym_discovery/0,"Although these word embeddings do not distinguish one semantic relation from another , we expect that true hypernyms will constitute a significant proportion of the predicted candidate hypernyms .",23,0,29
dataset/preprocessed/test-data/hypernym_discovery/0,"Indeed , we show that for the medical domain subtask , our system beats the other unsupervised systems , although it still ranks behind the supervised systems .",24,0,28
dataset/preprocessed/test-data/hypernym_discovery/0,"Even though unsupervised systems tend to rank behind supervised systems in NLP tasks in general , our motivation to focus on an unsupervised approach is derived from the fact that they do not require explicit hand - annotated data , and from the expectation that they are able to generalise more easily to unseen hypernym - hyponym pairs .",25,0,59
dataset/preprocessed/test-data/hypernym_discovery/0,The rest of this system description paper is organised as follows :,26,0,12
dataset/preprocessed/test-data/hypernym_discovery/0,Section 2 briefly surveys the relevant literature and explains the reasons for choosing to use a particular flavour of word embeddings .,27,0,22
dataset/preprocessed/test-data/hypernym_discovery/0,Section 3 describes the components of the system and its settings .,28,0,12
dataset/preprocessed/test-data/hypernym_discovery/0,Section 4 summarises the results and offers some insights behind the numbers .,29,0,13
dataset/preprocessed/test-data/hypernym_discovery/0,Section 5 concludes and proposes avenues for future work .,30,0,10
dataset/preprocessed/test-data/hypernym_discovery/0,Modern neural methods for natural language processing ( NLP ) use pre-trained word embeddings as fixed - sized vector representations of lexical units in running text as input data .,32,0,30
dataset/preprocessed/test-data/hypernym_discovery/0,"However , as mentioned previously , word embedding vectors can be used on their own to measure semantic relations between words in an unsupervised manner by , for example , taking the cosine similarity of two word embedding vectors for which semantic similarity is to be measured .",33,0,48
dataset/preprocessed/test-data/hypernym_discovery/0,There are several competing approaches for producing word embedding vectors .,34,0,11
dataset/preprocessed/test-data/hypernym_discovery/0,"One such approach is skip - gram with negative sampling ( SGNS ) , introduced by as part of their Word2 Vec software package .",35,0,25
dataset/preprocessed/test-data/hypernym_discovery/0,"The skip - gram approach assumes that a focus word occurring in text depends on its context words ( the words the focus word co-occurs with inside a fixed - sized window ) , but that those context words occur independently of each other .",36,0,45
dataset/preprocessed/test-data/hypernym_discovery/0,This conditional independence assumption in the context words makes computation more efficient and produces vectors that work well in practice .,37,0,21
dataset/preprocessed/test-data/hypernym_discovery/0,"The negative sampling portion of the algorithm is away of producing "" negative "" context words for the focus word by simply drawing random words from the corpus .",38,0,29
dataset/preprocessed/test-data/hypernym_discovery/0,"These random words are assumed to be "" bad "" context words for the focus word .",39,0,17
dataset/preprocessed/test-data/hypernym_discovery/0,Our team indeed implemented a variant of the Hypervec method but failed to obtain better per-formance scores on the training set than those obtained by using traditional SGNS ( see Section 4 ) .,40,0,34
dataset/preprocessed/test-data/hypernym_discovery/0,"Whilst it is possible that a software bug in our implementation could be the cause of this lower performance , we decided to submit the SGNS results to the official shared task due to time constraints .",41,0,37
dataset/preprocessed/test-data/hypernym_discovery/0,"Our system consists of two components : a trainer that learns word vectors using an implementation of the Skip - Gram with Negative Sampling algorithm , and a predictor that outputs ( predicts ) the top 10 hypernyms of an input word based on the trained vectors .",43,0,48
dataset/preprocessed/test-data/hypernym_discovery/0,These two components and their settings are described here .,44,0,10
dataset/preprocessed/test-data/hypernym_discovery/0,"The trainer is a modification of Py - Torch SGNS 1 , a freely available implementation of the Skip - Gram with Negative Sampling algorithm .",46,0,26
dataset/preprocessed/test-data/hypernym_discovery/0,"One set of vectors per specialised corpus ( medicine and music industry ) were trained on a vocabulary that consists of the 100,000 most frequent words in each corpus , using a word window of 5 words to the left and 5 words to the right of a sliding focus word .",47,0,52
dataset/preprocessed/test-data/hypernym_discovery/0,The windows do not cross sentence boundaries .,48,0,8
dataset/preprocessed/test-data/hypernym_discovery/0,"For negative sampling , 20 words were randomly selected from the vocabulary based on their frequency 2 .",49,0,18
dataset/preprocessed/test-data/hypernym_discovery/2,Hypernyms under Siege : Linguistically - motivated Artillery for Hypernymy Detection,2,1,11
dataset/preprocessed/test-data/hypernym_discovery/2,"The fundamental role of hypernymy in NLP has motivated the development of many methods for the automatic identification of this relation , most of which rely on word distribution .",4,0,30
dataset/preprocessed/test-data/hypernym_discovery/2,"We investigate an extensive number of such unsupervised measures , using several distributional semantic models that differ by context type and feature weighting .",5,0,24
dataset/preprocessed/test-data/hypernym_discovery/2,We analyze the performance of the different methods based on their linguistic motivation .,6,0,14
dataset/preprocessed/test-data/hypernym_discovery/2,"Comparison to the state - of - the - art supervised methods shows that while supervised methods generally outperform the unsupervised ones , the former are sensitive to the distribution of training instances , hurting their reliability .",7,0,38
dataset/preprocessed/test-data/hypernym_discovery/2,"Being based on general linguistic hypotheses and independent from training data , unsupervised measures are more robust , and therefore are still useful artillery for hypernymy detection .",8,0,28
dataset/preprocessed/test-data/hypernym_discovery/2,"In the last two decades , the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy .",10,1,22
dataset/preprocessed/test-data/hypernym_discovery/2,"Such effort is motivated by the role this semantic relation plays in a large number of tasks , such as taxonomy creation and recognizing textual entailment .",11,0,27
dataset/preprocessed/test-data/hypernym_discovery/2,"The task has appeared to be , however , a challenging one , and the numerous approaches proposed to tackle it have often shown limitations .",12,0,26
dataset/preprocessed/test-data/hypernym_discovery/2,"Early corpus - based methods have exploited patterns that may indicate hypernymy ( e.g. "" animals such as dogs "" ) , but the recall limitation of this approach , requiring both words to co-occur in a sentence , motivated the development of methods that rely on adaptations of the distributional hypothesis .",13,0,53
dataset/preprocessed/test-data/hypernym_discovery/2,"The first distributional approaches were unsupervised , assigning a score for each ( x , y ) wordpair , which is expected to be higher for hypernym pairs than for negative instances .",14,0,33
dataset/preprocessed/test-data/hypernym_discovery/2,"Evaluation is performed using ranking metrics inherited from information retrieval , such as Average Precision ( AP ) and Mean Average Precision ( MAP ) .",15,0,26
dataset/preprocessed/test-data/hypernym_discovery/2,Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis and the distributional informativeness hypothesis .,16,0,19
dataset/preprocessed/test-data/hypernym_discovery/2,"In the last couple of years , the focus of the research community shifted to supervised distributional methods , in which each ( x , y) word - pair is represented by a combination of x and y's word vectors ( e.g. concatenation or difference ) , and a classifier is trained on these resulting vectors to predict hypernymy .",17,0,60
dataset/preprocessed/test-data/hypernym_discovery/2,"While the original methods were based on count - based vectors , in recent years they have been used with word embeddings , and have gained popularity thanks to their ease of use and their high performance on several common datasets .",18,0,42
dataset/preprocessed/test-data/hypernym_discovery/2,"However , there have been doubts on whether they can actually learn to recognize hypernymy .",19,0,16
dataset/preprocessed/test-data/hypernym_discovery/2,"Additional recent hypernymy detection methods include a multimodal perspective , a supervised method using unsupervised measure scores as features , and a neural method integrating path - based and distributional information .",20,0,32
dataset/preprocessed/test-data/hypernym_discovery/2,"In this paper we perform an extensive evaluation of various unsupervised distributional measures for hypernymy detection , using several distributional semantic models that differ by context type and feature weighting .",21,0,31
dataset/preprocessed/test-data/hypernym_discovery/2,Some measure vari - ants and context - types are tested for the first time .,22,0,16
dataset/preprocessed/test-data/hypernym_discovery/2,"We demonstrate that since each of these measures captures a different aspect of the hypernymy relation , there is no single measure that consistently performs well in discriminating hypernymy from different semantic relations .",23,0,34
dataset/preprocessed/test-data/hypernym_discovery/2,"We analyze the performance of the measures in different settings and suggest a principled way to select the suitable measure , context type and feature weighting according to the task setting , yielding consistent performance across datasets .",24,0,38
dataset/preprocessed/test-data/hypernym_discovery/2,We also compare the unsupervised measures to the state - of - the - art supervised methods .,25,0,18
dataset/preprocessed/test-data/hypernym_discovery/2,"We show that supervised methods outperform the unsupervised ones , while also being more efficient , computed on top of low - dimensional vectors .",26,0,25
dataset/preprocessed/test-data/hypernym_discovery/2,"At the same time , however , our analysis reassesses previous findings suggesting that supervised methods do not actually learn the relation between the words , but only characteristics of a single word in the pair .",27,0,37
dataset/preprocessed/test-data/hypernym_discovery/2,"Moreover , since the features in embedding - based classifiers are latent , it is difficult to tell what the classifier has learned .",28,0,24
dataset/preprocessed/test-data/hypernym_discovery/2,"We demonstrate that unsupervised methods , on the other hand , do account for the relation between words in a pair , and are easily interpretable , being based on general linguistic hypotheses .",29,0,34
dataset/preprocessed/test-data/hypernym_discovery/2,Distributional Semantic Spaces,30,0,3
dataset/preprocessed/test-data/hypernym_discovery/2,We created multiple distributional semantic spaces that differ in their context type and feature weighting .,31,0,16
dataset/preprocessed/test-data/hypernym_discovery/2,"As an underlying corpus we used a concatenation of the following two corpora : uk WaC ( Ferraresi , 2007 ) , a 2 - billion word corpus constructed by crawling the .uk domain , and WaCkypedia EN ) , a 2009 dump of the English Wikipedia .",32,0,48
dataset/preprocessed/test-data/hypernym_discovery/2,"Both corpora include POS , lemma and dependency parse annotations .",33,0,11
dataset/preprocessed/test-data/hypernym_discovery/2,"Our vocabulary ( of target and context words ) includes only nouns , verbs and adjectives that occurred at least 100 times in the corpus .",34,0,26
dataset/preprocessed/test-data/hypernym_discovery/2,We use several context types :,36,0,6
dataset/preprocessed/test-data/hypernym_discovery/2,Window - based contexts :,37,0,5
dataset/preprocessed/test-data/hypernym_discovery/2,"the contexts of a target word w i are the words surrounding it in a ksized window : w i?k , ... , w i ?1 , w i + 1 , ... , w i+k .",38,0,37
dataset/preprocessed/test-data/hypernym_discovery/2,"If the context - type is directional , words occurring before and after w i are marked differently , i.e. : w i?k /l , ... , w i ?1 / l , w i + 1 /r , ... , w i+k /r .",39,0,45
dataset/preprocessed/test-data/hypernym_discovery/2,Out - of - vocabulary words are filtered out before applying the window .,40,0,14
dataset/preprocessed/test-data/hypernym_discovery/2,"We experimented with window sizes 2 and 5 , directional and indirectional ( win2 , win2d , win5 , win5d ) .",41,0,22
dataset/preprocessed/test-data/hypernym_discovery/2,"Dependency - based contexts : rather than adjacent words in a window , we consider neighbors in a dependency parse tree .",42,0,22
dataset/preprocessed/test-data/hypernym_discovery/2,The contexts of a target word w i are its parent and daughter nodes in the dependency tree ( dep ) .,43,0,22
dataset/preprocessed/test-data/hypernym_discovery/2,"We also experimented with a joint dependency context inspired by , in which the contexts of a target word are the parent - sister pairs in the dependency tree ( joint ) .",44,0,33
dataset/preprocessed/test-data/hypernym_discovery/2,See for an illustration .,45,0,5
dataset/preprocessed/test-data/hypernym_discovery/2,Each distributional semantic space is spanned by a matrix Min which each row corresponds to a target word while each column corresponds to a context .,47,0,26
dataset/preprocessed/test-data/hypernym_discovery/2,"The value of each cell M i , j represents the association between the target word w i and the context c j .",48,0,24
dataset/preprocessed/test-data/hypernym_discovery/2,We experimented with two feature weightings :,49,0,7
dataset/preprocessed/test-data/hypernym_discovery/4,Hypernym discovery aims to discover the hypernym word sets given a hyponym word and proper corpus .,3,1,17
dataset/preprocessed/test-data/hypernym_discovery/4,"This paper proposes a simple but effective method for the discovery of hypernym sets based on word embedding , which can be used to measure the contextual similarities between words .",4,0,31
dataset/preprocessed/test-data/hypernym_discovery/4,"Given a test hyponym word , we get its hypernym lists by computing the similarities between the hyponym word and words in the training data , and fill the test word 's hypernym lists with the hypernym list in the training set of the nearest similarity distance to the test word .",5,0,52
dataset/preprocessed/test-data/hypernym_discovery/4,"In SemEval 2018 task9 , our results , achieve 1st on Spanish , 2nd on Italian , 6th on English in the metric of MAP .",6,0,26
dataset/preprocessed/test-data/hypernym_discovery/4,"Hypernymy relationship plays a critical role in language understanding because it enables generalization , which lies at the core of human cognition ) .",8,0,24
dataset/preprocessed/test-data/hypernym_discovery/4,"It has been widely used in various NLP applications ) , from word sense dis ambiguation ) to information retrieval ) , question answering ) and textual entailment ) .",9,0,30
dataset/preprocessed/test-data/hypernym_discovery/4,"To date , the hypernymy relation also plays an important role in Knowledge Base Construction task .",10,0,17
dataset/preprocessed/test-data/hypernym_discovery/4,"In the past SemEval contest ( Sem Eval - 2015 task 17 1 , SemEval - 2016 task 13 2 ) , the "" Hypernym Detection "" task was treated as a classfication task , i.e. , given a ( hyponym , hypernym ) pair , deciding whether the pair is a true hypernymic relation or not .",11,1,58
dataset/preprocessed/test-data/hypernym_discovery/4,This has led to criticisms regarding its oversimplification .,12,0,9
dataset/preprocessed/test-data/hypernym_discovery/4,"In the SemEval 2018 Task 9 , the task has shifted to "" Hypernym Discovery "" , i.e. , given the search space of a domain 's vocabulary and an input hyponym , discover its best ( set of ) candidate hypernyms .",13,0,43
dataset/preprocessed/test-data/hypernym_discovery/4,"In this paper , the content is organized as follows :",14,0,11
dataset/preprocessed/test-data/hypernym_discovery/4,"Section 2 gives an introduction to the related work ; Section 3 describes our methods for this task , including word embedding projection learning as the baseline and the nearest - neighbourbased method as the submission result ; The experimental results are presented in Section 4 .",15,0,47
dataset/preprocessed/test-data/hypernym_discovery/4,We conclude the paper with Section 5 .,16,0,8
dataset/preprocessed/test-data/hypernym_discovery/4,The work of identifying hypernymy relationship can be categorized from different aspects according to the learning methods and the task formulization .,18,0,22
dataset/preprocessed/test-data/hypernym_discovery/4,"The earlier work ) formalized the task as an unsupervised hypernym discovery task , i.e. , none hyponym - hypernyms pairs ( x , y) are given as the training data .",19,0,32
dataset/preprocessed/test-data/hypernym_discovery/4,handcrafted a set of lexico - syntactic paths that connect the joint occurrences of x and y which indicate hypernymy in a large corpus .,20,0,25
dataset/preprocessed/test-data/hypernym_discovery/4,trained a logistic regression classifier using all dependency paths which connect a small number of known hyponym - hypernym pairs .,21,0,21
dataset/preprocessed/test-data/hypernym_discovery/4,Paths that were assigned high weights by the classifier are used to extract unseen hypernym pairs from a new corpus .,22,0,21
dataset/preprocessed/test-data/hypernym_discovery/4,"Variations of were later used in tasks such as taxonomy construction ; ; ) , analogy identification ( Turney ( 2006 ) ) , and definition extraction ; Navigli and Velardi ( 2010 ) ) .",23,0,36
dataset/preprocessed/test-data/hypernym_discovery/4,A major limitation in relying on lexicosyntactic paths is the requirement of the cooccurence of the hypernym pairs .,24,0,19
dataset/preprocessed/test-data/hypernym_discovery/4,Distributional methods are developed to overcome this limitation .,25,0,9
dataset/preprocessed/test-data/hypernym_discovery/4,developed symmetric similarity measures to detect hypernym in an unsupervised manner. ; employed directional measures based on the distributional inclusion hypothesis .,26,0,22
dataset/preprocessed/test-data/hypernym_discovery/4,"More recent work ; ) introduces new measures , based on the distributional informativeness hypothesis. ; learn directly the word embeddings which are optimized for capturing the hypernymy relationship .",27,0,30
dataset/preprocessed/test-data/hypernym_discovery/4,The supervised methods include ; .,28,0,6
dataset/preprocessed/test-data/hypernym_discovery/4,"These methods were originally wordcount - based , but can be easily adapted using word embeddings ; ) .",29,0,19
dataset/preprocessed/test-data/hypernym_discovery/4,"However , it was criticized that the supervised methods only learn prototypical hypernymy ) .",30,0,15
dataset/preprocessed/test-data/hypernym_discovery/4,3 Hyponym - hypernym Discovery method,31,0,6
dataset/preprocessed/test-data/hypernym_discovery/4,"For the corpus and the train / gold / test data , we have two preprocessing steps :",33,0,18
dataset/preprocessed/test-data/hypernym_discovery/4,1 ) Lowercase all the words ;,34,0,7
dataset/preprocessed/test-data/hypernym_discovery/4,"2 ) Concatenate the phrases ( hyponym or hypernym composed with more than one word ) which occur in the training set or the test set with underline , i.e. , "" executive president "" is replaced by "" executive president "" .",35,0,43
dataset/preprocessed/test-data/hypernym_discovery/4,It is quite useful for training word embedding models because we want to treat phrases as single words .,36,0,19
dataset/preprocessed/test-data/hypernym_discovery/4,"If there are multiple phrases in one sentence , we generate multiple sentences , one per phrase .",37,0,18
dataset/preprocessed/test-data/hypernym_discovery/4,"For example , "" executive president "" and "" vice executive president "" both exist in the corpus sentence "" Hoang Van Dung , vice executive president of the Vietnam Chamber of Commerce and Industry . "" .",38,0,38
dataset/preprocessed/test-data/hypernym_discovery/4,"After preprocessing , two more sentences are generated and included in the training corpus for word embeddings :",39,0,18
dataset/preprocessed/test-data/hypernym_discovery/4,"Hoang Van Dung , vice executive president of the Vietnam Chamber of Commerce and Industry .",40,0,16
dataset/preprocessed/test-data/hypernym_discovery/4,"Hoang Van Dung , vice executive president of the Vietnam Chamber of Commerce and Industry .",41,0,16
dataset/preprocessed/test-data/hypernym_discovery/4,"The size of the original corpus has increased after the preprocessing step , e.g. , The English corpus has increased from ? 18G to ?32G .",42,0,26
dataset/preprocessed/test-data/hypernym_discovery/4,We train our word embedding models using the Google word2vec ) tool 3 on the preprocessed corpus .,44,0,18
dataset/preprocessed/test-data/hypernym_discovery/4,We employ the skipgram model since the skip - gram model is shown to perform best in identifying semantic relations among words .,45,0,23
dataset/preprocessed/test-data/hypernym_discovery/4,The trained word embeddings are used in the projection learning and nearestneighbour based method .,46,0,15
dataset/preprocessed/test-data/hypernym_discovery/4,Method based on Projection Learning,47,0,5
dataset/preprocessed/test-data/hypernym_discovery/4,The intuition of this method is to assume that there is a linear transformation in the embedding space which maps hyponyms to their correspondent hypernyms .,48,0,26
dataset/preprocessed/test-data/hypernym_discovery/4,"We first learn a projection matrix from the training data , then apply the matrix to the test data .",49,0,20
dataset/preprocessed/test-data/constituency_parsing/7,What Do Recurrent Neural Network Grammars Learn About Syntax ?,2,1,10
dataset/preprocessed/test-data/constituency_parsing/7,Recurrent neural network grammars ( RNNG ) area recently proposed probabilistic generative modeling family for natural language .,4,1,18
dataset/preprocessed/test-data/constituency_parsing/7,They show state - of the - art language modeling and parsing performance .,5,0,14
dataset/preprocessed/test-data/constituency_parsing/7,"We investigate what information they learn , from a linguistic perspective , through various ablations to the model and the data , and by augmenting the model with an attention mechanism ( GA - RNNG ) to enable closer inspection .",6,0,41
dataset/preprocessed/test-data/constituency_parsing/7,We find that explicit modeling of composition is crucial for achieving the best performance .,7,0,15
dataset/preprocessed/test-data/constituency_parsing/7,"Through the attention mechanism , we find that headedness plays a central role in phrasal representation ( with the model 's latent attention largely agreeing with predictions made by hand - crafted head rules , albeit with some important differences ) .",8,0,42
dataset/preprocessed/test-data/constituency_parsing/7,"By training grammars without nonterminal labels , we find that phrasal representations depend minimally on nonterminals , providing support for the endocentricity hypothesis .",9,0,24
dataset/preprocessed/test-data/constituency_parsing/7,"In this paper , we focus on a recently proposed class of probability distributions , recurrent neural network grammars ( RNNGs ; ) , designed to model syntactic derivations of sentences .",11,0,32
dataset/preprocessed/test-data/constituency_parsing/7,"We focus on RNNGs as generative probabilistic models over trees , as summarized in 2 .",12,1,16
dataset/preprocessed/test-data/constituency_parsing/7,Fitting a probabilistic model to data has often been understood as away to test or confirm some aspect of a theory .,13,0,22
dataset/preprocessed/test-data/constituency_parsing/7,"We talk about a model 's assumptions and sometimes explore its parameters or posteriors over its latent variables in order to gain understanding of what it "" discovers "" from the data .",14,0,33
dataset/preprocessed/test-data/constituency_parsing/7,"In some sense , such models can bethought of as mini-scientists .",15,0,12
dataset/preprocessed/test-data/constituency_parsing/7,"Neural networks , including RNNGs , are capable of representing larger classes of hypotheses than traditional probabilistic models , giving them more freedom to explore .",16,0,26
dataset/preprocessed/test-data/constituency_parsing/7,"Unfortunately , they tend to be bad mini-scientists , because their parameters are difficult for human scientists to interpret .",17,0,20
dataset/preprocessed/test-data/constituency_parsing/7,RNNGs are striking because they obtain stateof - the - art parsing and language modeling performance .,18,0,17
dataset/preprocessed/test-data/constituency_parsing/7,"Their relative lack of independence assumptions , while still incorporating a degree of linguistically - motivated prior knowledge , affords the model considerable freedom to derive its own insights about syntax .",19,0,32
dataset/preprocessed/test-data/constituency_parsing/7,"If they are mini-scientists , the discoveries they make should be of particular interest as propositions about syntax ( at least for the particular genre and dialect of the data ) .",20,0,32
dataset/preprocessed/test-data/constituency_parsing/7,This paper manipulates the inductive bias of RNNGs to test linguistic hypotheses .,21,0,13
dataset/preprocessed/test-data/constituency_parsing/7,We begin with an ablation study to discover the importance of the composition function in 3 .,22,0,17
dataset/preprocessed/test-data/constituency_parsing/7,"Based on the findings , we augment the RNNG composition function with a novel gated attention mechanism ( leading to the GA - RNNG ) to incorporate more interpretability into the model in 4 .",23,0,35
dataset/preprocessed/test-data/constituency_parsing/7,"Using the GA - RNNG , we proceed by investigating the role that individual heads play in phrasal representation ( 5 ) and the role that nonterminal category labels play ( 6 ) .",24,0,34
dataset/preprocessed/test-data/constituency_parsing/7,"Our key findings are that lexical heads play an important role in representing most phrase types ( although compositions of multiple salient heads are not infrequent , especially 1 RNNGs have less inductive bias relative to traditional unlexicalized probabilistic context - free grammars , but more than models that parse by transducing word sequences to linearized parse trees represented as strings .",25,0,62
dataset/preprocessed/test-data/constituency_parsing/7,"Inductive bias is necessary for learning ; we believe the important question is not "" how little can a model getaway with ? "" but rather the benefit of different forms of inductive bias as data vary .",26,0,38
dataset/preprocessed/test-data/constituency_parsing/7,for conjunctions ) and that nonterminal labels provide little additional information .,27,0,12
dataset/preprocessed/test-data/constituency_parsing/7,"As a by-product of our investigation , a variant of the RNNG without ensembling achieved the best reported supervised phrase - structure parsing ( 93.6 F 1 ; English PTB ) and , through conversion , dependency parsing ( 95.8 UAS , 94.6 LAS ; PTB SD ) .",28,0,49
dataset/preprocessed/test-data/constituency_parsing/7,The code and pretrained models to replicate our results are publicly available 2 .,29,0,14
dataset/preprocessed/test-data/constituency_parsing/7,Recurrent Neural Network Grammars,30,0,4
dataset/preprocessed/test-data/constituency_parsing/7,An RNNG defines a joint probability distribution over string terminals and phrase - structure nonterminals .,31,0,16
dataset/preprocessed/test-data/constituency_parsing/7,"Formally , the RNNG is defined by a triple N , ? , ? , where N denotes the set of nonterminal symbols ( NP , VP , etc. ) , ?",33,0,32
dataset/preprocessed/test-data/constituency_parsing/7,"the set of all terminal symbols ( we assume that N ? ? = ? ) , and ?",34,0,19
dataset/preprocessed/test-data/constituency_parsing/7,the set of all model parameters .,35,0,7
dataset/preprocessed/test-data/constituency_parsing/7,"Unlike previous works that rely on hand - crafted rules to compose more fine - grained phrase representations , the RNNG implicitly parameterizes the information passed through compositions of phrases ( in ? and the neural network architecture ) , hence weakening the strong independence assumptions in classical probabilistic context - free grammars .",36,0,54
dataset/preprocessed/test-data/constituency_parsing/7,"The RNNG is based on an abstract state machine like those used in transition - based parsing , with its algorithmic state consisting of a stack of partially completed constituents , a buffer of already - generated terminal symbols , and a list of past actions .",37,0,47
dataset/preprocessed/test-data/constituency_parsing/7,"To generate a sentence x and its phrase - structure tree y , the RNNG samples a sequence of actions to construct y top - down .",38,0,27
dataset/preprocessed/test-data/constituency_parsing/7,"Given y , there is one such sequence ( easily identified ) , which we call the oracle , a = a 1 , . . . , an used during supervised training .",39,0,34
dataset/preprocessed/test-data/constituency_parsing/7,The RNNG uses three different actions :,40,0,7
dataset/preprocessed/test-data/constituency_parsing/7,"introduces an open nonterminal symbol onto the stack , e.g. , "" ( NP "" ; GEN ( x ) , where x ? ? , generates a terminal symbol and places it on the stack and buffer ; and REDUCE indicates a constituent is now complete .",41,0,48
dataset/preprocessed/test-data/constituency_parsing/7,The elements of the stack that comprise the current constituent ( going back to the last 2 https://github.com/clab/rnng/tree/,42,0,18
dataset/preprocessed/test-data/constituency_parsing/7,master / interpreting-rnng also defined a conditional version of the RNNG that can be used only for parsing ; here we focus on the generative version since it is more flexible and ( rather surprisingly ) even learns better estimates of p ( y | x ) . :,43,0,49
dataset/preprocessed/test-data/constituency_parsing/7,"The RNNG consists of a stack , buffer of generated words , and list of past actions that lead to the current configuration .",44,0,24
dataset/preprocessed/test-data/constituency_parsing/7,"Each component is embedded with LSTMs , and the parser state summary u t is used as top - layer features to predict a softmax over all feasible actions .",45,0,30
dataset/preprocessed/test-data/constituency_parsing/7,This figure is due to .,46,0,6
dataset/preprocessed/test-data/constituency_parsing/7,"open nonterminal ) are popped , a composition function is executed , yielding a composed representation that is pushed onto the stack .",47,0,23
dataset/preprocessed/test-data/constituency_parsing/7,"At each timestep , the model encodes the stack , buffer , and past actions , with a separate LSTM for each component as features to define a distribution over the next action to take ( conditioned on the full algorithmic state ) .",48,0,44
dataset/preprocessed/test-data/constituency_parsing/7,The over all architecture is illustrated in ; examples of full action sequences can be found in .,49,0,18
dataset/preprocessed/test-data/constituency_parsing/3,Grammar as a Foreign Language,2,0,5
dataset/preprocessed/test-data/constituency_parsing/3,Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades .,4,1,24
dataset/preprocessed/test-data/constituency_parsing/3,"As a result , the most accurate parsers are domain specific , complex , and inefficient .",5,0,17
dataset/preprocessed/test-data/constituency_parsing/3,"In this paper we show that the domain agnostic attention - enhanced sequence - to - sequence model achieves state - of - the - art results on the most widely used syntactic constituency parsing dataset , when trained on a large synthetic corpus that was annotated using existing parsers .",6,0,51
dataset/preprocessed/test-data/constituency_parsing/3,"It also matches the performance of standard parsers when trained only on a small human - annotated dataset , which shows that this model is highly data - efficient , in contrast to sequence - to - sequence models without the attention mechanism .",7,0,44
dataset/preprocessed/test-data/constituency_parsing/3,"Our parser is also fast , processing over a hundred sentences per second with an unoptimized CPU implementation .",8,0,19
dataset/preprocessed/test-data/constituency_parsing/3,* Equal contribution 1 arXiv:1412.7449v3 [ cs. CL ] 9 Jun 2015,9,0,12
dataset/preprocessed/test-data/constituency_parsing/3,Syntactic constituency parsing is a fundamental problem in linguistics and natural language processing that has a wide range of applications .,11,0,21
dataset/preprocessed/test-data/constituency_parsing/3,"This problem has been the subject of intense research for decades , and as a result , there exist highly accurate domain - specific parsers .",12,0,26
dataset/preprocessed/test-data/constituency_parsing/3,"The computational requirements of traditional parsers are cubic in sentence length , and while linear - time shift - reduce constituency parsers improved in accuracy in recent years , they never matched state - of - the - art .",13,0,40
dataset/preprocessed/test-data/constituency_parsing/3,"Furthermore , standard parsers have been designed with parsing in mind ; the concept of a parse tree is deeply ingrained into these systems , which makes these methods inapplicable to other problems .",14,0,34
dataset/preprocessed/test-data/constituency_parsing/3,"Recently , Sutskever et al .",15,0,6
dataset/preprocessed/test-data/constituency_parsing/3,"introduced a neural network model for solving the general sequenceto - sequence problem , and Bahdanau et al.",16,0,18
dataset/preprocessed/test-data/constituency_parsing/3,proposed a related model with an attention mechanism that makes it capable of handling long sequences well .,17,0,18
dataset/preprocessed/test-data/constituency_parsing/3,"Both models achieve state - of - the - art results on large scale machine translation tasks ( e.g. , ) .",18,0,22
dataset/preprocessed/test-data/constituency_parsing/3,"Syntactic constituency parsing can be formulated as a sequence - to - sequence problem if we linearize the parse tree ( cf. ) , so we can apply these models to parsing as well .",19,0,35
dataset/preprocessed/test-data/constituency_parsing/3,Our early experiments focused on the sequence - to - sequence model of Sutskever et al ..,20,0,17
dataset/preprocessed/test-data/constituency_parsing/3,"We found this model to work poorly when we trained it on standard human - annotated parsing datasets ( 1M tokens ) , so we constructed an artificial dataset by labelling a large corpus with the BerkeleyParser .",21,0,38
dataset/preprocessed/test-data/constituency_parsing/3,"To our surprise , the sequence - to - sequence model matched the BerkeleyParser that produced the annotation , having achieved an F 1 score of 90.5 on the test set ( section 23 of the WSJ ) .",22,0,39
dataset/preprocessed/test-data/constituency_parsing/3,We suspected that the attention model of Bahdanau et al.,23,0,10
dataset/preprocessed/test-data/constituency_parsing/3,might be more data efficient and we found that it is indeed the case .,24,0,15
dataset/preprocessed/test-data/constituency_parsing/3,"We trained a sequence - to - sequence model with attention on the small human - annotated parsing dataset and were able to achieve an F 1 score of 88.3 on section 23 of the WSJ without the use of an ensemble and 90.5 with an ensemble , which matches the performance of the BerkeleyParser ( 90.4 ) when trained on the same data .",25,0,65
dataset/preprocessed/test-data/constituency_parsing/3,"Finally , we constructed a second artificial dataset consisting of only high - confidence parse trees , as measured by the agreement of two parsers .",26,0,26
dataset/preprocessed/test-data/constituency_parsing/3,We trained a sequence - to - sequence model with attention on this data and achieved an F 1 score of 92.5 on section 23 of the WSJ - a new state - of - the - art .,27,0,39
dataset/preprocessed/test-data/constituency_parsing/3,"This result did not require an ensemble , and as a result , the parser is also very fast .",28,0,20
dataset/preprocessed/test-data/constituency_parsing/3,An ensemble further improves the score to 92.8 .,29,0,9
dataset/preprocessed/test-data/constituency_parsing/3,LSTM + A Parsing Model,30,0,5
dataset/preprocessed/test-data/constituency_parsing/3,Let us first recall the sequence - to - sequence LSTM model .,31,0,13
dataset/preprocessed/test-data/constituency_parsing/3,The Long Short - Term Memory model of is defined as follows .,32,0,13
dataset/preprocessed/test-data/constituency_parsing/3,"Let x t , ht , and mt be the input , control state , and memory state at timestep t.",33,0,21
dataset/preprocessed/test-data/constituency_parsing/3,"Given a sequence of inputs ( x 1 , . . . , x T ) , the LSTM computes the h-sequence ( h 1 , . . . , h T ) and the m-sequence ( m 1 , . . . , m T ) as follows .",34,0,50
dataset/preprocessed/test-data/constituency_parsing/3,"The operator denotes element - wise multiplication , the matrices W 1 , . . . , W 8 and the vector h 0 are the parameters of the model , and all the nonlinearities are computed element - wise .",35,0,41
dataset/preprocessed/test-data/constituency_parsing/3,"I na deep LSTM , each subsequent layer uses the h-sequence of the previous layer for its input sequence x .",36,0,21
dataset/preprocessed/test-data/constituency_parsing/3,The deep LSTM defines a distribution over output sequences given an input sequence :,37,0,14
dataset/preprocessed/test-data/constituency_parsing/3,"The above equation assumes a deep LSTM whose input sequence is x = ( A 1 , . . . , A TA , B 1 , . . . , B TB ) , so ht denotes t- th element of the h-sequence of topmost LSTM .",38,0,48
dataset/preprocessed/test-data/constituency_parsing/3,The matrix W o consists of the vector representations of each output symbol and the symbol ?,39,0,17
dataset/preprocessed/test-data/constituency_parsing/3,"b is a Kronecker delta with a dimension for each output symbol , so softmax ( W o h TA +t ) ?",40,0,23
dataset/preprocessed/test-data/constituency_parsing/3,Bt is precisely the B t 'th element of the distribution defined by the softmax .,41,0,16
dataset/preprocessed/test-data/constituency_parsing/3,Every output sequence terminates with a special end - of - sequence token which is necessary in order to define a distribution over sequences of variable lengths .,42,0,28
dataset/preprocessed/test-data/constituency_parsing/3,"We use two different sets of LSTM parameters , one for the input sequence and one for the output sequence , as shown in .",43,0,25
dataset/preprocessed/test-data/constituency_parsing/3,Stochastic gradient descent is used to maximize the training objective which is the average over the training set of the log probability of the correct output sequence given the input sequence .,44,0,32
dataset/preprocessed/test-data/constituency_parsing/3,An important extension of the sequence - to - sequence model is by adding an attention mechanism .,46,0,18
dataset/preprocessed/test-data/constituency_parsing/3,"We adapted the attention model from which , to produce each output symbol B t , uses an attention mechanism over the encoder LSTM states .",47,0,26
dataset/preprocessed/test-data/constituency_parsing/3,"Similar to our sequence - to - sequence model described in the previous section , we use two separate LSTMs ( one to encode the sequence of input words A i , and another one to produce or decode the output symbols Bi ) .",48,0,45
dataset/preprocessed/test-data/constituency_parsing/3,"Recall that the encoder hidden states are denoted ( h 1 , . . . , h TA ) and we denote the hidden states of the decoder by ( d 1 , . . . , d TB ) := ( h TA +1 , . . . , h TA +T B ) .",49,0,56
dataset/preprocessed/test-data/constituency_parsing/1,Cloze - driven Pretraining of Self - attention Networks,2,1,9
dataset/preprocessed/test-data/constituency_parsing/1,We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems .,4,1,24
dataset/preprocessed/test-data/constituency_parsing/1,"Our model solves a cloze - style word reconstruction task , where each word is ablated and must be predicted given the rest of the text .",5,0,27
dataset/preprocessed/test-data/constituency_parsing/1,"Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks , consistent with the concurrently introduced BERT model .",6,0,31
dataset/preprocessed/test-data/constituency_parsing/1,"We also present a detailed analysis of a number of factors that contribute to effective pretraining , including data domain and size , model capacity , and variations on the cloze objective .",7,0,33
dataset/preprocessed/test-data/constituency_parsing/1,Language model pretraining has recently been shown to provide significant performance gains for a range of challenging language understanding problems .,9,1,21
dataset/preprocessed/test-data/constituency_parsing/1,"However , existing work has either used unidirectional ( left - to - right ) language models ( LMs ) or bi-directional ( both left - to - right and right - to - left ) LMs ( BiLMs ) where each direction is trained with an independent loss function .",10,0,51
dataset/preprocessed/test-data/constituency_parsing/1,"In this paper , we show that even larger performance gains are possible by jointly pretraining both directions of a large language - model - inspired self - attention cloze model .",11,0,32
dataset/preprocessed/test-data/constituency_parsing/1,Our bi-directional transformer architecture predicts every token in the training data ( ) .,12,0,14
dataset/preprocessed/test-data/constituency_parsing/1,We achieve this by introducing a cloze - style training objective where the model must predict the center word given left - to - right and right - to - left context representations .,13,0,34
dataset/preprocessed/test-data/constituency_parsing/1,Our model separately computes both forward and backward states with * Equal contribution . :,14,0,15
dataset/preprocessed/test-data/constituency_parsing/1,Illustration of the model .,15,0,5
dataset/preprocessed/test-data/constituency_parsing/1,Block i is a standard transformer decoder block .,16,0,9
dataset/preprocessed/test-data/constituency_parsing/1,Green blocks operate left to right by masking future time - steps and blue blocks operate right to left .,17,0,20
dataset/preprocessed/test-data/constituency_parsing/1,"At the top , states are combined with a standard multi-head self - attention module whose output is fed to a classifier that predicts the center token .",18,0,28
dataset/preprocessed/test-data/constituency_parsing/1,"a masked self - attention architecture , that closely resembles a language model .",19,0,14
dataset/preprocessed/test-data/constituency_parsing/1,"At the top of the network , the forward and backward states are combined to jointly predict the center word .",20,0,21
dataset/preprocessed/test-data/constituency_parsing/1,"This approach allows us to consider both contexts when predicting words and to incur loss for every word in the training set , if the model does not assign it high likelihood .",21,0,33
dataset/preprocessed/test-data/constituency_parsing/1,"Experiments on the GLUE ) benchmark show strong gains over the state of the art for each task , including a 9.1 point gain on RTE over .",22,0,28
dataset/preprocessed/test-data/constituency_parsing/1,"These improvements are consistent with , if slightly behind , those achieved by the concurrently developed BERT pretraining approach , which we will discuss in more detail in the next section .",23,0,32
dataset/preprocessed/test-data/constituency_parsing/1,"We also show that it is possible to stack taskspecific architectures for NER and constituency parsing on top of our pretrained representations , and achieve new state - of - the - art performance lev-els for both tasks .",24,0,39
dataset/preprocessed/test-data/constituency_parsing/1,"We also present extensive experimental analysis to better understand these results , showing that ( 1 ) cross sentence pretraining is crucial for many tasks ; ( 2 ) pre-training continues to improve performance with up to 18B tokens and would likely continue to improve with more data ; and finally ( 3 ) our novel cloze - driven training regime is more effective than predicting left and right tokens separately .",25,0,72
dataset/preprocessed/test-data/constituency_parsing/1,There has been much recent work on learning sentence - specific representations for language understanding tasks .,27,0,17
dataset/preprocessed/test-data/constituency_parsing/1,learn contextualized word representations from a sequence to sequence translation task and uses the representations from the encoder network to improve a variety of language understanding tasks .,28,0,28
dataset/preprocessed/test-data/constituency_parsing/1,Subsequent work focused on language modeling pretraining which has been shown to be more effective and which does not require bilingual data .,29,0,23
dataset/preprocessed/test-data/constituency_parsing/1,Our work was inspired by ELMo and the generative pretraining ( GPT ) approach of .,30,0,16
dataset/preprocessed/test-data/constituency_parsing/1,ELMo introduces language models to pretrain word representations for downstream tasks including a novel mechanism to learn a combination of different layers in the language model that is most beneficial to the current task .,31,0,35
dataset/preprocessed/test-data/constituency_parsing/1,GPT relies on a left to right language model and an added projection layer for each downstream task without a task - specific model .,32,0,25
dataset/preprocessed/test-data/constituency_parsing/1,"Our approach mostly follows GPT , though we show that our model also works well with an ELMo module on NER and constituency parsing .",33,0,25
dataset/preprocessed/test-data/constituency_parsing/1,The concurrently introduced BERT model ( is a transformer encoder model that captures left and right context .,34,0,18
dataset/preprocessed/test-data/constituency_parsing/1,There is significant overlap between their work and ours but there are also significant differences : our model is a bi-directional transformer language model that predicts every single token in a sequence .,35,0,33
dataset/preprocessed/test-data/constituency_parsing/1,BERT is also a transformer encoder that has access to the entire input which makes it bi-directional but this choice requires a special training regime .,36,0,26
dataset/preprocessed/test-data/constituency_parsing/1,"In particular , they multi-task between predicting a subset of masked input tokens , similar to a denoising autoencoder , and a next sentence prediction task .",37,0,27
dataset/preprocessed/test-data/constituency_parsing/1,"In comparison , we optimize a single loss function that requires the model to predict each token of an input sentence given all surrounding tokens .",38,0,26
dataset/preprocessed/test-data/constituency_parsing/1,We use all tokens as training targets and therefore extract learning signal from every single token in the sentence and not just a subset .,39,0,25
dataset/preprocessed/test-data/constituency_parsing/1,BERT tailors pretraining to capture dependencies between sentences via a next sentence prediction task as well as by constructing training examples of sentence - pairs with input markers that distinguish between tokens of the two sentences .,40,0,37
dataset/preprocessed/test-data/constituency_parsing/1,Our model is trained similarly to a classical language model since we do not adapt the training examples to resemble the end task data and we do not solve a denoising task during training .,41,0,35
dataset/preprocessed/test-data/constituency_parsing/1,"Finally , BERT as well as consider only a single data source to pretrain their models , either Books Corpus , or BooksCorpus and additional Wikipedia data , whereas our study ablates the effect of various amounts of training data as well as different data sources .",42,0,47
dataset/preprocessed/test-data/constituency_parsing/1,Two tower model,43,0,3
dataset/preprocessed/test-data/constituency_parsing/1,"Our cloze model represents a probability distribution p ( t i |t 1 , . . . , t i?1 , t i + 1 , . . . , tn ) for a sentence with n tokens t 1 , . . . , tn .",44,0,47
dataset/preprocessed/test-data/constituency_parsing/1,There are two selfattentional towers each consisting of N stacked blocks : the forward tower operates left - to - right and the backward tower operates in the opposite direction .,45,0,31
dataset/preprocessed/test-data/constituency_parsing/1,"To predict a token , we combine the representations of the two towers , as described in more detail below , taking care that neither representation contains information about the current target token .",46,0,34
dataset/preprocessed/test-data/constituency_parsing/1,The forward tower computes the representation F l i for token i at layer l based on the forward representations of the previous layer F l?1 ? i via selfattention ; the backward tower computes representation B l i based on information from the opposite direction B l?1 ?i .,47,0,50
dataset/preprocessed/test-data/constituency_parsing/1,"When examples of uneven length are batched , one of the towers may not have any context at the beginning .",48,0,21
dataset/preprocessed/test-data/constituency_parsing/1,We deal with this issue by adding an extra zero state over which the selfattention mechanism can attend .,49,0,19
dataset/preprocessed/test-data/constituency_parsing/5,In- Order Transition - based Constituent Parsing,2,1,7
dataset/preprocessed/test-data/constituency_parsing/5,Both bottom - up and top - down strategies have been used for neural transition - based constituent parsing .,4,1,20
dataset/preprocessed/test-data/constituency_parsing/5,"The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree , where bottom - up strategies and top - down strategies take post-order and pre-order traversal over trees , respectively .",5,0,39
dataset/preprocessed/test-data/constituency_parsing/5,"Bottom - up parsers benefit from rich features from readily built partial parses , but lack lookahead guidance in the parsing process ; top - down parsers benefit from non-local guidance for local decisions , but rely on a strong encoder over the input to predict a constituent hierarchy before its construction .",6,0,53
dataset/preprocessed/test-data/constituency_parsing/5,"To mitigate both issues , we propose a novel parsing system based on in - order traversal over syntactic trees , designing a set of transition actions to find a compromise between bottom - up constituent information and top - down lookahead information .",7,0,44
dataset/preprocessed/test-data/constituency_parsing/5,"Based on stack - LSTM , our psycholinguistically motivated constituent parsing system achieves 91.8 F 1 on the WSJ benchmark .",8,0,21
dataset/preprocessed/test-data/constituency_parsing/5,"Furthermore , the system achieves 93.6 F 1 with supervised reranking and 94.2 F 1 with semi-supervised reranking , which are the best results on the WSJ benchmark .",9,0,29
dataset/preprocessed/test-data/constituency_parsing/5,Transition - based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences .,11,0,18
dataset/preprocessed/test-data/constituency_parsing/5,"There are two popular transition - based constituent parsing systems , namely bottom - up parsing and top - down parsing .",12,0,22
dataset/preprocessed/test-data/constituency_parsing/5,The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree .,13,0,19
dataset/preprocessed/test-data/constituency_parsing/5,The process of bottom - up parsing can be regarded as post -order traversal over a constituent tree .,14,0,19
dataset/preprocessed/test-data/constituency_parsing/5,"For example , given the sentence in , a bottom - up shift - reduce parser takes the action sequence in ( a ) 1 to build the output , where the word sequence "" The little boy "" is first read , and then an NP recognized for the word sequence .",15,0,53
dataset/preprocessed/test-data/constituency_parsing/5,"After the system reads the verb "" likes "" and its subsequent NP , a VP is recognized .",16,0,19
dataset/preprocessed/test-data/constituency_parsing/5,The full order of recognition for the tree nodes is 3 ? 4 ? 5 ? 2 ? 7 ? 9 ? 10 ? 8 ? 6 ? 11 ? 1 .,17,0,32
dataset/preprocessed/test-data/constituency_parsing/5,"When making local decisions , rich information is available from readily built partial trees , which contributes to local dis ambiguation .",18,0,22
dataset/preprocessed/test-data/constituency_parsing/5,"However , there is lack of top - down guidance from lookahead information , which can be useful .",19,0,19
dataset/preprocessed/test-data/constituency_parsing/5,"In addition , binarization must be applied to trees , as shown in , to ensure a constant number of actions , and to take advantage of lexical head information .",20,0,31
dataset/preprocessed/test-data/constituency_parsing/5,"However , such binarization requires a set of language - specific rules , which hampers adaptation of parsing to other languages .",21,0,22
dataset/preprocessed/test-data/constituency_parsing/5,"On the other hand , the process of top - down parsing can be regarded as pre-order traversal over a tree .",22,0,22
dataset/preprocessed/test-data/constituency_parsing/5,"Given the sentence in shift - reduce parser takes the action sequence in ( b ) to build the output , where an S is first made and then an NP is generated .",23,0,34
dataset/preprocessed/test-data/constituency_parsing/5,"After that , the system makes a decision to read the word sequence "" The little boy "" to complete the NP .",24,0,23
dataset/preprocessed/test-data/constituency_parsing/5,The full order of recognition for the tree nodes is 1 ? 2 ? 3 ? 4 ? 5 ? 6 ? 7 ? 8 ? 9 ? 10 ? 11 .,25,0,32
dataset/preprocessed/test-data/constituency_parsing/5,The top - down lookahead guidance contributes to non-local dis ambiguation .,26,0,12
dataset/preprocessed/test-data/constituency_parsing/5,"However , it is difficult to generate a constituent before its sub constituents have been realized , since no explicit features can be extracted from their subtree structures .",27,0,29
dataset/preprocessed/test-data/constituency_parsing/5,"Thanks to the use of recurrent neural networks , which make it possible to represent a sentence globally before syntactic tree construction , seminal work of neural top - down parsing directly generates bracketed constituent trees using sequence - to - sequence models . ( c ) in - order system :",28,0,52
dataset/preprocessed/test-data/constituency_parsing/5,Action sequences of three types of transition constituent parsing system .,29,0,11
dataset/preprocessed/test-data/constituency_parsing/5,"Details of the action system are introduced in Section 2.1 , Section 2.2 and Section 3 , respectively .",30,0,19
dataset/preprocessed/test-data/constituency_parsing/5,transition - based parsing .,31,0,5
dataset/preprocessed/test-data/constituency_parsing/5,"In this paper , we propose a novel transition system for constituent parsing , mitigating issues of both bottom - up and top - down systems by finding a compromise between bottom - up constituent information and top - down lookahead information .",32,0,43
dataset/preprocessed/test-data/constituency_parsing/5,The process of the proposed constituent parsing can be regarded as in - order traversal over a tree .,33,0,19
dataset/preprocessed/test-data/constituency_parsing/5,"Given the sentence in , the system takes the action sequence in ( c ) to build the output .",34,0,20
dataset/preprocessed/test-data/constituency_parsing/5,"The system reads the word "" The "" and then projects an NP , which is based on bottom - up evidence .",35,0,23
dataset/preprocessed/test-data/constituency_parsing/5,"After this , based on the projected NP , the system reads the word sequence "" little boy "" , with top - down guidance from NP .",36,0,28
dataset/preprocessed/test-data/constituency_parsing/5,"Similarly , based on the completed constituent "" ( NP The little boy ) "" , the system projects an S , with the bottom - up evidence .",37,0,29
dataset/preprocessed/test-data/constituency_parsing/5,"With the Sand the word "" likes "" , the system projects an VP , which can serve as top - down guidance .",38,0,24
dataset/preprocessed/test-data/constituency_parsing/5,"The full order of recognition for the tree nodes is 3 ? 2 ? 4 ? 5 ? 1 ? 7 ? 6 ? 9 ? 8 ? 10 ? 11 . Compared to post -order traversal , in - order traversal can potentially resolve non-local ambiguity better by top - down guidance .",39,0,54
dataset/preprocessed/test-data/constituency_parsing/5,"Compared to pre-order traversal , in - order traversal can potentially resolve local ambiguity better by bottom - up evidence .",40,0,21
dataset/preprocessed/test-data/constituency_parsing/5,"Furthermore , in - order traversal is psycholinguistically motivated .",41,0,10
dataset/preprocessed/test-data/constituency_parsing/5,"Empirically , a human reader comprehends sentences by giving lookahead guesses for constituents .",42,0,14
dataset/preprocessed/test-data/constituency_parsing/5,"For example , when reading a word "" likes "" , a human reader could guess that it could be a start of a constituent VP , instead of waiting to read the object "" red tomatoes "" , which is the procedure of a bottom - up system .",43,0,50
dataset/preprocessed/test-data/constituency_parsing/5,We compare our system with the two baseline systems ( i.e. a top - down system and a bottomup system ) under the same neural transition - based framework of .,44,0,31
dataset/preprocessed/test-data/constituency_parsing/5,"Our final models outperform both of the bottom - up and top - down transition - based constituent parsing by achieving a 91.8 F 1 in English and a 86.1 F 1 in Chinese for greedy fully - supervised parsing , respectively .",45,0,43
dataset/preprocessed/test-data/constituency_parsing/5,"Furthermore , our final model obtains a 93.6 F 1 with supervised reranking ( Choe and Charniak , 2016 ) and a 94.2 F 1 with semi-supervised reranking , achieving the state - of - the - art results on constituent parsing on the English benchmark .",46,0,47
dataset/preprocessed/test-data/constituency_parsing/5,"By converting to Stanford dependencies , our final model achieves the state - of the - art results on dependency parsing by obtaining a 96.2 % UAS and a 95.2 % LAS .",47,0,33
dataset/preprocessed/test-data/constituency_parsing/5,"To our knowledge , we are the first to systematically compare top - down and bottom - up constituent parsing under the same neural framework .",48,0,26
dataset/preprocessed/test-data/constituency_parsing/5,We release our code at https://github.com/LeonCrashCode/InOrderParser .,49,0,7
dataset/preprocessed/test-data/constituency_parsing/8,Constituency Parsing with a Self - Attentive Encoder,2,1,8
dataset/preprocessed/test-data/constituency_parsing/8,We demonstrate that replacing an LSTM encoder with a self - attentive architecture can lead to improvements to a state - of the - art discriminative constituency parser .,4,0,29
dataset/preprocessed/test-data/constituency_parsing/8,"The use of attention makes explicit the manner in which information is propagated between different locations in the sentence , which we use to both analyze our model and propose potential improvements .",5,0,33
dataset/preprocessed/test-data/constituency_parsing/8,"For example , we find that separating positional and content information in the encoder can lead to improved parsing accuracy .",6,0,21
dataset/preprocessed/test-data/constituency_parsing/8,"Additionally , we evaluate different approaches for lexical representation .",7,0,10
dataset/preprocessed/test-data/constituency_parsing/8,"Our parser achieves new state - of the - art results for single models trained on the Penn Treebank : 93.55 F1 without the use of any external data , and 95.13 F1 when using pre-trained word representations .",8,0,39
dataset/preprocessed/test-data/constituency_parsing/8,Our parser also outperforms the previous best - published accuracy figures on 8 of the 9 languages in the SPMRL dataset .,9,0,22
dataset/preprocessed/test-data/constituency_parsing/8,"In recent years , neural network approaches have led to improvements in constituency parsing .",11,0,15
dataset/preprocessed/test-data/constituency_parsing/8,"Many of these parsers can broadly be characterized as following an encoder - decoder design : an encoder reads the input sentence and summarizes it into a vector or set of vectors ( e.g. one for each word or span in the sentence ) , and then a decoder uses these vector summaries to incrementally buildup a labeled parse tree .",12,0,61
dataset/preprocessed/test-data/constituency_parsing/8,"In contrast to the large variety of decoder architectures investigated in recent work , the encoders in recent parsers have predominantly been built using recurrent neural networks ( RNNs ) , and in particular Long Short - Term Memory networks ( LSTMs ) .",13,0,44
dataset/preprocessed/test-data/constituency_parsing/8,RNNs have largely replaced approaches such as the fixed - window - size feed - forward networks of in part due to their ability to capture global context .,14,0,29
dataset/preprocessed/test-data/constituency_parsing/8,"However , RNNs are not the only architecture capable of summarizing large global contexts : recent work by presented a new state - of - the - art approach to machine translation with an architecture that entirely eliminates recurrent connections and relies instead on a repeated neural attention mechanism .",15,0,50
dataset/preprocessed/test-data/constituency_parsing/8,"In this paper , we introduce a parser that combines an encoder built using this kind of self - attentive architecture with a decoder customized for parsing ( ) .",16,0,30
dataset/preprocessed/test-data/constituency_parsing/8,"In Section 2 of this paper , we describe the architecture and present our finding that self - attention can outperform an LSTM - based approach .",17,0,27
dataset/preprocessed/test-data/constituency_parsing/8,"A neural attention mechanism makes explicit the manner in which information is transferred between different locations in the sentence , which we can use to study the relative importance of different kinds of context to the parsing task .",18,0,39
dataset/preprocessed/test-data/constituency_parsing/8,"Different locations in the sentence can attend to each other based on their positions , but also based on their contents ( i.e. based on the words at or around those positions ) .",19,0,34
dataset/preprocessed/test-data/constituency_parsing/8,"In Section 3 we present our find - ing that when our parser learns to make an implicit trade - off between these two types of attention , it predominantly makes use of position - based attention , and show that explicitly factoring the two types of attention can noticeably improve parsing accuracy .",20,0,54
dataset/preprocessed/test-data/constituency_parsing/8,"In Section 4 , we study our model 's use of attention and reaffirm the conventional wisdom that sentence - wide global context is important for parsing decisions .",21,0,29
dataset/preprocessed/test-data/constituency_parsing/8,"Like in most neural parsers , we find morphological ( or at least sub - word ) features to be important to achieving good results , particularly on unseen words or inflections .",22,0,33
dataset/preprocessed/test-data/constituency_parsing/8,"In Section 5.1 , we demonstrate that a simple scheme based on concatenating character embeddings of word prefixes / suffixes can outperform using part - of - speech tags from an external system .",23,0,34
dataset/preprocessed/test-data/constituency_parsing/8,"We also present a version of our model that uses a character LSTM , which performs better than other lexical representationseven if word embeddings are removed from the model .",24,0,30
dataset/preprocessed/test-data/constituency_parsing/8,"In Section 5.2 , we explore an alternative approach for lexical representations that makes use of pre-training on a large unsupervised corpus .",25,0,23
dataset/preprocessed/test-data/constituency_parsing/8,We find that using the deep contextualized representations proposed by can boost parsing accuracy .,26,0,15
dataset/preprocessed/test-data/constituency_parsing/8,"Our parser achieves 93.55 F1 on the Penn Treebank WSJ test set when not using external word representations , outperforming all previous singlesystem constituency parsers trained only on the WSJ training set .",27,0,33
dataset/preprocessed/test-data/constituency_parsing/8,"The addition of pre-trained word representations following increases parsing accuracy to 95.13 F1 , a new stateof - the - art for this dataset .",28,0,25
dataset/preprocessed/test-data/constituency_parsing/8,Our model also outperforms previous best published results on 8 of the 9 languages in the SPMRL 2013 / 2014 shared tasks .,29,0,23
dataset/preprocessed/test-data/constituency_parsing/8,Code and trained English models are publicly available .,30,0,9
dataset/preprocessed/test-data/constituency_parsing/8,"Our parser follows an encoder - decoder architecture , as shown in .",33,0,13
dataset/preprocessed/test-data/constituency_parsing/8,"The decoder , described in Section 2.1 , is borrowed from the chart parser of with additional modifications from .",34,0,20
dataset/preprocessed/test-data/constituency_parsing/8,"Their parser is architecturally streamlined yet achieves the highest performance among discriminative single - system parsers trained on WSJ data only , which is why we selected it as the starting point for our experiments with encoder variations .",35,0,39
dataset/preprocessed/test-data/constituency_parsing/8,"Sections 2.2 and 2.3 de-scribe the base version of our encoder , where the self - attentive architecture described in Section 2.2 is adapted from .",36,0,26
dataset/preprocessed/test-data/constituency_parsing/8,Tree Scores and Chart Decoder,37,0,5
dataset/preprocessed/test-data/constituency_parsing/8,"Our parser assigns a real - valued score s ( T ) to each tree T , which decomposes as",38,0,20
dataset/preprocessed/test-data/constituency_parsing/8,"Here s ( i , j , l ) is a real - valued score for a constituent that is located between fencepost positions i and j in a sentence and has label l .",39,0,35
dataset/preprocessed/test-data/constituency_parsing/8,"To handle unary chains , the set of labels includes a collapsed entry for each unary chain in the training set .",40,0,22
dataset/preprocessed/test-data/constituency_parsing/8,The model handles n-ary trees by binarizing them and introducing a dummy label ?,41,0,14
dataset/preprocessed/test-data/constituency_parsing/8,"to nodes created during binarization , with the property that ? i , j : s ( i , j , ? ) = 0 . Enforcing that scores associated with the dummy labels are always zero ensures that Here ?",42,0,41
dataset/preprocessed/test-data/constituency_parsing/8,"is the Hamming loss on labeled spans , and the tree corresponding to the most -violated constraint can be found using a slight modification of the inference algorithm used at test time .",43,0,33
dataset/preprocessed/test-data/constituency_parsing/8,"For further details , see .",44,0,6
dataset/preprocessed/test-data/constituency_parsing/8,"The remainder of this paper concerns itself with the functional form of s ( i , j , l ) , which is calculated using a neural network for all l = ?.",45,0,33
dataset/preprocessed/test-data/constituency_parsing/8,Context - Aware Word Representations,46,0,5
dataset/preprocessed/test-data/constituency_parsing/8,"The encoder portion of our model is split into two parts : a word - based portion that assigns a contextaware vector representation y t to each position tin the sentence ( described in this section ) , and a chart portion that combines the vectors y t to generate span scores s ( i , j , l) ( Section 2.3 ) .",47,0,64
dataset/preprocessed/test-data/constituency_parsing/8,The architecture for generating the vectors y t is adapted from . :,48,0,13
dataset/preprocessed/test-data/constituency_parsing/8,"An overview of our encoder , which produces a context - aware summary vector for each word in the sentence .",49,0,21
dataset/preprocessed/test-data/constituency_parsing/6,Parsing as Language Modeling,2,1,4
dataset/preprocessed/test-data/constituency_parsing/6,"We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing - 93.8 F 1 on section 23 , using 2 - 21 as training , 24 as development , plus tri-training .",4,1,53
dataset/preprocessed/test-data/constituency_parsing/6,"When trees are converted to Stanford dependencies , UAS and LAS are 95.9 % and 94.1 % .",5,0,18
dataset/preprocessed/test-data/constituency_parsing/6,"Recent work on deep learning syntactic parsing models has achieved notably good results , e.g. , with 92.4 F 1 on Penn Treebank constituency parsing and with 92.8 F 1 .",7,1,31
dataset/preprocessed/test-data/constituency_parsing/6,"In this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 F 1 , with a comparatively simple architecture .",8,0,37
dataset/preprocessed/test-data/constituency_parsing/6,In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem .,9,0,23
dataset/preprocessed/test-data/constituency_parsing/6,Section 2 looks more closely at three of the most relevant previous papers .,10,0,14
dataset/preprocessed/test-data/constituency_parsing/6,"We then describe our exact model ( Section 3 ) , followed by the experimental setup and results ( Sections 4 and 5 ) .",11,0,25
dataset/preprocessed/test-data/constituency_parsing/6,There is a one - to - one mapping between a tree and its sequential form .,12,0,17
dataset/preprocessed/test-data/constituency_parsing/6,( Part - of - speech tags are not used . ),13,0,12
dataset/preprocessed/test-data/constituency_parsing/6,"Formally , a language model ( LM ) is a probability distribution over strings of a language :",15,0,18
dataset/preprocessed/test-data/constituency_parsing/6,where x is a sentence and t indicates a word position .,16,0,12
dataset/preprocessed/test-data/constituency_parsing/6,"The efforts in language modeling go into computing P ( x t |x 1 , , x t?1 ) , which as described next is useful for parsing as well .",17,0,31
dataset/preprocessed/test-data/constituency_parsing/6,Parsing as Language Modeling,18,0,4
dataset/preprocessed/test-data/constituency_parsing/6,A generative parsing model parses a sentence ( x ) into it s phrasal structure ( y ) according to,19,0,20
dataset/preprocessed/test-data/constituency_parsing/6,where Y ( x ) lists all possible structures of x .,20,0,12
dataset/preprocessed/test-data/constituency_parsing/6,"If we think of a tree ( x , y) as a sequence ( z ) as illustrated in , we can define a probability distribution over ( x , y) as follows :",21,0,34
dataset/preprocessed/test-data/constituency_parsing/6,"P ( x , y ) = P ( z ) = P ( z 1 , , z m ) = m t=1 P ( z t | z 1 , , z t?1 ) , which is equivalent to Equation ( 1 ) .",22,0,46
dataset/preprocessed/test-data/constituency_parsing/6,"We have reduced parsing to language modeling and can use language modeling techniques of estimating P ( z t | z 1 , , z t?1 ) for parsing .",23,0,30
dataset/preprocessed/test-data/constituency_parsing/6,We look here at three neural net ( NN ) models closest to our research along various dimensions .,25,0,19
dataset/preprocessed/test-data/constituency_parsing/6,"The first gives the basic language modeling architecture that we have adopted , while the other two are parsing models that have the current best results in NN parsing .",26,0,30
dataset/preprocessed/test-data/constituency_parsing/6,LSTM - LM,27,0,3
dataset/preprocessed/test-data/constituency_parsing/6,"The LSTM - LM of turns ( x 1 , , x t?1 ) into ht , a hidden state of an LSTM , and uses ht to guess x t :",28,0,32
dataset/preprocessed/test-data/constituency_parsing/6,Wis a parameter matrix and [ i ] indexes ith element of a vector .,30,0,15
dataset/preprocessed/test-data/constituency_parsing/6,"The simplicity of the model makes it easily extendable and scalable , which has inspired a character - based LSTM - LM that works well for many languages and an ensemble of large LSTM - LMs for English with astonishing perplexity of 23.7 .",31,0,44
dataset/preprocessed/test-data/constituency_parsing/6,"In this paper , we build a parsing model based on the LSTM - LM of .",32,0,17
dataset/preprocessed/test-data/constituency_parsing/6,"observe that a phrasal structure ( y ) can be expressed as a sequence and build a machine translation parser ( MTP ) , a sequence - tosequence model , which translates x into y using a conditional probability :",33,0,40
dataset/preprocessed/test-data/constituency_parsing/6,"where the conditioning event ( x , y 1 , , y t?1 ) is modeled by an LSTM encoder and an LSTM decoder .",35,0,25
dataset/preprocessed/test-data/constituency_parsing/6,"The encoder maps x into he , a set of vectors that represents x , and the decoder obtains a summary vector ( h t ) which is concatenation of the decoder 's hidden state ( h d t ) and weighted sum of word representations ( n i =1 ?",36,0,51
dataset/preprocessed/test-data/constituency_parsing/6,i he i ) with an alignment vector ( ? ) .,37,0,12
dataset/preprocessed/test-data/constituency_parsing/6,Finally the decoder predicts y t given ht .,38,0,9
dataset/preprocessed/test-data/constituency_parsing/6,"Inspired by MTP , our model processes sequential trees .",39,0,10
dataset/preprocessed/test-data/constituency_parsing/6,"Recurrent Neural Network Grammars ( RNNG ) , a generative parsing model , defines a joint distribution over a tree in terms of actions the model takes to generate the tree :",41,0,32
dataset/preprocessed/test-data/constituency_parsing/6,"where a is a sequence of actions whose output precisely matches the sequence of symbols in z , which implies Equation is the same as Equation .",42,0,27
dataset/preprocessed/test-data/constituency_parsing/6,"RNNG and our model differ in how they compute the conditioning event ( z 1 , , z t?1 ) :",43,0,21
dataset/preprocessed/test-data/constituency_parsing/6,"RNNG combines hidden states of three LSTMs that keep track of actions the model has taken , an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM 's hidden state as shown in the next section .",44,0,46
dataset/preprocessed/test-data/constituency_parsing/6,"Our model , the model of applied to sequential trees and we call LSTM - LM from now on , is a joint distribution over trees :",46,0,27
dataset/preprocessed/test-data/constituency_parsing/6,where ht is a hidden state of an LSTM .,47,0,10
dataset/preprocessed/test-data/constituency_parsing/6,"Due to lack of an algorithm that searches through an exponentially large phrase - structure space , we use an n-best parser to reduce Y ( x ) to Y ( x ) , whose size is polynomial , and use LSTM - LM to find y that satisfies",48,0,49
dataset/preprocessed/test-data/constituency_parsing/6,( 4 ),49,0,3
dataset/preprocessed/test-data/constituency_parsing/0,Recurrent Neural Network Grammars,2,1,4
dataset/preprocessed/test-data/constituency_parsing/0,"This is modified version of a paper originally published at NAACL 2016 that contains a corrigendum at the end , with improved results after fixing an implementation bug in the RNNG composition function .",4,0,34
dataset/preprocessed/test-data/constituency_parsing/0,"We introduce recurrent neural network grammars , probabilistic models of sentences with explicit phrase structure .",5,0,16
dataset/preprocessed/test-data/constituency_parsing/0,We explain efficient inference procedures that allow application to both parsing and language modeling .,6,0,15
dataset/preprocessed/test-data/constituency_parsing/0,Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state - of - the - art sequential RNNs in English and Chinese 1 .,7,0,37
dataset/preprocessed/test-data/constituency_parsing/0,Sequential recurrent neural networks ( RNNs ) are remarkably effective models of natural language .,9,0,15
dataset/preprocessed/test-data/constituency_parsing/0,"In the last few years , language model results that substantially improve over long - established state - of the - art baselines have been obtained using RNNs as well as in various conditional language modeling tasks such as machine translation , image caption generation , and dialogue generation .",10,0,50
dataset/preprocessed/test-data/constituency_parsing/0,"Despite these impressive results , sequential models area priori inappropriate models of natural language , since relationships among words are largely organized in terms of latent nested structures rather than sequential surface order .",11,0,34
dataset/preprocessed/test-data/constituency_parsing/0,"In this paper , we introduce recurrent neural network grammars ( RNNGs ; 2 ) , a new generative probabilistic model of sentences that explicitly models nested , hierarchical relationships among words and phrases .",12,0,35
dataset/preprocessed/test-data/constituency_parsing/0,"RNNGs operate via a recursive syntactic process reminiscent of probabilistic context - free grammar generation , but decisions are parameterized using RNNs that condition on the entire syntactic derivation history , greatly relaxing context - free independence assumptions .",13,0,39
dataset/preprocessed/test-data/constituency_parsing/0,The foundation of this work is a top - down variant of transition - based parsing ( 3 ) .,14,0,20
dataset/preprocessed/test-data/constituency_parsing/0,"We give two variants of the algorithm , one for parsing ( given an observed sentence , transform it into a tree ) , and one for generation .",15,0,29
dataset/preprocessed/test-data/constituency_parsing/0,"While several transition - based neural models of syntactic generation exist , these have relied on structure building operations based on parsing actions in shift - reduce and leftcorner parsers which operate in a largely bottomup fashion .",16,0,38
dataset/preprocessed/test-data/constituency_parsing/0,"While this construction is appealing because inference is relatively straightforward , it limits the use of top - down grammar information , which is helpful for generation .",17,0,28
dataset/preprocessed/test-data/constituency_parsing/0,"2 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top - down ( i.e. , rootto - terminal ) syntactic information ( 4 ) .",18,0,27
dataset/preprocessed/test-data/constituency_parsing/0,"The top - down transition set that RNNGs are based on lends itself to discriminative modeling as well , where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures .",19,0,39
dataset/preprocessed/test-data/constituency_parsing/0,"Similar to previously published discriminative bottomup transition - based parsers , greedy prediction with our model yields a linear -",20,0,20
dataset/preprocessed/test-data/constituency_parsing/0,"The left - corner parsers used by incorporate limited top - down information , but a complete path from the root of the tree to a terminal is not generally present when a terminal is generated .",21,0,37
dataset/preprocessed/test-data/constituency_parsing/0,Refer to for an example .,22,0,6
dataset/preprocessed/test-data/constituency_parsing/0,"time deterministic parser ( provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed ) ; however , our algorithm generates arbitrary tree structures directly , without the binarization required by shift - reduce parsers .",23,0,43
dataset/preprocessed/test-data/constituency_parsing/0,"The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences , and this is used to solve a second practical challenge with RNNGs : approximating the marginal likelihood and MAP tree of a sentence under the generative model .",24,0,46
dataset/preprocessed/test-data/constituency_parsing/0,We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model ( 5 ) .,25,0,26
dataset/preprocessed/test-data/constituency_parsing/0,Experiments show that RNNGs are effective for both language modeling and parsing ( 6 ) .,26,0,16
dataset/preprocessed/test-data/constituency_parsing/0,Our generative model obtains ( i ) the best - known parsing results using a single supervised generative model and ( ii ) better perplexities in language modeling than state - of - the - art sequential LSTM language models .,27,0,41
dataset/preprocessed/test-data/constituency_parsing/0,Surprisingly - although inline with previous parsing results showing the effectiveness of generative models ) parsing with the generative model obtains significantly better results than parsing with the discriminative model .,28,0,31
dataset/preprocessed/test-data/constituency_parsing/0,"Formally , an RNNG is a triple ( N , ? , ? ) consisting of a finite set of nonterminal symbols ( N ) , a finite set of terminal symbols ( ? ) such that N ? ? = ? , and a collection of neural network parameters ?.",30,0,51
dataset/preprocessed/test-data/constituency_parsing/0,It does not explicitly define rules since these are implicitly characterized by ?.,31,0,13
dataset/preprocessed/test-data/constituency_parsing/0,"The algorithm that the grammar uses to generate trees and strings in the language is characterized in terms of a transition - based algorithm , which is outlined in the next section .",32,0,33
dataset/preprocessed/test-data/constituency_parsing/0,"In the section after that , the semantics of the parameters thatare used to turn this into a stochastic algorithm that generates pairs of trees and strings are discussed .",33,0,30
dataset/preprocessed/test-data/constituency_parsing/0,Top - down Parsing and Generation,34,0,6
dataset/preprocessed/test-data/constituency_parsing/0,RNNGs are based on a top - down generation algorithm that relies on a stack data structure of partially completed syntactic constituents .,35,0,23
dataset/preprocessed/test-data/constituency_parsing/0,"To emphasize the similarity of our algorithm to more familiar bottom - up shift - reduce recognition algorithms , we first present the parsing ( rather than generation ) version of our algorithm ( 3.1 ) and then present modifications to turn it into a generator ( 3.2 ) .",36,0,50
dataset/preprocessed/test-data/constituency_parsing/0,The parsing algorithm transforms a sequence of words x into a parse tree y using two data structures ( a stack and an input buffer ) .,38,0,27
dataset/preprocessed/test-data/constituency_parsing/0,"As with the bottomup algorithm of , our algorithm begins with the stack ( S ) empty and the complete sequence of words in the input buffer ( B ) .",39,0,31
dataset/preprocessed/test-data/constituency_parsing/0,"The buffer contains unprocessed terminal symbols , and the stack contains terminal symbols , "" open "" nonterminal symbols , and completed constituents .",40,0,24
dataset/preprocessed/test-data/constituency_parsing/0,"At each timestep , one of the following three classes of operations ( ) is selected by a classifier , based on the current contents on the stack and buffer :",41,0,31
dataset/preprocessed/test-data/constituency_parsing/0,"NT ( X ) introduces an "" open nonterminal "" X onto the top of the stack .",42,0,18
dataset/preprocessed/test-data/constituency_parsing/0,"Open nonterminals are written as a nonterminal symbol preceded by an open parenthesis , e.g. , "" ( VP "" , and they represent a nonterminal whose child nodes have not yet been fully constructed .",43,0,36
dataset/preprocessed/test-data/constituency_parsing/0,"Open nonterminals are "" closed "" to form complete constituents by subsequent REDUCE operations .",44,0,15
dataset/preprocessed/test-data/constituency_parsing/0,"SHIFT removes the terminal symbol x from the front of the input buffer , and pushes it onto the top of the stack .",45,0,24
dataset/preprocessed/test-data/constituency_parsing/0,"REDUCE repeatedly pops completed subtrees or terminal symbols from the stack until an open nonterminal is encountered , and then this open NT is popped and used as the label of a new constituent that has the popped subtrees as its children .",46,0,43
dataset/preprocessed/test-data/constituency_parsing/0,This new completed constituent is pushed onto the stack as a single composite item .,47,0,15
dataset/preprocessed/test-data/constituency_parsing/0,A single REDUCE operation can thus create constituents with an unbounded number of children .,48,0,15
dataset/preprocessed/test-data/constituency_parsing/0,The parsing algorithm terminates when there is a single completed constituent on the stack and the buffer is empty .,49,0,20
dataset/preprocessed/test-data/constituency_parsing/2,An Empirical Study of Building a Strong Baseline for Constituency Parsing,2,1,11
dataset/preprocessed/test-data/constituency_parsing/2,This paper investigates the construction of a strong baseline based on general purpose sequence - to - sequence models for constituency parsing .,4,0,23
dataset/preprocessed/test-data/constituency_parsing/2,"We incorporate several techniques that were mainly developed in natural language generation tasks , e.g. , machine translation and summarization , and demonstrate that the sequenceto - sequence model achieves the current top - notch parsers ' performance without requiring explicit task - specific knowledge or architecture of constituent parsing .",5,0,51
dataset/preprocessed/test-data/constituency_parsing/2,"Sequence - to - sequence ( Seq2seq ) models have successfully improved many well - studied NLP tasks , especially for natural language generation ( NLG ) tasks , such as machine translation ( MT ) and abstractive summarization .",7,0,40
dataset/preprocessed/test-data/constituency_parsing/2,Seq2seq models have also been applied to constituency parsing and provided a fairly good result .,8,0,16
dataset/preprocessed/test-data/constituency_parsing/2,"However one obvious , intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees , Thus , models that directly model them , such as RNNG , are an intuitively more promising approach .",9,0,57
dataset/preprocessed/test-data/constituency_parsing/2,"In fact , RNNG and its extensions provide the current stateof - the - art performance .",10,0,17
dataset/preprocessed/test-data/constituency_parsing/2,Sec2seq models are currently considered a simple baseline of neuralbased constituency parsing .,11,0,13
dataset/preprocessed/test-data/constituency_parsing/2,"After the first proposal of an Seq2seq constituency parser , many task - independent techniques have been developed , mainly in the NLG research area .",12,0,26
dataset/preprocessed/test-data/constituency_parsing/2,Our aim is to update the Seq2seq approach proposed in as a stronger baseline of constituency parsing .,13,0,18
dataset/preprocessed/test-data/constituency_parsing/2,Our motivation is basically identical to that described in .,14,0,10
dataset/preprocessed/test-data/constituency_parsing/2,A strong baseline is crucial for reporting reliable experimental results .,15,0,11
dataset/preprocessed/test-data/constituency_parsing/2,It offers a fair evaluation of promising new techniques if they solve new issues or simply resolve issues that have already been addressed by current generic technology .,16,0,28
dataset/preprocessed/test-data/constituency_parsing/2,"More specifically , it might become possible to analyze what types of implicit linguistic structures are easier or harder to capture for neural models by comparing the outputs of strong Seq2seq models and task - specific models , e.g. , RNNG .",17,0,42
dataset/preprocessed/test-data/constituency_parsing/2,"The contributions of this paper are summarized as follows : ( 1 ) a strong baseline for constituency parsing based on general purpose Seq2seq models 1 , ( 2 ) an empirical investigation of several generic techniques that can ( or can not ) contribute to improve the parser performance , ( 3 ) empirical evidence that Seq2seq models implicitly learn parse tree structures well without knowing taskspecific and explicit tree structure information .",18,0,74
dataset/preprocessed/test-data/constituency_parsing/2,Constituency Parsing by Seq2seq,19,0,4
dataset/preprocessed/test-data/constituency_parsing/2,Our starting point is an RNN - based Seq2seq model with an attention mechanism that was applied to constituency parsing .,20,0,21
dataset/preprocessed/test-data/constituency_parsing/2,"We omit detailed descriptions due to space limitations , but note that our model architecture is identical to the one introduced in",21,0,22
dataset/preprocessed/test-data/constituency_parsing/2,A key trick for applying Seq2seq models to constituency parsing is the linearization of parse 1 Our code and experimental configurations for reproducing our experiments are publicly available : https://github.com/nttcslab-nlp/strong,22,0,30
dataset/preprocessed/test-data/constituency_parsing/2,s 2s baseline parser,23,0,4
dataset/preprocessed/test-data/constituency_parsing/2,"2 More specifically , our Seq2seq model follows the one implemented in seq2seq - attn ( https://github.com/harvardnlp/seq2seq-attn ) , which is the alpha-version of the Open NMT tool ( http://opennmt.net ) .",24,0,32
dataset/preprocessed/test-data/constituency_parsing/2,John has a dog .,26,0,5
dataset/preprocessed/test-data/constituency_parsing/2,Output : S-exp.,27,0,3
dataset/preprocessed/test-data/constituency_parsing/2,( S ( NP NNP ) ( VP VBZ ( NP DT NN ) ) . ) Linearized form ( S ( NP NNP ) NP ( VP VBZ ( NP DT NN ) NP ) VP . ) S w/ POS normalized ( S ( NP XX ) NP ( VP XX ( NP XX XX ) NP ) VP . ) S : Examples of linearization and POS - tag normalization trees .,28,0,75
dataset/preprocessed/test-data/constituency_parsing/2,"Roughly speaking , a linearized parse tree consists of open , close bracketing and POS - tags that correspond to a given input raw sentence .",29,0,26
dataset/preprocessed/test-data/constituency_parsing/2,"Since a one - to - one mapping exists between a parse tree and it s linearized form ( if the linearized form is a valid tree ) , we can recover parse trees from the predicted linearized parse tree .",30,0,41
dataset/preprocessed/test-data/constituency_parsing/2,also introduced the part - of - speech ( POS ) tag normalization technique .,31,0,15
dataset/preprocessed/test-data/constituency_parsing/2,"They substituted each POS tag in a linearized parse tree to a single XX - tag 3 , which allows Seq2seq models to achieve a more competitive performance range than the current state - of the - art parses 4 . shows an example of a parse tree to which linearization and POS - tag normalization was applied .",32,0,59
dataset/preprocessed/test-data/constituency_parsing/2,Task - independent,33,0,3
dataset/preprocessed/test-data/constituency_parsing/2,This section describes several generic techniques that improve Seq2seq performance 5 . lists the notations used in this paper for a convenient reference .,35,0,24
dataset/preprocessed/test-data/constituency_parsing/2,Subword as input features,36,0,4
dataset/preprocessed/test-data/constituency_parsing/2,Applying subword decomposition has recently become a leading technique in NMT literature .,37,0,13
dataset/preprocessed/test-data/constituency_parsing/2,Its primary advantage is a significant reduction of the serious out - of - vocabulary ( OOV ) problem .,38,0,20
dataset/preprocessed/test-data/constituency_parsing/2,We incorporated subword information as an additional feature of the original input words .,39,0,14
dataset/preprocessed/test-data/constituency_parsing/2,A similar usage of subword features was previously proposed in .,40,0,11
dataset/preprocessed/test-data/constituency_parsing/2,"Formally , the encoder embedding vector at encoder position i , namely , e i , is calculated as follows :",41,0,21
dataset/preprocessed/test-data/constituency_parsing/2,"We did not substitute POS - tags for punctuation symbols such as "" . "" , and "" , "" .",42,0,21
dataset/preprocessed/test-data/constituency_parsing/2,4 Several recently developed neural - based constituency parsers ignore POS tags since they are not evaluated in the standard evaluation metric of constituency parsing ( Bracketing F- measure ) .,43,0,31
dataset/preprocessed/test-data/constituency_parsing/2,Figure in the supplementary material shows the brief sketch of the method explained in the following section .,44,0,18
dataset/preprocessed/test-data/constituency_parsing/2,D : dimension of the embeddings H : dimension of the hidden states i : index of the ( token ) position in input sentence j : index of the ( token ) position in output linearized format of parse tree V ( e ) :,45,0,46
dataset/preprocessed/test-data/constituency_parsing/2,vocabulary of word for input ( encoder ) side V ( s ) :,46,0,14
dataset/preprocessed/test-data/constituency_parsing/2,"vocabulary of subword for input ( encoder ) side E : encoder embedding matrix for V ( e ) , where E ? R D|V ( e ) | F : encoder embedding matrix for V ( s ) , where F ? R D|V ( s ) | wi : i - th word ( token ) in the input sentence , wi ? V ( e ) x k : one - hot vector representation of the k - th word in V ( e ) s k : one - hot vector representation of the k - th subword in V ( s ) u : encoder embedding vector of unknown token ?( ) :",47,0,118
dataset/preprocessed/test-data/constituency_parsing/2,function that returns the index of given word in the vocabulary V ( e ) ?( ) : function that returns a set of indices in the subword vocabulary V ( s ) generated from the given word .,48,0,39
dataset/preprocessed/test-data/constituency_parsing/2,"e.g. , k ? ? ( wi ) ei : encoder embedding vector at position i in encoder V ( d ) :",49,0,23
dataset/preprocessed/test-data/constituency_parsing/4,Improving Neural Parsing by Disentangling Model Combination and Reranking Effects,2,0,10
dataset/preprocessed/test-data/constituency_parsing/4,Recent work has proposed several generative neural models for constituency parsing that achieve state - of - the - art results .,4,1,22
dataset/preprocessed/test-data/constituency_parsing/4,"Since direct search in these generative models is difficult , they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward .",5,0,29
dataset/preprocessed/test-data/constituency_parsing/4,We first present an algorithm for direct search in these generative models .,6,0,13
dataset/preprocessed/test-data/constituency_parsing/4,We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects .,7,0,21
dataset/preprocessed/test-data/constituency_parsing/4,"Finally , we show that explicit model combination can improve performance even further , resulting in new state - of - the - art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data .",8,0,45
dataset/preprocessed/test-data/constituency_parsing/4,Recent work on neural constituency parsing has found multiple cases where generative scoring models for which inference is complex outperform base models for which inference is simpler .,10,1,28
dataset/preprocessed/test-data/constituency_parsing/4,"Abe a parser that we want to parse with ( here one of the generative models ) , and let B be a base parser that we use to propose candidate parses which are then scored by the less - tractable parser A .",12,0,44
dataset/preprocessed/test-data/constituency_parsing/4,We denote this cross - scoring setup by B ?,13,0,10
dataset/preprocessed/test-data/constituency_parsing/4,The papers above repeatedly saw that the cross - scoring setup B ?,15,0,13
dataset/preprocessed/test-data/constituency_parsing/4,A under which their generative models were applied outperformed the standard singleparser setup B ?,16,0,15
dataset/preprocessed/test-data/constituency_parsing/4,We term this a cross - scoring gain .,18,0,9
dataset/preprocessed/test-data/constituency_parsing/4,This paper asks two questions .,19,0,6
dataset/preprocessed/test-data/constituency_parsing/4,"First , why do recent discriminative - to - generative cross - scoring se - * Equal contribution .",20,0,19
dataset/preprocessed/test-data/constituency_parsing/4,tups B ?,21,0,3
dataset/preprocessed/test-data/constituency_parsing/4,A outperform their base parsers B ?,22,0,7
dataset/preprocessed/test-data/constituency_parsing/4,Perhaps generative models,23,0,3
dataset/preprocessed/test-data/constituency_parsing/4,A are simply superior to the base models B and direct generative parsing ( A ?,24,0,16
dataset/preprocessed/test-data/constituency_parsing/4,A ) would be better still if it were feasible .,25,0,11
dataset/preprocessed/test-data/constituency_parsing/4,"If so , we would characterize the cross - scoring gain from B ?",26,0,14
dataset/preprocessed/test-data/constituency_parsing/4,B to B ?,27,0,4
dataset/preprocessed/test-data/constituency_parsing/4,A as a reranking gain .,28,0,6
dataset/preprocessed/test-data/constituency_parsing/4,"However , it 's also possible that the hybrid system B ?",29,0,12
dataset/preprocessed/test-data/constituency_parsing/4,A shows gains merely from subtle model combination effects .,30,0,10
dataset/preprocessed/test-data/constituency_parsing/4,"If so , scoring candidates using some combined score A + B would be even better , which we would characterize as a model combination gain .",31,0,27
dataset/preprocessed/test-data/constituency_parsing/4,It might even be the case that B is a better parser over all ( i.e. B ?,32,0,18
dataset/preprocessed/test-data/constituency_parsing/4,B outperforms A ? A ) .,33,0,7
dataset/preprocessed/test-data/constituency_parsing/4,"Of course , many real hybrids will exhibit both reranking and model combination gains .",34,0,15
dataset/preprocessed/test-data/constituency_parsing/4,"In this paper , we present experiments to isolate the degree to which each gain occurs for each of two state - of - the - art generative neural parsing models : the Recurrent Neural Network Grammar generative parser ( RG ) of , and the LSTM language modeling generative parser ( LM ) of .",35,0,56
dataset/preprocessed/test-data/constituency_parsing/4,"In particular , we present and use a beam - based search procedure with an augmented state space that can search directly in the generative models , allowing us to explore A ?",36,0,33
dataset/preprocessed/test-data/constituency_parsing/4,A for these generative parsers A independent of any base parsers .,37,0,12
dataset/preprocessed/test-data/constituency_parsing/4,"Our findings suggest the presence of model combination effects in both generative parsers : when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser ( the RNNG discriminative parser , RD ) , performance decreases when compared to using just candidates from the base parser , i.e. , B ? A ?",38,0,64
dataset/preprocessed/test-data/constituency_parsing/4,A has lower evaluation performance than B ? A ( Section 3.1 ) .,39,0,14
dataset/preprocessed/test-data/constituency_parsing/4,"This result suggests that both generative models benefit from fortuitous search errors in the rescoring setting - there are trees with higher probability under the generative model than any tree proposed by the base parser , but which would decrease evaluation performance if selected .",40,0,45
dataset/preprocessed/test-data/constituency_parsing/4,"Because of this , we hypothesize that model combination effects between the base and generative models are partially responsible for the high performance of the generative reranking systems , rather than the generative model being generally superior .",41,0,38
dataset/preprocessed/test-data/constituency_parsing/4,"Here we consider our second question : if crossscoring gains are at least partly due to implicit model combination , can we gain even more by combining the models explicitly ?",42,0,31
dataset/preprocessed/test-data/constituency_parsing/4,"We find that this is indeed the case : simply taking a weighted average of the scores of both models when selecting a parse from the base parser 's candidate list improves over using only the score of the generative model , in many cases substantially ( Section 3.2 ) .",43,0,51
dataset/preprocessed/test-data/constituency_parsing/4,"Using this technique , in combination with ensembling , we obtain new state - of - the - art results on the Penn Treebank : 94.25 F1 when training only on gold parse trees and 94.66 F1 when using external silver data .",44,0,43
dataset/preprocessed/test-data/constituency_parsing/4,Decoding in generative neural models,45,0,5
dataset/preprocessed/test-data/constituency_parsing/4,"All of the parsers we investigate in this work ( the discriminative parser RD , and the two generative parsers RG and LM , see Section 1 ) produce parse trees in a depth - first , left - to - right traversal , using the same basic actions : NT ( X ) , which opens a new constituent with the non-terminal symbol X ; SHIFT / GEN ( w ) , which adds a word ; and RE - DUCE , which closes the current constituent .",46,0,89
dataset/preprocessed/test-data/constituency_parsing/4,"We refer to for a complete description of these actions , and the constraints on them necessary to ensure valid parse trees .",47,0,23
dataset/preprocessed/test-data/constituency_parsing/4,"The primary difference between the actions in the discriminative and generative models is that , whereas the discriminative model uses a SHIFT action which is fixed to produce the next word in the sentence , the generative models use GEN ( w ) to define a distribution over all possible words win the lexicon .",48,0,55
dataset/preprocessed/test-data/constituency_parsing/4,"This stems from the generative model 's definition of a joint probability p ( x , y) over all possible sentences x and parses y .",49,0,26
dataset/preprocessed/test-data/document_classification/7,A Corpus for Multilingual Document Classification in Eight Languages,2,1,9
dataset/preprocessed/test-data/document_classification/7,Cross - lingual document classification aims at training a document classifier on resources in one language and transferring it to a different language without any additional resources .,4,1,28
dataset/preprocessed/test-data/document_classification/7,Several approaches have been proposed in the literature and the current best practice is to evaluate them on a subset of the Reuters Corpus Volume,5,0,25
dataset/preprocessed/test-data/document_classification/7,"However , this subset covers only few languages ( English , German , French and Spanish ) and almost all published works focus on the the transfer between English and German .",7,0,32
dataset/preprocessed/test-data/document_classification/7,"In addition , we have observed that the class prior distributions differ significantly between the languages .",8,0,17
dataset/preprocessed/test-data/document_classification/7,We argue that this complicates the evaluation of the multilinguality .,9,0,11
dataset/preprocessed/test-data/document_classification/7,"In this paper , we propose a new subset of the Reuters corpus with balanced class priors for eight languages .",10,0,21
dataset/preprocessed/test-data/document_classification/7,"By adding Italian , Russian , Japanese and Chinese , we cover languages which are very different with respect to syntax , morphology , etc .",11,0,26
dataset/preprocessed/test-data/document_classification/7,We provide strong baselines for all language transfer directions using multilingual word and sentence embeddings respectively .,12,0,17
dataset/preprocessed/test-data/document_classification/7,"Our goal is to offer a freely available framework to evaluate cross - lingual document classification , and we hope to foster by these means , research in this important area .",13,0,32
dataset/preprocessed/test-data/document_classification/7,There are many tasks in natural language processing which require the classification of sentences or longer paragraphs into a set of predefined categories .,15,0,24
dataset/preprocessed/test-data/document_classification/7,"Typical applications are for instance topic identification ( e.g. sports , news , . . . ) or product reviews ( positive or negative ) .",16,0,26
dataset/preprocessed/test-data/document_classification/7,There is a large body of research on approaches for document classification .,17,0,13
dataset/preprocessed/test-data/document_classification/7,An important aspect to compare these different approaches is the availability of high quality corpora to train and evaluate them .,18,0,21
dataset/preprocessed/test-data/document_classification/7,"Unfortunately , most of these evaluation tasks focus on the English language only , while there is an ever increasing need to perform document classification in many other languages .",19,0,30
dataset/preprocessed/test-data/document_classification/7,"One could of course collect and label training data for other languages , but this would be costly and time consuming .",20,0,22
dataset/preprocessed/test-data/document_classification/7,"An interesting alternative is "" crosslingual document classification "" .",21,0,10
dataset/preprocessed/test-data/document_classification/7,The underlying idea is to use a representation of the words or whole documents which is independent of the language .,22,0,21
dataset/preprocessed/test-data/document_classification/7,"By these means , a classifier trained on one language can be transferred to a different one , without the need of resources in that transfer language .",23,0,28
dataset/preprocessed/test-data/document_classification/7,"Ideally , the performance obtained by crosslingual transfer should be as close as possible to training the entire system on language specific resources .",24,0,24
dataset/preprocessed/test-data/document_classification/7,Such a task was first proposed by using the Reuters Corpus Volume,25,0,12
dataset/preprocessed/test-data/document_classification/7,"The aim was to first train a classifier on English and then to transfer it to German , and vice versa .",27,0,22
dataset/preprocessed/test-data/document_classification/7,An extension to the transfer between English and French and Spanish respectively was proposed by .,28,0,16
dataset/preprocessed/test-data/document_classification/7,"However , only few comparative results are available for these transfer directions .",29,0,13
dataset/preprocessed/test-data/document_classification/7,The contributions of this work are as follows .,30,0,9
dataset/preprocessed/test-data/document_classification/7,"We extend previous works and use the data in the Reuters Corpus Volume 2 to define new cross - lingual document classification tasks for eight very different languages , namely English , French , Spanish , Italian , German , Russian , Chinese and Japanese .",31,0,46
dataset/preprocessed/test-data/document_classification/7,"For each language , we define a train , development and test corpus .",32,0,14
dataset/preprocessed/test-data/document_classification/7,"We also provide strong reference results for all transfer directions between the eight languages , e.g. not limited to the transfer between a foreign language and English .",33,0,28
dataset/preprocessed/test-data/document_classification/7,"We compare two approaches , based either on multilingual word or sentence embeddings respectively .",34,0,15
dataset/preprocessed/test-data/document_classification/7,"By these means , we hope to define a clear evaluation environment for highly multilingual document classification .",35,0,18
dataset/preprocessed/test-data/document_classification/7,"The Reuters Corpus Volume 2 , in short RCV2 1 , is a multilingual corpus with a collection of 487,000 news stories .",37,0,23
dataset/preprocessed/test-data/document_classification/7,"Each news story was manually classified into four hierarchical groups : CCAT ( Corporate / Industrial ) , ECAT ( Economics ) , GCAT ( Government / Social ) and MCAT ( Markets ) .",38,0,35
dataset/preprocessed/test-data/document_classification/7,Topic codes were assigned to capture the major subject of the news story .,39,0,14
dataset/preprocessed/test-data/document_classification/7,"The entire corpus covers thirteen languages , i.e. Dutch , French , German , Chinese , Japanese , Russian , Portuguese , Spanish , Latin American Spanish , Italian , Danish , Norwegian , and Swedish , written by local reporters in each language .",40,0,45
dataset/preprocessed/test-data/document_classification/7,The news stories are not parallel .,41,0,7
dataset/preprocessed/test-data/document_classification/7,"Single - label stories , i.e. those labeled with only one topic out of the four top categories , are often used for evaluations .",42,0,25
dataset/preprocessed/test-data/document_classification/7,"However , the class distributions vary significantly across all the thirteen languages ( see ) .",43,0,16
dataset/preprocessed/test-data/document_classification/7,"Therefore , using random samples to extract evaluation corpora may lead to very imbalanced test sets , i.e. undesired and misleading variability among the languages when the main focus is to evaluate cross - lingual transfer .",44,0,37
dataset/preprocessed/test-data/document_classification/7,Cross - lingual document classification,45,0,5
dataset/preprocessed/test-data/document_classification/7,A subset of the English and German sections of RCV2 was defined by to evaluate crosslingual document classification .,46,0,19
dataset/preprocessed/test-data/document_classification/7,This subset was used in several follow - up works and many comparative results are available for the transfer between German and English .,47,0,24
dataset/preprocessed/test-data/document_classification/7,extended the use of RCV2 for cross - lingual document classification to the French and Spanish language ( transfer from and to English ) .,48,0,25
dataset/preprocessed/test-data/document_classification/7,An analysis of these evaluation corpora has shown that the class prior distributions vary significantly between the classes ( see ) .,49,0,22
dataset/preprocessed/test-data/document_classification/3,Squeezed Very Deep Convolutional Neural Networks for Text Classification,2,1,9
dataset/preprocessed/test-data/document_classification/3,"Most of the research in convolutional neural networks has focused on increasing network depth to improve accuracy , resulting in a massive number of parameters which restricts the trained network to platforms with memory and processing constraints .",4,0,38
dataset/preprocessed/test-data/document_classification/3,We propose to modify the structure of the Very Deep Convolutional Neural Networks ( VDCNN ) model to fit mobile platforms constraints and keep performance .,5,0,26
dataset/preprocessed/test-data/document_classification/3,"In this paper , we evaluate the impact of Temporal Depthwise Separable Convolutions and Global Average Pooling in the network parameters , storage size , and latency .",6,0,28
dataset/preprocessed/test-data/document_classification/3,"The squeezed model ( SVDCNN ) is between 10x and 20x smaller , depending on the network depth , maintaining a maximum size of 6 MB .",7,0,27
dataset/preprocessed/test-data/document_classification/3,"Regarding accuracy , the network experiences a loss between 0.4 % and 1.3 % and obtains lower latencies compared to the baseline model .",8,0,24
dataset/preprocessed/test-data/document_classification/3,The general trend in deep learning approaches has been developing models with increasing layers .,10,0,15
dataset/preprocessed/test-data/document_classification/3,"Deeper neural networks have achieved high - quality results in different tasks such as image classification , detection , and segmentation .",11,0,22
dataset/preprocessed/test-data/document_classification/3,Deep models can also learn hierarchical feature representations from images .,12,0,11
dataset/preprocessed/test-data/document_classification/3,"In the Natural Language Processing ( NLP ) field , the belief that compositional models can also be used to textrelated tasks is more recent .",13,0,26
dataset/preprocessed/test-data/document_classification/3,The increasing availability of text data motivates research for models able to improve accuracy in different language tasks .,14,0,19
dataset/preprocessed/test-data/document_classification/3,"Following the image classification Convolutional Neural Network ( CNN ) tendency , the research in text classification has placed effort into developing deeper networks .",15,0,25
dataset/preprocessed/test-data/document_classification/3,The first CNN based approach for text was a shallow network with one layer .,16,0,15
dataset/preprocessed/test-data/document_classification/3,"Following this work , deeper architectures were proposed , .",17,0,10
dataset/preprocessed/test-data/document_classification/3,Conneau et al. were the first to propose Very Deep Convolutional Neural Networks ( VDCNN ) applied to text classification .,18,0,21
dataset/preprocessed/test-data/document_classification/3,VDCNN accuracy increases with depth .,19,0,6
dataset/preprocessed/test-data/document_classification/3,The approach with 29 layers is the state - of - the - art accuracy of CNNs for text classification .,20,0,21
dataset/preprocessed/test-data/document_classification/3,* Authors contributed equally and are both first writers .,21,0,10
dataset/preprocessed/test-data/document_classification/3,"However , regardless of making networks deeper to improve accuracy , little effort has been made to build text classification models to constrained resources .",22,0,25
dataset/preprocessed/test-data/document_classification/3,"It is a very different scenario compared to image approaches , where size and speed constrained models have been proposed , .",23,0,22
dataset/preprocessed/test-data/document_classification/3,"In real - world applications , size and speed are constraints to an efficient mobile and embedded deployment of deep models .",24,0,22
dataset/preprocessed/test-data/document_classification/3,"Several relevant real - world applications depend on text classification tasks such as sentiment analysis , recommendation and opinion mining .",25,0,21
dataset/preprocessed/test-data/document_classification/3,The appeal for these applications combined with the boost in mobile devices usage motivates the need for research in restrained text classification models .,26,0,24
dataset/preprocessed/test-data/document_classification/3,"Concerning mobile development , there are numerous benefits to developing smaller models .",27,0,13
dataset/preprocessed/test-data/document_classification/3,Some of the most relevant are requiring fewer data transferring while updating the client model and increasing usability by diminishing the inference time .,28,0,24
dataset/preprocessed/test-data/document_classification/3,Such advantages would boost the usage of deep neural models in text - based applications for embedded platforms .,29,0,19
dataset/preprocessed/test-data/document_classification/3,"In this paper , we investigate modifications on the network proposed by Conneau et al. with the aim of reducing its number of parameters , storage size and latency with minimal performance degradation .",30,0,34
dataset/preprocessed/test-data/document_classification/3,To achieve these improvements we used Temporal Depthwise Separable Convolution and Global Average Pooling techniques .,31,0,16
dataset/preprocessed/test-data/document_classification/3,"Therefore , our main contribution is to propose the Squeezed Very Deep Convolutional Neural Networks ( SVDCNN ) , a text classification model which requires significantly fewer parameters compared to the stateof - the - art CNNs .",32,0,38
dataset/preprocessed/test-data/document_classification/3,Section II provides an overview of the state - of - the - art in CNNs for text classification .,33,0,20
dataset/preprocessed/test-data/document_classification/3,Section III presents the VDCNN model .,34,0,7
dataset/preprocessed/test-data/document_classification/3,Section IV explains the proposed model SVDCNN and the subsequent impact in the total number of parameters of the network .,35,0,21
dataset/preprocessed/test-data/document_classification/3,V details how we perform experiments .,37,0,7
dataset/preprocessed/test-data/document_classification/3,"Section VI analyses the results and lastly , Section VII , presents conclusions and direction for future works .",38,0,19
dataset/preprocessed/test-data/document_classification/3,CNNs were originally designed for Computer Vision with the aim of considering feature extraction and classification as one task .,41,0,20
dataset/preprocessed/test-data/document_classification/3,"Although CNNs are very successful in image classification tasks , its use in text classification is relatively new and has some peculiarities .",42,0,23
dataset/preprocessed/test-data/document_classification/3,"Contrasting with traditional image bi-dimensional representations , texts are onedimensionally represented .",43,0,12
dataset/preprocessed/test-data/document_classification/3,"Due to this property , the convolutions are designed as temporal convolutions .",44,0,13
dataset/preprocessed/test-data/document_classification/3,"Furthermore , it is necessary to generate a numerical representation from the text so the network can be trained using this representation .",45,0,23
dataset/preprocessed/test-data/document_classification/3,"This representation , namely embeddings , is usually obtained through the application of a lookup table , generated from a given dictionary .",46,0,23
dataset/preprocessed/test-data/document_classification/3,An early approach for text classification tasks consisted of a shallow neural network working on the word level and using only one convolutional layer .,47,0,25
dataset/preprocessed/test-data/document_classification/3,The author reported results in smaller datasets .,48,0,8
dataset/preprocessed/test-data/document_classification/16,A C - LSTM Neural Network for Text Classification,2,1,9
dataset/preprocessed/test-data/document_classification/16,Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling .,4,0,19
dataset/preprocessed/test-data/document_classification/16,"Convolutional neural network ( CNN ) and recurrent neural network ( RNN ) are two mainstream architectures for such modeling tasks , which adopt totally different ways of understanding natural languages .",5,0,32
dataset/preprocessed/test-data/document_classification/16,"In this work , we combine the strengths of both architectures and propose a novel and unified model called C - LSTM for sentence representation and text classification .",6,0,29
dataset/preprocessed/test-data/document_classification/16,"C - LSTM utilizes CNN to extract a sequence of higher - level phrase representations , and are fed into along short - term memory recurrent neural network ( LSTM ) to obtain the sentence representation .",7,0,37
dataset/preprocessed/test-data/document_classification/16,C - LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics .,8,0,21
dataset/preprocessed/test-data/document_classification/16,We evaluate the proposed architecture on sentiment classification and question classification tasks .,9,0,13
dataset/preprocessed/test-data/document_classification/16,The experimental results show that the C - LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks .,10,0,23
dataset/preprocessed/test-data/document_classification/16,"As one of the core steps in NLP , sentence modeling aims at representing sentences as meaningful features for tasks such as sentiment classification .",12,0,25
dataset/preprocessed/test-data/document_classification/16,"Traditional sentence modeling uses the bag - ofwords model which often suffers from the curse of dimensionality ; others use composition based methods instead , e.g. , an algebraic operation over semantic word vectors to produce the semantic sentence vector .",13,0,41
dataset/preprocessed/test-data/document_classification/16,"However , such methods may not perform well due to the loss of word order information .",14,0,17
dataset/preprocessed/test-data/document_classification/16,More recent models for distributed sentence representation fall into two categories according to the form of input sentence : sequence - based models and tree - structured models .,15,0,29
dataset/preprocessed/test-data/document_classification/16,Sequence - based models construct sentence representations from word sequences by taking in account the relationship between successive words .,16,0,20
dataset/preprocessed/test-data/document_classification/16,Tree- structured models treat each word token as anode in a syntactic parse tree and learn sentence representations from leaves to the root in a recursive manner .,17,0,28
dataset/preprocessed/test-data/document_classification/16,Convolutional neural networks ( CNNs ) and recurrent neural networks ( RNNs ) have emerged as two widely used architectures and are often combined with sequence - based or tree - structured models .,18,0,34
dataset/preprocessed/test-data/document_classification/16,"Owing to the capability of capturing local correlations of spatial or temporal structures , CNNs have achieved top performance in computer vision , speech recognition and NLP .",19,0,28
dataset/preprocessed/test-data/document_classification/16,"For sentence modeling , CNNs perform excellently in extracting n-gram features at different positions of a sentence through convolutional filters , and can learn short and long - range relations through pooling operations .",20,0,34
dataset/preprocessed/test-data/document_classification/16,CNNs have been successfully combined with both sequence - based model and tree - structured model in sentence modeling .,21,0,20
dataset/preprocessed/test-data/document_classification/16,The other popular neural network architecture - RNN - is able to handle sequences of any length and capture long - term dependencies .,22,0,24
dataset/preprocessed/test-data/document_classification/16,"To avoid the problem of gradient exploding or vanishing in the standard RNN , Long Short - term Memory RNN ( LSTM ) and other variants were designed for better remembering and memory accesses .",23,0,35
dataset/preprocessed/test-data/document_classification/16,"Along with the sequence - based or the tree - structured models , RNNs have achieved remarkable results in sentence or document modeling .",24,0,24
dataset/preprocessed/test-data/document_classification/16,"To conclude , CNN is able to learn local response from temporal or spatial data but lacks the ability of learning sequential correlations ; on the other hand , RNN is specilized for sequential modelling but unable to extract features in a parallel way .",25,0,45
dataset/preprocessed/test-data/document_classification/16,"It has been shown that higher - level modeling of x t can help to disentangle underlying factors of variation within the input , which should then make it easier to learn temporal structure between successive time steps .",26,0,39
dataset/preprocessed/test-data/document_classification/16,"For example , Sainath et al. have obtained respectable improvements in WER by learning a deep LSTM from multi-scale inputs .",27,0,21
dataset/preprocessed/test-data/document_classification/16,We explore training the LSTM model directly from sequences of higherlevel representaions while preserving the sequence order of these representaions .,28,0,21
dataset/preprocessed/test-data/document_classification/16,"In this paper , we introduce a new architecture short for C - LSTM by combining CNN and LSTM to model sentences .",29,0,23
dataset/preprocessed/test-data/document_classification/16,"To benefit from the advantages of both CNN and RNN , we design a simple end - to - end , unified architecture by feeding the output of a one - layer CNN into LSTM .",30,0,36
dataset/preprocessed/test-data/document_classification/16,The CNN is constructed on top of the pre-trained word vectors from massive unlabeled text data to learn higher - level representions of n-grams .,31,0,25
dataset/preprocessed/test-data/document_classification/16,"Then to learn sequential correlations from higher - level suqence representations , the feature maps of CNN are organized as sequential window features to serve as the input of LSTM .",32,0,31
dataset/preprocessed/test-data/document_classification/16,"In this way , instead of constructing LSTM directly from the input sentence , we first transform each sentence into successive window ( n- gram ) features to help disentangle factors of variations within sentences .",33,0,36
dataset/preprocessed/test-data/document_classification/16,"We choose sequence - based input other than relying on the syntactic parse trees before feeding in the neural network , thus our model does n't rely on any external language knowledge and complicated pre-processing .",34,0,36
dataset/preprocessed/test-data/document_classification/16,"In our experiments , we evaluate the semantic sentence representations learned from C - LSTM with two tasks : sentiment classification and 6 - way question classification .",35,0,28
dataset/preprocessed/test-data/document_classification/16,Our evaluations show that the C - LSTM model can achieve excellent results with several benchmarks as compared with a wide range of baseline models .,36,0,26
dataset/preprocessed/test-data/document_classification/16,"We also show that the combination of CNN and LSTM outperforms individual multi - layer CNN models and RNN models , which indicates that LSTM can learn longterm dependencies from sequences of higher - level representations better than the other models .",37,0,42
dataset/preprocessed/test-data/document_classification/16,"Deep learning based neural network models have achieved great success in many NLP tasks , including learning distributed word , sentence and document representation , statistical machine translation , sentiment classification , etc .",39,0,34
dataset/preprocessed/test-data/document_classification/16,"Learning distributed sentence representation through neural network models requires little external domain knowledge and can reach satisfactory results in related tasks like sentiment classification , text categorization .",40,0,28
dataset/preprocessed/test-data/document_classification/16,"In many recent sentence representation learning works , neural network models are constructed upon either the input word sequences or the transformed syntactic parse tree .",41,0,26
dataset/preprocessed/test-data/document_classification/16,"Among them , convolutional neural network ( CNN ) and recurrent neural network ( RNN ) are two popular ones .",42,0,21
dataset/preprocessed/test-data/document_classification/16,The capability of capturing local correlations along with extracting higher - level correlations through pooling empowers CNN to model sentences naturally from consecutive context windows .,43,0,26
dataset/preprocessed/test-data/document_classification/16,"In , Collobert et al. applied convolutional filters to successive windows for a given sequence to extract global features by max - pooling .",44,0,24
dataset/preprocessed/test-data/document_classification/16,"As a slight variant , proposed a CNN architecture with multiple filters ( with a varying window size ) and two ' channels ' of word vectors .",45,0,28
dataset/preprocessed/test-data/document_classification/16,"To capture word relations of varying sizes , proposed a dynamic k-max pooling mechanism .",46,0,15
dataset/preprocessed/test-data/document_classification/16,"In a more recent work , Tao et al .",47,0,10
dataset/preprocessed/test-data/document_classification/16,apply tensor - based operations between words to replace linear operations on concatenated word vectors in the standard convolutional layer and explore the non-linear interactions between nonconsective n-grams .,48,0,29
dataset/preprocessed/test-data/document_classification/16,also explores convolutional models on tree - structured sentences .,49,0,10
dataset/preprocessed/test-data/document_classification/10,Neural Attentive Bag - of - Entities Model for Text Classification,2,1,11
dataset/preprocessed/test-data/document_classification/10,"This study proposes a Neural Attentive Bagof - Entities model , which is a neural network model that performs text classification using entities in a knowledge base .",4,0,28
dataset/preprocessed/test-data/document_classification/10,Entities provide unambiguous and relevant semantic signals that are beneficial for capturing semantics in texts .,5,0,16
dataset/preprocessed/test-data/document_classification/10,"We combine simple high - recall entity detection based on a dictionary , to detect entities in a document , with a novel neural attention mechanism that enables the model to focus on a small number of unambiguous and relevant entities .",6,0,42
dataset/preprocessed/test-data/document_classification/10,"We tested the effectiveness of our model using two standard text classification datasets ( i.e. , the 20 Newsgroups and R8 datasets ) and a popular factoid question answering dataset based on a trivia quiz game .",7,0,37
dataset/preprocessed/test-data/document_classification/10,"As a result , our model achieved state - of - the - art results on all datasets .",8,0,19
dataset/preprocessed/test-data/document_classification/10,The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.,9,0,12
dataset/preprocessed/test-data/document_classification/10,"Text classification is an important task , and its applications span a wide range of activities such as topic classification , spam detection , and sentiment classification .",11,0,28
dataset/preprocessed/test-data/document_classification/10,"Recent studies showed that models based on neural networks can outperform conventional models ( e.g. , nave Bayes ) on text classification tasks .",12,0,24
dataset/preprocessed/test-data/document_classification/10,Typical neural network - based text classification models are based on words .,13,0,13
dataset/preprocessed/test-data/document_classification/10,"They typically use words in the target documents as inputs , map words into continuous vectors ( embeddings ) , and capture the semantics in documents by using compositional functions over word embeddings such as averaging or summation of word embeddings , convolutional neural networks ( CNN ) , and recurrent neural networks ( RNN ) .",14,0,57
dataset/preprocessed/test-data/document_classification/10,"Apart from the aforementioned approaches , past studies attempted to use entities in a knowledge base ( KB ) ( e.g. , Wikipedia ) to capture the semantics in documents .",15,0,31
dataset/preprocessed/test-data/document_classification/10,These models typically represent a document by using a set of entities ( or bag of entities ) relevant to the document .,16,0,23
dataset/preprocessed/test-data/document_classification/10,"The main benefit of using entities instead of words is that unlike words , entities provide unambiguous semantic signals because they are uniquely identified in a KB .",17,0,28
dataset/preprocessed/test-data/document_classification/10,One key issue here is to determine the way in which to associate a document with its relevant entities .,18,0,20
dataset/preprocessed/test-data/document_classification/10,An existing straightforward approach involves creating a set of relevant entities using an entity linking system to detect and dis ambiguate the names of entities in a document .,19,0,29
dataset/preprocessed/test-data/document_classification/10,"However , this approach is problematic because ( 1 ) entity linking systems produce dis ambiguation errors , and ( 2 ) entities appearing in a document are not necessarily relevant to the given document .",20,0,36
dataset/preprocessed/test-data/document_classification/10,"This study proposes the Neural Attentive Bagof - Entities ( NABoE ) model , which is a neural network model that addresses the text classification problem by modeling the semantics in the target documents using entities in the KB .",21,0,40
dataset/preprocessed/test-data/document_classification/10,"For each entity name in a document ( e.g. , "" Apple "" ) , our model first detects entities that maybe referred to by this name ( e.g. , Apple Inc. , Apple ( food ) ) , and then represents the document using the weighted average of the embeddings of these entities .",22,0,55
dataset/preprocessed/test-data/document_classification/10,The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of the entities that are less ambiguous in meaning and more relevant to the document .,23,0,36
dataset/preprocessed/test-data/document_classification/10,"In other words , the attention mechanism is designed to compute weights by jointly addressing entity linking and entity salience detection tasks .",24,0,23
dataset/preprocessed/test-data/document_classification/10,"Furthermore , the attention mechanism improves the interpretability of the model because it enables us to inspect the small number of entities that strongly affect the classification decisions .",25,0,29
dataset/preprocessed/test-data/document_classification/10,"We validate the effectiveness of our proposed model by addressing two important natural language tasks : a text classification task using two standard datasets ( i.e. , the 20 Newsgroups and R8 datasets ) , and a factoid question answering task based on a popular dataset derived from the quiz bowl trivia quiz game .",26,0,55
dataset/preprocessed/test-data/document_classification/10,"As a result , our model achieved state - of - the - art results on both tasks .",27,0,19
dataset/preprocessed/test-data/document_classification/10,The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.,28,0,12
dataset/preprocessed/test-data/document_classification/10,"Given a document , our model addresses the text classification task by using the following two steps : it first detects entities from the document , and then classifies the document using the proposed model with the detected entities as inputs .",30,0,42
dataset/preprocessed/test-data/document_classification/10,"As the target KB , we used the September 2018 version of Wikipedia , which contains a total of 7,333,679 entities .",32,0,22
dataset/preprocessed/test-data/document_classification/10,"Regarding the entity dictionary described in Section 2.1 , we excluded an entity name if its link probability was lower than 1 % and a referent entity if its commonness given the entity name was lower than 3 % for computational efficiency .",34,0,43
dataset/preprocessed/test-data/document_classification/10,Entity names were treated as case-insensitive .,35,0,7
dataset/preprocessed/test-data/document_classification/10,"As a result , the dictionary contained 18,785,550 entity names , and each name had 1.14 referent entities on average .",36,0,21
dataset/preprocessed/test-data/document_classification/10,"Furthermore , to detect entities from a document , we also tested two publicly available entity linking systems , Wikifier and TAGME , instead of using dictionarybased entity detection .",37,0,30
dataset/preprocessed/test-data/document_classification/10,"We selected these systems because they are capable of detecting non-named entities ( e.g. , technical terms ) that are useful for addressing the text classification task .",38,0,28
dataset/preprocessed/test-data/document_classification/10,"Here , we used the entities detected and dis ambiguated by these systems as inputs to our neural network model .",40,0,21
dataset/preprocessed/test-data/document_classification/10,where v w ?,42,0,4
dataset/preprocessed/test-data/document_classification/10,Rd is the embedding of word w .,43,0,8
dataset/preprocessed/test-data/document_classification/10,We then derive the entity - based representation of D as a weighted average of the embeddings of the entities :,44,0,21
dataset/preprocessed/test-data/document_classification/10,where v e ?,45,0,4
dataset/preprocessed/test-data/document_classification/10,Rd is the embedding of entity e and a e the normalized attention weight corresponding toe computed using the following softmax - based attention function :,46,0,26
dataset/preprocessed/test-data/document_classification/10,where w a ?,47,0,4
dataset/preprocessed/test-data/document_classification/10,"R l is a weight vector , b a ?",48,0,10
dataset/preprocessed/test-data/document_classification/10,"R is the bias , and ?( e , D ) is a function that generates an l-dimensional vector consisting of the features of the attention function .",49,0,28
dataset/preprocessed/test-data/document_classification/9,Investigating Capsule Networks with Dynamic Routing for Text Classification,2,1,9
dataset/preprocessed/test-data/document_classification/9,"In this study , we explore capsule networks with dynamic routing for text classification .",4,0,15
dataset/preprocessed/test-data/document_classification/9,"We propose three strategies to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information or have not been successfully trained .",5,0,32
dataset/preprocessed/test-data/document_classification/9,A series of experiments are conducted with capsule networks on six text classification benchmarks .,6,0,15
dataset/preprocessed/test-data/document_classification/9,"Capsule networks achieve competitive results over the compared baseline methods on 4 out of 6 datasets , which shows the effectiveness of capsule networks for text classification .",7,0,28
dataset/preprocessed/test-data/document_classification/9,We additionally show that capsule networks exhibit significant improvement when transfer single - label to multi-label text classification over the competitors .,8,0,22
dataset/preprocessed/test-data/document_classification/9,"To the best of our knowledge , this is the first work that capsule networks have been empirically investigated for text modeling 1 . * Corresponding author ( min.yang@siat.ac.cn )",9,0,30
dataset/preprocessed/test-data/document_classification/9,1 Codes are publicly available at : https : //github.com/andyweizhao/capsule_text_ classification .,10,0,12
dataset/preprocessed/test-data/document_classification/9,Modeling articles or sentences computationally is a fundamental topic in natural language processing .,12,0,14
dataset/preprocessed/test-data/document_classification/9,"It could be as simple as a keyword / phrase matching problem , but it could also be a nontrivial problem if compositions , hierarchies , and structures of texts are considered .",13,0,33
dataset/preprocessed/test-data/document_classification/9,"For example , a news article which mentions a single phrase "" US election "" maybe categorized into the political news with high probability .",14,0,25
dataset/preprocessed/test-data/document_classification/9,"But it could be very difficult for a computer to predict which presidential candidate is favored by its author , or whether the author 's view in the article is more liberal or more conservative .",15,0,36
dataset/preprocessed/test-data/document_classification/9,"Earlier efforts in modeling texts have achieved limited success on text categorization using a simple bag - of - words classifier , implying understanding the meaning of the individual word or n-gram is a necessary step towards more sophisticated models .",16,0,41
dataset/preprocessed/test-data/document_classification/9,"It is therefore not a surprise that distributed representations of words , a.k.a. word embeddings , have received great attention from NLP community addressing the question "" what "" to be modeled at the basic level .",17,0,37
dataset/preprocessed/test-data/document_classification/9,"In order to model higher level concepts and facts in texts , an NLP researcher has to think cautiously the so - called "" what "" question : what is actually modeled beyond word meanings .",18,0,36
dataset/preprocessed/test-data/document_classification/9,"A common approach to the question is to treat the texts as sequences and focus on their spatial patterns , whose representatives include convolutional neural networks ( CNNs ) and long shortterm memory networks ( LSTMs ) .",19,0,38
dataset/preprocessed/test-data/document_classification/9,"Another common approach is to completely ignore the order of words but focus on their compositions as a collection , whose representatives include probabilistic topic modeling and Earth Mover 's Distance based modeling .",20,0,34
dataset/preprocessed/test-data/document_classification/9,"Those two approaches , albeit quite different from the computational perspective , actually follow a common measure to be diagnosed regarding their answers to the "" what "" question .",21,0,30
dataset/preprocessed/test-data/document_classification/9,"In neural network approaches , spatial patterns aggregated at lower levels contribute to representing higher level concepts .",22,0,18
dataset/preprocessed/test-data/document_classification/9,"Here , they form a recursive process to articulate what to be modeled .",23,0,14
dataset/preprocessed/test-data/document_classification/9,"For example , CNN builds convolutional feature detectors to extract local patterns from a window of vector sequences and uses max - pooling to select the most prominent ones .",24,0,30
dataset/preprocessed/test-data/document_classification/9,It then hierarchically builds such pattern extraction pipelines at multiple levels .,25,0,12
dataset/preprocessed/test-data/document_classification/9,"Being a spatially sensitive model , CNN pays a price for the inefficiency of replicating feature detectors on a grid .",26,0,21
dataset/preprocessed/test-data/document_classification/9,"As argued in , one has to choose between replicating detectors whose size grows exponentially with the number of dimensions , or increasing the volume of the labeled training set in a similar exponential way .",27,0,36
dataset/preprocessed/test-data/document_classification/9,"On the other hand , methods thatare spatially insensitive are perfectly efficient at the inference time regardless of any order of words or local patterns .",28,0,26
dataset/preprocessed/test-data/document_classification/9,"However , they are unavoidably more restricted to encode rich structures presented in a sequence .",29,0,16
dataset/preprocessed/test-data/document_classification/9,Improving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue .,30,0,21
dataset/preprocessed/test-data/document_classification/9,A recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue .,31,0,18
dataset/preprocessed/test-data/document_classification/9,They introduce an iterative routing process to decide the credit attribution between nodes from lower and higher layers .,32,0,19
dataset/preprocessed/test-data/document_classification/9,A metaphor ( also as an argument ) they made is that human visual system intelligently assigns parts to wholes at the inference time without hard - coding patterns to be perspective relevant .,33,0,34
dataset/preprocessed/test-data/document_classification/9,"As an outcome , their model could encode the intrinsic spatial relationship between apart and a whole constituting viewpoint invariant knowledge that automatically generalizes to novel viewpoints .",34,0,28
dataset/preprocessed/test-data/document_classification/9,"In our work , we follow a similar spirit to use this technique in modeling texts .",35,0,17
dataset/preprocessed/test-data/document_classification/9,"Three strategies are proposed to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words thatare unrelated to specific categories .",36,0,38
dataset/preprocessed/test-data/document_classification/9,We conduct a series of experiments with capsule networks on top of the pre-trained word vectors for six text classification benchmarks .,37,0,22
dataset/preprocessed/test-data/document_classification/9,"More importantly , we show that capsule networks achieves significant improvement when transferring singlelabel to multi-label text classifications over strong baseline methods .",38,0,23
dataset/preprocessed/test-data/document_classification/9,"Our capsule network , depicted in , is a variant of the capsule networks proposed in .",40,0,17
dataset/preprocessed/test-data/document_classification/9,"It consists of four layers : ngram convolutional layer , primary capsule layer , convolutional capsule layer , and fully connected capsule layer .",41,0,24
dataset/preprocessed/test-data/document_classification/9,"In addition , we explore two capsule frameworks to integrate these four components in different ways .",42,0,17
dataset/preprocessed/test-data/document_classification/9,"In the rest of this section , we elaborate the key components in detail .",43,0,15
dataset/preprocessed/test-data/document_classification/9,N - gram Convolutional Layer,44,0,5
dataset/preprocessed/test-data/document_classification/9,This layer is a standard convolutional layer which extracts n-gram features at different positions of a sentence through various convolutional filters .,45,0,22
dataset/preprocessed/test-data/document_classification/9,Suppose x ?,46,0,3
dataset/preprocessed/test-data/document_classification/9,R LV denotes the input sentence representation where L is the length of the sentence and V is the embedding size of words .,47,0,24
dataset/preprocessed/test-data/document_classification/9,Let xi ?,48,0,3
dataset/preprocessed/test-data/document_classification/9,RV be the V - dimensional word vector corresponding to the i - th word in the sentence .,49,0,19
dataset/preprocessed/test-data/document_classification/19,Text Classification Improved by Integrating Bidirectional LSTM with Two - dimensional Max Pooling,2,1,13
dataset/preprocessed/test-data/document_classification/19,Recurrent Neural Network ( RNN ) is one of the most popular architectures used in Natural Language Processsing ( NLP ) tasks because its recurrent structure is very suitable to process variablelength text .,4,0,34
dataset/preprocessed/test-data/document_classification/19,"RNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors , which form a matrix .",5,0,23
dataset/preprocessed/test-data/document_classification/19,And this matrix includes two dimensions : the time - step dimension and the feature vector dimension .,6,0,18
dataset/preprocessed/test-data/document_classification/19,Then most existing models usually utilize one - dimensional ( 1D ) max pooling operation or attention - based operation only on the time - step dimension to obtain a fixed - length vector .,7,0,35
dataset/preprocessed/test-data/document_classification/19,"However , the features on the feature vector dimension are not mutually independent , and simply applying 1 D pooling operation over the time - step dimension independently may destroy the structure of the feature representation .",8,0,37
dataset/preprocessed/test-data/document_classification/19,"On the other hand , applying two - dimensional ( 2D ) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks .",9,0,28
dataset/preprocessed/test-data/document_classification/19,"To integrate the features on both dimensions of the matrix , this paper explores applying 2 D max pooling operation to obtain a fixed - length representation of the text .",10,0,31
dataset/preprocessed/test-data/document_classification/19,This paper also utilizes 2D convolution to sample more meaningful information of the matrix .,11,0,15
dataset/preprocessed/test-data/document_classification/19,"Experiments are conducted on six text classification tasks , including sentiment analysis , question classification , subjectivity classification and newsgroup classification .",12,0,22
dataset/preprocessed/test-data/document_classification/19,"Compared with the state - of - the - art models , the proposed models achieve excellent performance on 4 out of 6 tasks .",13,0,25
dataset/preprocessed/test-data/document_classification/19,"Specifically , one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine - grained classification tasks .",14,0,23
dataset/preprocessed/test-data/document_classification/19,"Text classification is an essential component in many NLP applications , such as sentiment analysis , relation extraction and spam detection .",16,0,22
dataset/preprocessed/test-data/document_classification/19,"Therefore , it has attracted considerable attention from many researchers , and various types of models have been proposed .",17,0,20
dataset/preprocessed/test-data/document_classification/19,"As a traditional method , the bag - of - words ( BoW ) model treats texts as unordered sets of words .",18,0,23
dataset/preprocessed/test-data/document_classification/19,"In this way , however , it fails to encode word order and syntactic feature .",19,0,16
dataset/preprocessed/test-data/document_classification/19,"Recently , order- sensitive models based on neural networks have achieved tremendous success for text classification , and shown more significant progress compared with BoW models .",20,0,27
dataset/preprocessed/test-data/document_classification/19,"The challenge for textual modeling is how to capture features for different text units , such as phrases , sentences and documents .",21,0,23
dataset/preprocessed/test-data/document_classification/19,"Benefiting from its recurrent structure , RNN , as an alternative type of neural networks , is very suitable to process the variable - length text .",22,0,27
dataset/preprocessed/test-data/document_classification/19,"RNN can capitalize on distributed representations of words by first converting the tokens comprising each text into vectors , which form a matrix .",23,0,24
dataset/preprocessed/test-data/document_classification/19,"This matrix includes two dimensions : the time - step dimension and the feature vector dimension , and it will be updated in the process of learning feature representation .",24,0,30
dataset/preprocessed/test-data/document_classification/19,"Then RNN utilizes 1 D max pooling operation or attention - based operation , which extracts maximum values or generates a weighted representation over the time - step dimension of the matrix , to obtain a fixed - length vector .",25,0,41
dataset/preprocessed/test-data/document_classification/19,"Both of the two operators ignore features on the feature vector dimension , which maybe important for sentence representation , therefore the use of 1 D max pooling and attention - based operators may pose a serious limitation .",26,0,39
dataset/preprocessed/test-data/document_classification/19,"Convolutional Neural Networks ( CNN ) utilizes 1 D convolution to perform the feature mapping , and then applies 1 D max pooling operation over the time - step dimension to obtain a fixed - length output .",27,0,38
dataset/preprocessed/test-data/document_classification/19,"However the elements in the matrix learned by RNN are not independent , as RNN reads a sentence word byword , one can effectively treat the matrix as an ' image ' .",28,0,33
dataset/preprocessed/test-data/document_classification/19,"Unlike in NLP , CNN in image processing tasks applies 2D convolution and 2D pooling operation to get a representation of the input .",29,0,24
dataset/preprocessed/test-data/document_classification/19,It is a good choice to utilize 2D convolution and 2D pooling to sample more meaningful features on both the time - step dimension and the feature vector dimension for text classification .,30,0,33
dataset/preprocessed/test-data/document_classification/19,"Above all , this paper proposes Bidirectional Long Short - Term Memory Networks with Two - Dimensional Max Pooling ( BLSTM - 2DPooling ) to capture features on both the time - step dimension and the feature vector dimension .",31,0,40
dataset/preprocessed/test-data/document_classification/19,It first utilizes Bidirectional Long Short - Term Memory Networks ( BLSTM ) to transform the text into vectors .,32,0,20
dataset/preprocessed/test-data/document_classification/19,And then 2D max pooling operation is utilized to obtain a fixed - length vector .,33,0,16
dataset/preprocessed/test-data/document_classification/19,This paper also applies 2D convolution ( BLSTM - 2DCNN ) to capture more meaningful features to represent the input text .,34,0,22
dataset/preprocessed/test-data/document_classification/19,The contributions of this paper can be summarized as follows :,35,0,11
dataset/preprocessed/test-data/document_classification/19,"This paper proposes a combined framework , which utilizes BLSTM to capture long - term sentence dependencies , and extracts features by 2D convolution and 2D max pooling operation for sequence modeling tasks .",36,0,34
dataset/preprocessed/test-data/document_classification/19,"To the best of our knowledge , this work is the first example of using 2D convolution and 2D max pooling operation in NLP tasks .",37,0,26
dataset/preprocessed/test-data/document_classification/19,"This work introduces two combined models BLSTM - 2DPooling and BLSTM - 2DCNN , and verifies them on six text classification tasks , including sentiment analysis , question classification , subjectivity classification , and newsgroups classification .",38,0,37
dataset/preprocessed/test-data/document_classification/19,"Compared with the state - of - the - art models , BLSTM - 2DCNN achieves excellent performance on 4 out of 6 tasks .",39,0,25
dataset/preprocessed/test-data/document_classification/19,"Specifically , it achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine - grained classification tasks .",40,0,19
dataset/preprocessed/test-data/document_classification/19,"To better understand the effect of 2D convolution and 2D max pooling operation , this paper conducts experiments on Stanford Sentiment Treebank fine - grained task .",41,0,27
dataset/preprocessed/test-data/document_classification/19,"It first depicts the performance of the proposed models on different length of sentences , and then conducts a sensitivity analysis of 2D filter and max pooling size .",42,0,29
dataset/preprocessed/test-data/document_classification/19,The remainder of the paper is organized as follows .,43,0,10
dataset/preprocessed/test-data/document_classification/19,"In Section 2 , the related work about text classification is reviewed .",44,0,13
dataset/preprocessed/test-data/document_classification/19,Section 3 presents the BLSTM - 2DCNN architectures for text classification in detail .,45,0,14
dataset/preprocessed/test-data/document_classification/19,Section 4 describes details about the setup of the experiments .,46,0,11
dataset/preprocessed/test-data/document_classification/19,Section 5 presents the experimental results .,47,0,7
dataset/preprocessed/test-data/document_classification/19,The conclusion is drawn in the section 6 .,48,0,9
dataset/preprocessed/test-data/document_classification/1,BRIDGING THE DOMAIN GAP IN CROSS - LINGUAL DOCUMENT CLASSIFICATION,2,1,10
dataset/preprocessed/test-data/document_classification/1,The scarcity of labeled training data often prohibits the internationalization of NLP models to multiple languages .,4,0,17
dataset/preprocessed/test-data/document_classification/1,"Recent developments in cross - lingual understanding ( XLU ) has made progress in this area , trying to bridge the language barrier using language universal representations .",5,1,28
dataset/preprocessed/test-data/document_classification/1,"However , even if the language problem was resolved , models trained in one language would not transfer to another language perfectly due to the natural domain drift across languages and cultures .",6,0,33
dataset/preprocessed/test-data/document_classification/1,"We consider the setting of semi-supervised cross - lingual understanding , where labeled data is available in a source language ( English ) , but only unlabeled data is available in the target language .",7,0,35
dataset/preprocessed/test-data/document_classification/1,We combine state - of - the - art cross - lingual methods with recently proposed methods for weakly supervised learning such as unsupervised pre-training and unsupervised data augmentation to simultaneously close both the language gap and the domain gap in XLU .,8,1,43
dataset/preprocessed/test-data/document_classification/1,We show that addressing the domain gap is crucial .,9,0,10
dataset/preprocessed/test-data/document_classification/1,We improve over strong baselines and achieve a new state - of - the - art for cross - lingual document classification .,10,0,23
dataset/preprocessed/test-data/document_classification/1,Work completed at Facebook AI 1 ar Xiv :1909.07009v2 [ cs.CL ],12,0,12
dataset/preprocessed/test-data/document_classification/1,Recent advances in Natural Language Processing have enabled us to train high - accuracy systems for many language tasks .,14,0,20
dataset/preprocessed/test-data/document_classification/1,"However , training an accurate system still requires a large amount of training data .",15,0,15
dataset/preprocessed/test-data/document_classification/1,It is inefficient to collect data for a new task and it is virtually impossible to annotate a separate data set for each language .,16,0,25
dataset/preprocessed/test-data/document_classification/1,"To go beyond English and a few popular languages , we need methods that can learn from data in one language and apply it to others .",17,0,27
dataset/preprocessed/test-data/document_classification/1,Cross - Lingual,18,0,3
dataset/preprocessed/test-data/document_classification/1,Understanding ( XLU ) has emerged as afield concerned with learning models on data in one language and applying it to others .,19,0,23
dataset/preprocessed/test-data/document_classification/1,"Much of the work in XLU focuses on the zero - shot setting , which assumes that labeled data is available in one source language ( usually English ) and not in any of the target languages in which the model is evaluated .",20,0,44
dataset/preprocessed/test-data/document_classification/1,The labeled data can be used to train a high quality model in the source language .,21,0,17
dataset/preprocessed/test-data/document_classification/1,One then relies on general domain parallel corpora and monolingual corpora to learn to ' transfer ' from the source language to the target language .,22,0,26
dataset/preprocessed/test-data/document_classification/1,Transfer methods can explicitly rely on machine translation models built from such parallel corpora .,23,0,15
dataset/preprocessed/test-data/document_classification/1,"Alternatively , one can use such corpora to learn language universal representations to produce features to train a model in one language , which one can directly apply to other languages .",24,0,32
dataset/preprocessed/test-data/document_classification/1,"Such representations can be in the form of cross - lingual word embeddings , contextual word embeddings , or sentence embeddings ; ; ) .",25,0,25
dataset/preprocessed/test-data/document_classification/1,"Using such techniques , recent work has demonstrated reasonable zero - shot performance for crosslingual document classification ) and natural language inference ) .",26,0,24
dataset/preprocessed/test-data/document_classification/1,"What we have so far described is a simplified view of XLU , which focuses solely on the problem of aligning languages .",27,0,23
dataset/preprocessed/test-data/document_classification/1,"This view assumes that , if we had access to a perfect translation system , and translated our source training data into the target language , the resulting model would perform as well as if we had collected a similarly sized labeled dataset directly in our target language .",28,0,49
dataset/preprocessed/test-data/document_classification/1,Existing work in XLU to date also works under this assumption .,29,0,12
dataset/preprocessed/test-data/document_classification/1,"However , in real world applications , we must also bridge the domain gap across different languages , as well as the language gap .",30,0,25
dataset/preprocessed/test-data/document_classification/1,"No task is ever identical in two languages , even if we group them under the same label , e.g. ' news document classification ' or ' product reviews ' .",31,0,31
dataset/preprocessed/test-data/document_classification/1,A Chinese customer might express sentiment differently than his American counterpart .,32,0,12
dataset/preprocessed/test-data/document_classification/1,Or French news might simply cover different topics than English news .,33,0,12
dataset/preprocessed/test-data/document_classification/1,"As a result , any approach which ignores this domain drift will fall short of native in - language performance in real world XLU .",34,0,25
dataset/preprocessed/test-data/document_classification/1,"In this paper , we propose to jointly tackle both language and domain transfer .",35,0,15
dataset/preprocessed/test-data/document_classification/1,"We consider the semi-supervised XLU setting , wherein addition to labeled data in a source language , we have access to unlabeled data in the target language .",36,0,28
dataset/preprocessed/test-data/document_classification/1,"Using this unlabeled data , we combine the aforementioned cross - lingual methods with recently proposed unsupervised domain adaptation and weak supervision techniques on the task of cross - lingual document classification .",37,0,33
dataset/preprocessed/test-data/document_classification/1,"In particular , we focus on two approaches for domain adaptation .",38,0,12
dataset/preprocessed/test-data/document_classification/1,The first method is based on masked language model ( MLM ) pre-training ( as in ) using unlabeled target language corpora .,39,0,23
dataset/preprocessed/test-data/document_classification/1,Such methods have been shown to improve over general purpose pre-trained models such as BERT in the weakly supervised setting ; ) .,40,0,23
dataset/preprocessed/test-data/document_classification/1,"The second method is unsupervised data augmentation ( UDA ) ) , where synthetic paraphrases are generated from the unlabeled corpus , and the model is trained on a label consistency loss .",41,0,33
dataset/preprocessed/test-data/document_classification/1,"While both of these techniques were proposed previously , in both cases it is non-trivial to extend them to the cross - lingual setting .",42,0,25
dataset/preprocessed/test-data/document_classification/1,"For instance when performing data augmentation , one could generate paraphrases in either the source or the target language or both .",43,0,22
dataset/preprocessed/test-data/document_classification/1,We experiment with various approaches and provide guidelines with ablation studies .,44,0,12
dataset/preprocessed/test-data/document_classification/1,"Furthermore , we find that the value of additional labeled data in the source language is limited due to the train - test discrepancy of XLU tasks .",45,0,28
dataset/preprocessed/test-data/document_classification/1,We propose to alleviate this issue by using self - training technique to do the domain adaptation from the source language into the target language .,46,0,26
dataset/preprocessed/test-data/document_classification/1,"By combining these methods , we are able to reduce error rates by an average 44 % over a strong XLM baseline , setting a new state - of - the - art for cross - lingual document classification .",47,0,40
dataset/preprocessed/test-data/document_classification/1,CROSS - LINGUAL UNDERSTANDING,49,0,4
dataset/preprocessed/test-data/document_classification/5,HDLTex : Hierarchical Deep Learning for Text Classification,2,1,8
dataset/preprocessed/test-data/document_classification/5,"Increasingly large document collections require improved information processing methods for searching , retrieving , and organizing text .",4,0,18
dataset/preprocessed/test-data/document_classification/5,"Central to these information processing methods is document classification , which has become an important application for supervised learning .",5,1,20
dataset/preprocessed/test-data/document_classification/5,Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased .,6,0,17
dataset/preprocessed/test-data/document_classification/5,This is because along with growth in the number of documents has come an increase in the number of categories .,7,0,21
dataset/preprocessed/test-data/document_classification/5,This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification .,8,0,19
dataset/preprocessed/test-data/document_classification/5,Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,9,1,20
dataset/preprocessed/test-data/document_classification/5,HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy .,10,0,19
dataset/preprocessed/test-data/document_classification/5,Each year scientific researchers produce a massive number of documents .,12,0,11
dataset/preprocessed/test-data/document_classification/5,"In 2014 the 28,100 active , scholarly , peerreviewed , English - language journals published about 2.5 million articles , and there is evidence that the rate of growth in both new journals and publications is accelerating .",13,0,38
dataset/preprocessed/test-data/document_classification/5,The volume of these documents has made automatic organization and classification an essential element for the advancement of basic and applied research .,14,0,23
dataset/preprocessed/test-data/document_classification/5,"Much of the recent work on automatic document classification has involved supervised learning techniques such as classification trees , nave Bayes , support vector machines ( SVM ) , neural nets , and ensemble methods .",15,0,36
dataset/preprocessed/test-data/document_classification/5,Classification trees and nave Bayes approaches provide good interpretability but tend to be less accurate than the other methods .,16,0,20
dataset/preprocessed/test-data/document_classification/5,"However , automatic classification has become increasingly challenging over the last several years due to growth in corpus sizes and the number of fields and sub-fields .",17,0,27
dataset/preprocessed/test-data/document_classification/5,Areas of research that were little known only five years ago have now become areas of high growth and interest .,18,0,21
dataset/preprocessed/test-data/document_classification/5,"This growth in sub-fields has occurred across a range of disciplines including biology ( e.g. , CRISPR - CA9 ) , material science ( e.g. , chemical programming ) , and health sciences ( e.g. , precision medicine ) .",19,0,40
dataset/preprocessed/test-data/document_classification/5,This growth in sub-fields means that it is important to not just label a document by specialized area but to also organize it within its over all field and the accompanying sub-field .,20,0,33
dataset/preprocessed/test-data/document_classification/5,This is hierarchical classification .,21,0,5
dataset/preprocessed/test-data/document_classification/5,"Although many existing approaches to document classification can quickly identify the over all area of a document , few of them can rapidly organize documents into the correct subfields or areas of specialization .",22,0,34
dataset/preprocessed/test-data/document_classification/5,"Further , the combination of top - level fields and all sub-fields presents current document classification approaches with a combinatorially increasing number of class labels that they can not handle .",23,0,31
dataset/preprocessed/test-data/document_classification/5,This paper presents a new approach to hierarchical document classification that we call Hierarchical Deep Learning for Text classification ( HDLTex ) .,24,0,23
dataset/preprocessed/test-data/document_classification/5,HDLTex combines deep learning architectures to allow both over all and specialized learning by level of the document hierarchy .,25,0,20
dataset/preprocessed/test-data/document_classification/5,"This paper reports our experiments with HDLTex , which exhibits improved accuracy over traditional document classification methods .",26,0,18
dataset/preprocessed/test-data/document_classification/5,"Document classification is necessary to organize documents for retrieval , analysis , curation , and annotation .",29,0,17
dataset/preprocessed/test-data/document_classification/5,Researchers have studied and developed a variety of methods for document classification .,30,0,13
dataset/preprocessed/test-data/document_classification/5,Work in the information retrieval community has focused on search engine fundamentals such as indexing and dictionaries thatare considered core technologies in this field .,31,0,25
dataset/preprocessed/test-data/document_classification/5,"Considerable work has built on these foundational methods to provide improvements through feedback and query reformulation , .",32,0,18
dataset/preprocessed/test-data/document_classification/5,More recent work has employed methods from data mining and machine learning .,33,0,13
dataset/preprocessed/test-data/document_classification/5,Among the most accurate of these techniques is the support vector machine ( SVM ) - .,34,0,17
dataset/preprocessed/test-data/document_classification/5,SVMs use kernel functions to find separating hyperplanes in high - dimensional spaces .,35,0,14
dataset/preprocessed/test-data/document_classification/5,"Other kernel methods used for information retrieval include string kernels such as the spectrum kernel and the mismatch kernel , which are widely used with DNA and RNA sequence data .",36,0,31
dataset/preprocessed/test-data/document_classification/5,SVM and related methods are difficult to interpret .,37,0,9
dataset/preprocessed/test-data/document_classification/5,"For this reason many information retrieval systems use decision trees and nave Bayes , methods .",38,0,16
dataset/preprocessed/test-data/document_classification/5,"These methods are easier to understand and , as such , can support query reformulation , but they lack accuracy .",39,0,21
dataset/preprocessed/test-data/document_classification/5,Some recent work has investigated topic modeling to provide similar interpretations as nave Bayes methods but with improved accuracy .,40,0,20
dataset/preprocessed/test-data/document_classification/5,This paper uses newer methods of machine learning for document classification taken from deep learning .,41,0,16
dataset/preprocessed/test-data/document_classification/5,"Deep learning is an efficient version of neural networks that can perform unsupervised , supervised , and semi-supervised learning .",42,0,20
dataset/preprocessed/test-data/document_classification/5,"Deep learning has been extensively used for image processing , but many recent studies have applied deep learning in other domains such as text and data mining .",43,0,28
dataset/preprocessed/test-data/document_classification/5,The basic architecture in a neural network is a fully connected network of nonlinear processing nodes organized as layers .,44,0,20
dataset/preprocessed/test-data/document_classification/5,"The first layer is the input layer , the final layer is the output layer , and all other layers are hidden .",45,0,23
dataset/preprocessed/test-data/document_classification/5,"In this paper , we will refer to these fully connected networks as Deep Neural Networks ( DNN ) .",46,0,20
dataset/preprocessed/test-data/document_classification/5,Convolutional Neural Networks ( CNNs ) are modeled after the architecture of the visual cortex where neurons are not fully connected but are spatially distinct .,47,0,26
dataset/preprocessed/test-data/document_classification/5,CNNs provide excellent results in generalizing the classification of objects in images .,48,0,13
dataset/preprocessed/test-data/document_classification/5,More recent work has used CNNs for text mining .,49,0,10
dataset/preprocessed/test-data/document_classification/18,Character - level Convolutional Networks for Text Classification,2,1,8
dataset/preprocessed/test-data/document_classification/18,This article offers an empirical exploration on the use of character - level convolutional networks ( ConvNets ) for text classification .,5,0,22
dataset/preprocessed/test-data/document_classification/18,We constructed several largescale datasets to show that character - level convolutional networks could achieve state - of - the - art or competitive results .,6,0,26
dataset/preprocessed/test-data/document_classification/18,"Comparisons are offered against traditional models such as bag of words , n-grams and their TFIDF variants , and deep learning models such as word - based ConvNets and recurrent neural networks .",7,0,33
dataset/preprocessed/test-data/document_classification/18,There are also related works that use character - level features for language processing .,8,0,15
dataset/preprocessed/test-data/document_classification/18,"These include using character - level n-grams with linear classifiers [ 15 ] , and incorporating character - level features to ConvNets [ 28 ] [ 29 ] .",9,0,29
dataset/preprocessed/test-data/document_classification/18,"In particular , these ConvNet approaches use words as a basis , in which character - level features extracted at word [ 28 ] or word n-gram [ 29 ] level form a distributed representation .",10,0,36
dataset/preprocessed/test-data/document_classification/18,Improvements for part - of - speech tagging and information retrieval were observed .,11,0,14
dataset/preprocessed/test-data/document_classification/18,"Text classification is a classic topic for natural language processing , in which one needs to assign predefined categories to free - text documents .",13,0,25
dataset/preprocessed/test-data/document_classification/18,The range of text classification research goes from designing the best features to choosing the best possible machine learning classifiers .,14,0,21
dataset/preprocessed/test-data/document_classification/18,"To date , almost all techniques of text classification are based on words , in which simple statistics of some ordered word combinations ( such as n-grams ) usually perform the best .",15,0,33
dataset/preprocessed/test-data/document_classification/18,"On the other hand , many researchers have found convolutional networks ( ConvNets ) are useful in extracting information from raw signals , ranging from computer vision applications to speech recognition and others .",16,0,34
dataset/preprocessed/test-data/document_classification/18,"In particular , time - delay networks used in the early days of deep learning research are essentially convolutional networks that model sequential data .",17,0,25
dataset/preprocessed/test-data/document_classification/18,"In this article we explore treating text as a kind of raw signal at character level , and applying temporal ( one-dimensional ) ConvNets to it .",18,0,27
dataset/preprocessed/test-data/document_classification/18,For this article we only used a classification task as away to exemplify ConvNets ' ability to understand texts .,19,0,20
dataset/preprocessed/test-data/document_classification/18,"Historically we know that ConvNets usually require large - scale datasets to work , therefore we also build several of them .",20,0,22
dataset/preprocessed/test-data/document_classification/18,An extensive set of comparisons is offered with traditional models and other deep learning models .,21,0,16
dataset/preprocessed/test-data/document_classification/18,Applying convolutional networks to text classification or natural language processing at large was explored in literature .,22,0,17
dataset/preprocessed/test-data/document_classification/18,"It has been shown that ConvNets can be directly applied to distributed or discrete embedding of words , without any knowledge on the syntactic or semantic structures of a language .",23,0,31
dataset/preprocessed/test-data/document_classification/18,These approaches have been proven to be competitive to traditional models .,24,0,12
dataset/preprocessed/test-data/document_classification/18,from previous research that ConvNets do not require the knowledge about the syntactic or semantic structure of a language .,25,0,20
dataset/preprocessed/test-data/document_classification/18,"This simplification of engineering could be crucial for a single system that can work for different languages , since characters always constitute a necessary construct regardless of whether segmentation into words is possible .",26,0,34
dataset/preprocessed/test-data/document_classification/18,Working on only characters also has the advantage that abnormal character combinations such as misspellings and emoticons maybe naturally learnt .,27,0,21
dataset/preprocessed/test-data/document_classification/18,Character - level Convolutional Networks,28,0,5
dataset/preprocessed/test-data/document_classification/18,"In this section , we introduce the design of character - level ConvNets for text classification .",29,0,17
dataset/preprocessed/test-data/document_classification/18,"The design is modular , where the gradients are obtained by back - propagation to perform optimization .",30,0,18
dataset/preprocessed/test-data/document_classification/18,"The main component is the temporal convolutional module , which simply computes a 1 - D convolution .",32,0,18
dataset/preprocessed/test-data/document_classification/18,Suppose we have a discrete input function g ( x ) ?,33,0,12
dataset/preprocessed/test-data/document_classification/18,"[ 1 , l ] ?",34,0,6
dataset/preprocessed/test-data/document_classification/18,Rand a discrete kernel function,35,0,5
dataset/preprocessed/test-data/document_classification/18,where c = k ? d + 1 is an offset constant .,36,0,13
dataset/preprocessed/test-data/document_classification/18,"Just as in traditional convolutional networks in vision , the module is parameterized by a set of such kernel functions f ij ( x ) ( i = 1 , 2 , . . . , m and j = 1 , 2 , . . . , n) which we call weights , on a set of inputs g i ( x ) and outputs h j ( y ) .",37,0,72
dataset/preprocessed/test-data/document_classification/18,"We call each g i ( or h j ) input ( or output ) features , and m ( or n) input ( or output ) feature size .",38,0,30
dataset/preprocessed/test-data/document_classification/18,The outputs h j ( y ) is obtained by a sum over i of the convolutions between g i ( x ) and f ij ( x ) .,39,0,30
dataset/preprocessed/test-data/document_classification/18,One key module that helped us to train deeper models is temporal max - pooling .,40,0,16
dataset/preprocessed/test-data/document_classification/18,It is the 1 - D version of the max - pooling module used in computer vision .,41,0,18
dataset/preprocessed/test-data/document_classification/18,Given a discrete input function g ( x ) ?,42,0,10
dataset/preprocessed/test-data/document_classification/18,"[ 1 , l ] ?",43,0,6
dataset/preprocessed/test-data/document_classification/18,"R , the max - pooling function h ( y ) ?",44,0,12
dataset/preprocessed/test-data/document_classification/18,"[ 1 , ( l ? k) / d + 1 ] ?",45,0,13
dataset/preprocessed/test-data/document_classification/18,R of g ( x ) is defined as,46,0,9
dataset/preprocessed/test-data/document_classification/18,where c = k ? d + 1 is an offset constant .,47,0,13
dataset/preprocessed/test-data/document_classification/18,"This very pooling module enabled us to train ConvNets deeper than 6 layers , where all others fail .",48,0,19
dataset/preprocessed/test-data/document_classification/18,The analysis by might shed some light on this .,49,0,10
dataset/preprocessed/test-data/document_classification/15,ADVERSARIAL TRAINING METHODS FOR SEMI - SUPERVISED TEXT CLASSIFICATION,2,1,9
dataset/preprocessed/test-data/document_classification/15,Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting .,4,0,26
dataset/preprocessed/test-data/document_classification/15,"However , both methods require making small perturbations to numerous entries of the input vector , which is inappropriate for sparse high - dimensional inputs such as one - hot word representations .",5,0,33
dataset/preprocessed/test-data/document_classification/15,We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself .,6,0,31
dataset/preprocessed/test-data/document_classification/15,The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks .,7,0,18
dataset/preprocessed/test-data/document_classification/15,"We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training , the model is less prone to overfitting .",8,0,28
dataset/preprocessed/test-data/document_classification/15,Adversarial examples are examples thatare created by making small perturbations to the input designed to significantly increase the loss incurred by a machine learning model .,10,0,26
dataset/preprocessed/test-data/document_classification/15,"Several models , including state of the art convolutional neural networks , lack the ability to classify adversarial examples correctly , sometimes even when the adversarial perturbation is constrained to be so small that a human observer can not perceive it .",11,0,42
dataset/preprocessed/test-data/document_classification/15,Adversarial training is the process of training a model to correctly classify both unmodified examples and adversarial examples .,12,0,19
dataset/preprocessed/test-data/document_classification/15,"It improves not only robustness to adversarial examples , but also generalization performance for original examples .",13,0,17
dataset/preprocessed/test-data/document_classification/15,"Adversarial training requires the use of labels when training models that use a supervised cost , because the label appears in the cost function that the adversarial perturbation is designed to maximize .",14,0,33
dataset/preprocessed/test-data/document_classification/15,Virtual adversarial training extends the idea of adversarial training to the semi-supervised regime and unlabeled examples .,15,0,17
dataset/preprocessed/test-data/document_classification/15,"This is done by regularizing the model so that given an example , the model will produce the same output distribution as it produces on an adversarial perturbation of that example .",16,0,32
dataset/preprocessed/test-data/document_classification/15,Virtual adversarial training achieves good generalization performance for both supervised and semi-supervised learning tasks .,17,0,15
dataset/preprocessed/test-data/document_classification/15,Previous work has primarily applied adversarial and virtual adversarial training to image classification tasks .,18,0,15
dataset/preprocessed/test-data/document_classification/15,"In this work , we extend these techniques to text classification tasks and sequence models .",19,0,16
dataset/preprocessed/test-data/document_classification/15,Adversarial perturbations typically consist of making small modifications to very many real - valued inputs .,20,0,16
dataset/preprocessed/test-data/document_classification/15,"For text classification , the input is discrete , and usually represented as a series of highdimensional one - hot vectors .",21,0,22
dataset/preprocessed/test-data/document_classification/15,"Because the set of high - dimensional one - hot vectors does not admit infinitesimal perturbation , we define the perturbation on continuous word embeddings instead of discrete word inputs .",22,0,31
dataset/preprocessed/test-data/document_classification/15,Traditional adversarial and virtual adversarial training can be interpreted both as a regularization strategy and as defense against an adversary who can supply malicious inputs .,23,0,26
dataset/preprocessed/test-data/document_classification/15,"Since the perturbed embedding does not map to any word and the adversary presumably does not have access to the word embedding layer , our proposed training strategy is no longer intended as a defense against an adversary .",24,0,39
dataset/preprocessed/test-data/document_classification/15,We thus propose this approach exclusively as a means of regularizing a text classifier by stabilizing the classification function .,25,0,20
dataset/preprocessed/test-data/document_classification/15,"We show that our approach with neural language model unsupervised pretraining as proposed by achieves state of the art performance for multiple semi-supervised text classification tasks , including sentiment classification and topic classification .",26,0,34
dataset/preprocessed/test-data/document_classification/15,"We emphasize that optimization of only one additional hyperparameter ? , the norm constraint limiting the size of the adversarial perturbations , achieved such state of the art performance .",27,0,30
dataset/preprocessed/test-data/document_classification/15,These results strongly encourage the use of our proposed method for other text classification tasks .,28,0,16
dataset/preprocessed/test-data/document_classification/15,We believe that text classification is an ideal setting for semi-supervised learning because there are abundant unlabeled corpora for semi-supervised learning algorithms to leverage .,29,0,25
dataset/preprocessed/test-data/document_classification/15,This work is the first work we know of to use adversarial and virtual adversarial training to improve a text or RNN model .,30,0,24
dataset/preprocessed/test-data/document_classification/15,We also analyzed the trained models to qualitatively characterize the effect of adversarial and virtual adversarial training .,31,0,18
dataset/preprocessed/test-data/document_classification/15,We found that adversarial and virtual adversarial training improved word embeddings over the baseline methods .,32,0,16
dataset/preprocessed/test-data/document_classification/15,"We denote a sequence of T words as {w ( t ) |t = 1 , . . . , T } , and a corresponding target as y .",34,0,30
dataset/preprocessed/test-data/document_classification/15,"To transform a discrete word input to a continuous vector , we define the word embedding matrix V ?",35,0,19
dataset/preprocessed/test-data/document_classification/15,R ( K+1 ) D where K is the number of words in the vocabulary and each row v k corresponds to the word embedding of the i - th word .,36,0,32
dataset/preprocessed/test-data/document_classification/15,"Note that the ( K + 1 ) - th word embedding is used as an embedding of an ' end of sequence ( eos ) ' token , v eos .",37,0,32
dataset/preprocessed/test-data/document_classification/15,"As a text classification model , we used a simple LSTM - based neural network model , shown in .",38,0,20
dataset/preprocessed/test-data/document_classification/15,"At time step t , the input is the discrete word w ( t ) , and the corresponding word embedding is v ( t ) .",39,0,27
dataset/preprocessed/test-data/document_classification/15,We additionally tried the bidirectional v eos,40,0,7
dataset/preprocessed/test-data/document_classification/15,The model with perturbed embeddings .,41,0,6
dataset/preprocessed/test-data/document_classification/15,LSTM architecture since this is used by the current state of the art method .,42,0,15
dataset/preprocessed/test-data/document_classification/15,"For constructing the bidirectional LSTM model for text classification , we add an additional LSTM on the reversed sequence to the unidirectional LSTM model described in .",43,0,27
dataset/preprocessed/test-data/document_classification/15,The model then predicts the label on the concatenated LSTM outputs of both ends of the sequence .,44,0,18
dataset/preprocessed/test-data/document_classification/15,"In adversarial and virtual adversarial training , we train the classifier to be robust to perturbations of the embeddings , shown in .",45,0,23
dataset/preprocessed/test-data/document_classification/15,These perturbations are described in detail in Section 3 .,46,0,10
dataset/preprocessed/test-data/document_classification/15,"At present , it is sufficient to understand that the perturbations are of bounded norm .",47,0,16
dataset/preprocessed/test-data/document_classification/15,The model could trivially learn to make the perturbations insignificant by learning embeddings with very large norm .,48,0,18
dataset/preprocessed/test-data/document_classification/15,"To prevent this pathological solution , when we apply adversarial and virtual adversarial training to the model we defined above , we replace the embeddings v k with normalized embeddingsv k , defined as : v",49,0,36
dataset/preprocessed/test-data/document_classification/8,Disconnected Recurrent Neural Networks for Text Categorization,2,1,7
dataset/preprocessed/test-data/document_classification/8,Recurrent neural network ( RNN ) has achieved remarkable performance in text categorization .,4,0,14
dataset/preprocessed/test-data/document_classification/8,"RNN can model the entire sequence and capture long - term dependencies , but it does not do well in extracting key patterns .",5,0,24
dataset/preprocessed/test-data/document_classification/8,"In contrast , convolutional neural network ( CNN ) is good at extracting local and position - invariant features .",6,0,20
dataset/preprocessed/test-data/document_classification/8,"In this paper , we present a novel model named disconnected recurrent neural network ( DRNN ) , which incorporates position - invariance into RNN .",7,0,26
dataset/preprocessed/test-data/document_classification/8,"By limiting the distance of information flow in RNN , the hidden state at each time step is restricted to represent words near the current position .",8,0,27
dataset/preprocessed/test-data/document_classification/8,The proposed model makes great improvements over RNN and CNN models and achieves the best performance on several benchmark datasets for text categorization .,9,0,24
dataset/preprocessed/test-data/document_classification/8,"Text categorization is a fundamental and traditional task in natural language processing ( NLP ) , which can be applied in various applications such as sentiment analysis , question classification and topic classification .",11,0,34
dataset/preprocessed/test-data/document_classification/8,"Nowadays , one of the most commonly used methods to handle the task is to represent a text with a low dimensional vector , then feed the vector into a softmax function to calculate the probability of each category .",12,0,40
dataset/preprocessed/test-data/document_classification/8,Recurrent neural network ( RNN ) and convolutional neural network ( CNN ) are two kinds of neural networks usually used to represent the text .,13,0,26
dataset/preprocessed/test-data/document_classification/8,RNN can model the whole sequence and capture long - term dependencies .,14,0,13
dataset/preprocessed/test-data/document_classification/8,"However , modeling the entire sequence sometimes case1 :",15,0,9
dataset/preprocessed/test-data/document_classification/8,One of the seven great unsolved mysteries of mathematics may have been cracked by a reclusive Russian .,16,0,18
dataset/preprocessed/test-data/document_classification/8,case2 : A reclusive Russian may have cracked one of the seven great unsolved mysteries of mathematics .,17,0,18
dataset/preprocessed/test-data/document_classification/8,"can be a burden , and it may neglect key parts for text categorization .",18,0,15
dataset/preprocessed/test-data/document_classification/8,"In contrast , CNN is able to extract local and position - invariant features well .",19,0,16
dataset/preprocessed/test-data/document_classification/8,"is an example of topic classification , where both sentences should be classified as Science and Technology .",20,0,18
dataset/preprocessed/test-data/document_classification/8,"The key phrase that determines the category is unsolved mysteries of mathematics , which can be well extracted by CNN due to position - invariance .",21,0,26
dataset/preprocessed/test-data/document_classification/8,"RNN , however , does n't address such issues well because the representation of the key phrase relies on all the previous terms and the representation changes as the key phrase moves .",22,0,33
dataset/preprocessed/test-data/document_classification/8,"In this paper , we incorporate positioninvariance into RNN and propose a novel model named Disconnected Recurrent Neural Network ( DRNN ) .",23,0,23
dataset/preprocessed/test-data/document_classification/8,"Concretely , we disconnect the information transmission of RNN and limit the maximal transmission step length as a fixed value k , so that the representation at each step only depends on the previous k ?",24,0,36
dataset/preprocessed/test-data/document_classification/8,1 words and the current word .,25,0,7
dataset/preprocessed/test-data/document_classification/8,"In this way , DRNN can also alleviate the burden of modeling the entire document .",26,0,16
dataset/preprocessed/test-data/document_classification/8,"To maintain the position - invariance , we utilize max pooling to extract the important information , which has been suggested by .",27,0,23
dataset/preprocessed/test-data/document_classification/8,Our proposed model can also be regarded as a special 1D CNN where convolution kernels are replaced with recurrent units .,28,0,21
dataset/preprocessed/test-data/document_classification/8,"Therefore , the maximal transmission step length can also be consid-ered as the window size in CNN .",29,0,18
dataset/preprocessed/test-data/document_classification/8,Another difference to CNN is that DRNN can increase the window size k arbitrarily without increasing the number of parameters .,30,0,21
dataset/preprocessed/test-data/document_classification/8,We also find that there is a trade - off between position - invariance and long - term dependencies in the DRNN .,31,0,23
dataset/preprocessed/test-data/document_classification/8,"When the window size is too large , the position - invariance will dis appear like RNN .",32,0,18
dataset/preprocessed/test-data/document_classification/8,"By contrast , when the window size is too small , we will lose the ability to model long - term dependencies just like CNN .",33,0,26
dataset/preprocessed/test-data/document_classification/8,"We find that the optimal window size is related to the type of task , but affected little by training dataset sizes .",34,0,23
dataset/preprocessed/test-data/document_classification/8,"Thus , we can search the optimal window size by training on a small dataset .",35,0,16
dataset/preprocessed/test-data/document_classification/8,We conduct experiments on seven large - scale text classification datasets introduced by .,36,0,14
dataset/preprocessed/test-data/document_classification/8,The experimental results show that our proposed model outperforms the other models on all of these datasets .,37,0,18
dataset/preprocessed/test-data/document_classification/8,Our contributions can be concluded as follows :,38,0,8
dataset/preprocessed/test-data/document_classification/8,1 . We propose a novel model to incorporate position - variance into RNN .,39,0,15
dataset/preprocessed/test-data/document_classification/8,Our proposed model can both capture long - term dependencies and local information well .,40,0,15
dataset/preprocessed/test-data/document_classification/8,"We study the effect of different recurrent units , pooling operations and window sizes on model performance .",42,0,18
dataset/preprocessed/test-data/document_classification/8,"Based on this , we propose an empirical method to find the optimal window size .",43,0,16
dataset/preprocessed/test-data/document_classification/8,Our proposed model outperforms the other models and achieves the best performance on seven text classification datasets .,45,0,18
dataset/preprocessed/test-data/document_classification/8,"Deep neural networks have shown great success in many NLP tasks such as machine translation , sentiment classification , etc .",47,0,21
dataset/preprocessed/test-data/document_classification/8,"Nowadays , nearly most of deep neural networks models are based on CNN or RNN .",48,0,16
dataset/preprocessed/test-data/document_classification/8,"Below , we will introduce some important works about text classification based on them .",49,0,15
dataset/preprocessed/test-data/document_classification/6,Explicit Interaction Model towards Text Classification,2,1,6
dataset/preprocessed/test-data/document_classification/6,Text classification is one of the fundamental tasks in natural language processing .,4,0,13
dataset/preprocessed/test-data/document_classification/6,"Recently , deep neural networks have achieved promising performance in the text classification task compared to shallow models .",5,0,19
dataset/preprocessed/test-data/document_classification/6,"Despite of the significance of deep models , they ignore the fine - grained ( matching signals between words and classes ) classification clues since their classifications mainly rely on the text - level representations .",6,0,36
dataset/preprocessed/test-data/document_classification/6,"To address this problem , we introduce the interaction mechanism to incorporate word - level matching signals into the text classification task .",7,0,23
dataset/preprocessed/test-data/document_classification/6,"In particular , we design a novel framework , EXplicit interAction Model ( dubbed as EXAM ) , equipped with the interaction mechanism .",8,0,24
dataset/preprocessed/test-data/document_classification/6,We justified the proposed approach on several benchmark datasets including both multilabel and multi-class text classification tasks .,9,0,18
dataset/preprocessed/test-data/document_classification/6,Extensive experimental results demonstrate the superiority of the proposed method .,10,0,11
dataset/preprocessed/test-data/document_classification/6,"As a byproduct , we have released the codes and parameter settings to facilitate other researches .",11,0,17
dataset/preprocessed/test-data/document_classification/6,"Text classification is one of the fundamental tasks in natural language processing , targeting at classifying apiece of text content into one or multiple categories .",13,0,26
dataset/preprocessed/test-data/document_classification/6,"According to the number of desired categories , text classification can be divided into two groups , namely , multi-label ( multiple categories ) and multi-class ( unique category ) .",14,0,31
dataset/preprocessed/test-data/document_classification/6,"For instance , classifying an article into different topics ( e.g. , machine learning or data mining ) falls into the former one since an article could be under several topics simultaneously .",15,0,33
dataset/preprocessed/test-data/document_classification/6,"By contrast , classifying a comment of a movie into its corresponding rating level lies into the multi -class group .",16,0,21
dataset/preprocessed/test-data/document_classification/6,"Both multi-label and multi-class text classifications have been widely applied in many fields like sentimental analysis , topic tagging , and document classification .",17,0,24
dataset/preprocessed/test-data/document_classification/6,Feature engineering dominates the performance of traditional shallow text classification methods for a very longtime .,18,0,16
dataset/preprocessed/test-data/document_classification/6,"Various rule - based and statistical features like bag - of - words and N - grams ) are designed to describe the text , and fed into the shallow machine learning models such as Linear Regression and Support Vector Machine to make the judgment .",19,0,46
dataset/preprocessed/test-data/document_classification/6,Traditional solutions suffer from two defects :,20,0,7
dataset/preprocessed/test-data/document_classification/6,"1 ) High labor intensity for the manually crafted features , and 2 ) data sparsity ( a N -grams could occur only several times in a given dataset ) .",21,0,31
dataset/preprocessed/test-data/document_classification/6,"Recently , owing to the ability of tackling the aforementioned problems , deep neural networks ; Liu , Qiu , and Huang 2016 ; Grave et al. ) have become the promising solutions for the text classification .",22,0,38
dataset/preprocessed/test-data/document_classification/6,"Deep neural networks typically learn a word - level representation for the input text , which is usually a matrix with each row / column as an embedding of a word in the text .",23,0,35
dataset/preprocessed/test-data/document_classification/6,"They then compress the word - level representation into a text - level representation ( vector ) with aggregation operations ( e.g. , pooling ) .",24,0,26
dataset/preprocessed/test-data/document_classification/6,"Thereafter , a fullyconnected ( FC ) layer at the topmost of the network is appended to make the final decision .",25,0,22
dataset/preprocessed/test-data/document_classification/6,"Note that these solutions are also called encoding - based methods , since they encode the textual content into a latent vector representation .",26,0,24
dataset/preprocessed/test-data/document_classification/6,"Although great success has been achieved , these deep neural network based solutions naturally ignore the finegrained classification clues ( i.e. , matching signals between words and classes ) , since their classifications are based on text - level representations .",27,0,41
dataset/preprocessed/test-data/document_classification/6,"As shown in , the classification ( i.e. , FC ) layer of these solutions matches the text - level representation with class representations via a dotproduct operation .",28,0,29
dataset/preprocessed/test-data/document_classification/6,"Mathematically , it interprets the parameter matrix of the FC layer as a set of class representations ( each column is associated with a class ) .",29,0,27
dataset/preprocessed/test-data/document_classification/6,"As such , the probability of the text belonging to a class is largely determined by their over all matching score regardless of word - level matching signals , which would provide explicit signals for classification ( e.g. , missile strongly indicates the topic of military ) .",30,0,48
dataset/preprocessed/test-data/document_classification/6,"To address the aforementioned problems , we introduce the interaction mechanism ( Wang and Jiang 2016 b ) , which is capable of incorporating the word - level matching signals for text classification .",31,0,34
dataset/preprocessed/test-data/document_classification/6,The key idea behind the interaction mechanism is to explicitly calculate the matching scores between the words and classes .,32,0,20
dataset/preprocessed/test-data/document_classification/6,"From the word - level representation , it computes an interaction matrix , in which each entry is the matching score between a word and a class ( dot -product between their representations ) , illustrating the word - level matching signals .",33,0,43
dataset/preprocessed/test-data/document_classification/6,"By taking the interaction matrix as a text representation , the later classification layer could incorporate fine - grained word level signals for the finer classification rather than simply making the text - level matching .",34,0,36
dataset/preprocessed/test-data/document_classification/6,"Based upon the interaction mechanism , we devise an EXplicit interAction Model ( dubbed as EXAM ) .",35,0,18
dataset/preprocessed/test-data/document_classification/6,"Specifically , the proposed framework consists of three main components : word - level encoder , interaction layer , and aggregation layer .",36,0,23
dataset/preprocessed/test-data/document_classification/6,The word - level encoder projects the textual contents into the word - level representations .,37,0,16
dataset/preprocessed/test-data/document_classification/6,"Hereafter , the interaction layer calculates the matching scores between the words and classes ( i.e. , constructs the interaction matrix ) .",38,0,23
dataset/preprocessed/test-data/document_classification/6,"Then , the last layer aggregates those matching scores into predictions over each class , respectively .",39,0,17
dataset/preprocessed/test-data/document_classification/6,We justify our proposed EXAM model over both the multi-label and multi-class text classifications .,40,0,15
dataset/preprocessed/test-data/document_classification/6,"Extensive experiments on several benchmarks demonstrate the effectiveness of the proposed method , surpassing the corresponding state - of - the - art methods remarkably .",41,0,26
dataset/preprocessed/test-data/document_classification/6,"In summary , the contributions of this work are threefold : We present a novel framework , EXAM , which leverages the interaction mechanism to explicitly compute the wordlevel interaction signals for the text classification .",42,0,36
dataset/preprocessed/test-data/document_classification/6,We justify the proposed EXAM model over both multilabel and multi-class text classifications .,43,0,14
dataset/preprocessed/test-data/document_classification/6,Extensive experimental results demonstrate the effectiveness of the proposed method .,44,0,11
dataset/preprocessed/test-data/document_classification/6,We release the implementation of our method ( including some baselines ) and the involved parameter settings to facilitate later researchers 1 .,45,0,23
dataset/preprocessed/test-data/document_classification/6,"In this section , we introduce two widely - used word - level encoders :",47,0,15
dataset/preprocessed/test-data/document_classification/6,"Gated Recurrent Units notations in this paper , we use bold capital letters ( e.g. , X ) and bold lowercase letters ( e.g. , x ) to denote matrices and vectors , respectively .",48,0,35
dataset/preprocessed/test-data/document_classification/6,"We employ non-bold letters ( e.g. , x ) to represent scalars , and Greek letters ( e.g. , ? ) as parameters .",49,0,24
dataset/preprocessed/test-data/document_classification/11,Task - oriented Word Embedding for Text Classification,2,1,8
dataset/preprocessed/test-data/document_classification/11,Distributed word representation plays a pivotal role in various natural language processing tasks .,4,0,14
dataset/preprocessed/test-data/document_classification/11,"In spite of its success , most existing methods only consider contextual information , which is suboptimal when used in various tasks due to alack of task - specific features .",5,0,31
dataset/preprocessed/test-data/document_classification/11,The rational word embeddings should have the ability to capture both the semantic features and task - specific features of words .,6,0,22
dataset/preprocessed/test-data/document_classification/11,"In this paper , we propose a task - oriented word embedding method and apply it to the text classification task .",7,0,22
dataset/preprocessed/test-data/document_classification/11,"With the function - aware component , our method regularizes the distribution of words to enable the embedding space to have a clear classification boundary .",8,0,26
dataset/preprocessed/test-data/document_classification/11,We evaluate our method using five text classification datasets .,9,0,10
dataset/preprocessed/test-data/document_classification/11,The experiment results show that our method significantly outperforms the state - of - the - art methods .,10,0,19
dataset/preprocessed/test-data/document_classification/11,* Corresponding author,11,0,3
dataset/preprocessed/test-data/document_classification/11,This work is licenced under a Creative Commons Attribution 4.0 International Licence .,12,0,13
dataset/preprocessed/test-data/document_classification/11,Licence details : http://creativecommons.org/licenses/by/4.0/,13,0,4
dataset/preprocessed/test-data/document_classification/11,Text Classification Expected Distribution Word Embeddings Actual Distribution,14,0,8
dataset/preprocessed/test-data/document_classification/11,AI : a combination of active learning and self learning for named entity recognition on twitter using conditional random fields learning :,15,0,22
dataset/preprocessed/test-data/document_classification/11,Law : we assess the relationship between legal origin and a range of correlated indicators of social responsibility,16,0,18
dataset/preprocessed/test-data/document_classification/11,Learning word representation is a fundamental step in various natural language processing tasks .,18,0,14
dataset/preprocessed/test-data/document_classification/11,"Tremendous advances have been made by distributed representations ( also known as word embeddings ) which learn a transformation of each word from raw text data to a dense , lower - dimensional vector space .",19,0,36
dataset/preprocessed/test-data/document_classification/11,"Most existing methods leverage contextual information from the corpus and other complementary information , such as subword information , implicitly syntactic dependencies , and semantic relations .",20,0,27
dataset/preprocessed/test-data/document_classification/11,"In traditional evaluations such as word similarity and word analogy , the aforementioned context - aware word embeddings work well since semantic information plays a vital role in these tasks , and this information is naturally addressed byword contexts .",21,0,40
dataset/preprocessed/test-data/document_classification/11,"However , in real - world applications , such as text classification and information retrieval , word contexts alone are insufficient to achieve success in the absence of task - specific features .",22,0,33
dataset/preprocessed/test-data/document_classification/11,illustrates this problem with the classification task as an example .,23,0,11
dataset/preprocessed/test-data/document_classification/11,Several sentences from different categories are given at the far left of the figure where the words in bold are salient words for the category distinction .,24,0,27
dataset/preprocessed/test-data/document_classification/11,We also illustrated expected word distribution of these salient words in the embedding space .,25,0,15
dataset/preprocessed/test-data/document_classification/11,"To obtain a good classification performance , the expected word distribution should have a clear classification boundary : words within the same category are close to each other and faraway from words in other categories as illustrated in .",26,0,39
dataset/preprocessed/test-data/document_classification/11,"However , the actual distribution obtained from Word2 Vec at the far right of is normally not satisfactory because Word2 Vec only focuses on context similarity .",27,0,27
dataset/preprocessed/test-data/document_classification/11,"For example , although learning and educational are with similar context as recognized by Word2 Vec , they are salient words to distinguish categories of AI and Sociology , so they should be faraway from each other .",28,0,38
dataset/preprocessed/test-data/document_classification/11,"Apparently , using word embedding directly from Word2 Vec would not obtain good performance on the text classification task due to the fact that words ' functional features in the real tasks are ignored in the training process .",29,0,39
dataset/preprocessed/test-data/document_classification/11,"In this paper , we propose a task - oriented word embedding method ( denoted as ToWE ) to solve the aforementioned problem .",30,0,24
dataset/preprocessed/test-data/document_classification/11,It learns the distributed representation of words according to the given specific :,31,0,13
dataset/preprocessed/test-data/document_classification/11,The example sentences from the text classification dataset .,32,0,9
dataset/preprocessed/test-data/document_classification/11,Words in bold are salient words to distinguish the sentence category .,33,0,12
dataset/preprocessed/test-data/document_classification/11,Their most similar words in the Word2 Vec space are shown in the right - hand column .,34,0,18
dataset/preprocessed/test-data/document_classification/11,"The word color indicates the category , and the words in black are general words for the task .",35,0,19
dataset/preprocessed/test-data/document_classification/11,NLP task .,36,0,3
dataset/preprocessed/test-data/document_classification/11,"Specifically , we focus on text classification .",37,0,8
dataset/preprocessed/test-data/document_classification/11,"In our method , the words ' contextual information and task information are inherently jointed to construct the word embeddings .",38,0,21
dataset/preprocessed/test-data/document_classification/11,"In the joint learning framework , the contextual information is captured following the context prediction task introduced by .",39,0,19
dataset/preprocessed/test-data/document_classification/11,"To model the task information , we regularize the distribution of the salient words to have a clear classification boundary , and then adjust the distribution of the other words in the embedding space correspondingly .",40,0,36
dataset/preprocessed/test-data/document_classification/11,"To give an intuitive understanding on how our method works from the classification perspective , we design a 5 Abstracts",41,0,20
dataset/preprocessed/test-data/document_classification/11,Group dataset ( detailed in Section 4.1 ) and conduct a qualitative analysis .,42,0,14
dataset/preprocessed/test-data/document_classification/11,Experiments show qualitative improvements of our method over context - based Skip - gram method on word neighbors for classification .,43,0,21
dataset/preprocessed/test-data/document_classification/11,"We also perform empirical comparisons on five text classification datasets , which demonstrate the effectiveness of our method over the other state - of the - art methods .",44,0,29
dataset/preprocessed/test-data/document_classification/11,The contributions of this paper can be summarized as following :,45,0,11
dataset/preprocessed/test-data/document_classification/11,We propose a task - oriented word embedding method that is specially designed for text classification .,46,0,17
dataset/preprocessed/test-data/document_classification/11,It introduces the function - aware component and highlights word 's functional attributes in the embedding space by regularizing the distribution of words to have a clear classification boundary .,47,0,30
dataset/preprocessed/test-data/document_classification/11,We design a 5 Abstracts,48,0,5
dataset/preprocessed/test-data/document_classification/11,"Group dataset and present a qualitative analysis , giving an intuitive understanding on how our method works from the classification perspective .",49,0,22
dataset/preprocessed/test-data/document_classification/14,Supervised and Semi- Supervised Text Categorization using LSTM for Region Embeddings,2,1,11
dataset/preprocessed/test-data/document_classification/14,"One - hot CNN ( convolutional neural network ) has been shown to be effective for text categorization ( Johnson & Zhang , 2015a ; b ) .",4,1,28
dataset/preprocessed/test-data/document_classification/14,We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of ' text region embedding + pooling ' .,5,0,32
dataset/preprocessed/test-data/document_classification/14,"Under this framework , we explore a more sophisticated region embedding method using Long Short - Term Memory ( LSTM ) .",6,0,22
dataset/preprocessed/test-data/document_classification/14,"LSTM can embed text regions of variable ( and possibly large ) sizes , whereas the region size needs to be fixed in a CNN .",7,0,26
dataset/preprocessed/test-data/document_classification/14,We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings .,8,0,18
dataset/preprocessed/test-data/document_classification/14,The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data .,9,0,22
dataset/preprocessed/test-data/document_classification/14,"The results indicate that on this task , embeddings of text regions , which can convey complex concepts , are more useful than embeddings of single words in isolation .",10,0,30
dataset/preprocessed/test-data/document_classification/14,We report performances exceeding the previous best results on four benchmark datasets .,11,0,13
dataset/preprocessed/test-data/document_classification/14,"Text categorization is the task of assigning labels to documents written in a natural language , and it has numerous real - world applications including sentiment analysis as well as traditional topic assignment tasks .",13,0,35
dataset/preprocessed/test-data/document_classification/14,"The state - of - the art methods for text categorization had long been linear predictors ( e.g. , SVM with a linear kernel ) with either bag - ofword or bag - of - n- gram vectors ( hereafter bow ) as input , e.g. , .",14,0,48
dataset/preprocessed/test-data/document_classification/14,"This , however , A convolutional neural network ( CNN ) ) is a feedforward neural network with convolution layers interleaved with pooling layers , originally developed for image processing .",15,0,31
dataset/preprocessed/test-data/document_classification/14,"In its convolution layer , a small region of data ( e.g. , a small square of image ) at every location is converted to a low-dimensional vector with information relevant to the task being preserved , which we loosely term ' embedding ' .",16,0,45
dataset/preprocessed/test-data/document_classification/14,"The embedding function is shared among all the locations , so that useful features can be detected irrespective of their locations .",17,0,22
dataset/preprocessed/test-data/document_classification/14,"In its simplest form , onehot CNN works as follows .",18,0,11
dataset/preprocessed/test-data/document_classification/14,"A document is represented as a sequence of one - hot vectors ( each of which indicates a word by the position of a 1 ) ; a convolution layer converts small regions of the document ( e.g. , "" I love it "" ) to low-dimensional vectors at every location ( embedding of text regions ) ; a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average ; and the top layer classifies a document vector with a linear model .",19,0,90
dataset/preprocessed/test-data/document_classification/14,The onehot CNN and its semi-supervised extension were shown to be superior to a number of previous methods .,20,0,19
dataset/preprocessed/test-data/document_classification/14,"In this work , we consider a more general framework ( subsuming one - hot CNN ) which jointly trains a feature generator and a linear model , where the feature generator consists of ' region embedding + pooling ' .",21,0,41
dataset/preprocessed/test-data/document_classification/14,"The specific region embedding function of one - hot CNN takes the simple form v ( x ) = max (0 , Wx + b ) ,",22,0,27
dataset/preprocessed/test-data/document_classification/14,"where x is a concatenation of one - hot vectors ( therefore , ' one - hot ' in the name ) of the words in the - th region ( of a fixed size ) , and the weight matrix W and the bias vector b need to be trained .",23,0,52
dataset/preprocessed/test-data/document_classification/14,"It is simple and fast to compute , and considering its simplicity , the method works surprisingly well if the region size is appropriately set .",24,0,26
dataset/preprocessed/test-data/document_classification/14,"However , there are also potential shortcomings .",25,0,8
dataset/preprocessed/test-data/document_classification/14,"The region size must be fixed , which may not be optimal as the size of relevant regions may vary .",26,0,21
dataset/preprocessed/test-data/document_classification/14,"Practically , the region size can not be very large as the number of parameters to be learned ( components of W ) depends on it .",27,0,27
dataset/preprocessed/test-data/document_classification/14,JZ15 proposed variations to alleviate these issues .,28,0,8
dataset/preprocessed/test-data/document_classification/14,"For example , a bow - input variation allows x above to be a bow vector of the region .",29,0,20
dataset/preprocessed/test-data/document_classification/14,"This enables a larger region , but at the expense of losing word order in the region and so its use maybe limited .",30,0,24
dataset/preprocessed/test-data/document_classification/14,"In this work , we build on the general framework of ' region embedding + pooling ' and explore a more sophisticated region embedding via Long Short - Term Memory ( LSTM ) , seeking to overcome the shortcomings above , in the supervised and semi-supervised settings .",31,0,48
dataset/preprocessed/test-data/document_classification/14,LSTM ) is a recurrent neural network .,32,0,8
dataset/preprocessed/test-data/document_classification/14,"In its typical applications to text , an LSTM takes words in a sequence one by one ; i.e. , at time t , it takes as input the t- th word and the output from time t ?",33,0,39
dataset/preprocessed/test-data/document_classification/14,"1 . Therefore , the output from each time step can be regarded as the embedding of the sequence of words that have been seen so far ( or a relevant part of it ) .",34,0,36
dataset/preprocessed/test-data/document_classification/14,It is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks .,35,0,19
dataset/preprocessed/test-data/document_classification/14,"That is , an LSTM can be used to embed text regions of variable ( and possibly large ) sizes .",36,0,21
dataset/preprocessed/test-data/document_classification/14,"We pursue the best use of LSTM for our purpose , and then compare the resulting model with the previous best methods including one - hot CNN and previous LSTM .",37,0,31
dataset/preprocessed/test-data/document_classification/14,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .",38,0,27
dataset/preprocessed/test-data/document_classification/14,Our findings are threefold .,39,0,5
dataset/preprocessed/test-data/document_classification/14,"First , in the supervised setting , our simplification strategy leads to higher accuracy and faster training than previous LSTM .",40,0,21
dataset/preprocessed/test-data/document_classification/14,"Second , accuracy can be further improved by training LSTMs on unlabeled data for learning useful region embeddings and using them to produce additional input .",41,0,26
dataset/preprocessed/test-data/document_classification/14,"Third , both our LSTM models and one - hot CNN strongly outperform other methods including previous LSTM .",42,0,19
dataset/preprocessed/test-data/document_classification/14,"The best results are obtained by combining the two types of region embeddings ( LSTM embed - dings and CNN embeddings ) trained on unlabeled data , indicating that their strengths are complementary .",43,0,34
dataset/preprocessed/test-data/document_classification/14,"Overall , our results show that for text categorization , embeddings of text regions , which can convey higher - level concepts than single words in isolation , are useful , and that useful region embeddings can be learned without going through word embedding learning .",44,0,46
dataset/preprocessed/test-data/document_classification/14,We report performances exceeding the previous best results on four benchmark datasets .,45,0,13
dataset/preprocessed/test-data/document_classification/14,Our code and experimental details are available at http://riejohnson.com/cnn download.html .,46,0,11
dataset/preprocessed/test-data/document_classification/14,"On text , LSTM has been used for labeling or generating words .",48,0,13
dataset/preprocessed/test-data/document_classification/14,"It has been also used for representing short sentences mostly for sentiment analysis , and some of them rely on syntactic parse trees ; see e.g. , .",49,0,28
dataset/preprocessed/test-data/document_classification/17,Very Deep Convolutional Networks for Text Classification,2,1,7
dataset/preprocessed/test-data/document_classification/17,"The dominant approach for many NLP tasks are recurrent neural networks , in particular LSTMs , and convolutional neural networks .",4,0,21
dataset/preprocessed/test-data/document_classification/17,"However , these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state - of - the - art in computer vision .",5,0,29
dataset/preprocessed/test-data/document_classification/17,We present a new architecture ( VD - CNN ) for text processing which operates directly at the character level and uses only small convolutions and pooling operations .,6,0,29
dataset/preprocessed/test-data/document_classification/17,"We are able to show that the performance of this model increases with the depth : using up to 29 convolutional layers , we report improvements over the state - of the - art on several public text classification tasks .",7,0,41
dataset/preprocessed/test-data/document_classification/17,"To the best of our knowledge , this is the first time that very deep convolutional nets have been applied to text processing .",8,0,24
dataset/preprocessed/test-data/document_classification/17,"The goal of natural language processing ( NLP ) is to process text with computers in order to analyze it , to extract information and eventually to represent the same information differently .",10,0,33
dataset/preprocessed/test-data/document_classification/17,"We may want to associate categories to parts of the text ( e.g. POS tagging or sentiment analysis ) , structure text differently ( e.g. parsing ) , or convert it to some other form which preserves all or part of the content ( e.g. machine translation , summarization ) .",11,0,51
dataset/preprocessed/test-data/document_classification/17,The level of granularity of this processing can range from individual characters to subword units or words up to whole sentences or even paragraphs .,12,0,25
dataset/preprocessed/test-data/document_classification/17,"After a couple of pioneer works , , among others ) , the use of neural networks for NLP applications is attracting huge in - terest in the research community and they are systematically applied to all NLP tasks .",13,0,40
dataset/preprocessed/test-data/document_classification/17,"However , while the use of ( deep ) neural networks in NLP has shown very good results for many tasks , it seems that they have not yet reached the level to outperform the state - of - the - art by a large margin , as it was observed in computer vision and speech recognition .",14,0,58
dataset/preprocessed/test-data/document_classification/17,"Convolutional neural networks , in short Con -vNets , are very successful in computer vision .",15,0,16
dataset/preprocessed/test-data/document_classification/17,"In early approaches to computer vision , handcrafted features were used , for instance "" scale - invariant feature transform ( SIFT ) "" , followed by some classifier .",16,0,30
dataset/preprocessed/test-data/document_classification/17,The fundamental idea of is to consider feature extraction and classification as one jointly trained task .,17,0,17
dataset/preprocessed/test-data/document_classification/17,"This idea has been improved over the years , in particular by using many layers of convolutions and pooling to sequentially extract a hierarchical representation of the input .",18,0,29
dataset/preprocessed/test-data/document_classification/17,The best networks are using more than 150 layers as in .,19,0,12
dataset/preprocessed/test-data/document_classification/17,Many NLP approaches consider words as basic units .,20,0,9
dataset/preprocessed/test-data/document_classification/17,An important step was the introduction of continuous representations of words .,21,0,12
dataset/preprocessed/test-data/document_classification/17,These word embeddings are now the state - of - the - art in NLP .,22,0,16
dataset/preprocessed/test-data/document_classification/17,"However , it is less clear how we should best represent a sequence of words , e.g. a whole sentence , which has complicated syntactic and semantic relations .",23,0,29
dataset/preprocessed/test-data/document_classification/17,"In general , in the same sentence , we maybe faced with local and long - range dependencies .",24,0,19
dataset/preprocessed/test-data/document_classification/17,"Currently , the mainstream approach is to consider a sentence as a sequence of tokens ( characters or words ) and to process them with a recurrent neural network ( RNN ) .",25,0,33
dataset/preprocessed/test-data/document_classification/17,"Tokens are usually processed in sequential order , from left to right , and the RNN is expected to "" memorize "" the whole sequence in its internal states .",26,0,30
dataset/preprocessed/test-data/document_classification/17,The most popular and successful RNN variant are certainly LSTMs ( Hochreiter and Schmid - to name just a few .,27,0,21
dataset/preprocessed/test-data/document_classification/17,"However , we argue that LSTMs are generic learning machines for sequence processing which are lacking task - specific structure .",28,0,21
dataset/preprocessed/test-data/document_classification/17,We propose the following analogy .,29,0,6
dataset/preprocessed/test-data/document_classification/17,"It is well known that a fully connected one hidden layer neural network can in principle learn any realvalued function , but much better results can be obtained with a deep problem - specific architecture which develops hierarchical representations .",30,0,40
dataset/preprocessed/test-data/document_classification/17,"By these means , the search space is heavily constrained and efficient solutions can be learned with gradient descent .",31,0,20
dataset/preprocessed/test-data/document_classification/17,ConvNets are namely adapted for computer vision because of the compositional structure of an image .,32,0,16
dataset/preprocessed/test-data/document_classification/17,"Texts have similar properties : characters combine to form n-grams , stems , words , phrase , sentences etc .",33,0,20
dataset/preprocessed/test-data/document_classification/17,"We believe that a challenge in NLP is to develop deep architectures which are able to learn hierarchical representations of whole sentences , jointly with the task .",34,0,28
dataset/preprocessed/test-data/document_classification/17,"In this paper , we propose to use deep architectures of many convolutional layers to approach this goal , using up to 29 layers .",35,0,25
dataset/preprocessed/test-data/document_classification/17,"The design of our architecture is inspired by recent progress in computer vision , in particular .",36,0,17
dataset/preprocessed/test-data/document_classification/17,This paper is structured as follows .,37,0,7
dataset/preprocessed/test-data/document_classification/17,There have been previous attempts to use ConvNets for text processing .,38,0,12
dataset/preprocessed/test-data/document_classification/17,We summarize the previous works in the next section and discuss the relations and differences .,39,0,16
dataset/preprocessed/test-data/document_classification/17,Our architecture is described in detail in section 3 .,40,0,10
dataset/preprocessed/test-data/document_classification/17,"We have evaluated our approach on several sentence classification tasks , initially proposed by .",41,0,15
dataset/preprocessed/test-data/document_classification/17,These tasks and our experimental results are detailed in section 4 .,42,0,12
dataset/preprocessed/test-data/document_classification/17,The proposed deep convolutional network shows significantly better results than previous ConvNets approach .,43,0,14
dataset/preprocessed/test-data/document_classification/17,The paper concludes with a discussion of future research directions for very deep approach in NLP .,44,0,17
dataset/preprocessed/test-data/document_classification/17,"There is a large body of research on sentiment analysis , or more generally on sentence classification tasks .",46,0,19
dataset/preprocessed/test-data/document_classification/17,"Initial approaches followed the classical two stage scheme of extraction of ( handcrafted ) features , followed by a classification stage .",47,0,22
dataset/preprocessed/test-data/document_classification/17,"Typical features include bag - of - words or ngrams , and their TF - IDF .",48,0,17
dataset/preprocessed/test-data/document_classification/17,These techniques have been compared with ConvNets by .,49,0,9
dataset/preprocessed/test-data/document_classification/0,Bag of Tricks for Efficient Text Classification,2,1,7
dataset/preprocessed/test-data/document_classification/0,This paper explores a simple and efficient baseline for text classification .,4,0,12
dataset/preprocessed/test-data/document_classification/0,"Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy , and many orders of magnitude faster for training and evaluation .",5,0,33
dataset/preprocessed/test-data/document_classification/0,"We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU , and classify half a million sentences among 312K classes in less than a minute .",6,0,36
dataset/preprocessed/test-data/document_classification/0,"Text classification is an important task in Natural Language Processing with many applications , such as web search , information retrieval , ranking and document classification .",8,0,27
dataset/preprocessed/test-data/document_classification/0,"Recently , models based on neural networks have become increasingly popular .",9,0,12
dataset/preprocessed/test-data/document_classification/0,"While these models achieve very good performance in practice , they tend to be relatively slow both at train and test time , limiting their use on very large datasets .",10,0,31
dataset/preprocessed/test-data/document_classification/0,"Meanwhile , linear classifiers are often considered as strong baselines for text classification problems .",11,0,15
dataset/preprocessed/test-data/document_classification/0,"Despite their simplicity , they often obtain stateof - the - art performances if the right features are used .",12,0,20
dataset/preprocessed/test-data/document_classification/0,They also have the potential to scale to very large corpus .,13,0,12
dataset/preprocessed/test-data/document_classification/0,"In this work , we explore ways to scale these baselines to very large corpus with a large output space , in the context of text classification .",14,0,28
dataset/preprocessed/test-data/document_classification/0,"Inspired by the recent work in efficient word representation learning , we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes , while achieving performance on par with the state - of - the - art .",15,0,50
dataset/preprocessed/test-data/document_classification/0,"We evaluate the quality of our approach fastText 1 on two different tasks , namely tag prediction and sentiment analysis .",16,0,21
dataset/preprocessed/test-data/document_classification/0,"A simple and efficient baseline for sentence classification is to represent sentences as bag of words ( BoW ) and train a linear classifier , e.g. , a logistic regression or an SVM .",18,0,34
dataset/preprocessed/test-data/document_classification/0,"However , linear classifiers do not share parameters among features and classes .",19,0,13
dataset/preprocessed/test-data/document_classification/0,This possibly limits their generalization in the context of large output space where some classes have very few examples .,20,0,20
dataset/preprocessed/test-data/document_classification/0,Common solutions to this problem are to factorize the linear classifier into low rank matrices or to use multilayer neural networks .,21,0,22
dataset/preprocessed/test-data/document_classification/0,shows a simple linear model with rank constraint .,22,0,9
dataset/preprocessed/test-data/document_classification/0,The first weight matrix A is a look - up table over the words .,23,0,15
dataset/preprocessed/test-data/document_classification/0,"The word representations are then averaged into a text representation , which is in turn fed to a linear classifier .",24,0,21
dataset/preprocessed/test-data/document_classification/0,The text representa - tion is an hidden variable which can be potentially be reused .,25,0,16
dataset/preprocessed/test-data/document_classification/0,"This architecture is similar to the cbow model of , where the middle word is replaced by a label .",26,0,20
dataset/preprocessed/test-data/document_classification/0,We use the softmax function f to compute the probability distribution over the predefined classes .,27,0,16
dataset/preprocessed/test-data/document_classification/0,"For a set of N documents , this leads to minimizing the negative loglikelihood over the classes :",28,0,18
dataset/preprocessed/test-data/document_classification/0,"where x n is the normalized bag of features of the nth document , y n the label , A and B the weight matrices .",29,0,26
dataset/preprocessed/test-data/document_classification/0,This model is trained asynchronously on multiple CPUs using stochastic gradient descent and a linearly decaying learning rate .,30,0,19
dataset/preprocessed/test-data/document_classification/0,"When the number of classes is large , computing the linear classifier is computationally expensive .",32,0,16
dataset/preprocessed/test-data/document_classification/0,"More precisely , the computational complexity is O ( kh ) where k is the number of classes and h the dimension of the text representation .",33,0,27
dataset/preprocessed/test-data/document_classification/0,"In order to improve our running time , we use a hierarchical softmax ) based on the Huffman coding tree .",34,0,21
dataset/preprocessed/test-data/document_classification/0,"During training , the computational complexity drops to O ( h log 2 ( k ) ) .",35,0,18
dataset/preprocessed/test-data/document_classification/0,The hierarchical softmax is also advantageous at test time when searching for the most likely class .,36,0,17
dataset/preprocessed/test-data/document_classification/0,Each node is associated with a probability that is the probability of the path from the root to that node .,37,0,21
dataset/preprocessed/test-data/document_classification/0,"If the node is at depth l + 1 with parents n 1 , . . . , n l , it s probability is",38,0,25
dataset/preprocessed/test-data/document_classification/0,This means that the probability of anode is always lower than the one of its parent .,39,0,17
dataset/preprocessed/test-data/document_classification/0,Exploring the tree with a depth first search and tracking the maximum probability among the leaves allows us to discard any branch associated with a small probability .,40,0,28
dataset/preprocessed/test-data/document_classification/0,"In practice , we observe a reduction of the complexity to O ( h log 2 ( k ) ) at test time .",41,0,24
dataset/preprocessed/test-data/document_classification/0,"This approach is further extended to compute the T - top targets at the cost of O ( log ( T ) ) , using a binary heap .",42,0,29
dataset/preprocessed/test-data/document_classification/0,N - gram features,43,0,4
dataset/preprocessed/test-data/document_classification/0,Bag of words is invariant to word order but taking explicitly this order into account is often computationally very expensive .,44,0,21
dataset/preprocessed/test-data/document_classification/0,"Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .",45,0,22
dataset/preprocessed/test-data/document_classification/0,This is very efficient in practice while achieving comparable results to methods that explicitly use the order .,46,0,18
dataset/preprocessed/test-data/document_classification/0,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",47,0,36
dataset/preprocessed/test-data/document_classification/0,We evaluate fastText on two different tasks .,49,0,8
dataset/preprocessed/test-data/document_classification/12,Graph Convolutional Networks for Text Classification,2,1,6
dataset/preprocessed/test-data/document_classification/12,Text classification is an important and classical problem in natural language processing .,4,0,13
dataset/preprocessed/test-data/document_classification/12,"There have been a number of studies that applied convolutional neural networks ( convolution on regular grid , e.g. , sequence ) to classification .",5,0,25
dataset/preprocessed/test-data/document_classification/12,"However , only a limited number of studies have explored the more flexible graph convolutional neural networks ( convolution on non-grid , e.g. , arbitrary graph ) for the task .",6,0,31
dataset/preprocessed/test-data/document_classification/12,"In this work , we propose to use graph convolutional networks for text classification .",7,0,15
dataset/preprocessed/test-data/document_classification/12,"We build a single text graph for a corpus based on word co-occurrence and document word relations , then learn a Text Graph Convolutional Network ( Text GCN ) for the corpus .",8,0,33
dataset/preprocessed/test-data/document_classification/12,"Our Text GCN is initialized with one - hot representation for word and document , it then jointly learns the embeddings for both words and documents , as supervised by the known class labels for documents .",9,0,37
dataset/preprocessed/test-data/document_classification/12,Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state - of - the - art methods for text classification .,10,0,33
dataset/preprocessed/test-data/document_classification/12,"On the other hand , Text GCN also learns predictive word and document embeddings .",11,0,15
dataset/preprocessed/test-data/document_classification/12,"In addition , experimental results show that the improvement of Text GCN over state - of - the - art comparison methods become more prominent as we lower the percentage of training data , suggesting the robustness of Text GCN to less training data in text classification .",12,0,48
dataset/preprocessed/test-data/document_classification/12,Text classification is a fundamental problem in natural language processing ( NLP ) .,14,0,14
dataset/preprocessed/test-data/document_classification/12,"There are numerous applications of text classification such as document organization , news filtering , spam detection , opinion mining , and computational phenotyping ) .",15,0,26
dataset/preprocessed/test-data/document_classification/12,An essential intermediate step for text classification is text representation .,16,0,11
dataset/preprocessed/test-data/document_classification/12,"Traditional methods represent text with hand - crafted features , such as sparse lexical features ( e.g. , bag - of - words and n-grams ) .",17,0,27
dataset/preprocessed/test-data/document_classification/12,"Recently , deep learning models have been widely used to learn text representations , including convolutional neural networks ( CNN ) ( Kim 2014 ) and recurrent neural networks ( RNN ) such as long short - term memory ( LSTM ) .",18,0,43
dataset/preprocessed/test-data/document_classification/12,"As CNN and RNN prioritize locality and sequentiality , these deep learning models can capture semantic and syntactic information in local consecutive word sequences well , but may ignore global word cooccurrence in a corpus which carries non-consecutive and long - distance semantics .",19,0,44
dataset/preprocessed/test-data/document_classification/12,"Recently , a new research direction called graph neural networks or graph embeddings has attracted wide attention .",20,0,18
dataset/preprocessed/test-data/document_classification/12,Graph neural networks have been effective at tasks thought to have rich relational structure and can preserve global structure information of a graph in graph embeddings .,21,0,27
dataset/preprocessed/test-data/document_classification/12,"In this work , we propose a new graph neural networkbased method for text classification .",22,0,16
dataset/preprocessed/test-data/document_classification/12,"We construct a single large graph from an entire corpus , which contains words and documents as nodes .",23,0,19
dataset/preprocessed/test-data/document_classification/12,"We model the graph with a Graph Convolutional Network ( GCN ) , a simple and effective graph neural network that captures high order neighborhoods information .",24,0,27
dataset/preprocessed/test-data/document_classification/12,The edge between two word nodes is built byword co-occurrence information and the edge between a word node and document node is built using word frequency and word 's document frequency .,25,0,32
dataset/preprocessed/test-data/document_classification/12,We then turn text classification problem into anode classification problem .,26,0,11
dataset/preprocessed/test-data/document_classification/12,The method can achieve strong classification performances with a small proportion of labeled documents and learn interpretable word and document node embeddings .,27,0,23
dataset/preprocessed/test-data/document_classification/12,Our source code is available at https://github. com/yao8839836/text_gcn .,28,0,9
dataset/preprocessed/test-data/document_classification/12,"To summarize , our contributions are as follows :",29,0,9
dataset/preprocessed/test-data/document_classification/12,We propose a novel graph neural network method for text classification .,30,0,12
dataset/preprocessed/test-data/document_classification/12,"To the best of our knowledge , this is the first study to model a whole corpus as a heterogeneous graph and learn word and document embeddings with graph neural networks jointly .",31,0,33
dataset/preprocessed/test-data/document_classification/12,"Results on several benchmark datasets demonstrate that our method outperforms state - of - the - art text classification methods , without using pre-trained word embeddings or external knowledge .",32,0,30
dataset/preprocessed/test-data/document_classification/12,Our method also learn predictive word and document embeddings automatically .,33,0,11
dataset/preprocessed/test-data/document_classification/12,Related Work Traditional Text Classification,34,0,5
dataset/preprocessed/test-data/document_classification/12,Traditional text classification studies mainly focus on feature engineering and classification algorithms .,35,0,13
dataset/preprocessed/test-data/document_classification/12,"For feature engineering , the most commonly used feature is the bagof - words feature .",36,0,16
dataset/preprocessed/test-data/document_classification/12,"In addition , some more complex features have been designed , such as n-grams and entities in ontologies ) .",37,0,20
dataset/preprocessed/test-data/document_classification/12,There are also existing studies on converting texts to graphs and perform feature engineering on graphs and subgraphs .,38,0,19
dataset/preprocessed/test-data/document_classification/12,"Unlike these methods , our method can learn text representations as node embeddings automatically .",39,0,15
dataset/preprocessed/test-data/document_classification/12,Deep Learning for Text Classification,40,0,5
dataset/preprocessed/test-data/document_classification/12,Deep learning text classification studies can be categorized into two groups .,41,0,12
dataset/preprocessed/test-data/document_classification/12,One group of studies focused on models based on word embeddings .,42,0,12
dataset/preprocessed/test-data/document_classification/12,Several recent studies showed that the success of deep learning on text classification largely depends on the effectiveness of the word embeddings .,43,0,23
dataset/preprocessed/test-data/document_classification/12,Some authors aggregated unsupervised word embeddings as document embeddings then fed these document embeddings into a classifier .,44,0,18
dataset/preprocessed/test-data/document_classification/12,Others jointly learned word / document and document label embeddings ) .,45,0,12
dataset/preprocessed/test-data/document_classification/12,"Our work is connected to these methods , the major difference is that these methods build text representations after learning word embeddings while we learn word and document embeddings simultaneously for text classification .",46,0,34
dataset/preprocessed/test-data/document_classification/12,Another group of studies employed deep neural networks .,47,0,9
dataset/preprocessed/test-data/document_classification/12,Two representative deep networks are CNN and RNN .,48,0,9
dataset/preprocessed/test-data/document_classification/12,( Kim 2014 ) used CNN for sentence classification .,49,0,10
dataset/preprocessed/test-data/document_classification/2,Rethinking Complex Neural Network Architectures for Document Classification,2,1,8
dataset/preprocessed/test-data/document_classification/2,"Neural network models for many NLP tasks have grown increasingly complex in recent years , making training and deployment more difficult .",4,0,22
dataset/preprocessed/test-data/document_classification/2,"A number of recent papers have questioned the necessity of such architectures and found that well - executed , simpler models are quite effective .",5,0,25
dataset/preprocessed/test-data/document_classification/2,"We show that this is also the case for document classification : in a large - scale reproducibility study of several recent neural models , we find that a simple BiLSTM architecture with appropriate regularization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets .",6,0,57
dataset/preprocessed/test-data/document_classification/2,"Surprisingly , our simple model is able to achieve these results without attention mechanisms .",7,0,15
dataset/preprocessed/test-data/document_classification/2,"While these regularization techniques , borrowed from language modeling , are not novel , to our knowledge we are the first to apply them in this context .",8,0,28
dataset/preprocessed/test-data/document_classification/2,Our work provides an opensource platform and the foundation for future work in document classification .,9,0,16
dataset/preprocessed/test-data/document_classification/2,Recent developments in neural architectures for a wide range of NLP tasks can be characterized as a drive towards increasingly complex network components and modeling techniques .,11,0,27
dataset/preprocessed/test-data/document_classification/2,"Worryingly , these new models are accompanied by smaller and smaller improvements in effectiveness on standard benchmark datasets , which leads us to wonder if observed improvements are "" real "" .",12,0,32
dataset/preprocessed/test-data/document_classification/2,"There is , however , ample evidence to the contrary .",13,0,11
dataset/preprocessed/test-data/document_classification/2,To provide a few examples : report that standard LSTM architectures outperform more recent models when properly tuned .,14,0,19
dataset/preprocessed/test-data/document_classification/2,"show that sequence transduction using encoder - decoder networks with attention mechanisms work just as well with the attention module only , making most of the complex * Equal contribution .",15,0,31
dataset/preprocessed/test-data/document_classification/2,neural machinery unnecessary .,16,0,4
dataset/preprocessed/test-data/document_classification/2,show that simple RNN - and CNN - based models yield accuracies rivaling far more complex architectures in simple question answering over knowledge graphs .,17,0,25
dataset/preprocessed/test-data/document_classification/2,"Perhaps most damning are the indictments of , who lament the lack of empirical rigor in our field and cite even more examples where improvements can be attributed to far more mundane reasons ( e.g. , hyperparameter tuning ) or are simply noise .",18,0,44
dataset/preprocessed/test-data/document_classification/2,"concur with these sentiments , adding that authors often use fancy mathematics to obfuscate or to impress ( reviewers ) rather than to clarify .",19,0,25
dataset/preprocessed/test-data/document_classification/2,"Complex architectures are more difficult to train , more sensitive to hyperparameters , and brittle with respect to domains with different data characteristics - thus both exacerbating the "" crisis of reproducibility "" and making it difficult for practitioners to deploy networks that tackle real - world problems in production environments .",20,0,52
dataset/preprocessed/test-data/document_classification/2,"Like the papers cited above , we question the need for overly complex neural architectures , focusing on the problem of document classification .",21,0,24
dataset/preprocessed/test-data/document_classification/2,"Starting with a large - scale reproducibility study of several recent neural models , we find that a simple bi-directional LSTM ( BiLSTM ) architecture with appropriate regularization yields accuracy and F 1 that are either competitive or exceed the state of the art on four standard benchmark datasets .",22,0,50
dataset/preprocessed/test-data/document_classification/2,"As the closest comparison point , we find no benefit to the hierarchical modeling proposed by and we are able to achieve good classification results without attention mechanisms .",23,0,29
dataset/preprocessed/test-data/document_classification/2,"While these regularization techniques , borrowed from language modeling , are not novel , we are to our knowledge the first to apply them in this context .",24,0,28
dataset/preprocessed/test-data/document_classification/2,Our work provides an opensource platform and the foundation for future work in document classification .,25,0,16
dataset/preprocessed/test-data/document_classification/2,Background and Related Work,26,0,4
dataset/preprocessed/test-data/document_classification/2,"Over the last few years , deep neural networks have achieved the state of the art in document classification .",28,0,20
dataset/preprocessed/test-data/document_classification/2,"One popular model , hierarchical attention network ( HAN ) , uses word - and sentence - level attention in classifying documents .",29,0,23
dataset/preprocessed/test-data/document_classification/2,"Although this model nicely captures the intuition that modeling word sequences in sentences should be handled separately from sentence - level discourse modeling , one wonders if such complex architectures are really necessary , especially given the size of training data available today .",30,0,44
dataset/preprocessed/test-data/document_classification/2,"An important variant of document classification is the multi-label , multi-class case .",31,0,13
dataset/preprocessed/test-data/document_classification/2,"develop XML - CNNs for multi-label text classification , basing the architecture on with increased filter sizes and an additional fully - connected layer .",32,0,25
dataset/preprocessed/test-data/document_classification/2,They also incorporate dynamic adaptive max - pooling instead of the vanilla max - pooling overtime in KimCNN .,33,0,19
dataset/preprocessed/test-data/document_classification/2,"The paper compares with CNN - based approaches for the multi-label task , but only reports precision and disregards recall .",34,0,21
dataset/preprocessed/test-data/document_classification/2,instead adopts encoder - decoder sequence generation models ( SGMs ) for generating multiple labels for each document .,35,0,19
dataset/preprocessed/test-data/document_classification/2,"Similar to our critique of HAN , we opine against the high complexity of these multi-label approaches .",36,0,18
dataset/preprocessed/test-data/document_classification/2,There have been attempts to extend dropout from feedforward neural networks to recurrent ones .,38,0,15
dataset/preprocessed/test-data/document_classification/2,"Unfortunately , direct application of dropout on the hidden units of an RNN empirically harms its ability to retain longterm information .",39,0,22
dataset/preprocessed/test-data/document_classification/2,"Recently , however , successfully apply dropout - like techniques to regularize RNNs for language modeling , achieving competitive word - level perplexity on multiple datasets .",40,0,27
dataset/preprocessed/test-data/document_classification/2,"Inspired by this development , we adopt two of their regularization techniques , embedding dropout and weight - dropped LSTMs , to our task of document classification .",41,0,28
dataset/preprocessed/test-data/document_classification/2,Weight - dropped LSTM .,42,0,5
dataset/preprocessed/test-data/document_classification/2,"LSTMs comprise eight total input-hidden and hidden - hidden weight matrices ; in weight dropping , regularize the four hidden - hidden matrices with DropConnect .",43,0,26
dataset/preprocessed/test-data/document_classification/2,"The operation is applied only once per sequence , using the same dropout mask across multiple timesteps .",44,0,18
dataset/preprocessed/test-data/document_classification/2,"Conveniently , this allows practitioners to use fast , out - of the - box LSTM implementations without affecting the RNN formulation or training performance .",45,0,26
dataset/preprocessed/test-data/document_classification/2,Embedding Dropout .,46,0,3
dataset/preprocessed/test-data/document_classification/2,"Introduced in and successfully employed for neural language modeling , embedding dropout performs dropout on entire word embeddings , effectively removing some of the words at each training iteration .",47,0,30
dataset/preprocessed/test-data/document_classification/2,"As a result , the technique conditions the model to be robust against missing input ; for document classification , this discourages the model from relying on a small set of input words for prediction .",48,0,36
dataset/preprocessed/test-data/document_classification/4,Joint Embedding of Words and Labels for Text Classification,2,1,9
dataset/preprocessed/test-data/document_classification/4,"Word embeddings are effective intermediate representations for capturing semantic regularities between words , when learning the representations of text sequences .",4,0,21
dataset/preprocessed/test-data/document_classification/4,We propose to view text classification as a label - word joint embedding problem : each label is embedded in the same space with the word vectors .,5,0,28
dataset/preprocessed/test-data/document_classification/4,We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels .,6,0,17
dataset/preprocessed/test-data/document_classification/4,"The attention is learned on a training set of labeled samples to ensure that , given a text sequence , the relevant words are weighted higher than the irrelevant ones .",7,0,31
dataset/preprocessed/test-data/document_classification/4,"Our method maintains the interpretability of word embeddings , and enjoys a built - in ability to leverage alternative sources of information , in addition to input text sequences .",8,0,30
dataset/preprocessed/test-data/document_classification/4,"Extensive results on the several large text datasets show that the proposed framework outperforms the state - of - the - art methods by a large margin , in terms of both accuracy and speed .",9,0,36
dataset/preprocessed/test-data/document_classification/4,U B Q r rd W a j 2 Y 9 N 2 Mb D h P x o TD / 4 R L x 6 0 u 3 IQ c J K m L + + 9 a W em D W L O j PX 9 D 6 + w s r q 2,10,0,56
dataset/preprocessed/test-data/document_classification/4,J A W sq J M Z 2 6 H 9 t e Sr R ll I M 7 M z E Q E / p MB t B x U B I B pp f m M 5 n i Y 8 e E O FL a L W l x z v 7 NS I kw W X 3 O 6 Z o em k Ut I / / T O om N Ln op k 3 F i Q d L f i 6 K E Y 6 t w N m Ac Mg 3 U 8 r E D h Gr ma s V 0 SD S h 1 j 1 Dy Q 2 h v t j y M mi d 1 i 5 r / u 1 Z t X E 1 m 0 Y R Ha I j d IL q 6 B w 1 0 A 1 q oh a i 6 B V 9 om 8 P e e / e U B Q r rd W a j 2 Y 9 N 2 Mb D h P x o TD / 4 R L x 6 0 u 3 IQ c J K m L + + 9 a W em D W L O j PX 9 D 6 + w s r q 2 v l H c LG 2 Vt 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G OJ P Q s s x ya M ca i Ag 4 PA b P 1 5 n + O A J t m J L 3 d h x D T 5 CB Z B G j x D q q X 3 n q,11,0,302
dataset/preprocessed/test-data/document_classification/4,Sn i h S g g i w 7 R 7 J 4 id pm k 3 i PD dd F q a 0 w K w Z J S Li o d m L N y G c 3 L R O F l 0 T T L H Z NS v VP 2 an w d e B v U Z q K J Z NP u Vt 2 6 o a C J A W sq J M Z 2 6 H 9 t e Sr R ll I M 7 M z E Q E / p MB t B x U B I B pp f m M 5 n i Y 8 e E O FL a L W l x z v 7 NS I kw W X 3 O 6 Z o em k Ut I / / T O om N Ln op k 3 F i Q d L f i 6 K E Y 6 t w N m Ac Mg 3 U 8 r E D h Gr ma s V 0 SD S h 1 j 1 Dy Q 2 h v t j y M mi d 1 i 5 r / u 1 Z t X E 1 m 0 Y R Ha I j d IL q 6 B w 1 0 A 1 q oh a i 6 B V 9 om 8 P e e / e U B Q r rd W a j 2 Y 9 N 2 Mb D h P x o TD / 4 R L x 6 0 u 3 IQ c J K m L + + 9 a W em D W L O j PX 9 D 6 + w s r q 2 v l H c LG 2 Vt 3 d 2 K 3 v 7 D 0 Y l m k K L K q 5 0 O y A G OJ P Q s s x ya M ca i Ag 4 PA b P 1 5 n + O A J t m J L 3 d h x D T 5 CB Z B G j x D q q X 3 n q,12,0,377
dataset/preprocessed/test-data/document_classification/4,Sn i h S g g i w 7 R 7 J 4 id pm k 3 i PD dd F q a 0 w K w Z J S Li o d m L N y G c 3 L R O F l 0 T T L H Z NS v VP 2 an w d e B v U Z q K J Z NP u Vt 2 6 o a C J A W sq J M Z 2 6 H 9 t e Sr R ll I M 7 M z E Q E / p MB t B x U B I B pp f m M 5 n i Y 8 e E O FL a L W l x z v 7 NS I kw W X 3 O 6 Z o em k Ut I / / T O om N Ln op k 3 F i Q d L f i 6 K E Y 6 t w N m Ac Mg 3 U 8 r E D h Gr ma s V 0 SD S h 1 j 1 Dy Q 2 h v t j y M mi d 1 i 5 r / u 1 Z t X E 1 m 0 Y R Ha I j d IL q 6 B w 1 0 A 1 q oh a i 6 B V 9 om 8 P e e / e,13,0,245
dataset/preprocessed/test-data/document_classification/4,Text classification is a fundamental problem in natural language processing ( NLP ) .,15,0,14
dataset/preprocessed/test-data/document_classification/4,The task is to annotate a given text sequence with one ( or multiple ) class label ( s ) describing its textual content .,16,0,25
dataset/preprocessed/test-data/document_classification/4,A key intermediate step is the text representation .,17,0,9
dataset/preprocessed/test-data/document_classification/4,"Traditional methods represent text with hand - crafted features , such as sparse lexical features ( e.g. , n-grams ) .",18,0,21
dataset/preprocessed/test-data/document_classification/4,"Recently , neural models have been employed to learn text representations , including convolutional neural networks ( CNNs ) and recurrent neural networks ( RNNs ) based on long short - term memory ( LSTM ) .",19,0,37
dataset/preprocessed/test-data/document_classification/4,"To further increase the representation flexibility of such models , attention mechanisms have been introduced as an integral part of models employed for text classification .",20,0,26
dataset/preprocessed/test-data/document_classification/4,"The attention module is trained to capture the dependencies that make significant contributions to the task , regardless of the distance between the elements in the sequence .",21,0,28
dataset/preprocessed/test-data/document_classification/4,It can thus provide complementary information to the distance - aware dependencies modeled by RNN / CNN .,22,0,18
dataset/preprocessed/test-data/document_classification/4,The increasing representation power of the attention mechanism comes with increased model complexity .,23,0,14
dataset/preprocessed/test-data/document_classification/4,"Alternatively , several recent studies show that the success of deep learning on text classification largely depends on the effectiveness of the word embeddings .",24,0,25
dataset/preprocessed/test-data/document_classification/4,"Particularly , quantitatively show that the word - embeddings - based text classification tasks can have the similar level of difficulty regardless of the employed models , using the concept of intrinsic dimension .",25,0,34
dataset/preprocessed/test-data/document_classification/4,"Thus , simple models are preferred .",26,0,7
dataset/preprocessed/test-data/document_classification/4,"As the basic building blocks in neural - based NLP , word embeddings capture the similarities / regularities between words .",27,0,21
dataset/preprocessed/test-data/document_classification/4,"This idea has been extended to compute embeddings that capture the semantics of word sequences ( e.g. , phrases , sentences , paragraphs and documents ) .",28,0,27
dataset/preprocessed/test-data/document_classification/4,"These representations are built upon various types of compositions of word vectors , ranging from simple averaging to sophisticated architectures .",29,0,21
dataset/preprocessed/test-data/document_classification/4,"Further , they suggest that simple models are efficient and interpretable , and have the poten - ar Xiv : 1805.04174v1 [ cs. CL ] 10 May 2018 tial to outperform sophisticated deep neural models .",30,0,36
dataset/preprocessed/test-data/document_classification/4,"It is therefore desirable to leverage the best of both lines of works : learning text representations to capture the dependencies that make significant contributions to the task , while maintaining low computational cost .",31,0,35
dataset/preprocessed/test-data/document_classification/4,"For the task of text classification , labels play a central role of the final performance .",32,0,17
dataset/preprocessed/test-data/document_classification/4,A natural question to ask is how we can directly use label information in constructing the text - sequence representations .,33,0,21
dataset/preprocessed/test-data/document_classification/4,"Our primary contribution is therefore to propose such a solution by making use of the label embedding framework , and propose the Label - Embedding Attentive Model ( LEAM ) to improve text classification .",35,0,35
dataset/preprocessed/test-data/document_classification/4,"While there is an abundant literature in the NLP community on word embeddings ( how to describe a word ) for text representations , much less work has been devoted in comparison to label embeddings ( how to describe a class ) .",36,0,43
dataset/preprocessed/test-data/document_classification/4,"The proposed LEAM is implemented by jointly embedding the word and label in the same latent space , and the text representations are constructed directly using the text - label compatibility .",37,0,32
dataset/preprocessed/test-data/document_classification/4,"Our label embedding framework has the following salutary properties : ( i ) Label - attentive text representation is informative for the downstream classification task , as it directly learns from a shared joint space , whereas traditional methods proceed in multiple steps by solving intermediate problems .",38,0,48
dataset/preprocessed/test-data/document_classification/4,"( ii ) The LEAM learning procedure only involves a series of basic algebraic operations , and hence it retains the interpretability of simple models , especially when the label description is available .",39,0,34
dataset/preprocessed/test-data/document_classification/4,"( iii ) Our attention mechanism ( derived from the text - label compatibility ) has fewer parameters and less computation than related methods , and thus is much cheaper in both training and testing , compared with sophisticated deep attention models .",40,0,43
dataset/preprocessed/test-data/document_classification/4,"( iv ) We perform extensive experiments on several text - classification tasks , demonstrating the effectiveness of our label - embedding attentive model , providing state - of - the - art results on benchmark datasets .",41,0,38
dataset/preprocessed/test-data/document_classification/4,( v ) We further apply LEAM to predict the medical codes from clinical text .,42,0,16
dataset/preprocessed/test-data/document_classification/4,"As an interesting by - product , our attentive model can highlight the informative key words for prediction , which in practice can reduce a doctor 's burden on reading clinical notes .",43,0,33
dataset/preprocessed/test-data/document_classification/4,Label embedding has been shown to be effective in various domains and tasks .,45,0,14
dataset/preprocessed/test-data/document_classification/4,"In computer vision , there has been avast amount of research on leveraging label embeddings for image classification , multimodal learning between images and text , and text recognition in images .",46,0,32
dataset/preprocessed/test-data/document_classification/4,"It is particularly successful on the task of zero - shot learning , where the label correlation captured in the embedding space can improve the prediction when some classes are unseen .",47,0,32
dataset/preprocessed/test-data/document_classification/4,"In NLP , labels embedding for text classification has been studied in the context of heterogeneous networks in and multitask learning in , respectively .",48,0,25
dataset/preprocessed/test-data/document_classification/4,"To the authors ' knowledge , there is little research on investigating the effectiveness of label embeddings to design efficient attention models , and how to joint embedding of words and labels to make full use of label information for text classification has not been studied previously , representing a contribution of this paper .",49,0,55
dataset/preprocessed/test-data/document_classification/13,Deep Pyramid Convolutional Neural Networks for Text Categorization,2,1,8
dataset/preprocessed/test-data/document_classification/13,This paper proposes a low - complexity word - level deep convolutional neural network ( CNN ) architecture for text categorization that can efficiently represent longrange associations in text .,4,0,30
dataset/preprocessed/test-data/document_classification/13,"In the literature , several deep and complex neural networks have been proposed for this task , assuming availability of relatively large amounts of training data .",5,0,27
dataset/preprocessed/test-data/document_classification/13,"However , the associated computational complexity increases as the networks go deeper , which poses serious challenges in practical applications .",6,0,21
dataset/preprocessed/test-data/document_classification/13,"Moreover , it was shown recently that shallow word - level CNNs are more accurate and much faster than the state - of - the - art very deep nets such as character - level CNN s even in the setting of large training data .",7,0,46
dataset/preprocessed/test-data/document_classification/13,"Motivated by these findings , we carefully studied deepening of word - level CNNs to capture global representations of text , and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much .",8,0,47
dataset/preprocessed/test-data/document_classification/13,We call it deep pyramid CNN .,9,0,7
dataset/preprocessed/test-data/document_classification/13,The proposed model with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic categorization .,10,0,23
dataset/preprocessed/test-data/document_classification/13,"Text categorization is an important task whose applications include spam detection , sentiment classification , and topic classification .",12,0,19
dataset/preprocessed/test-data/document_classification/13,"In recent years , neural networks that can make use of word order have been shown to be effective for text categorization .",13,0,23
dataset/preprocessed/test-data/document_classification/13,"While simple and shallow convolutional neural networks ( CNNs ) ) were proposed for this task earlier , more recently , deep and more complex neural networks have also been studied , assuming availability of relatively large amounts of training data ( e.g. , one million documents ) .",14,0,49
dataset/preprocessed/test-data/document_classification/13,"Examples are deep character - level CNNs , a complex combination of CNNs and recurrent neural networks ( RNNs ) , and RNNs in a wordsentence hierarchy .",15,0,28
dataset/preprocessed/test-data/document_classification/13,A CNN is a feedforward network with convolution layers interleaved with pooling layers .,16,0,14
dataset/preprocessed/test-data/document_classification/13,"Essentially , a convolution layer converts to a vector every small patch of data ( either the original data such as text or image or the output of the previous layer ) at every location ( e.g. , 3 - word windows around every word ) , which can be processed in parallel .",17,0,54
dataset/preprocessed/test-data/document_classification/13,"By contrast , an RNN has connections that form a cycle .",18,0,12
dataset/preprocessed/test-data/document_classification/13,"In its typical application to text , a recurrent unit takes words one by one as well as its own output on the previous word , which is parallel - processing unfriendly .",19,0,33
dataset/preprocessed/test-data/document_classification/13,"While both CNNs and RNNs can take advantage of word order , the simple nature and parallel - processing friendliness of CNNs make them attractive particularly when large training data causes computational challenges .",20,0,34
dataset/preprocessed/test-data/document_classification/13,There have been several recent studies of CNN for text categorization in the large training data setting .,21,0,18
dataset/preprocessed/test-data/document_classification/13,"For example , in , very deep 32 - layer character - level CNNs were shown to outperform deep 9 - layer character - level CNNs of .",22,0,28
dataset/preprocessed/test-data/document_classification/13,"However , in , very shallow 1 - layer word - level CNNs were shown to be more accurate and much faster than the very deep characterlevel CNNs of .",23,0,30
dataset/preprocessed/test-data/document_classification/13,"Although character - level approaches have merit in not having to deal with millions of distinct words , shallow word - level CNNs turned out to be superior even when used with only a manageable number ( 30 K ) of the most frequent words .",24,0,46
dataset/preprocessed/test-data/document_classification/13,This demonstrates the basic fact - knowledge of word leads to a powerful representation .,25,0,15
dataset/preprocessed/test-data/document_classification/13,These results motivate us to pursue an effective and efficient design of deep wordlevel CNNs for text categorization .,26,0,19
dataset/preprocessed/test-data/document_classification/13,"Note , however , that it is not as simple as merely replacing characters with words in character - level CNNs ; doing so rather degraded accuracy in .",27,0,29
dataset/preprocessed/test-data/document_classification/13,We carefully studied deepening of word - level CNNs in the large - data setting and found a deep but low - complexity network architecture with which the best accuracy can be obtained by increasing the depth but not the order of computation time - the total computation time is bounded by a constant .,28,0,55
dataset/preprocessed/test-data/document_classification/13,"We call it deep pyramid CNN ( DPCNN ) , as the computation time per layer decreases exponentially in a ' pyramid shape ' .",29,0,25
dataset/preprocessed/test-data/document_classification/13,"After converting discrete text to continuous representation , the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size ( as well as per-layer computation ) shrinks in a pyramid shape .",30,0,48
dataset/preprocessed/test-data/document_classification/13,The network depth can be treated as a meta-parameter .,31,0,10
dataset/preprocessed/test-data/document_classification/13,The computational complexity of this network is bounded to be no more than twice that of one convolution block .,32,0,20
dataset/preprocessed/test-data/document_classification/13,"At the same time , as described later , the ' pyramid ' enables efficient discovery of long - range associations in the text ( and so more global information ) , as the network is deepened .",33,0,38
dataset/preprocessed/test-data/document_classification/13,"This is why DPCNN can achieve better accuracy than the shallow CNN mentioned above ( hereafter ShallowCNN ) , which can use only short - range associations .",34,0,28
dataset/preprocessed/test-data/document_classification/13,"Moreover , DPCNN can be regarded as a deep extension of ShallowCNN , which we proposed in and later tested with large datasets in .",35,0,25
dataset/preprocessed/test-data/document_classification/13,We show that DPCNN with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic classification ..,36,0,24
dataset/preprocessed/test-data/document_classification/13,"The first layer performs text region embedding , which generalizes commonly used word embedding to the embedding of text regions covering one or more words .",37,0,26
dataset/preprocessed/test-data/document_classification/13,It is followed by stacking of convolution blocks ( two convolution layers and a shortcut ) interleaved with pooling layers with stride 2 for downsampling .,38,0,26
dataset/preprocessed/test-data/document_classification/13,The final pooling layer aggregates internal data for each document into one vector .,39,0,14
dataset/preprocessed/test-data/document_classification/13,We use max pooling for all pooling layers .,40,0,9
dataset/preprocessed/test-data/document_classification/13,The key features of DPCNN are as follows .,41,0,9
dataset/preprocessed/test-data/document_classification/13,"Downsampling without increasing the number of feature maps ( dimensionality of layer output , 250 in ) .",42,0,18
dataset/preprocessed/test-data/document_classification/13,Downsampling enables efficient representation of long - range associations ( and so more global information ) in the text .,43,0,20
dataset/preprocessed/test-data/document_classification/13,"By keeping the same number of feature maps , every 2 - stride downsampling reduces the per-block computation by half and thus the total computation time is bounded by a constant .",44,0,32
dataset/preprocessed/test-data/document_classification/13,Shortcut connections with pre-activation and identity mapping for enabling training of deep networks .,45,0,14
dataset/preprocessed/test-data/document_classification/13,Text region embedding enhanced with unsupervised embeddings ( embeddings trained in an unsupervised manner ) for improving accuracy .,46,0,19
dataset/preprocessed/test-data/document_classification/13,Downsampling with the number of feature maps fixed,48,0,8
dataset/preprocessed/test-data/document_classification/13,"After each convolution block , we perform max - pooling with size 3 and stride",49,0,15
dataset/preprocessed/test-data/document_classification/20,Practical Text Classification With Large Pre-Trained Language Models,2,1,8
dataset/preprocessed/test-data/document_classification/20,Multi-emotion sentiment classification is a natural language processing ( NLP ) problem with valuable use cases on realworld data .,4,0,20
dataset/preprocessed/test-data/document_classification/20,"We demonstrate that large - scale unsupervised language modeling combined with finetuning offers a practical solution to this task on difficult datasets , including those with label class imbalance and domain - specific context .",5,0,35
dataset/preprocessed/test-data/document_classification/20,"By training an attention - based Transformer network ( Vaswani et al. 2017 ) on 40 GB of text ( Amazon reviews ) ( McAuley et al. 2015 ) and fine - tuning on the training set , our model achieves a 0.69 F1 score on the SemEval Task 1:E - c multidimensional emotion classification problem ( Mohammad et al. 2018 ) , based on the Plutchik wheel of emotions ( Plutchik 1979 ) .",6,0,75
dataset/preprocessed/test-data/document_classification/20,"These results are competitive with state of the art models , including strong F 1 scores on difficult ( emotion ) categories such as Fear ( 0.73 ) , Disgust ( 0.77 ) and Anger ( 0.78 ) , as well as competitive results on rare categories such as Anticipation ( 0.42 ) and Surprise ( 0.37 ) .",7,0,59
dataset/preprocessed/test-data/document_classification/20,"Furthermore , we demonstrate our application on a real world text classification task .",8,0,14
dataset/preprocessed/test-data/document_classification/20,"We create a narrowly collected text dataset of real tweets on several topics , and show that our finetuned model outperforms general purpose commercially available APIs for sentiment and multidimensional emotion classification on this dataset by a significant margin .",9,0,40
dataset/preprocessed/test-data/document_classification/20,"We also perform a variety of additional studies , investigating properties of deep learning architectures , datasets and algorithms for achieving practical multidimensional sentiment classification .",10,0,26
dataset/preprocessed/test-data/document_classification/20,"Overall , we find that unsupervised language modeling and finetuning is a simple framework for achieving high quality results on realworld sentiment classification .",11,0,24
dataset/preprocessed/test-data/document_classification/20,"Recent work has shown that language models - both RNN variants like the multiplicative LSTM ( m LSTM ) , as well as the attention - based Transformer network ) - can be trained efficiently over very large datasets , and that the resulting models can be transferred to downstream language understanding problems , often matching or exceeding the previous state of the art approaches on academic datasets .",13,0,69
dataset/preprocessed/test-data/document_classification/20,"However , how well do these models perform on practical text classification problems , with real world data ?",14,0,19
dataset/preprocessed/test-data/document_classification/20,"Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",15,0,15
dataset/preprocessed/test-data/document_classification/20,All rights reserved .,16,0,4
dataset/preprocessed/test-data/document_classification/20,"In this work , we train both mLSTM and Transformer language models on a large 40 GB text dataset , then transfer those models to two text classification problems : binary sentiment ( including Neutral labels ) , and multidimensional emotion classification based on the Plutchik wheel of emotions .",17,0,50
dataset/preprocessed/test-data/document_classification/20,"We examine our performance on these tasks , both against large academic datasets , and on an original text dataset that we compiled from social media messages about several specific topics , such as video games .",18,0,37
dataset/preprocessed/test-data/document_classification/20,We demonstrate that our approach matches the state of the art on the academic datasets without domain - specific training and without excessive hyper - parameter tuning .,19,0,28
dataset/preprocessed/test-data/document_classification/20,"Meanwhile on the social media dataset , our approach outperforms commercially available APIs by significant margins , even when those models are re-calibrated to the test set .",20,0,28
dataset/preprocessed/test-data/document_classification/20,"Furthermore , we notice that 1 ) the Transformer model generally out - performs the m LSTM model , especially when fine - tuning on multidimensional emotion classification , and 2 ) fine - tuning the model significantly improves performance on the emotion tasks , both for the m LSTM and the Transformer model .",21,0,55
dataset/preprocessed/test-data/document_classification/20,"We suggest that our approach creates models with good generalization to increasingly difficult text classification problems , and we offer ablation studies to demonstrate that effect .",22,0,27
dataset/preprocessed/test-data/document_classification/20,"It is difficult to fit a single model for text classification across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .",23,0,31
dataset/preprocessed/test-data/document_classification/20,"For example , words such as war and sick are not necessarily negative in the context of video games , which are significantly represented in our dataset .",24,0,28
dataset/preprocessed/test-data/document_classification/20,"By training a language model across a large text dataset , we expose our model to many contexts .",25,0,19
dataset/preprocessed/test-data/document_classification/20,Perhaps a small amount of downstream transfer is enough to choose the right context features for emotion classification in the appropriate setting .,26,0,23
dataset/preprocessed/test-data/document_classification/20,"Our work shows that unsupervised language modeling combined with finetuning offers a practical solution to specialized text classification problems , including those with large category class imbalance , and significant human label dis agreement .",27,0,35
dataset/preprocessed/test-data/document_classification/20,Supervised learning is difficult to apply to NLP problems because labels are expensive .,29,0,14
dataset/preprocessed/test-data/document_classification/20,"Following , and , we train unsupervised text models on large amounts of unlabelled text data , and transfer the model features to small supervised text problems .",30,0,28
dataset/preprocessed/test-data/document_classification/20,The supervised text classification problem used for transfer is binary sentiment on the Stanford Sentiment Treebank ( SST ) .,31,0,20
dataset/preprocessed/test-data/document_classification/20,Some of these binary text examples are subtle .,32,0,9
dataset/preprocessed/test-data/document_classification/20,"Prior works show that unsupervised language models can learn nuanced features of text , such as word ordering and double negation , just from the underlying task of next - word prediction .",33,0,33
dataset/preprocessed/test-data/document_classification/20,"However , while this includes difficult examples , it does not necessarily represent sentiment on practical text problems .",34,0,19
dataset/preprocessed/test-data/document_classification/20,The source material ( professionally written movie reviews ) does not include colloquial language .,35,0,15
dataset/preprocessed/test-data/document_classification/20,The dataset excludes Neutral sentiment texts and those with weak directional sentiment .,36,0,13
dataset/preprocessed/test-data/document_classification/20,The dataset does not include dimensions of sentiment apart from Positive and Negative .,37,0,14
dataset/preprocessed/test-data/document_classification/20,Plutchik 's Wheel of Emotions,38,0,5
dataset/preprocessed/test-data/document_classification/20,We focus our multidimension emotion classification on Plutchik 's wheel of emotions .,39,0,13
dataset/preprocessed/test-data/document_classification/20,"This taxonomy , in use since 1979 , aims to classify human emotions as a combination of four dualities :",40,0,20
dataset/preprocessed/test-data/document_classification/20,"Joy - Sadness , Anger - Fear , Trust - Disgust , and Surprise - Anticipation .",41,0,17
dataset/preprocessed/test-data/document_classification/20,"According to the basic emotion model , while humans experience hundreds of emotions , some emotions are more fundamental than others .",42,0,22
dataset/preprocessed/test-data/document_classification/20,"The commercial general purpose emotion classification API that we compare against , IBM 's Watson 1 , offers classification scores for the Joy , Sadness , Fear , Disgust and Anger emotions - all present in Plutchik 's wheel (.",43,0,40
dataset/preprocessed/test-data/document_classification/20,Sem Eval Multidimension Emotion Dataset,44,0,5
dataset/preprocessed/test-data/document_classification/20,"The Se-m Eval Task 1:E - c problem ) offers a training set of 6,857 tweets , with binary labels for the eight Plutchik categories , plus Optimism , Pessimism , and Love .",45,0,34
dataset/preprocessed/test-data/document_classification/20,This dataset was created through a process of text selection and human labeling .,46,0,14
dataset/preprocessed/test-data/document_classification/20,We show our results on this dataset and compare it to the current state of the art performance .,47,0,19
dataset/preprocessed/test-data/document_classification/20,"While it is not possible to report rater agreement on these categories for the compilation of the dataset , the authors note that 2 out of 7 raters had to agree for a positive label to be applied , as requiring larger agreement caused a scarcity of .",48,0,48
dataset/preprocessed/test-data/document_classification/20,labels for some categories .,49,0,5
dataset/preprocessed/test-data/face_alignment/7,Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network,2,1,12
dataset/preprocessed/test-data/face_alignment/7,We propose a straightforward method that simultaneously reconstructs the 3D facial structure and provides dense alignment .,4,0,17
dataset/preprocessed/test-data/face_alignment/7,"To achieve this , we design a 2D representation called UV position map which records the 3D shape of a complete face in UV space , then train a simple Convolutional Neural Network to regress it from a single 2D image .",5,0,42
dataset/preprocessed/test-data/face_alignment/7,We also integrate a weight mask into the loss function during training to improve the performance of the network .,6,0,20
dataset/preprocessed/test-data/face_alignment/7,"Our method does not rely on any prior face model , and can reconstruct full facial geometry along with semantic meaning .",7,0,22
dataset/preprocessed/test-data/face_alignment/7,"Meanwhile , our network is very light - weighted and spends only 9.8 ms to process an image , which is extremely faster than previous works .",8,0,27
dataset/preprocessed/test-data/face_alignment/7,Experiments on multiple challenging datasets show that our method surpasses other state - of - the - art methods on both reconstruction and alignment tasks by a large margin .,9,0,30
dataset/preprocessed/test-data/face_alignment/7,Code is available at https://github.com/YadiraF/PRNet.,10,0,5
dataset/preprocessed/test-data/face_alignment/7,3 D face reconstruction and face alignment are two fundamental and highly related topics in computer vision .,12,1,18
dataset/preprocessed/test-data/face_alignment/7,"In the last decades , researches in these two fields benefit each other .",13,0,14
dataset/preprocessed/test-data/face_alignment/7,"In the beginning , face alignment that aims at detecting a special 2D fiducial points is commonly used as a prerequisite for other facial tasks such as face recognition and assists 3 D face reconstruction to a great extent .",14,0,40
dataset/preprocessed/test-data/face_alignment/7,"However , researchers find that 2D alignment has difficulties in dealing with problems of large poses or occlusions .",15,0,19
dataset/preprocessed/test-data/face_alignment/7,"With the development of deep learning , many computer vision problems have been well solved by utilizing Convolution Neural Networks ( CNNs ) .",16,0,24
dataset/preprocessed/test-data/face_alignment/7,"Thus , some works start to use CNNs to estimate the 3D Morphable Model ( 3 DMM ) coefficients or 3D model warping functions to restore the corresponding 3D information from a single 2D facial image , which provides both dense face alignment and 3D face reconstruction results .",17,0,49
dataset/preprocessed/test-data/face_alignment/7,"However , the performance of these methods is restricted due to the limitation of the 3D space defined by face model basis or arXiv : 1803.07835v1 [ cs . CV ] 21 Mar 2018 templates .",18,0,36
dataset/preprocessed/test-data/face_alignment/7,The required operations including perspective projection or 3D Thin Plate Spline ( TPS ) transformation also add complexity to the overall process .,19,0,23
dataset/preprocessed/test-data/face_alignment/7,"Recently , two end - to - end works , which bypass the limitation of model space , achieve the state - of - the - art performances on their respective tasks .",20,0,33
dataset/preprocessed/test-data/face_alignment/7,"trains a complex network to regress 68 facial landmarks with 2D coordinates from a single image , but needs an extra network to estimate the depth value .",21,0,28
dataset/preprocessed/test-data/face_alignment/7,"Besides , dense alignment is not provided by this method .",22,0,11
dataset/preprocessed/test-data/face_alignment/7,develops a volumetric representation of 3 D face and uses a network to regress it from a 2D image .,23,0,20
dataset/preprocessed/test-data/face_alignment/7,"However , this representation discards the semantic meaning of points , thus the network needs to regress the whole volume in order to restore the facial shape , which is only part of the volume .",24,0,36
dataset/preprocessed/test-data/face_alignment/7,"So this representation limits the resolution of the recovered shape , and need a complex network to regress it .",25,0,20
dataset/preprocessed/test-data/face_alignment/7,"We find that these limitations do not exist in previous model - based methods , it motivates us to find anew approach to obtaining the 3D reconstruction and dense alignment simultaneously in a model - free manner .",26,0,38
dataset/preprocessed/test-data/face_alignment/7,"In this paper , we propose an end - to - end method called Position map Regression Network ( PRN ) to jointly predict dense alignment and reconstruct 3 D face shape .",27,0,33
dataset/preprocessed/test-data/face_alignment/7,Our method surpasses all other previous works on both 3 D face alignment and reconstruction on multiple datasets .,28,0,19
dataset/preprocessed/test-data/face_alignment/7,"Meanwhile , our method is straightforward with a very light - weighted model which provides the result in one pass with 9.8 ms .",29,0,24
dataset/preprocessed/test-data/face_alignment/7,All of these are achieved by the elaborate design of the 2D representation of 3 D facial structure and the corresponding loss function .,30,0,24
dataset/preprocessed/test-data/face_alignment/7,"Specifically , we design a UV position map , which is a 2D image recording the 3D coordinates of a complete facial point cloud , and at the same time keeping the semantic meaning at each UV place .",31,0,39
dataset/preprocessed/test-data/face_alignment/7,We then train a simple encoder - decoder network with a weighted loss that focuses more on discriminative region to regress the UV position map from a single 2 D facial image .,32,0,33
dataset/preprocessed/test-data/face_alignment/7,"Figure1 shows our method is robust to poses , illuminations and occlusions .",33,0,13
dataset/preprocessed/test-data/face_alignment/7,"In summary , our main contributions are :",34,0,8
dataset/preprocessed/test-data/face_alignment/7,"- For the first time , we solve the problems of face alignment and 3D face reconstruction together in an end - to - end fashion without the restriction of low - dimensional solution space .",35,0,36
dataset/preprocessed/test-data/face_alignment/7,"- To directly regress the 3D facial structure and dense alignment , we develop a novel representation called UV position map , which records the position information of 3 D face and provides dense correspondence to the semantic meaning of each point on UV space .",36,0,46
dataset/preprocessed/test-data/face_alignment/7,"- For training , we proposed a weight mask which assigns different weight to each point on position map and compute a weighted loss .",37,0,25
dataset/preprocessed/test-data/face_alignment/7,We show that this design helps improving the performance of our network .,38,0,13
dataset/preprocessed/test-data/face_alignment/7,- We finally provide a light - weighted framework that runs at over 100 FPS to directly obtain 3 D face reconstruction and alignment result from a single 2 D facial image .,39,0,33
dataset/preprocessed/test-data/face_alignment/7,- Comparison on the AFLW2000 - 3D and Florence datasets shows that our method achieves more than 25 % relative improvements over other stateof - the - art methods on both tasks of 3 D face reconstruction and dense face alignment .,40,0,42
dataset/preprocessed/test-data/face_alignment/7,"As described above , the main problems our method can solve are 3 D face reconstruction and face alignment .",42,0,20
dataset/preprocessed/test-data/face_alignment/7,We will talk about closely related works on these tasks in the following subsections .,43,0,15
dataset/preprocessed/test-data/face_alignment/7,"In this part , we evaluate our method on 3D face reconstruction task and compare with 3 DDFA , DeFA and VRN - Guided on AFLW2000 - 3D and Florence datasets .",46,0,32
dataset/preprocessed/test-data/face_alignment/7,We use the same set of points as in evaluating dense alignment and changes the metric so as to keep consistency with other 3 D face reconstruction evaluation methods .,47,0,30
dataset/preprocessed/test-data/face_alignment/7,"We first use Iterative Closest Points ( ICP ) algorithm to find the corresponding nearest points between the network output and ground truth point cloud , then calculate Mean Squared Error ( MSE ) normalized by outer interocular distance of 3D coordinates .",48,0,43
dataset/preprocessed/test-data/face_alignment/7,The result is shown in .,49,0,6
dataset/preprocessed/test-data/face_alignment/3,Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression,2,1,10
dataset/preprocessed/test-data/face_alignment/3,Heatmap regression with a deep network has become one of the mainstream approaches to localize facial landmarks .,4,0,18
dataset/preprocessed/test-data/face_alignment/3,"However , the loss function for heatmap regression is rarely studied .",5,0,12
dataset/preprocessed/test-data/face_alignment/3,"In this paper , we analyze the ideal loss function properties for heatmap regression in face alignment problems .",6,0,19
dataset/preprocessed/test-data/face_alignment/3,"Then we propose a novel loss function , named Adaptive Wing loss , that is able to adapt its shape to different types of ground truth heatmap pixels .",7,0,29
dataset/preprocessed/test-data/face_alignment/3,This adaptability penalizes loss more on foreground pixels while lesson background pixels .,8,0,13
dataset/preprocessed/test-data/face_alignment/3,"To address the imbalance between foreground and background pixels , we also propose Weighted Loss Map , which assigns high weights on foreground and difficult background pixels to help training process focus more on pixels that are crucial to landmark localization .",9,0,42
dataset/preprocessed/test-data/face_alignment/3,"To further improve face alignment accuracy , we introduce boundary prediction and CoordConv with boundary coordinates .",10,0,17
dataset/preprocessed/test-data/face_alignment/3,"Extensive experiments on different benchmarks , including COFW , 300 W and WFLW , show our approach outperforms the state - of - the - art by a significant margin on various evaluation metrics .",11,0,35
dataset/preprocessed/test-data/face_alignment/3,"Besides , the Adaptive Wing loss also helps other heatmap regression tasks .",12,0,13
dataset/preprocessed/test-data/face_alignment/3,Code will be made publicly available at https://github.com/protossw512/AdaptiveWingLoss.,13,0,8
dataset/preprocessed/test-data/face_alignment/3,"Face alignment , also known as facial landmark localization , seeks to localize pre-defined landmarks on human faces .",15,1,19
dataset/preprocessed/test-data/face_alignment/3,"Face alignment plays an essential role in many face related applications such as face recognition , face frontalization and 3D face reconstruction .",16,0,23
dataset/preprocessed/test-data/face_alignment/3,"In recent years , Convolutional Neural Network ( CNN ) based heatmap regression has become one of the mainstream approaches for face alignment problems and achieved considerable performance on frontal faces .",17,0,32
dataset/preprocessed/test-data/face_alignment/3,"However , landmarks on faces with large pose , occlusion and significant blur are still challenging to localize .",18,0,19
dataset/preprocessed/test-data/face_alignment/3,"Heatmap regression , which regresses a heatmap generated from landmark coordinates , is widely used for face alignment .",19,0,19
dataset/preprocessed/test-data/face_alignment/3,"In heatmap regression , the ground truth heatmap is generated by plotting a Gaussian distribution centered at each landmark on each channel .",20,0,23
dataset/preprocessed/test-data/face_alignment/3,The model regresses against the ground truth heatmap at pixel level and then use the predicted heatmaps to infer landmark locations .,21,0,22
dataset/preprocessed/test-data/face_alignment/3,"Prediction accuracy on foreground pixels ( pixels with positive values ) , especially the ones near the mode of each Gaussian distribution ( , is essential to accurately localize landmarks , even small prediction errors on these pixels can cause the prediction to shift from the correct modes .",22,0,49
dataset/preprocessed/test-data/face_alignment/3,"On the contrary , accurately predicting the values of background pixels ( pixels with zero values ) is less important , since small errors on these pixels will not affect landmark prediction inmost cases .",23,0,35
dataset/preprocessed/test-data/face_alignment/3,"However , prediction accuracy on difficult background pixels ( background pixels near foreground pixels ) are also important since they are often incorrectly regressed as foreground pixels and could cause inaccurate predictions .",24,0,33
dataset/preprocessed/test-data/face_alignment/3,"From this discussion , we locate two issues of the widely used Mean Square Error ( MSE ) loss in heatmap regression : i ) MSE is not sensitive to small errors , which hurts the capability to correctly locate the mode of the Gaussian dis-tribution ; ii ) During training all pixels have the same loss function and equal weights , however , background pixels absolutely dominates foreground pixels on a heatmap .",25,0,74
dataset/preprocessed/test-data/face_alignment/3,"As a result of i ) and ii ) , models trained with the MSE loss tend to predict a blurry and dilated heatmap with low intensity on foreground pixels compared to the ground truth ( .",26,0,37
dataset/preprocessed/test-data/face_alignment/3,This low quality heatmap could cause wrong estimation of facial landmarks .,27,0,12
dataset/preprocessed/test-data/face_alignment/3,"Wing loss is shown to be effective to improve coordinate regression , however , according to our experiment , it is not applicable for heatmap regression .",28,0,27
dataset/preprocessed/test-data/face_alignment/3,Small errors on background pixels will accumulate significant gradients and thus cause the training process to diverge .,29,0,18
dataset/preprocessed/test-data/face_alignment/3,"We thus propose a new loss function and name it Adaptive Wing loss ( Sec. , that is able to significantly improve the quality of heatmap regression results .",30,0,29
dataset/preprocessed/test-data/face_alignment/3,"Due to the translation invariance of the convolution operation in bottom - up and top - down CNN structures such as stacked Hourglass ( HG ) , the network is notable to capture coordinate information , which we believe is useful for facial landmark localization , since the structure of human faces is relatively stable .",31,0,56
dataset/preprocessed/test-data/face_alignment/3,"Inspired by the Coord - Conv layer proposed by Liu et al. , we encode into our model the full coordinate information and the information only on boundaries predicted from the previous HG module into our model .",32,0,38
dataset/preprocessed/test-data/face_alignment/3,The encoded coordinate information further improves the performance of our approach .,33,0,12
dataset/preprocessed/test-data/face_alignment/3,"To encode boundary coordinates , we also add a sub-task of boundary prediction by concatenating an additional boundary channel into the ground truth heatmap which is jointly trained with other channels .",34,0,32
dataset/preprocessed/test-data/face_alignment/3,"In summary , our main contributions include :",35,0,8
dataset/preprocessed/test-data/face_alignment/3,"Propose a novel loss function for heatmap regression named Adaptive Wing loss , that is able to adapt its curvature to ground truth pixel values .",36,0,26
dataset/preprocessed/test-data/face_alignment/3,"This adaptive property reduces small errors on foreground pixels for accurate landmark localization , while tolerates small errors on background pixels fora better convergence rate .",37,0,26
dataset/preprocessed/test-data/face_alignment/3,With proposed Weighted Loss Map it is also able to focus on foreground pixels and difficult background pixels during training .,38,0,21
dataset/preprocessed/test-data/face_alignment/3,"Encode coordinate information , including coordinates on boundary , into the face alignment algorithm using CoordConv .",39,0,17
dataset/preprocessed/test-data/face_alignment/3,"Our approach outperforms the state - of - the - art algorithms by a significant margin on mainstream face alignment datasets including 300W , COFW and WFLW .",40,0,28
dataset/preprocessed/test-data/face_alignment/3,We also show the validity of the Adaptive Wing loss in the human pose estimation task which also utilizes heatmap regression .,41,0,22
dataset/preprocessed/test-data/face_alignment/3,CNN based heatmap regression models leverage CNN to perform heatmap regression .,43,0,12
dataset/preprocessed/test-data/face_alignment/3,"In recent work , joint bottom - up and top - down architectures such as stacked HG were able to achieve the state - of - the - art performance .",44,0,31
dataset/preprocessed/test-data/face_alignment/3,Bulat et al .,45,0,4
dataset/preprocessed/test-data/face_alignment/3,"proposed a hierarchical , parallel and multi-scale block as a replacement for the original ResNet block to further improve the localization accuracy of HG .",46,0,25
dataset/preprocessed/test-data/face_alignment/3,Tang et al. was able to achieve current state - of - the - art with quantized densely connected U - Nets with fewer parameters than stacked HG models .,47,0,30
dataset/preprocessed/test-data/face_alignment/3,Other architectures are also able to achieve excellent performance .,48,0,10
dataset/preprocessed/test-data/face_alignment/3,Merget et al .,49,0,4
dataset/preprocessed/test-data/face_alignment/16,Deep Multi- Center Learning for Face Alignment,2,1,7
dataset/preprocessed/test-data/face_alignment/16,Facial landmarks are highly correlated with each other since a certain landmark can be estimated by its neighboring landmarks .,4,0,20
dataset/preprocessed/test-data/face_alignment/16,Most of the existing deep learning methods only use one fully - connected layer called shape prediction layer to estimate the locations of facial landmarks .,5,0,26
dataset/preprocessed/test-data/face_alignment/16,"In this paper , we propose a novel deep learning framework named Multi - Center Learning with multiple shape prediction layers for face alignment .",6,0,25
dataset/preprocessed/test-data/face_alignment/16,"In particular , each shape prediction layer emphasizes on the detection of a certain cluster of semantically relevant landmarks respectively .",7,0,21
dataset/preprocessed/test-data/face_alignment/16,"Challenging landmarks are focused firstly , and each cluster of landmarks is further optimized respectively .",8,0,16
dataset/preprocessed/test-data/face_alignment/16,"Moreover , to reduce the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .",9,0,26
dataset/preprocessed/test-data/face_alignment/16,Extensive experiments demonstrate that our method is effective for handling complex occlusions and appearance variations with real - time performance .,10,0,21
dataset/preprocessed/test-data/face_alignment/16,The code for our method is available at https://github.com/ZhiwenShao/MCNet-Extension .,11,0,10
dataset/preprocessed/test-data/face_alignment/16,"Index Terms - Multi- Center Learning , Model Assembling , Face Alignment * Corresponding author .",12,0,16
dataset/preprocessed/test-data/face_alignment/16,( a ),13,0,3
dataset/preprocessed/test-data/face_alignment/16,Chin is occluded .,14,0,4
dataset/preprocessed/test-data/face_alignment/16,( b ) Right contour is invisible .,15,0,8
dataset/preprocessed/test-data/face_alignment/16,"Face alignment refers to detecting facial landmarks such as eye centers , nose tip , and mouth corners .",17,0,19
dataset/preprocessed/test-data/face_alignment/16,"It is the preprocessor stage of many face analysis tasks like face animation , face beautification , and face recognition .",18,0,21
dataset/preprocessed/test-data/face_alignment/16,"A robust and accurate face alignment is still challenging in unconstrained scenarios , owing to severe occlusions and large appearance variations .",19,0,22
dataset/preprocessed/test-data/face_alignment/16,"Most conventional methods , , , only use low - level handcrafted features and are not based on the prevailing deep neural networks , which limits their capacity to represent highly complex faces .",20,0,34
dataset/preprocessed/test-data/face_alignment/16,"Recently , several methods use deep networks to estimate shapes from input faces .",21,0,14
dataset/preprocessed/test-data/face_alignment/16,"Sun et al. ,",22,0,4
dataset/preprocessed/test-data/face_alignment/16,"Zhou et al. , and Zhang et al .",23,0,9
dataset/preprocessed/test-data/face_alignment/16,employed cascaded deep networks to refine predicted shapes successively .,24,0,10
dataset/preprocessed/test-data/face_alignment/16,"Due to the use of multiple networks , these methods have high model complexity with complicated training processes .",25,0,19
dataset/preprocessed/test-data/face_alignment/16,"Taking this into account , Zhang et al. , proposed a Tasks - Constrained Deep Convolutional Network ( TCDCN ) , which uses only one deep network with excellent performance .",26,0,31
dataset/preprocessed/test-data/face_alignment/16,"However , it needs extra labels of facial attributes for training samples , which limits its universality .",27,0,18
dataset/preprocessed/test-data/face_alignment/16,Each facial landmark is not isolated but highly correlated with adjacent landmarks .,28,0,13
dataset/preprocessed/test-data/face_alignment/16,"As shown in , facial landmarks along the chin are all occluded , and landmarks around the mouth are partially occluded .",29,0,22
dataset/preprocessed/test-data/face_alignment/16,shows that landmarks on the right side of face are almost invisible .,30,0,13
dataset/preprocessed/test-data/face_alignment/16,"Therefore , landmarks in the same local face region have similar properties including occlusion and visibility .",31,0,17
dataset/preprocessed/test-data/face_alignment/16,It is observed that the nose can be localized roughly with the locations of eyes and mouth .,32,0,18
dataset/preprocessed/test-data/face_alignment/16,There are also structural correlations among different facial parts .,33,0,10
dataset/preprocessed/test-data/face_alignment/16,"Motivated by this fact , facial landmarks are divided into several clusters based on their semantic relevance .",34,0,18
dataset/preprocessed/test-data/face_alignment/16,"In this work 1 , we propose a novel deep learning framework named Multi - Center Learning ( MCL ) to exploit the strong correlations among landmarks .",35,0,28
dataset/preprocessed/test-data/face_alignment/16,"In particular , our network uses multiple shape prediction layers to predict the locations of landmarks , and each shape prediction layer emphasizes on the detection of a certain cluster of landmarks respectively .",36,0,34
dataset/preprocessed/test-data/face_alignment/16,"By weighting the loss of each landmark , challenging landmarks are focused firstly , and each cluster of landmarks is further optimized respectively .",37,0,24
dataset/preprocessed/test-data/face_alignment/16,"Moreover , to decrease the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .",38,0,26
dataset/preprocessed/test-data/face_alignment/16,The entire framework reinforces the learning process of each landmark with a low model complexity .,39,0,16
dataset/preprocessed/test-data/face_alignment/16,The main contributions of this study can be summarized as follows :,40,0,12
dataset/preprocessed/test-data/face_alignment/16,We propose a novel multi-center learning framework for exploiting the strong correlations among landmarks .,41,0,15
dataset/preprocessed/test-data/face_alignment/16,We propose a model assembling method which ensures a low model complexity .,42,0,13
dataset/preprocessed/test-data/face_alignment/16,Extensive experiments demonstrate that our method is effective for handling complex occlusions and appearance variations with real - time performance .,43,0,21
dataset/preprocessed/test-data/face_alignment/16,The remainder of this paper is structured as below .,44,0,10
dataset/preprocessed/test-data/face_alignment/16,We discuss related works in the next section .,45,0,9
dataset/preprocessed/test-data/face_alignment/16,"In Section III , we illuminate the structure of our network and the learning algorithm .",46,0,16
dataset/preprocessed/test-data/face_alignment/16,Extensive experiments are carried out in Section IV .,47,0,9
dataset/preprocessed/test-data/face_alignment/16,Section V concludes this work .,48,0,6
dataset/preprocessed/test-data/face_alignment/10,Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks,2,1,11
dataset/preprocessed/test-data/face_alignment/10,"We present a new loss function , namely Wing loss , for robust facial landmark localisation with Convolutional Neural Networks ( CNNs ) .",4,0,24
dataset/preprocessed/test-data/face_alignment/10,"We first compare and analyse different loss functions including L2 , L1 and smooth L1 .",5,0,16
dataset/preprocessed/test-data/face_alignment/10,"The analysis of these loss functions suggests that , for the training of a CNN - based localisation model , more attention should be paid to small and medium range errors .",6,0,32
dataset/preprocessed/test-data/face_alignment/10,"To this end , we design a piece - wise loss function .",7,0,13
dataset/preprocessed/test-data/face_alignment/10,"The new loss amplifies the impact of errors from the interval ( - w , w) by switching from L1 loss to a modified logarithm function .",8,0,27
dataset/preprocessed/test-data/face_alignment/10,"To address the problem of under-representation of samples with large out - of - plane head rotations in the training set , we propose a simple but effective boosting strategy , referred to as pose - based data balancing .",9,0,40
dataset/preprocessed/test-data/face_alignment/10,"In particular , we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation , bounding box translation and other data augmentation approaches .",10,0,34
dataset/preprocessed/test-data/face_alignment/10,"Last , the proposed approach is extended to create a two - stage framework for robust facial landmark localisation .",11,0,20
dataset/preprocessed/test-data/face_alignment/10,"The experimental results obtained on AFLW and 300W demonstrate the merits of the Wing loss function , and prove the superiority of the proposed method over the state - of - the - art approaches .",12,0,36
dataset/preprocessed/test-data/face_alignment/10,"Facial landmark localisation , or face alignment , aims at finding the coordinates of a set of pre-defined key points for 2 D face images .",14,1,26
dataset/preprocessed/test-data/face_alignment/10,"A facial landmark usually has specific semantic meaning , e.g. nose tip or eye centre , which provides rich geometric information for other face analysis tasks such as face recognition , emotion estimation and 3D face reconstruction .",15,0,38
dataset/preprocessed/test-data/face_alignment/10,"Thanks to the successive developments in this area of research during the past decades , we are able to perform very accurate facial landmark localisation in constrained scenarios , even using traditional approaches such as Active Shape Model ( ASM ) , Active Appearance Model ( AAM ) and Constrained Local Model ( CLM ) .",16,0,56
dataset/preprocessed/test-data/face_alignment/10,"The existing challenge is to achieve robust and accurate landmark localisation of unconstrained faces that are impacted by a variety of appearance variations , e.g. in pose , expression , illumination , image blurring and occlusion .",17,0,37
dataset/preprocessed/test-data/face_alignment/10,"To this end , cascaded - regression - based approaches have been widely used , in which a set of weak regressors are cascaded to form a strong regressor .",18,0,30
dataset/preprocessed/test-data/face_alignment/10,"However , the capability of cascaded regression is nearly saturated due to its shallow structure .",19,0,16
dataset/preprocessed/test-data/face_alignment/10,"After cascading more than four or five weak regressors , the performance of cascaded regression is hard to improve further .",20,0,21
dataset/preprocessed/test-data/face_alignment/10,"More recently , deep neural networks have been put forward as a more powerful alternative in a wide range of computer vision and pattern recognition tasks , including facial landmark localisation .",21,0,32
dataset/preprocessed/test-data/face_alignment/10,"To perform robust facial landmark localisation us - ing deep neural networks , different network types have been explored , such as the Convolutional Neural Network ( CNN ) , Auto - Encoder Network and Recurrent Neural Network ( RNN ) .",22,0,42
dataset/preprocessed/test-data/face_alignment/10,"In addition , different network architectures have been extensively studied during the recent years along with the development of deep neural networks in other AI applications .",23,0,27
dataset/preprocessed/test-data/face_alignment/10,"For example , the Fully Convolutional Network ( FCN ) and hourglass network with residual blocks have been found very effective .",24,0,22
dataset/preprocessed/test-data/face_alignment/10,One crucial aspect of deep learning is to define a loss function leading to better - learnt representation from underlying data .,25,0,22
dataset/preprocessed/test-data/face_alignment/10,"However , this aspect of the design seems to belittle investigated by the facial landmark localisation community .",26,0,18
dataset/preprocessed/test-data/face_alignment/10,"To the best of our knowledge , most existing facial landmark localisation approaches using deep learning are based on the L2 loss .",27,0,23
dataset/preprocessed/test-data/face_alignment/10,"However , the L2 loss function is sensitive to outliers , which has been noted in connection with the bounding box regression problem in the well - known Fast R - CNN algorithm .",28,0,34
dataset/preprocessed/test-data/face_alignment/10,Rashid et al. also notice this issue and use the smooth L1 loss instead of L2 .,29,0,17
dataset/preprocessed/test-data/face_alignment/10,"To further address the issue , we propose a new loss function , namely Wing loss ( ) , for robust facial landmark localisation .",30,0,25
dataset/preprocessed/test-data/face_alignment/10,The main contributions of our work include :,31,0,8
dataset/preprocessed/test-data/face_alignment/10,"presenting a systematic analysis of different loss functions that could be used for regression - based facial landmark localisation with CNNs , which to our best knowledge is the first such study carried out in connection with the landmark localisation problem .",32,0,42
dataset/preprocessed/test-data/face_alignment/10,"We empirically and theoretically compare L1 , L2 and smooth L1 loss functions and find that L1 and smooth L1 perform much better than the widely used L2 loss .",33,0,30
dataset/preprocessed/test-data/face_alignment/10,"a novel loss function , namely the Wing loss , which is designed to improve the deep neural network training capability for small and medium range errors .",34,0,28
dataset/preprocessed/test-data/face_alignment/10,"a data augmentation strategy , i.e. pose - based data balancing , that compensates the low frequency of occurrence of samples with large out - of - plane head rotations in the training set .",35,0,35
dataset/preprocessed/test-data/face_alignment/10,a two - stage facial landmark localisation framework for performance boosting .,36,0,12
dataset/preprocessed/test-data/face_alignment/10,The paper is organised as follows .,37,0,7
dataset/preprocessed/test-data/face_alignment/10,Section 2 presents a brief review of the related literature .,38,0,11
dataset/preprocessed/test-data/face_alignment/10,The regression - based facial landmarking problem with CNNs is formulated in Section 3 .,39,0,15
dataset/preprocessed/test-data/face_alignment/10,The properties of common loss functions ( L1 and L2 ) are discussed in Section 4 which also motivate the introduction of the novel Wing loss function .,40,0,28
dataset/preprocessed/test-data/face_alignment/10,The pose - based data balancing strategy is the subject of Section 5 .,41,0,14
dataset/preprocessed/test-data/face_alignment/10,The twostage localisation framework is proposed in Section 6 .,42,0,10
dataset/preprocessed/test-data/face_alignment/10,The advocated approach is validated experimentally in Section 7 and the paper is drawn to conclusion in Section 8 .,43,0,20
dataset/preprocessed/test-data/face_alignment/10,Network Architectures :,45,0,3
dataset/preprocessed/test-data/face_alignment/10,Most deep - learning - based facial landmark localisation approaches are regression - based .,46,0,15
dataset/preprocessed/test-data/face_alignment/10,"For such a task , the most straightforward way is to use a CNN model with regression output layers .",47,0,20
dataset/preprocessed/test-data/face_alignment/10,The input fora regression CNN is usually an image patch enclosing the whole face region and the output is a vector consisting of the 2D coordinates of facial landmarks .,48,0,30
dataset/preprocessed/test-data/face_alignment/10,"Besides the classical CNN architecture , newly developed CNN systems have also been used for facial landmark localisation and shown promising results , e.g. FCN and the hourglass network .",49,0,30
dataset/preprocessed/test-data/face_alignment/9,Semantic Alignment : Finding Semantically Consistent Ground - truth for Facial Landmark Detection,2,1,13
dataset/preprocessed/test-data/face_alignment/9,"Recently , deep learning based facial landmark detection has achieved great success .",4,0,13
dataset/preprocessed/test-data/face_alignment/9,"Despite this , we notice that the semantic ambiguity greatly degrades the detection performance .",5,0,15
dataset/preprocessed/test-data/face_alignment/9,"Specifically , the semantic ambiguity means that some landmarks ( e.g. those evenly distributed along the face contour ) do not have clear and accurate definition , causing inconsistent annotations by annotators .",6,0,33
dataset/preprocessed/test-data/face_alignment/9,"Accordingly , these inconsistent annotations , which are usually provided by public databases , commonly work as the groundtruth to supervise network training , leading to the degraded accuracy .",7,0,30
dataset/preprocessed/test-data/face_alignment/9,"To our knowledge , little research has investigated this problem .",8,0,11
dataset/preprocessed/test-data/face_alignment/9,"In this paper , we propose a novel probabilistic model which introduces a latent variable , i.e. the ' real ' ground - truth which is semantically consistent , to optimize .",9,0,32
dataset/preprocessed/test-data/face_alignment/9,This framework couples two parts ( 1 ) training landmark detection CNN and ( 2 ) searching the ' real ' groundtruth .,10,0,23
dataset/preprocessed/test-data/face_alignment/9,These two parts are alternatively optimized : the searched ' real ' ground - truth supervises the CNN training ; and the trained CNN assists the searching of ' real ' groundtruth .,11,0,33
dataset/preprocessed/test-data/face_alignment/9,"In addition , to recover the unconfidently predicted landmarks due to occlusion and low quality , we propose a global heatmap correction unit ( GHCU ) to correct outliers by considering the global face shape as a constraint .",12,0,39
dataset/preprocessed/test-data/face_alignment/9,Extensive experiments on both image - based ( 300 W and AFLW ) and video - based ( 300 - VW ) databases demonstrate that our method effectively improves the landmark detection accuracy and achieves the state of the art performance .,13,0,42
dataset/preprocessed/test-data/face_alignment/9,Deep learning methods have achieved great success on landmark detection due to the strong modeling capacity .,15,0,17
dataset/preprocessed/test-data/face_alignment/9,"Despite this success , precise and credible landmark detection still has many challenges , one * equal contribution .",16,0,19
dataset/preprocessed/test-data/face_alignment/9,Non semantic moving,17,0,3
dataset/preprocessed/test-data/face_alignment/9,Semantic moving .,18,0,3
dataset/preprocessed/test-data/face_alignment/9,The landmark updates in training after the model is roughly converged .,19,0,12
dataset/preprocessed/test-data/face_alignment/9,"Due to ' semantic ambiguity ' , we can see that many optimization directions , which are random guided by random annotation noises along with the contour and ' non semantic ' .",20,0,33
dataset/preprocessed/test-data/face_alignment/9,The others move to the semantically accurate positions .,21,0,9
dataset/preprocessed/test-data/face_alignment/9,"Red and green dots denote the predicted and annotation landmarks , respectively .",22,0,13
dataset/preprocessed/test-data/face_alignment/9,of which is the degraded performance caused by ' semantic ambiguity ' .,23,0,13
dataset/preprocessed/test-data/face_alignment/9,This ambiguity results from the lack of clear definition on those weak semantic landmarks on the contours ( e.g. those on face contour and nose bridge ) .,24,0,28
dataset/preprocessed/test-data/face_alignment/9,"In comparison , strong semantic landmarks on the corners ( e.g. eye corner ) suffer less from such ambiguity .",25,0,20
dataset/preprocessed/test-data/face_alignment/9,"The ' semantic ambiguity ' can make human annotators confused about the positions of weak semantic points , and it is inevitable for annotators to introduce random noises during annotating .",26,0,31
dataset/preprocessed/test-data/face_alignment/9,The inconsistent and imprecise annotations can mislead CNN training and cause degraded performance .,27,0,14
dataset/preprocessed/test-data/face_alignment/9,"Specifically , when the deep model roughly converges to the ground - truth provided by public databases , the network training is misguided by random annotation noises caused by ' semantic ambiguity ' , shown in .",28,0,37
dataset/preprocessed/test-data/face_alignment/9,"Clearly these noises can make the network training trapped into local minima , leading to degraded results .",29,0,18
dataset/preprocessed/test-data/face_alignment/9,"In this paper , we propose a novel Semantic Alignment method which reduces the ' semantic ambiguity ' intrinsi-cally .",30,0,20
dataset/preprocessed/test-data/face_alignment/9,We assume that there exist ' real ' ground - truths which are semantically consistent and more accurate than human annotations provided by databases .,31,0,25
dataset/preprocessed/test-data/face_alignment/9,"We model the ' real ' ground - truth as a latent variable to optimize , and the optimized ' real ' ground - truth then supervises the landmark detection network training .",32,0,33
dataset/preprocessed/test-data/face_alignment/9,"Accordingly , we propose a probabilistic model which can simultaneously search the ' real ' ground - truth and train the landmark detection network in an end - to - end way .",33,0,33
dataset/preprocessed/test-data/face_alignment/9,"In this probabilistic model , the prior model is to constrain the latent variable to be close to the observations of the ' real ' ground truth , one of which is the human annotation .",34,0,36
dataset/preprocessed/test-data/face_alignment/9,The likelihood model is to reduce the Pearson Chi-square distance between the expected and the predicted distributions of ' real ' ground - truth .,35,0,25
dataset/preprocessed/test-data/face_alignment/9,The heatmap generated by the hourglass architecture represents the confidence of each pixel and this confidence distribution is used to model the predicted distribution of likelihood .,36,0,27
dataset/preprocessed/test-data/face_alignment/9,"Apart from the proposed probabilistic framework , we further propose a global heatmap correction unit ( GHCU ) which maintains the global face shape constraint and recovers the unconfidently predicted landmarks caused by challenging factors such as occlusions and low resolution of images .",37,0,44
dataset/preprocessed/test-data/face_alignment/9,"We conduct experiments on 300W , AFLW and 300 - VW databases and achieve the state of the art performance .",38,0,21
dataset/preprocessed/test-data/face_alignment/9,"In recent years , convolutional neural networks ( CNN ) achieves very impressive results on many computer vision tasks including face alignment .",40,0,23
dataset/preprocessed/test-data/face_alignment/9,Sun et al proposes to cascade several DCNN to predict the shape stage by stage .,41,0,16
dataset/preprocessed/test-data/face_alignment/9,"Zhang et al proposes a single CNN and jointly optimizes facial landmark detection together with facial attribute recognition , further enhancing the speed and performance .",42,0,26
dataset/preprocessed/test-data/face_alignment/9,"The methods above use shallow CNN models to directly regress facial landmarks , which are difficult to cope the complex task with dense landmarks and large pose variations .",43,0,29
dataset/preprocessed/test-data/face_alignment/9,"To further improve the performance , many popular semantic segmentation and human pose estimation frameworks are used for face alignment .",44,0,21
dataset/preprocessed/test-data/face_alignment/9,"For each landmark , they predict a heatmap which contains the probability of the corresponding landmark .",45,0,17
dataset/preprocessed/test-data/face_alignment/9,Yang et al .,46,0,4
dataset/preprocessed/test-data/face_alignment/9,"uses a two parts network , i.e. , a supervised transformation to normalize faces and a stacked hourglass network to get prediction heatmaps .",47,0,24
dataset/preprocessed/test-data/face_alignment/9,"Most recently , JMFA and FAN also achieve the state of the art accuracy by leveraging stacked hourglass network .",48,0,20
dataset/preprocessed/test-data/face_alignment/9,"However , these methods do not consider the ' semantic ambiguity ' problem which potentially degrades the detection performance .",49,0,20
dataset/preprocessed/test-data/face_alignment/1,Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression,2,1,14
dataset/preprocessed/test-data/face_alignment/1,"Figure 1 : A few results from our VRN - Guided method , on a full range of pose , including large expressions .",4,0,24
dataset/preprocessed/test-data/face_alignment/1,Abstract 3 D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty .,5,1,15
dataset/preprocessed/test-data/face_alignment/1,"Current systems often assume the availability of multiple facial images ( sometimes from the same subject ) as input , and must address a number of methodological challenges such as establishing dense correspondences across large facial poses , expressions , and non-uniform illumination .",6,0,44
dataset/preprocessed/test-data/face_alignment/1,In general these methods require complex and inefficient pipelines for model building and fitting .,7,0,15
dataset/preprocessed/test-data/face_alignment/1,"In this work , we propose to address many of these limitations by training a Convolutional Neural Network ( CNN ) on an appropriate dataset consisting of 2D images and 3D facial models or scans .",8,0,36
dataset/preprocessed/test-data/face_alignment/1,"Our CNN works with just a single 2 D facial image , does not require accurate alignment nor establishes dense correspondence between images , works for arbitrary facial poses and expressions , and can be used to reconstruct the whole 3 D facial geometry ( including the non-visible parts of the face ) bypassing the construction ( during training ) and fitting ( during testing ) of a 3D Morphable Model .",9,0,72
dataset/preprocessed/test-data/face_alignment/1,We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image .,10,0,27
dataset/preprocessed/test-data/face_alignment/1,"We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality , especially for the cases of large poses and facial expressions .",11,0,35
dataset/preprocessed/test-data/face_alignment/1,Code and models will be made available at,12,0,8
dataset/preprocessed/test-data/face_alignment/1,3 D face reconstruction is the problem of recovering the 3D facial geometry from 2D images .,14,0,17
dataset/preprocessed/test-data/face_alignment/1,"Despite many years of research , it is still an open problem in Vision and Graphics research .",15,0,18
dataset/preprocessed/test-data/face_alignment/1,"Depending on the setting and the assumptions made , there are many variations of it as well as a multitude of approaches to solve it .",16,0,26
dataset/preprocessed/test-data/face_alignment/1,This work is on 3D face reconstruction using only a single image .,17,0,13
dataset/preprocessed/test-data/face_alignment/1,"Under this setting , the problem is considered far from being solved .",18,0,13
dataset/preprocessed/test-data/face_alignment/1,"In this paper , we propose to approach it , for the first time to the best of our knowledge , by directly learning a mapping from pixels to 3D coordinates using a Convolutional Neural Network ( CNN ) .",19,0,40
dataset/preprocessed/test-data/face_alignment/1,"Besides its simplicity , our approach works with totally unconstrained images downloaded from the web , including facial images of arbitrary poses , facial expressions and occlusions , as shown in .",20,0,32
dataset/preprocessed/test-data/face_alignment/1,"No matter what the underlying assumptions are , what the input ( s ) and output ( s ) to the algorithm are , 3 D face reconstruction requires in general complex pipelines and solving non-convex difficult optimization problems for both model building ( during training ) and model fitting ( during testing ) .",22,0,55
dataset/preprocessed/test-data/face_alignment/1,"In the following paragraph , we provide examples from 5 predominant approaches :",23,0,13
dataset/preprocessed/test-data/face_alignment/1,"In the 3D Morphable Model ( 3 DMM ) , the most popular approach for estimating the full 3D facial structure from a single image ( among others ) , training includes an iterative flow procedure for dense image correspondence which is prone to failure .",25,0,46
dataset/preprocessed/test-data/face_alignment/1,"Additionally , testing requires a careful initialisation for solving a difficult highly non-convex optimization problem , which is slow .",26,0,20
dataset/preprocessed/test-data/face_alignment/1,"2 . The work of , a popular approach for 2.5 D reconstruction from a single image , formulates and solves a carefully initialised ( for frontal images only ) non-convex optimization problem for recovering the lighting , depth , and albedo in an alternating manner where each of the sub-problems is a difficult optimization problem per se .",27,0,59
dataset/preprocessed/test-data/face_alignment/1,"3 . In , a quite popular recent approach for creating a neutral subject - specific 2.5 D model from a near frontal image , an iterative procedure is proposed which entails localising facial landmarks , face frontalization , solving a photometric stereo problem , local surface normal estimation , and finally shape integration .",28,0,55
dataset/preprocessed/test-data/face_alignment/1,"4 . In , a state - of - the - art pipeline for reconstructing a highly detailed 2.5 D facial shape for each video frame , an average shape and an illumination subspace for the specific person is firstly computed ( offline ) , while testing is an iterative process requiring a sophisticated pose estimation algorithm , 3 D flow computation between the model and the video frame , and finally shape refinement by solving a shape - from - shading optimization problem .",29,0,85
dataset/preprocessed/test-data/face_alignment/1,"5 . More recently , the state - of - the - art method of that produces the average ( neutral ) 3 D face from a collection of personal photos , firstly performs landmark detection , then fits a 3 DMM using a sparse set of points , then solves an optimization problem similar to the one in , then performs surface normal estimation as in and finally performs surface reconstruction by solving another energy minimisation problem .",30,0,79
dataset/preprocessed/test-data/face_alignment/1,Simplifying the technical challenges involved in the aforementioned works is the main motivation of this paper .,31,0,17
dataset/preprocessed/test-data/face_alignment/1,"We describe a very simple approach which bypasses many of the difficulties encountered in 3D face reconstruction by using a novel volumetric representation of the 3D facial geometry , and an appropriate CNN architecture that is trained to regress directly from a 2 D facial image to the corresponding 3D volume .",33,0,52
dataset/preprocessed/test-data/face_alignment/1,An overview of our method is shown in .,34,0,9
dataset/preprocessed/test-data/face_alignment/1,"In summary , our contributions are :",35,0,7
dataset/preprocessed/test-data/face_alignment/1,"Given a dataset consisting of 2D images and 3D face scans , we investigate whether a CNN can learn directly , in an end - to - end fashion , the mapping from image pixels to the full 3 D facial structure geometry ( including the non-visible facial parts ) .",36,0,51
dataset/preprocessed/test-data/face_alignment/1,"Indeed , we show that the answer to this question is positive .",37,0,13
dataset/preprocessed/test-data/face_alignment/1,"We demonstrate that our CNN works with just a single 2 D facial image , does not require accurate alignment nor establishes dense correspondence between images , works for arbitrary facial poses and expressions , and can be used to reconstruct the whole 3 D facial geometry bypassing the construction ( during training ) and fitting ( during testing ) of a 3 DMM .",38,0,65
dataset/preprocessed/test-data/face_alignment/1,We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image .,39,0,27
dataset/preprocessed/test-data/face_alignment/1,3 DMM fitting is not used .,40,0,7
dataset/preprocessed/test-data/face_alignment/1,Our method uses only 2D images as input to the proposed CNN architecture .,41,0,14
dataset/preprocessed/test-data/face_alignment/1,"We show how the related task of 3D facial landmark localisation can be incorporated into the proposed framework and help improve reconstruction quality , especially for the cases of large poses and facial expressions .",42,0,35
dataset/preprocessed/test-data/face_alignment/1,"We report results fora large number of experiments on both controlled and completely unconstrained images from the web , illustrating that our method outperforms prior work on single image 3 D face reconstruction by a large margin .",43,0,38
dataset/preprocessed/test-data/face_alignment/1,Closely related work,44,0,3
dataset/preprocessed/test-data/face_alignment/1,"This section reviews closely related work in 3 D face reconstruction , depth estimation using CNNs and work on 3D representation modelling with CNNs .",45,0,25
dataset/preprocessed/test-data/face_alignment/1,3 D face reconstruction .,46,0,5
dataset/preprocessed/test-data/face_alignment/1,"A full literature review of 3D face reconstruction falls beyond the scope of the paper ; we simply note that our method makes minimal assumptions i.e. it requires just a single 2D image to reconstruct the full 3 D facial structure , and works under arbitrary poses and expressions .",47,0,50
dataset/preprocessed/test-data/face_alignment/1,"Under the single image setting , the most related works to our method are based on 3 DMM fitting and the work of which performs joint face reconstruction and alignment , reconstructing however a neutral frontal face .",48,0,38
dataset/preprocessed/test-data/face_alignment/1,"The work of describes a multi-feature based approach to 3 DMM fitting using non-linear least - squares optimization ( Levenberg - Marquardt ) , which given appropriate initialisation produces results of good accuracy .",49,0,34
dataset/preprocessed/test-data/face_alignment/5,Look at Boundary : A Boundary - Aware Face Alignment Algorithm,2,1,11
dataset/preprocessed/test-data/face_alignment/5,We present a novel boundary - aware face alignment algorithm by utilising boundary lines as the geometric structure of a human face to help facial landmark localisation .,4,0,28
dataset/preprocessed/test-data/face_alignment/5,"Unlike the conventional heatmap based method and regression based method , our approach derives face landmarks from boundary lines which remove the ambiguities in the landmark definition .",5,0,28
dataset/preprocessed/test-data/face_alignment/5,Three questions are explored and answered by this work :,6,0,10
dataset/preprocessed/test-data/face_alignment/5,1 . Why using boundary ?,7,0,6
dataset/preprocessed/test-data/face_alignment/5,2 . How to use boundary ?,8,0,7
dataset/preprocessed/test-data/face_alignment/5,3 . What is the relationship between boundary estimation and landmarks localisation ?,9,0,13
dataset/preprocessed/test-data/face_alignment/5,"Our boundaryaware face alignment algorithm achieves 3.49 % mean error on 300 - W Fullset , which outperforms state - of - the - art methods by a large margin .",10,0,31
dataset/preprocessed/test-data/face_alignment/5,Our method can also easily integrate information from other datasets .,11,0,11
dataset/preprocessed/test-data/face_alignment/5,"By utilising boundary information of 300 - W dataset , our method achieves 3.92 % mean error with 0.39 % failure rate on COFW dataset , and 1.25 % mean error on AFLW - Full dataset .",12,0,37
dataset/preprocessed/test-data/face_alignment/5,"Moreover , we propose anew dataset WFLW to unify training and testing across different factors , including poses , expressions , illuminations , makeups , occlusions , and blurriness .",13,0,30
dataset/preprocessed/test-data/face_alignment/5,Dataset and model will be publicly available at https://wywu.github.io/projects/LAB/LAB.html,14,0,9
dataset/preprocessed/test-data/face_alignment/5,"Face alignment , which refers to facial landmark detection in this work , serves as a key step for many face applications , e.g. , face recognition , face verification and face frontalisation .",16,1,34
dataset/preprocessed/test-data/face_alignment/5,The objective of this paper is to devise an effective face alignment algorithm to handle faces with unconstrained pose variation and occlusion across multiple datasets and annotation protocols . *,17,0,30
dataset/preprocessed/test-data/face_alignment/5,This work was done during an internship at SenseTime Research .,18,0,11
dataset/preprocessed/test-data/face_alignment/5,datasets with different number of landmarks .,19,0,7
dataset/preprocessed/test-data/face_alignment/5,The second column illustrates the universally defined facial boundaries estimated by our methods .,20,0,14
dataset/preprocessed/test-data/face_alignment/5,"With the help of boundary information , our approach achieves high accuracy localisation results across multiple datasets and annotation protocols , as shown in the third column .",21,0,28
dataset/preprocessed/test-data/face_alignment/5,"Different to face detection and recognition , face alignment identifies geometry structure of human face which can be viewed as modeling highly structured output .",22,0,25
dataset/preprocessed/test-data/face_alignment/5,"Each facial landmark is strongly associated with a well - defined facial boundary , e.g. , eyelid and nose bridge .",23,0,21
dataset/preprocessed/test-data/face_alignment/5,"However , compared to boundaries , facial landmarks are not so well - defined .",24,0,15
dataset/preprocessed/test-data/face_alignment/5,Facial landmarks other than corners can hardly remain the same semantical locations with large pose variation and occlusion .,25,0,19
dataset/preprocessed/test-data/face_alignment/5,"Besides , different annotation schemes of existing datasets lead to a different number of landmarks ( 19/29/68/194 points ) and annotation scheme of future face alignment datasets can hardly be determined .",26,0,32
dataset/preprocessed/test-data/face_alignment/5,We believe the reasoning of a unique facial structure is the key to localise facial landmarks since human face does not include ambiguities .,27,0,24
dataset/preprocessed/test-data/face_alignment/5,"To this end , we use well - defined facial boundaries to represent the geometric structure of the human face .",28,0,21
dataset/preprocessed/test-data/face_alignment/5,It is easier to identify facial boundaries comparing to facial landmarks under large pose and occlusion .,29,0,17
dataset/preprocessed/test-data/face_alignment/5,"In this work , we represent facial structure using 13 boundary lines .",30,0,13
dataset/preprocessed/test-data/face_alignment/5,"Each facial boundary line can be interpolated from a sufficient number of facial landmarks across multiple datasets , which will not suffer from inconsistency of the annotation schemes .",31,0,29
dataset/preprocessed/test-data/face_alignment/5,Our boundary - aware face alignment algorithm contains two stages .,32,0,11
dataset/preprocessed/test-data/face_alignment/5,We first estimate facial boundary heatmaps and then regress landmarks with the help of boundary heatmaps .,33,0,17
dataset/preprocessed/test-data/face_alignment/5,"As noticed in , facial landmarks of different annotation schemes can be derived from boundary heatmaps with the same definition .",34,0,21
dataset/preprocessed/test-data/face_alignment/5,"To explore the relationship between facial boundaries and landmarks , we introduce adversarial learning ideas by using a landmark - based boundary effectiveness discriminator .",35,0,25
dataset/preprocessed/test-data/face_alignment/5,"Experiments have shown that the better quality estimated boundaries have , the more accurate landmarks will be .",36,0,18
dataset/preprocessed/test-data/face_alignment/5,"The boundary heatmap estimator , landmark regressor , and boundary effectiveness discriminator can be jointly learned in an end - to - end manner .",37,0,25
dataset/preprocessed/test-data/face_alignment/5,We used stacked hourglass structure to estimate facial boundary heatmap and model the structure between facial boundaries through message passing to increase its robustness to occlusion .,38,0,27
dataset/preprocessed/test-data/face_alignment/5,"After generating facial boundary heatmaps , the next step is deriving facial landmarks using boundaries .",39,0,16
dataset/preprocessed/test-data/face_alignment/5,The boundary heatmaps serve as structure cue to guide feature learning for the landmark regressor .,40,0,16
dataset/preprocessed/test-data/face_alignment/5,We observe that a model guided by ground truth boundary heatmaps can achieve 76. 26 % AUC on 300 W test while the state - of - the - art method can only achieve 54.85 % .,41,0,37
dataset/preprocessed/test-data/face_alignment/5,This suggests the richness of information contained in boundary heatmaps .,42,0,11
dataset/preprocessed/test-data/face_alignment/5,"To fully utilise the structure information , we apply boundary heatmaps at multiple stages in the landmark regression network .",43,0,20
dataset/preprocessed/test-data/face_alignment/5,"Our experiment shows that the more stages boundary heatmaps are used in feature learning , the better landmark prediction results we will get .",44,0,24
dataset/preprocessed/test-data/face_alignment/5,"We evaluate the proposed method on three popular face alignment benchmarks including 300W , COFW , and AFLW .",45,0,19
dataset/preprocessed/test-data/face_alignment/5,Our approach significantly outperforms previous state - of - the - art methods by a large margin .,46,0,18
dataset/preprocessed/test-data/face_alignment/5,"3.49 % mean error on 300 - W Fullset , 3.92 % mean error with 0.39 % failure rate on COFW and 1.25 % mean error on AFLW - Full dataset respectively .",47,0,33
dataset/preprocessed/test-data/face_alignment/5,"To unify the evaluation , we propose anew large dataset named Wider Facial Landmarks in - the - wild ( WFLW ) which contain 10 , 000 images .",48,0,29
dataset/preprocessed/test-data/face_alignment/5,"Our new dataset introduces large pose , expression , and occlusion variance .",49,0,13
dataset/preprocessed/test-data/face_alignment/18,Deep Alignment Network : A convolutional neural network for robust face alignment,2,1,12
dataset/preprocessed/test-data/face_alignment/18,"In this paper , we propose Deep Alignment Network ( DAN ) , a robust face alignment method based on a deep neural network architecture .",4,0,26
dataset/preprocessed/test-data/face_alignment/18,"DAN consists of multiple stages , where each stage improves the locations of the facial landmarks estimated by the previous stage .",5,0,22
dataset/preprocessed/test-data/face_alignment/18,"Our method uses entire face images at all stages , contrary to the recently proposed face alignment methods that rely on local patches .",6,0,24
dataset/preprocessed/test-data/face_alignment/18,This is possible thanks to the use of landmark heatmaps which provide visual information about landmark locations estimated at the previous stages of the algorithm .,7,0,26
dataset/preprocessed/test-data/face_alignment/18,The use of entire face images rather than patches allows DAN to handle face images with large variation in head pose and difficult initializations .,8,0,25
dataset/preprocessed/test-data/face_alignment/18,An extensive evaluation on two publicly available datasets shows that DAN reduces the state - of - theart failure rate by up to 70 % .,9,0,26
dataset/preprocessed/test-data/face_alignment/18,Our method has also been submitted for evaluation as part of the Menpo challenge .,10,0,15
dataset/preprocessed/test-data/face_alignment/18,"The goal of face alignment is to localize a set of predefined facial landmarks ( eye corners , mouth corners etc. ) in an image of a face .",12,1,29
dataset/preprocessed/test-data/face_alignment/18,"Face alignment is an important component of many computer vision applications , such as face verification , facial emotion recognition , humancomputer interaction and facial motion capture .",13,0,28
dataset/preprocessed/test-data/face_alignment/18,Most of the face alignment methods introduced in the recent years are based on shape indexed features .,14,0,18
dataset/preprocessed/test-data/face_alignment/18,"In these approaches image features , such as SIFT or learned features , are extracted from image patches extracted around each of the landmarks .",15,0,25
dataset/preprocessed/test-data/face_alignment/18,The features are then used to iteratively refine the estimates of landmark locations .,16,0,14
dataset/preprocessed/test-data/face_alignment/18,"While those approaches can be successfully applied to face alignment in many photos , their performance on the most challenging datasets leaves room for improvement .",17,0,26
dataset/preprocessed/test-data/face_alignment/18,We believe that this is due to the fact that for the most difficult images the features extracted at disjoint patches do not provide enough information and can lead the method into a local minimum .,18,0,36
dataset/preprocessed/test-data/face_alignment/18,"In this work , we address the above shortcoming by proposing a novel face alignment method which we dub Deep Alignment Network ( DAN ) .",19,0,26
dataset/preprocessed/test-data/face_alignment/18,"It is based on a multistage neural network where each stage refines the landmark positions estimated at the previous stage , iteratively improving the landmark locations .",20,0,27
dataset/preprocessed/test-data/face_alignment/18,The input to each stage of our algorithm ( except the first stage ) area face image normalized to a canonical pose and an image learned from the dense layer of the previous stage .,21,0,35
dataset/preprocessed/test-data/face_alignment/18,"To make use of the entire face image during the process of face alignment , we additionally input at each stage a landmark heatmap , which is a key element of our system .",22,0,34
dataset/preprocessed/test-data/face_alignment/18,A landmark heatmap is an image with high intensity values around landmark locations where intensity decreases with the distance from the nearest landmark .,23,0,24
dataset/preprocessed/test-data/face_alignment/18,The convolutional neural network can use the heatmaps to infer the current estimates of landmark locations in the image and thus refine them .,24,0,24
dataset/preprocessed/test-data/face_alignment/18,An example of a landmark heatmap can be seen in which shows an outline of our method .,25,0,18
dataset/preprocessed/test-data/face_alignment/18,"By using landmark heatmaps , our DAN algorithm is able to reduce the failure rate on the 300W public test set by a large margin of 72 % with respect to the state of the art .",26,0,37
dataset/preprocessed/test-data/face_alignment/18,"To summarize , the three main contributions of this work are the following :",27,0,14
dataset/preprocessed/test-data/face_alignment/18,We introduce landmark heatmaps which transfer the information about current landmark location estimates between the stages of our method .,29,0,20
dataset/preprocessed/test-data/face_alignment/18,"This improvement allows our method to make use of the entire image of a face , instead of local patches , and avoid falling into local minima .",30,0,28
dataset/preprocessed/test-data/face_alignment/18,The resulting robust face alignment method we propose in this paper reduces the failure rate by 60 % on .,32,0,20
dataset/preprocessed/test-data/face_alignment/18,A diagram showing an outline of the proposed method .,33,0,10
dataset/preprocessed/test-data/face_alignment/18,"Each stage of the neural network refines the landmark location estimates produced by the previous stage , starting with an initial estimate S0 .",34,0,24
dataset/preprocessed/test-data/face_alignment/18,The connection layers form a link between the consecutive stages of the network by producing the landmark heatmaps,35,0,18
dataset/preprocessed/test-data/face_alignment/18,"Ht , feature images",36,0,4
dataset/preprocessed/test-data/face_alignment/18,Ft and a transform,37,0,4
dataset/preprocessed/test-data/face_alignment/18,Tt which is used to warp the input image to a canonical pose .,38,0,14
dataset/preprocessed/test-data/face_alignment/18,"By introducing landmark heatmaps and feature images we can transmit crucial information , including the landmark location estimates , between the stages of our method .",39,0,26
dataset/preprocessed/test-data/face_alignment/18,the 300W private test set and 72 % on the 300 - W public test set compared to the state of the art .,40,0,24
dataset/preprocessed/test-data/face_alignment/18,"Finally , we publish both the source code of our implementation of the proposed method and the models used in the experiments .",42,0,23
dataset/preprocessed/test-data/face_alignment/18,The remainder of the paper is organized in the following manner .,43,0,12
dataset/preprocessed/test-data/face_alignment/18,In section 2 we give an overview of the related work .,44,0,12
dataset/preprocessed/test-data/face_alignment/18,In section 3 we provide a detailed description of the proposed method .,45,0,13
dataset/preprocessed/test-data/face_alignment/18,"Finally , in section 4 we perform an evaluation of DAN and compare it to the state of the art .",46,0,21
dataset/preprocessed/test-data/face_alignment/18,"Face alignment has along history , starting with the early Active Appearance Models , moving to Constrained Local Models and recently shifting to methods based on Cascaded Shape Regression ( CSR ) and deep learning .",48,0,36
dataset/preprocessed/test-data/face_alignment/18,"In CSR based methods , the face alignment begins with an initial estimate of the landmark locations which is then refined in an iterative manner .",49,0,26
dataset/preprocessed/test-data/face_alignment/15,Face Alignment Across Large Poses : A 3D Solution,2,1,9
dataset/preprocessed/test-data/face_alignment/15,"Face alignment , which fits a face model to an image and extracts the semantic meanings of facial pixels , has been an important topic in CV community .",4,1,29
dataset/preprocessed/test-data/face_alignment/15,"However , most algorithms are designed for faces in small to medium poses ( below 45 ) , lacking the ability to align faces in large poses up to 90 .",5,0,31
dataset/preprocessed/test-data/face_alignment/15,The challenges are three - fold :,6,0,7
dataset/preprocessed/test-data/face_alignment/15,"Firstly , the commonly used landmark - based face model assumes that all the landmarks are visible and is therefore not suitable for profile views .",7,0,26
dataset/preprocessed/test-data/face_alignment/15,"Secondly , the face appearance varies more dramatically across large poses , ranging from frontal view to profile view .",8,0,20
dataset/preprocessed/test-data/face_alignment/15,"Thirdly , labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed .",9,0,19
dataset/preprocessed/test-data/face_alignment/15,"In this paper , we propose a solution to the three problems in an new alignment framework , called 3D Dense Face Alignment ( 3DDFA ) , in which a dense 3 D face model is fitted to the image via convolutional neutral network ( CNN ) .",10,0,48
dataset/preprocessed/test-data/face_alignment/15,We also propose a method to synthesize large - scale training samples in profile views to solve the third problem of data labelling .,11,0,24
dataset/preprocessed/test-data/face_alignment/15,Experiments on the challenging AFLW database show that our approach achieves significant improvements over state - of - the - art methods .,12,0,23
dataset/preprocessed/test-data/face_alignment/15,"Traditional face alignment aims to locate face fiducial points like "" eye corner "" , "" nose tip "" and "" chin center "" , based on which the face image can be normalized .",14,0,35
dataset/preprocessed/test-data/face_alignment/15,"It is an essential preprocessing step for many face analysis tasks , e.g. , face recognition , expression recognition and inverse rendering .",15,0,23
dataset/preprocessed/test-data/face_alignment/15,The researches in face alignment can be divided into two categories : the analysis - by - synthesis based and regression based .,16,0,23
dataset/preprocessed/test-data/face_alignment/15,The former simulates the process of image generation and achieves alignment by minimizing the difference between model appearance and input image .,17,0,22
dataset/preprocessed/test-data/face_alignment/15,The latter extracts features around key points and regresses it to the ground truth landmarks .,18,0,16
dataset/preprocessed/test-data/face_alignment/15,"With the development in the last decade , face alignment across medium poses , where the yaw angle is less than 45 and all the landmarks are visible , has been well addressed .",19,0,34
dataset/preprocessed/test-data/face_alignment/15,"However , face alignment across large poses ( 90 ) is still a challenging problem without much attention and achievements .",20,0,21
dataset/preprocessed/test-data/face_alignment/15,There are three main challenges : .,21,0,7
dataset/preprocessed/test-data/face_alignment/15,Fitting results of 3DDFA .,22,0,5
dataset/preprocessed/test-data/face_alignment/15,"For each pair of the four results , on the left is the rendering of the fitted 3D shape with the mean texture , which is made transparent to demonstrate the fitting accuracy .",23,0,34
dataset/preprocessed/test-data/face_alignment/15,"On the right is the landmarks overlayed on the 3D face model , in which the blue / red ones indicate visible / invisible landmarks .",24,0,26
dataset/preprocessed/test-data/face_alignment/15,The visibility is directly computed from the fitted dense model by .,25,0,12
dataset/preprocessed/test-data/face_alignment/15,More results are demonstrated in supplemental material .,26,0,8
dataset/preprocessed/test-data/face_alignment/15,Landmark shape model implicitly assumes that each landmark can be robustly detected based on its distinctive visual patterns .,28,0,19
dataset/preprocessed/test-data/face_alignment/15,"However , when faces deviate from the frontal view , some landmarks become invisible due to self - occlusion .",29,0,20
dataset/preprocessed/test-data/face_alignment/15,"In medium poses , this problem can be addressed by changing the semantic positions of face contour landmarks to the silhouette , which is termed landmark marching .",30,0,28
dataset/preprocessed/test-data/face_alignment/15,"However , in large poses where half of face is occluded , some landmarks are inevitably invisible and have no image data .",31,0,23
dataset/preprocessed/test-data/face_alignment/15,"As a result , the landmark shape model no longer works well .",32,0,13
dataset/preprocessed/test-data/face_alignment/15,Fitting : Face alignment across large poses is more challenging than medium poses due to the dramatic appearance variations when close to the profile views .,33,0,26
dataset/preprocessed/test-data/face_alignment/15,The cascaded linear regression or traditional nonlinear models are not sophisticated enough to cover such complicated patterns in a unified way .,34,0,22
dataset/preprocessed/test-data/face_alignment/15,"The view - based framework , which adopts different landmark configurations and fitting models for each view category , may significantly increase computation cost since every view has to be tested .",35,0,32
dataset/preprocessed/test-data/face_alignment/15,Data Labelling :,36,0,3
dataset/preprocessed/test-data/face_alignment/15,The most serious problem comes from the data .,37,0,9
dataset/preprocessed/test-data/face_alignment/15,Manual labelling landmarks on large - pose faces is a very tedious task .,38,0,14
dataset/preprocessed/test-data/face_alignment/15,"Firstly , no algorithm can provide a good initialization to reduce the workload .",39,0,14
dataset/preprocessed/test-data/face_alignment/15,"Secondly , the occluded landmarks have to be "" guessed "" which is impossible for most of people .",40,0,19
dataset/preprocessed/test-data/face_alignment/15,"As a result , almost all public face alignment databases such as AFW , LFPW , HELEN and IBUG are collected in medium poses .",41,0,25
dataset/preprocessed/test-data/face_alignment/15,"Existing large - pose databases such as AFLW only contains visible landmarks , which could be ambiguous in invisible landmarks and hard to train a unified face alignment model .",42,0,30
dataset/preprocessed/test-data/face_alignment/15,"In this paper , we address all the three challenges with the goal of improving the face alignment performance across large poses .",43,0,23
dataset/preprocessed/test-data/face_alignment/15,To address the problem of invisible landmarks in large,44,0,9
dataset/preprocessed/test-data/face_alignment/15,"poses , we propose to fit the 3D dense face model rather than the sparse landmark shape model to the image .",45,0,22
dataset/preprocessed/test-data/face_alignment/15,"By incorporating 3D information , the appearance variations and self - occlusion caused by 3D transformations can be inherently addressed .",46,0,21
dataset/preprocessed/test-data/face_alignment/15,We call this method 3D Dense Face Alignment ( 3DDFA ) .,47,0,12
dataset/preprocessed/test-data/face_alignment/15,Some results are shown in .,48,0,6
dataset/preprocessed/test-data/face_alignment/8,Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D - Assisted Self - Supervised Learning,2,1,20
dataset/preprocessed/test-data/face_alignment/8,Dense face alignment ( odd rows ) and 3D face reconstruction ( even rows ) results from our proposed method .,5,0,21
dataset/preprocessed/test-data/face_alignment/8,"For alignment , only 68 key points are plotted for clear display ; for 3D reconstruction , reconstructed shapes are rendered with headlight for better view .",6,0,27
dataset/preprocessed/test-data/face_alignment/8,"Our method offers strong robustness and good performance even in presence of large poses ( the 3th , 4th and 5th columns ) and occlusions ( the 6th , 7th and 8th columns ) .",7,0,35
dataset/preprocessed/test-data/face_alignment/8,Best viewed in color .,8,0,5
dataset/preprocessed/test-data/face_alignment/8,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,9,1,17
dataset/preprocessed/test-data/face_alignment/8,Recent methods typically aim to learn a CNN - based 3 D face model that regresses coefficients of 3D Morphable Model ( 3 DMM ) from 2D images to render 3 D face reconstruction or dense face alignment .,10,0,39
dataset/preprocessed/test-data/face_alignment/8,"However , the shortage of training data with 3D annotations considerably limits performance of those methods .",11,0,17
dataset/preprocessed/test-data/face_alignment/8,"To alleviate this issue , we propose a novel 2D - assisted self - supervised learning ( 2DASL ) method that can effectively use "" in - the -wild "" 2 D face images with noisy landmark information to substantially improve 3 D face model learning .",12,0,47
dataset/preprocessed/test-data/face_alignment/8,"Specifically , taking the sparse 2 D facial landmarks as additional information , 2 DSAL introduces four novel self - supervision schemes that view the 2D landmark and 3D landmark prediction as a self - mapping process , including the 2D and 3D landmark self - prediction consistency , cycle - consistency over the 2D landmark prediction and self - critic over the predicted 3 DMM coefficients based on landmark predictions .",13,0,72
dataset/preprocessed/test-data/face_alignment/8,"Using these four self - supervision schemes , the 2DASL method significantly relieves demands on the the conventional paired 2D - to - 3D annotations and gives much higher - quality 3 D face models without requiring any additional 3D annotations .",14,0,42
dataset/preprocessed/test-data/face_alignment/8,Experiments on multiple challenging datasets show that our method outperforms state - of - the - arts for both 3 D face reconstruction and dense face alignment by a large margin .,15,0,32
dataset/preprocessed/test-data/face_alignment/8,Dense face alignment ( odd rows ) and 3D face reconstruction ( even rows ) results from our proposed method .,18,0,21
dataset/preprocessed/test-data/face_alignment/8,"For alignment , only 68 key points are plotted for clear display ; for 3D reconstruction , reconstructed shapes are rendered with headlight for better view .",19,0,27
dataset/preprocessed/test-data/face_alignment/8,"Our method offers strong robustness and good performance even in presence of large poses ( the 3th , 4th and 5th columns ) and occlusions ( the 6th , 7th and 8th columns ) .",20,0,35
dataset/preprocessed/test-data/face_alignment/8,Best viewed in color .,21,0,5
dataset/preprocessed/test-data/face_alignment/8,3 D face reconstruction from a single 2D image is a challenging problem with broad applications .,23,1,17
dataset/preprocessed/test-data/face_alignment/8,Recent methods typically aim to learn a CNN - based 3 D face model that regresses coefficients of 3D Morphable Model ( 3 DMM ) from 2D images to render 3 D face reconstruction or dense face alignment .,24,0,39
dataset/preprocessed/test-data/face_alignment/8,"However , the shortage of training data with 3D annotations considerably limits performance of those methods .",25,0,17
dataset/preprocessed/test-data/face_alignment/8,"To alleviate this issue , we propose a novel 2D - assisted self - supervised learning ( 2DASL ) method that can effectively use "" in - the -wild "" 2 D face images with noisy landmark information to substantially improve 3 D face model learning .",26,0,47
dataset/preprocessed/test-data/face_alignment/8,"Specifically , taking the sparse 2 D facial landmarks as additional information , 2 DSAL introduces four novel self - supervision schemes that view the 2D landmark and 3D landmark prediction as a self - mapping process , including the 2D and 3D landmark self - prediction consistency , cycle - consistency over the 2D landmark prediction and self - critic over the predicted 3 DMM coefficients based on landmark predictions .",27,0,72
dataset/preprocessed/test-data/face_alignment/8,"Using these four self - supervision schemes , the 2DASL method significantly relieves demands on the the conventional paired 2D - to - 3D annotations and gives much higher - quality 3 D face models without requiring any additional 3D annotations .",28,0,42
dataset/preprocessed/test-data/face_alignment/8,Experiments on multiple challenging datasets show that our method outperforms state - of - the - arts for both 3 D face reconstruction and dense face alignment by a large margin .,29,0,32
dataset/preprocessed/test-data/face_alignment/8,3 D face reconstruction is an important task in the field of computer vision and graphics .,31,1,17
dataset/preprocessed/test-data/face_alignment/8,"For instance , the recovery of 3D face geometry from a single image can help address many challenges ( e.g. , large pose and occlusion ) for 2 D face alignment through dense face alignment .",32,0,36
dataset/preprocessed/test-data/face_alignment/8,"Traditional 3 D face reconstruction methods are mainly based on optimization algorithms , e.g. , iterative closest point , to obtain coefficients for the 3D Morphable Model ( 3 DMM ) model and render the corresponding 3 D faces from a single face image .",33,0,45
dataset/preprocessed/test-data/face_alignment/8,"However , such methods are usually time - consuming due to the high optimization complexity and suffer from local optimal solution and bad initialization .",34,0,25
dataset/preprocessed/test-data/face_alignment/8,Recent works thus propose to use CNNs to learn to regress the 3 DMM coefficients and significantly improve the reconstruction quality and efficiency .,35,0,24
dataset/preprocessed/test-data/face_alignment/8,CNN - based methods have achieved remarkable success in 3D face reconstruction and dense face alignment .,36,0,17
dataset/preprocessed/test-data/face_alignment/8,"However , obtaining an accurate 3 D face CNN regression model ( from input 2D images to 3 DMM coefficients ) requires a large amount of training faces with 3D annotations , which are expensive to collect and even not achievable in some cases .",37,0,45
dataset/preprocessed/test-data/face_alignment/8,"Even some 3 D face datasets , like 300W - LP , are publicly available , they generally lack diversity in face appearance , expression , occlusions and environment conditions , limiting the generalization performance of resulted 3 D face regression models .",38,0,43
dataset/preprocessed/test-data/face_alignment/8,A model trained on such datasets can not deal well with various potential cases in - the - wild that are not present in the training examples .,39,0,28
dataset/preprocessed/test-data/face_alignment/8,"Although some recent works bypass the 3 DMM parameter regression and use image - to - volume or image - to - image strategy instead , the ground truths are all still needed and generated from 3 DMM using 300W - LP , still lacking diversity .",40,0,47
dataset/preprocessed/test-data/face_alignment/8,"In order to overcome the intrinsic limitation of existing 3 D face recovery models , we propose a novel learning method that leverages 2D "" in - the - wild "" face images to effectively supervise and facilitate the 3D face model learning .",41,0,44
dataset/preprocessed/test-data/face_alignment/8,"With the method , the trained 3 D face model can perform 3 D face reconstruction and dense face alignment well .",42,0,22
dataset/preprocessed/test-data/face_alignment/8,"This is inspired by the observation that a large number of 2 D face datasets are available with obtainable 2D landmark annotations , that could provide valuable information for 3D model learning , without requiring new data with 3D annotations .",43,0,41
dataset/preprocessed/test-data/face_alignment/8,"Since these 2D images do not have any 3D annotations , it is not straightforward to exploit them in 3 D face model learning .",44,0,25
dataset/preprocessed/test-data/face_alignment/8,We design a novel self - supervised learning method that is able to train a 3 D face model with weak supervision from 2D images .,45,0,26
dataset/preprocessed/test-data/face_alignment/8,"In particular , the proposed method takes the sparse annotated 2D landmarks as input and fully leverage the consistency within the 2 Dto - 2D and 3D - to - 3D self - mapping procedure as supervi-sion .",46,0,38
dataset/preprocessed/test-data/face_alignment/8,The model should be able to recover 2D landmarks from predicted 3D ones via direct 3D - to - 2D projection .,47,0,22
dataset/preprocessed/test-data/face_alignment/8,"Meanwhile , the 3D landmarks predicted from the annotated and recovered 2D landmarks via the model should be the same .",48,0,21
dataset/preprocessed/test-data/face_alignment/8,"Additionally , our proposed method also exploits cycle - consistency over the 2D landmark predictions , i.e. , taking the recovered 2D landmarks as input , the model should be able to generate 2D landmarks ( by projecting its predicted 3D landmarks ) that have small difference with the annotated ones .",49,0,52
dataset/preprocessed/test-data/face_alignment/6,Face Alignment using a 3D Deeply - initialized Ensemble of Regression Trees,2,1,12
dataset/preprocessed/test-data/face_alignment/6,Face alignment algorithms locate a set of landmark points in images of faces taken in unrestricted situations .,4,0,18
dataset/preprocessed/test-data/face_alignment/6,"State - of - the - art approaches typically fail or lose accuracy in the presence of occlusions , strong deformations , large pose variations and ambiguous configurations .",5,0,29
dataset/preprocessed/test-data/face_alignment/6,"In this paper we present 3DDE , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ensembles of regression trees .",6,0,29
dataset/preprocessed/test-data/face_alignment/6,It is initialized by robustly fitting a 3 D face model to the probability maps produced by a convolutional neural network .,7,0,22
dataset/preprocessed/test-data/face_alignment/6,With this initialization we address self - occlusions and large face rotations .,8,0,13
dataset/preprocessed/test-data/face_alignment/6,"Further , the regressor implicitly imposes a prior face shape on the solution , addressing occlusions and ambiguous face configurations .",9,0,21
dataset/preprocessed/test-data/face_alignment/6,Its coarse - to - fine structure tackles the combinatorial explosion of parts deformation .,10,0,15
dataset/preprocessed/test-data/face_alignment/6,"In the experiments performed , 3 DDE improves the state - of - the - art in 300W , COFW , AFLW and WFLW data sets .",11,0,27
dataset/preprocessed/test-data/face_alignment/6,"Finally , we perform cross - dataset experiments that reveal the existence of a significant data set bias in these benchmarks .",12,0,22
dataset/preprocessed/test-data/face_alignment/6,Face alignment algorithms precisely locate a set of points of interest in the images of faces taken in unrestricted conditions .,14,0,21
dataset/preprocessed/test-data/face_alignment/6,"It has received much attention from the research community since it is a preliminary step for estimating 3 D facial structure and many other face image analysis problems such as verification and recognition , attributes estimation or facial expression recognition , to name a few .",15,0,46
dataset/preprocessed/test-data/face_alignment/6,"Present approaches typically fail or lose precision in the presence of occlusions , strong deformations produced by facial expressions , large pose variations and ambiguous configurations caused , for example , by strong make - up or the existence of other nearby faces .",16,0,44
dataset/preprocessed/test-data/face_alignment/6,"Top performers in the most popular benchmarks are based on Convolutional Neural Networks ( CNNs ) and Ensemble of Regression Trees ( ERT ) , see e.g. , Tables 1 , 2 , 3 , 4 and 5 .",17,0,39
dataset/preprocessed/test-data/face_alignment/6,The large effective receptive field of deep models ) enable them to model context better and produce robust landmark estimations .,18,0,21
dataset/preprocessed/test-data/face_alignment/6,"However , in these models it is not easy to enforce facial shape consistency , something that limits their accuracy in the presence of occlusions and ambiguous facial configurations .",19,0,30
dataset/preprocessed/test-data/face_alignment/6,"ERT - based models , on the other hand , are difficult to initialize , but may implicitly impose face shape consistency in their estimations .",20,0,26
dataset/preprocessed/test-data/face_alignment/6,This increases their performance in occluded and ambiguous situations .,21,0,10
dataset/preprocessed/test-data/face_alignment/6,"They are also much more efficient than deep models and , as we demonstrate in our experiments , with a good initialization they are also very accurate .",22,0,28
dataset/preprocessed/test-data/face_alignment/6,"In this paper we present the 3 DDE ( 3D Deeply - initialized Ensemble ) regressor , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ERTs .",23,0,36
dataset/preprocessed/test-data/face_alignment/6,"It is a hybrid approach that inherits good properties of ERT , such as the ability to impose a face shape prior , and the robustness of deep models .",24,0,30
dataset/preprocessed/test-data/face_alignment/6,It is initialized by robustly fitting a 3 D face model to the probability maps produced by a CNN .,25,0,20
dataset/preprocessed/test-data/face_alignment/6,"With this initialization we tackle one of the main drawbacks of ERT , namely the difficulty in initializing the regressor in the presence of occlusions and large face rotations .",26,0,30
dataset/preprocessed/test-data/face_alignment/6,"On the other hand , the ERT implicitly imposes a prior face shape on the solution , addressing the shortcomings of deep models when occlusions and ambiguous face configurations are present .",27,0,32
dataset/preprocessed/test-data/face_alignment/6,"Finally , its coarse - to - fine structure tackles the combinatorial explosion of parts deformation , which is also a key limitation of approaches using shape constraints .",28,0,29
dataset/preprocessed/test-data/face_alignment/6,A preliminary version of our work appeared in .,29,0,9
dataset/preprocessed/test-data/face_alignment/6,Here we refine and extend it in several ways .,30,0,10
dataset/preprocessed/test-data/face_alignment/6,First we improve the initialization by using a RANSAC - like procedure that increases its robustness in the presence of occlusions .,31,0,22
dataset/preprocessed/test-data/face_alignment/6,We have also introduced early stopping and better data augmentation techniques for increasing the regularization when training both the ERT and the CNN .,32,0,24
dataset/preprocessed/test-data/face_alignment/6,We also extend the evaluation including the newly released WFLW database and a detailed ablation study .,33,0,17
dataset/preprocessed/test-data/face_alignment/6,"Finally , 3 DDE may also be trained in presence of missing and occluded landmarks in the training set .",34,0,20
dataset/preprocessed/test-data/face_alignment/6,This has enabled us to perform cross - dataset experiments that reveal the existence of significant data set bias that may limit the generalization capabilities of regressors trained on present data bases .,35,0,33
dataset/preprocessed/test-data/face_alignment/6,"To the best of our knowledge , this is the first time such a problem has been raised in the field .",36,0,22
dataset/preprocessed/test-data/face_alignment/6,Face alignment has been a topic of intense research for more than twenty years .,38,0,15
dataset/preprocessed/test-data/face_alignment/6,Initial successful results were based on 2D and 3D generative approaches such as the Active Appearance Models ( AAM ) or the 3D Morphable Models ( 3 DMM ) .,39,0,30
dataset/preprocessed/test-data/face_alignment/6,Recent approaches are based on a cascaded combination of discriminative regressors .,40,0,12
dataset/preprocessed/test-data/face_alignment/6,"In the earliest case these regressors are Random Ferns , Ensembles of Regression Trees or linear models .",41,0,18
dataset/preprocessed/test-data/face_alignment/6,"Key ideas in this approach are indexing image description relative to the current shape estimate , and the use of a regressor whose predictions lie on the subspace spanned by the training face shapes , this is the so - called Cascade Shape Regressor ( CSR ) framework .",42,0,49
dataset/preprocessed/test-data/face_alignment/6,improved the original cascade framework by proposing a realtime ensemble of regression trees .,43,0,14
dataset/preprocessed/test-data/face_alignment/6,used locally binary features to boost the performance up to 3000 FPS .,44,0,13
dataset/preprocessed/test-data/face_alignment/6,included occlusion estimation and decreased the influence of occluded landmarks .,45,0,11
dataset/preprocessed/test-data/face_alignment/6,refine the initial location of face landmarks using a random forest and SIFT features .,46,0,15
dataset/preprocessed/test-data/face_alignment/6,also use SIFT features and learn the linear regressor dividing the search space into individual regions with similar gradient directions .,47,0,21
dataset/preprocessed/test-data/face_alignment/6,"Overall , this set of approaches are very sensitive to the starting point of the regression process .",48,0,18
dataset/preprocessed/test-data/face_alignment/6,For this reason an important part of recent work revolves around how to find good initializations .,49,0,17
dataset/preprocessed/test-data/face_alignment/11,Unsupervised Training for 3D Morphable Model Regression,2,1,7
dataset/preprocessed/test-data/face_alignment/11,We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs .,4,0,22
dataset/preprocessed/test-data/face_alignment/11,"The training loss is based on features from a facial recognition network , computed onthe-fly by rendering the predicted faces with a differentiable renderer .",5,0,25
dataset/preprocessed/test-data/face_alignment/11,"To make training from features feasible and avoid network fooling effects , we introduce three objectives : a batch distribution loss that encourages the output distribution to match the distribution of the morphable model , a loopback loss that ensures the network can correctly reinterpret its own output , and a multi-view identity loss that compares the features of the predicted 3 D face and the input photograph from multiple viewing angles .",6,0,73
dataset/preprocessed/test-data/face_alignment/11,"We train a regression network using these objectives , a set of unlabeled photographs , and the morphable model itself , and demonstrate state - of - the - art results .",7,0,32
dataset/preprocessed/test-data/face_alignment/11,"A 3D morphable face model ( 3 DMM ) provides a smooth , low - dimensional "" face space "" spanning the range of human appearance .",9,0,27
dataset/preprocessed/test-data/face_alignment/11,"Finding the coordinates of a person in this space from a single image of that person is a common task for applications such as 3D avatar creation , facial animation transfer , and video editing ( e.g. ) .",10,0,39
dataset/preprocessed/test-data/face_alignment/11,"The conventional approach is to search the space through inverse rendering , which generates a face that matches the photograph by optimizing shape , texture , pose , and lighting parameters .",11,0,32
dataset/preprocessed/test-data/face_alignment/11,"This approach requires a complex , nonlinear optimization that can be difficult to solve in practice .",12,0,17
dataset/preprocessed/test-data/face_alignment/11,"Recent work has demonstrated fast , robust fitting by regressing from image pixels to morphable model coordinates using a neural network .",13,0,22
dataset/preprocessed/test-data/face_alignment/11,The major issue with the regression approach is the lack of ground - truth 3 D face data for training .,14,0,21
dataset/preprocessed/test-data/face_alignment/11,"Scans of face geometry and texture are difficult to acquire , both because of expense and privacy considerations .",15,0,19
dataset/preprocessed/test-data/face_alignment/11,"Previous approaches have explored synthesizing training pairs of image and morphable model coordinates in a preprocess , or training an image -.",16,0,22
dataset/preprocessed/test-data/face_alignment/11,Neutral 3D faces computed from input photographs using our regression network .,17,0,12
dataset/preprocessed/test-data/face_alignment/11,We map features from a facial recognition network into identity parameters for the Basel 2017 Morphable Face Model .,18,0,19
dataset/preprocessed/test-data/face_alignment/11,"to - image autoencoder with a fixed , morphable - model - based decoder and an image - based loss .",19,0,21
dataset/preprocessed/test-data/face_alignment/11,This paper presents a method for training a regression network that removes both the need for supervised training data and the reliance on inverse rendering to reproduce image pixels .,20,0,30
dataset/preprocessed/test-data/face_alignment/11,"Instead , the network learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG - Face or Google 's FaceNet .",21,0,31
dataset/preprocessed/test-data/face_alignment/11,"These features are robust to pose , expression , lighting , and even non-photorealistic inputs .",22,0,16
dataset/preprocessed/test-data/face_alignment/11,We exploit this invariance to apply a loss that matches the identity features between the input photograph and a synthetic rendering of the predicted face .,23,0,26
dataset/preprocessed/test-data/face_alignment/11,"The synthetic rendering need not have the same pose , expression , or lighting of the photograph , allowing our network to predict only shape and texture .",24,0,28
dataset/preprocessed/test-data/face_alignment/11,"Simply optimizing for similarity between identity features , however , can teach the regression network to fool the recognition network by producing faces that match closely in feature space but look unnatural .",25,0,33
dataset/preprocessed/test-data/face_alignment/11,"We alleviate the fooling problem by applying three novel losses : a batch distribution loss to match the statistics of each training batch to the statistics of the morphable model , a loopback loss to ensure the regression network can correctly reinterpret its own output , and a multi-view identity loss that combines features from multiple , independent views of the predicted shape .",26,0,64
dataset/preprocessed/test-data/face_alignment/11,"Using this scheme , we train a 3D shape and texture regression network using only a face recognition network , a morphable face model , and a dataset of unlabeled face images .",27,0,33
dataset/preprocessed/test-data/face_alignment/11,"We show that despite learning from unlabeled photographs , the 3D face results improve on the accuracy of previous work and are often recognizable as the original subjects .",28,0,29
dataset/preprocessed/test-data/face_alignment/11,Blanz and Vetter introduced the 3D morphable face model as an extension of the 2D active appearance model .,32,0,19
dataset/preprocessed/test-data/face_alignment/11,"They demonstrated face reconstruction from a single image by iteratively fitting a linear combination of registered scans and pose , camera , and lighting parameters .",33,0,26
dataset/preprocessed/test-data/face_alignment/11,"They decomposed the geometry and texture of the face scans using PCA to produce separate , reduced - dimension geometry and texture spaces .",34,0,24
dataset/preprocessed/test-data/face_alignment/11,Later work added more face scans and extended the model to include expressions as another separate space .,35,0,18
dataset/preprocessed/test-data/face_alignment/11,We build directly off of this work by using the PCA weights as the output of our network .,36,0,19
dataset/preprocessed/test-data/face_alignment/11,"Convergence of iterative fitting is sensitive to the initial conditions and the complexity of the scene ( i.e. , lighting , expression , and pose ) .",37,0,27
dataset/preprocessed/test-data/face_alignment/11,"Subsequent work ( and others ) has applied a range of techniques to improve the accuracy and stability of the fitting , producing very accurate results under good conditions .",38,0,30
dataset/preprocessed/test-data/face_alignment/11,"However , iterative approaches are still unreliable under general , in - the - wild , conditions , leading to the interest in regression - based approaches .",39,0,28
dataset/preprocessed/test-data/face_alignment/11,Learning to Generate 3D Face Models,40,0,6
dataset/preprocessed/test-data/face_alignment/11,Deep neural networks provide the ability to learn a regression from image pixels to 3D model parameters .,41,0,18
dataset/preprocessed/test-data/face_alignment/11,The chief difficulty becomes how to collect enough training data to feed the network .,42,0,15
dataset/preprocessed/test-data/face_alignment/11,One solution is to generate synthetic training data by drawing random samples from the morphable model and rendering the resulting faces .,43,0,22
dataset/preprocessed/test-data/face_alignment/11,"However , a network trained on purely synthetic data may perform poorly when faced with occlusions , unusual lighting , or ethnicities that are not well - represented by the morphable model .",44,0,33
dataset/preprocessed/test-data/face_alignment/11,"We include randomly generated , synthetic faces in each training batch to provide ground truth 3D coordinates , but train the network on real photographs at the same time .",45,0,30
dataset/preprocessed/test-data/face_alignment/11,Tran et al .,46,0,4
dataset/preprocessed/test-data/face_alignment/11,"address the lack of training data by using an iterative optimization to fit an expressionless model to a large number of photographs , and treat results where the optimization converged as ground truth .",47,0,34
dataset/preprocessed/test-data/face_alignment/11,"To generalize to faces with expression , identity labels and at least one neutral image are required , so the potential size of the training dataset is restricted .",48,0,29
dataset/preprocessed/test-data/face_alignment/11,"We also directly predict a neutral expression , but our unsupervised approach removes the need for an initial iterative fitting step .",49,0,22
dataset/preprocessed/test-data/face_alignment/14,Faster Than Real - time Facial Alignment : A 3D Spatial Transformer Network Approach in Unconstrained Poses,2,1,17
dataset/preprocessed/test-data/face_alignment/14,Facial alignment involves finding a set of landmark points on an image with a known semantic meaning .,4,0,18
dataset/preprocessed/test-data/face_alignment/14,"However , this semantic meaning of landmark points is often lost in 2D approaches where landmarks are either moved to visible boundaries or ignored as the pose of the face changes .",5,0,32
dataset/preprocessed/test-data/face_alignment/14,"In order to extract consistent alignment points across large poses , the 3D structure of the face must be considered in the alignment step .",6,0,25
dataset/preprocessed/test-data/face_alignment/14,"However , extracting a 3D structure from a single 2D image usually requires alignment in the first place .",7,0,19
dataset/preprocessed/test-data/face_alignment/14,We present our novel approach to simultaneously extract the 3D shape of the face and the semantically consistent 2D alignment through a 3D Spatial Transformer Network ( 3DSTN ) to model both the camera projection matrix and the warping parameters of a 3D model .,8,0,45
dataset/preprocessed/test-data/face_alignment/14,"By utilizing a generic 3D model and a Thin Plate Spline ( TPS ) warping function , we are able to generate subject specific 3 D shapes without the need fora large 3D shape basis .",9,0,36
dataset/preprocessed/test-data/face_alignment/14,"In addition , our proposed network can be trained in an end - to - end framework on entirely synthetic data from the 300W - LP dataset .",10,0,28
dataset/preprocessed/test-data/face_alignment/14,"Unlike other 3D methods , our approach only requires one pass through the network resulting in a faster than realtime alignment .",11,0,22
dataset/preprocessed/test-data/face_alignment/14,Evaluations of our model on the Annotated Facial Landmarks in the Wild ( AFLW ) and AFLW2000 - 3D datasets show our method achieves state - of - the - art performance over other 3D approaches to alignment .,12,0,39
dataset/preprocessed/test-data/face_alignment/14,Robust face recognition and analysis are contingent upon accurate localization of facial features .,14,1,14
dataset/preprocessed/test-data/face_alignment/14,"When modeling faces , the landmark points of interest consist of points that lie along the shape boundaries of facial features , e.g. eyes , lips , mouth , etc .",15,0,31
dataset/preprocessed/test-data/face_alignment/14,"When dealing with face images collected in the wild conditions , facial occlusion of landmarks becomes a common problem for off - angle faces .",16,0,25
dataset/preprocessed/test-data/face_alignment/14,Predicting the occlusion state of each landmarking points is one of the .,17,0,13
dataset/preprocessed/test-data/face_alignment/14,A subject from the CMU Multi - PIE dataset landmarked and frontalized by our method at various poses .,18,0,19
dataset/preprocessed/test-data/face_alignment/14,Landmarks found by our model are overlaid in green if they are determined to be a visible landmark and blue if self - occluded .,19,0,25
dataset/preprocessed/test-data/face_alignment/14,The non-visible regions of the face are determined by the estimated camera center and the estimated 3D shape .,20,0,19
dataset/preprocessed/test-data/face_alignment/14,Best viewed in color .,21,0,5
dataset/preprocessed/test-data/face_alignment/14,"challenges due to variations of objects in faces , e.g. beards and mustaches , sunglasses and other noisy objects .",22,0,20
dataset/preprocessed/test-data/face_alignment/14,"Additionally , face images of interest nowadays usually contain off - angle poses , illumination variations , low resolutions , and partial occlusions .",23,0,24
dataset/preprocessed/test-data/face_alignment/14,Many complex factors could affect the appearance of a face image in real - world scenarios and providing tolerance to these factors is the main challenge for researchers .,24,0,29
dataset/preprocessed/test-data/face_alignment/14,"Among these factors , pose is often the most important factor to be dealt with .",25,0,16
dataset/preprocessed/test-data/face_alignment/14,"It is known that as facial pose deviates from a frontal view , most face recognition systems have difficulty in performing robustly .",26,0,23
dataset/preprocessed/test-data/face_alignment/14,"In order to handle a wide range of pose changes , it becomes necessary to utilize 3D structural information of faces .",27,0,22
dataset/preprocessed/test-data/face_alignment/14,"However , many of the existing 3 D face modeling schemes have many drawbacks , such as computation time and complexity .",28,0,22
dataset/preprocessed/test-data/face_alignment/14,"Though these can be mitigated by using depth sensors or by tracking results from frame to frame in video , this can cause difficulty when they have to be applied in real - world large scale unconstrained face recognition scenarios where video and depth information is not available .",29,0,49
dataset/preprocessed/test-data/face_alignment/14,The 3D generic elastic model ( 3D - GEM ) approach was proposed as an efficient and reliable 3D modeling method from a single 2D image .,30,0,27
dataset/preprocessed/test-data/face_alignment/14,Heo et al.,31,0,3
dataset/preprocessed/test-data/face_alignment/14,claim that the depth information of a face is not extremely discriminative when factoring out the 2D spatial location of facial features .,32,0,23
dataset/preprocessed/test-data/face_alignment/14,"In our method , we follow this idea and observe that fairly accurate 3 D models can be generated by using a simple mean shape deformed to the input image at a relatively low computational cost compared to other approaches .",33,0,41
dataset/preprocessed/test-data/face_alignment/14,"We take the approach of using a simple mean shape and using a parametric , non-linear warping of that shape through alignment on the image to be able to model any unseen example .",34,0,34
dataset/preprocessed/test-data/face_alignment/14,A key flaw in many approaches that rely on a 3D Morphable Model ( 3 DMM ) is that it needs enough examples of the data to be able to model unseen samples .,35,0,34
dataset/preprocessed/test-data/face_alignment/14,"However , in the case of 3D faces , most datasets are very small .",36,0,15
dataset/preprocessed/test-data/face_alignment/14,Our Contributions in this Work,37,0,5
dataset/preprocessed/test-data/face_alignment/14,( 2 ) Our approach is efficiently implemented in an endto - end deep learning framework allowing for the alignment and 3D modeling tasks to be codependent .,38,0,28
dataset/preprocessed/test-data/face_alignment/14,This ensures that alignment points are semantically consistent across changing poses of the object which also allows for more consistent 3 D model generation and frontalization on images in the wild as shown in Figs. 1 and 2 .,39,0,39
dataset/preprocessed/test-data/face_alignment/14,( 3 ) Our method only requires a single pass through the network allowing us to achieve faster than real - time processing of images with state - of - the - art performance over other 2D and 3D approaches to alignment .,40,0,43
dataset/preprocessed/test-data/face_alignment/14,There have been numerous studies related to face alignment since the first work of Active Shape Models ( ASM ) in 1995 .,42,0,23
dataset/preprocessed/test-data/face_alignment/14,A comprehensive literature review in face alignment is beyond the scope of this work .,43,0,15
dataset/preprocessed/test-data/face_alignment/14,"In this paper , we mainly focus on recent Convolutional Neural Network ( CNN ) approaches to solve the face alignment problem .",44,0,23
dataset/preprocessed/test-data/face_alignment/14,Especially those methods aimed at using 3D approaches to achieve robust alignment results .,45,0,14
dataset/preprocessed/test-data/face_alignment/14,"While Principal Component Analysis ( PCA ) and its variants were successfully used to model the facial shapes and appearances , there have since been many advances in facial alignment .",48,0,31
dataset/preprocessed/test-data/face_alignment/14,Landmark locations can be directly predicted by a regression from a learned feature space .,49,0,15
dataset/preprocessed/test-data/face_alignment/17,Aggregation via Separation : Boosting Facial Landmark Detector with Semi-Supervised,2,0,10
dataset/preprocessed/test-data/face_alignment/17,"Facial landmark detection , or face alignment , is a fundamental task that has been extensively studied .",5,1,18
dataset/preprocessed/test-data/face_alignment/17,"In this paper , we investigate a new perspective of facial landmark detection and demonstrate it leads to further notable improvement .",6,0,22
dataset/preprocessed/test-data/face_alignment/17,"Given that any face images can be factored into space of style that captures lighting , texture and image environment , and a style - invariant structure space , our key idea is to leverage disentangled style and shape space of each individual to augment existing structures via style translation .",7,0,51
dataset/preprocessed/test-data/face_alignment/17,"With these augmented synthetic samples , our semi-supervised model surprisingly outperforms the fully - supervised one by a large margin .",8,0,21
dataset/preprocessed/test-data/face_alignment/17,"Extensive experiments verify the effectiveness of our idea with state - of - the - art results on WFLW [ 69 ] , 300W [ 56 ] , COFW [ 7 ] , and AFLW [ 36 ] datasets .",9,0,40
dataset/preprocessed/test-data/face_alignment/17,Our proposed structure is general and could be assembled into any face alignment frameworks .,10,0,15
dataset/preprocessed/test-data/face_alignment/17,The code is made publicly available at https://github.com/thesouthfrog/stylealign.,11,0,8
dataset/preprocessed/test-data/face_alignment/17,"Facial landmark detection is a fundamentally important step in many face applications , such as face recognition , 3 D face reconstruction , face tracking and face editing .",13,0,29
dataset/preprocessed/test-data/face_alignment/17,Accurate facial landmark localization was intensively studied with impressive progress made in these years .,14,0,15
dataset/preprocessed/test-data/face_alignment/17,"The main streams are learning a robust and discriminative model through effective network structure , usage of geometric information , and correction of loss functions .",15,0,26
dataset/preprocessed/test-data/face_alignment/17,"It is common wisdom now that factors such as variation of expression , pose , shape , and occlusion could greatly affect performance of landmark localization .",16,0,27
dataset/preprocessed/test-data/face_alignment/17,"Almost all prior work aims to alleviate these problems from the perspective of structural characteristics , such as disentangling 3 D pose to provide shape constraint , and utilizing dense bound - :",17,0,33
dataset/preprocessed/test-data/face_alignment/17,Problem in a well - trained facial landmark detector .,18,0,10
dataset/preprocessed/test-data/face_alignment/17,"It is biased towards unconstrained environment factors , including lighting , image quality , and occlusion .",19,0,17
dataset/preprocessed/test-data/face_alignment/17,"We regard these degradations as "" style "" in our analysis. ary information .",20,0,14
dataset/preprocessed/test-data/face_alignment/17,"The influence of "" environment "" still lacks principled discussion beyond structure .",21,0,13
dataset/preprocessed/test-data/face_alignment/17,"Also , considering limited labeled data for this task , how to optimally utilize limited training samples remains unexplored .",22,0,20
dataset/preprocessed/test-data/face_alignment/17,"About "" environment "" effect , distortion brought by explicit image style variance was observed recently .",23,0,17
dataset/preprocessed/test-data/face_alignment/17,"We instead utilize style transfer and disentangled representation learning to tackle the face alignment problem , since style transfer aims at altering style while preserving content .",24,0,27
dataset/preprocessed/test-data/face_alignment/17,"In practice , image content refers to objects , semantics and sharp edge maps , whereas style could be color and texture .",25,0,23
dataset/preprocessed/test-data/face_alignment/17,"Our idea is based on the purpose of facial landmark detection , which is to regress "" facial content "" - the principal component of facial geometry - by filtering unconstrained "" styles "" .",26,0,35
dataset/preprocessed/test-data/face_alignment/17,"The fundamental difference to define "" style "" from that of is that we refer it to image background , lighting , quality , existence of glasses , and other factors that prevent detectors from recognizing facial geometry .",27,0,39
dataset/preprocessed/test-data/face_alignment/17,We note every face image can be decomposed into its facial structure along with a distinctive attribute .,28,0,18
dataset/preprocessed/test-data/face_alignment/17,It is a natural conjecture that face alignment could be more robust if we augment images only regarding their styles .,29,0,21
dataset/preprocessed/test-data/face_alignment/17,"To this end , we propose a new framework to augment training for facial landmark detection without using extra knowledge .",30,0,21
dataset/preprocessed/test-data/face_alignment/17,"Instead of directly generating images , we first map face images into the space of structure and style .",31,0,19
dataset/preprocessed/test-data/face_alignment/17,"To guarantee the disentanglement of these two spaces , we design a conditional variational auto - encoder model , in which Kullback - Leiber ( KL ) divergence loss and skip connections are incorporated for compact representation of style and structure respectively .",32,0,43
dataset/preprocessed/test-data/face_alignment/17,"By factoring these features , we perform visual style translation between existing facial geometry .",33,0,15
dataset/preprocessed/test-data/face_alignment/17,"Given existing facial structure , faces with glasses , of poor quality , under blur or strong lighting are rerendered from corresponding style , which are used to further train the facial landmark detectors for a rather general and robust system to recognize facial geometry .",34,0,46
dataset/preprocessed/test-data/face_alignment/17,Our main contribution is as follows .,35,0,7
dataset/preprocessed/test-data/face_alignment/17,We offer a new perspective for facial landmark localization by factoring style and structure .,37,0,15
dataset/preprocessed/test-data/face_alignment/17,"Consequently , a face image is decomposed and rendered from distinctive image style and facial geometry .",38,0,17
dataset/preprocessed/test-data/face_alignment/17,A novel semi-supervised framework based on conditional variational auto - encoder is built upon this new perspective .,40,0,18
dataset/preprocessed/test-data/face_alignment/17,"By disentangling style and structure , our model generates style - augmented images via style translation , further boosting facial landmark detection .",41,0,23
dataset/preprocessed/test-data/face_alignment/17,We propose a new dataset based on AFLW with new 68 - point annotation .,43,0,15
dataset/preprocessed/test-data/face_alignment/17,It provides challenging benchmark considering large pose variation .,44,0,9
dataset/preprocessed/test-data/face_alignment/17,"With extensive experiments on popular benchmark datasets including WFLW , 300W , COFW and AFLW , our approach outperforms previous state - of the - arts by a large margin .",45,0,31
dataset/preprocessed/test-data/face_alignment/17,It is general to be incorporated into various frameworks for further performance improvement .,46,0,14
dataset/preprocessed/test-data/face_alignment/17,Our method also works well under limited training computation resource .,47,0,11
dataset/preprocessed/test-data/face_alignment/17,"This work has close connection with the areas of facial landmark detection , disentangled representation and selfsupervised learning .",49,0,19
dataset/preprocessed/test-data/face_alignment/0,"3D Face Morphable Models "" In - the - Wild """,2,0,11
dataset/preprocessed/test-data/face_alignment/0,"3D Morphable Models ( 3 DMMs ) are powerful statistical models of 3D facial shape and texture , and among the stateof - the - art methods for reconstructing facial shape from single images .",4,1,35
dataset/preprocessed/test-data/face_alignment/0,"With the advent of new 3D sensors , many 3 D facial datasets have been collected containing both neutral as well as expressive faces .",5,0,25
dataset/preprocessed/test-data/face_alignment/0,"However , all datasets are captured under controlled conditions .",6,0,10
dataset/preprocessed/test-data/face_alignment/0,"Thus , even though powerful 3 D facial shape models can be learnt from such data , it is difficult to build statistical texture models that are sufficient to reconstruct faces captured in unconstrained conditions ( "" in - the - wild "" ) .",7,0,45
dataset/preprocessed/test-data/face_alignment/0,"In this paper , we propose the first , to the best of our knowledge , "" in - the - wild "" 3 DMM by combining a powerful statistical model of facial shape , which describes both identity and expression , with an "" in - the -wild "" texture model .",8,0,53
dataset/preprocessed/test-data/face_alignment/0,"We show that the employment of such an "" in - thewild "" texture model greatly simplifies the fitting procedure , because there is no need to optimize with regards to the illumination parameters .",9,0,35
dataset/preprocessed/test-data/face_alignment/0,"Furthermore , we propose anew fast algorithm for fitting the 3 DMM in arbitrary images .",10,0,16
dataset/preprocessed/test-data/face_alignment/0,"Finally , we have captured the first 3 D facial database with relatively unconstrained conditions and report quantitative evaluations with state - of - the - art performance .",11,0,29
dataset/preprocessed/test-data/face_alignment/0,"Complementary qualitative reconstruction results are demonstrated on standard "" in - the - wild "" facial databases .",12,0,18
dataset/preprocessed/test-data/face_alignment/0,An open source implementation of our technique is released as part of the Menpo Project [ 1 ] .,13,0,19
dataset/preprocessed/test-data/face_alignment/0,"During the past few years , we have witnessed significant improvements in various face analysis tasks such as face detection and 2D facial landmark localization on static images .",15,0,29
dataset/preprocessed/test-data/face_alignment/0,"This is primarily attributed to the fact that the community has made a considerable effort to collect and annotate facial images captured under unconstrained conditions ( commonly referred to as "" in - the -wild "" ) and to the discriminative methodologies that can capitalise on the availability of such large amount of data .",16,0,55
dataset/preprocessed/test-data/face_alignment/0,"Nevertheless , discriminative techniques can not be applied for 3D facial shape estimation "" in - the -wild "" , due to lack of ground - truth data .",17,0,29
dataset/preprocessed/test-data/face_alignment/0,3 D facial shape estimation from single images has attracted the attention of many researchers the past twenty years .,18,1,20
dataset/preprocessed/test-data/face_alignment/0,The two main lines of research are ( i ) fitting a 3D Morphable Model ( 3 DMM ) and ( ii ) applying Shape from Shading ( SfS ) techniques .,19,0,32
dataset/preprocessed/test-data/face_alignment/0,The 3 DMM fitting proposed in the work of Blanz and Vetter was among the first model - based 3 D facial recovery approaches .,20,0,25
dataset/preprocessed/test-data/face_alignment/0,The method requires the construction of a 3 DMM which is a statistical model of facial texture and shape in a space where there are explicit correspondences .,21,0,28
dataset/preprocessed/test-data/face_alignment/0,The first 3 DMM was built using 200 faces captured in well - controlled conditions displaying only the neutral expression .,22,0,21
dataset/preprocessed/test-data/face_alignment/0,"That is the reason why the method was only shown to work on real - world , but not "" in - the -wild "" , images .",23,0,28
dataset/preprocessed/test-data/face_alignment/0,State - of - the - art,24,0,7
dataset/preprocessed/test-data/face_alignment/0,SfS techniques capitalise on special multi-linear decompositions that find an approximate spherical harmonic decomposition of the illumination .,25,0,18
dataset/preprocessed/test-data/face_alignment/0,"Furthermore , in order to benefit from the large availability of "" in - the - wild "" images , these methods jointly reconstruct large collections of images .",26,0,29
dataset/preprocessed/test-data/face_alignment/0,"Nevertheless , even thought the results of are quite interesting , given that there is no prior of the facial surface , the methods only recover 2.5 D representations of the faces and particular smooth approximations of the facial normals .",27,0,41
dataset/preprocessed/test-data/face_alignment/0,"3 D facial shape recovery from a single image under "" inthe - wild "" conditions is still an open and challenging problem in computer vision mainly due to the fact that :",28,0,33
dataset/preprocessed/test-data/face_alignment/0,The general problem of extracting the 3D facial shape from a single image is an ill - posed problem which is notoriously difficult to be solved without the use of any statistical priors for the shape and texture of faces .,29,0,41
dataset/preprocessed/test-data/face_alignment/0,"That is , without prior knowledge regarding the shape of the object at - hand there are inherent ambiguities present in the problem .",30,0,24
dataset/preprocessed/test-data/face_alignment/0,"The pixel intensity at a location in an image is the result of a complex combination of the underlying shape of the object , the surface albedo and normal characteristics , camera parameters and the arrangement of scene lighting and other objects in the scene .",31,0,46
dataset/preprocessed/test-data/face_alignment/0,"Hence , there are potentially infinite solutions to the problem .",32,0,11
dataset/preprocessed/test-data/face_alignment/0,"Learning statistical priors of the 3D facial shape and texture for "" in - the - wild "" images is currently very difficult by using modern acquisition devices .",33,0,29
dataset/preprocessed/test-data/face_alignment/0,"That is , even though there is a considerable improvement in 3D acquisition devices , they still can not operate in arbitrary conditions .",34,0,24
dataset/preprocessed/test-data/face_alignment/0,"Hence , all the current 3 D facial databases have been captured in controlled conditions .",35,0,16
dataset/preprocessed/test-data/face_alignment/0,"With the available 3 D facial data , it is feasible to learn a powerful statistical model of the facial shape that generalises well for both identity and expression .",36,0,30
dataset/preprocessed/test-data/face_alignment/0,"However , it is not possible to construct a statistical model of the facial texture that generalises well for "" in - the - wild "" images and is , at the same time , in correspondence with the statistical shape model .",37,0,43
dataset/preprocessed/test-data/face_alignment/0,That is the reason why current stateof - the - art 3 D face reconstruction methodologies rely solely on fitting a statistical 3 D facial shape prior on a sparse set of landmarks .,38,0,34
dataset/preprocessed/test-data/face_alignment/0,"In this paper , we make a number of contributions that enable the use of 3 DMMs for "" in - the -wild "" face reconstruction ( ) .",39,0,29
dataset/preprocessed/test-data/face_alignment/0,"In particular , our contributions are :",40,0,7
dataset/preprocessed/test-data/face_alignment/0,"We propose a methodology for learning a statistical texture model from "" in - the -wild "" facial images , which is in full correspondence with a statistical shape prior that exhibits both identity and expression variations .",41,0,38
dataset/preprocessed/test-data/face_alignment/0,"Motivated by the success of feature - based ( e.g. , HOG , SIFT ) Active Appearance Models ( AAMs ) we further show how to learn featurebased texture models for 3 DMMs .",42,0,34
dataset/preprocessed/test-data/face_alignment/0,"We show that the advantage of using the "" in - the -wild "" feature - based texture model is that the fitting strategy gets simplified since there is not need to optimize with respect to the illumination parameters .",43,0,40
dataset/preprocessed/test-data/face_alignment/0,"By capitalising on the recent advancements in fitting statistical deformable models , we propose a novel and fast algorithm for fitting "" in - the -wild "" 3 DMMs .",44,0,30
dataset/preprocessed/test-data/face_alignment/0,"Furthermore , we make the implementation of our algorithm publicly available , which we believe can be of great benefit to the community , given the lack of robust open - source implementations for fitting 3 DMMs .",45,0,38
dataset/preprocessed/test-data/face_alignment/0,"Due to lack of ground - truth data , the majority of the 3D face reconstruction papers report only qualitative results .",46,0,22
dataset/preprocessed/test-data/face_alignment/0,"In this paper , in order to provide quantitative evaluations , we collected anew dataset of 3D facial surfaces , using Kinect Fusion , which has many "" in - the -wild "" characteristics , even though it is captured indoors .",47,0,42
dataset/preprocessed/test-data/face_alignment/0,We release an open source implementation of our technique as part of the Menpo Project .,48,0,16
dataset/preprocessed/test-data/face_alignment/0,The remainder of the paper is structured as follows .,49,0,10
dataset/preprocessed/test-data/face_alignment/12,Dense Face Alignment,2,1,3
dataset/preprocessed/test-data/face_alignment/12,Face alignment is a classic problem in the computer vision field .,4,1,12
dataset/preprocessed/test-data/face_alignment/12,"Previous works mostly focus on sparse alignment with a limited number of facial landmark points , i.e. , facial landmark detection .",5,0,22
dataset/preprocessed/test-data/face_alignment/12,"In this paper , for the first time , we aim at providing a very dense 3D alignment for largepose face images .",6,0,23
dataset/preprocessed/test-data/face_alignment/12,"To achieve this , we train a CNN to estimate the 3D face shape , which not only aligns limited facial landmarks but also fits face contours and SIFT feature points .",7,0,32
dataset/preprocessed/test-data/face_alignment/12,"Moreover , we also address the bottleneck of training CNN with multiple datasets , due to different landmark markups on different datasets , such as 5 , 34 , 68 .",8,0,31
dataset/preprocessed/test-data/face_alignment/12,"Experimental results show our method not only provides highquality , dense 3 D face fitting but also outperforms the stateof - the - art facial landmark detection methods on the challenging datasets .",9,0,33
dataset/preprocessed/test-data/face_alignment/12,Our model can run at real time during testing and it 's available at,10,0,14
dataset/preprocessed/test-data/face_alignment/12,"Face alignment is a long - standing problem in the computer vision field , which is the process of aligning facial components , e.g. , eye , nose , mouth , and contour .",12,0,34
dataset/preprocessed/test-data/face_alignment/12,"An accurate face alignment is an essential prerequisite for many face related tasks , such as face recognition , 3 D face reconstruction and face animation .",13,0,27
dataset/preprocessed/test-data/face_alignment/12,"There are fruitful previous works on face alignment , which can be categorized as generative methods such as the early Active Shape Model and Active Appearance Model ( AAM ) based approaches , and discriminative methods such as regression - based approaches .",14,0,43
dataset/preprocessed/test-data/face_alignment/12,"Most previous methods estimate a sparse set of landmarks , e.g. , 68 landmarks .",15,0,15
dataset/preprocessed/test-data/face_alignment/12,"As this field is being developed , we believe that Dense Face Alignment ( DeFA ) becomes highly desired .",16,0,20
dataset/preprocessed/test-data/face_alignment/12,"Here , DeFA denotes that it 's doable to map any face - region pixel to the pixel in other face images , which has the same anatomical position inhuman faces .",17,0,32
dataset/preprocessed/test-data/face_alignment/12,"For example , given two face images from the same .",18,0,11
dataset/preprocessed/test-data/face_alignment/12,"A pair of images with their dense 3D shapes obtained by imposing landmark fitting constraint , contour fitting constraint and sift pair constraint .",19,0,24
dataset/preprocessed/test-data/face_alignment/12,"individual but with different poses , lightings or expressions , a perfect DeFA can even predict the mole ( i.e. darker pigment ) on two faces as the same position .",20,0,31
dataset/preprocessed/test-data/face_alignment/12,"Moreover , DeFA should offer dense correspondence not only between two face images , but also between the face image and the canonical 3 D face model .",21,0,28
dataset/preprocessed/test-data/face_alignment/12,This level of detailed geometry interpretation of a face image is invaluable to many conventional facial analysis problems mentioned above .,22,0,21
dataset/preprocessed/test-data/face_alignment/12,"Since this interpretation has gone beyond the sparse set of landmarks , fitting a dense 3 D face model to the face image is a reasonable way to achieve DeFA .",23,0,31
dataset/preprocessed/test-data/face_alignment/12,"In this work , we choose to develop the idea of fitting a dense 3 D face model to an image , where the model with thousands of vertexes makes it possible for face alignment to go very "" dense "" .",24,0,42
dataset/preprocessed/test-data/face_alignment/12,3 D face model fitting is well studied in the seminal work of 3D Morphorbal Model ( 3 DMM ) .,25,0,21
dataset/preprocessed/test-data/face_alignment/12,"We see a recent surge when it is applied to problems such as large - pose face alignment , 3D reconstruction , and face recognition , especially using the convolutional neural network ( CNN ) architecture .",26,0,37
dataset/preprocessed/test-data/face_alignment/12,"However , most prior works on 3D - model - fitting - based face alignment only utilize the sparse landmarks as supervision .",27,0,23
dataset/preprocessed/test-data/face_alignment/12,"There are two main challenges to be addressed in 3D face model fitting , in order to enable high - quality DeFA .",28,0,23
dataset/preprocessed/test-data/face_alignment/12,"First of all , to the best of our knowledge , no public face dataset has dense face shape labeling .",29,0,21
dataset/preprocessed/test-data/face_alignment/12,All of the in - the - wild face alignment datasets have no more than 68 landmarks in the labeling .,30,0,21
dataset/preprocessed/test-data/face_alignment/12,"Apparently , to provide a high - quality alignment for face - region pixels , we need information more than just the landmark labeling .",31,0,25
dataset/preprocessed/test-data/face_alignment/12,"Hence , the first challenge is to seek valuable information for additional supervision and in - tegrate them in the learning framework .",32,0,23
dataset/preprocessed/test-data/face_alignment/12,"Secondly , similar to many other data - driven problems and solutions , it is preferred that multiple datasets can be involved for solving face alignment task since a single dataset has limited types of variations .",33,0,37
dataset/preprocessed/test-data/face_alignment/12,"However , many face alignment methods can not leverage multiple datasets , because each dataset either is labeled differently .",34,0,20
dataset/preprocessed/test-data/face_alignment/12,"For instance , AFLW dataset contains a significant variation of poses , but has a few number of visible landmarks .",35,0,21
dataset/preprocessed/test-data/face_alignment/12,"In contrast , 300W dataset contains a large number of faces with 68 visible landmarks , but all faces are in a near - frontal view .",36,0,27
dataset/preprocessed/test-data/face_alignment/12,"Therefore , the second challenge is to allow the proposed method to leverage multiple face datasets .",37,0,17
dataset/preprocessed/test-data/face_alignment/12,"With the objective of addressing both challenges , we learn a CNN to fit a 3 D face model to the face image .",38,0,24
dataset/preprocessed/test-data/face_alignment/12,"While the proposed method works for any face image , we mainly pay attention to faces with large poses .",39,0,20
dataset/preprocessed/test-data/face_alignment/12,"Large - pose face alignment is a relatively new topic , and the performances in still have room to improve .",40,0,21
dataset/preprocessed/test-data/face_alignment/12,"To tackle first challenge of limited landmark labeling , we propose to employ additional constraints .",41,0,16
dataset/preprocessed/test-data/face_alignment/12,"We include contour constraint where the contour of the predicted shape should match the detected 2 D face boundary , and SIFT constraint where the SIFT key points detected on two face images of the same individual should map to the same vertexes on the 3D face model .",42,0,49
dataset/preprocessed/test-data/face_alignment/12,"Both constraints are integrated into the CNN training as additional loss function terms , where the end - to - end training results in an enhanced CNN for 3 D face model fitting .",43,0,34
dataset/preprocessed/test-data/face_alignment/12,"For the second challenge of leveraging multiple datasets , the 3D face model fitting approach has the inherent advantage in handling multiple training databases .",44,0,25
dataset/preprocessed/test-data/face_alignment/12,"Regardless of the landmark labeling number in a particular dataset , we can always define the corresponding 3 D vertexes to guide the training .",45,0,25
dataset/preprocessed/test-data/face_alignment/12,"Generally , our main contributions can be summarized as : 1 . We identify and define anew problem of dense face alignment , which seeks alignment of face - region pixels beyond the sparse set of landmarks .",46,0,38
dataset/preprocessed/test-data/face_alignment/12,"To achieve dense face alignment , we develop a novel 3 D face model fitting algorithm that adopts multiple constraints and leverages multiple datasets .",48,0,25
dataset/preprocessed/test-data/face_alignment/2,DeCaFA : Deep Convolutional Cascade for Face Alignment In The Wild,2,1,11
dataset/preprocessed/test-data/face_alignment/2,"Face Alignment is an active computer vision domain , that consists in localizing a number of facial landmarks that vary across datasets .",4,1,23
dataset/preprocessed/test-data/face_alignment/2,"State - of - the - art face alignment methods either consist in end - to - end regression , or in refining the shape in a cascaded manner , starting from an initial guess .",5,0,36
dataset/preprocessed/test-data/face_alignment/2,"In this paper , we introduce DeCaFA , an end - to - end deep convolutional cascade architecture for face alignment .",6,0,22
dataset/preprocessed/test-data/face_alignment/2,DeCaFA uses fully - convolutional stages to keep full spatial resolution throughout the cascade .,7,0,15
dataset/preprocessed/test-data/face_alignment/2,"Between each cascade stage , DeCaFA uses multiple chained transfer layers with spatial softmax to produce landmark - wise attention maps for each of several landmark alignment tasks .",8,0,29
dataset/preprocessed/test-data/face_alignment/2,"Weighted intermediate supervision , as well as efficient feature fusion between the stages allow to learn to progressively refine the attention maps in an end - to - end manner .",9,0,31
dataset/preprocessed/test-data/face_alignment/2,"We show experimentally that DeCaFA significantly outperforms existing approaches on 300W , CelebA and WFLW databases .",10,0,17
dataset/preprocessed/test-data/face_alignment/2,"In addition , we show that DeCaFA can learn fine alignment with reasonable accuracy from very few images using coarsely annotated data .",11,0,23
dataset/preprocessed/test-data/face_alignment/2,"Face alignment consists in localizing landmarks ( e.g. lips and eyes corners , pupils , nose tip ) on an face image .",13,0,23
dataset/preprocessed/test-data/face_alignment/2,"It is an important computer vision field , as it is an essential preprocess for face recognition , tracking , expression analysis , and face synthesis .",14,0,27
dataset/preprocessed/test-data/face_alignment/2,"Most recent face alignment approaches either belongs to cascaded regression methods , or to deep end - to - end regression methods .",15,0,23
dataset/preprocessed/test-data/face_alignment/2,"On the one 's hand , cascaded regression consists in learning a sequence of updates , starting from an initial guess , to refine the landmark localization in a coarse - to - fine manner .",16,0,36
dataset/preprocessed/test-data/face_alignment/2,"This allows to robustly learn rigid transformations , such as translation and rotation , in the first cascade stages , while learning non-rigid deformation ( e.g. due to facial expression or non-planar rotation ) later on .",17,0,37
dataset/preprocessed/test-data/face_alignment/2,"On the other hand , many deep approaches aim at regressing the landmark position from the original image directly .",18,0,20
dataset/preprocessed/test-data/face_alignment/2,"However , because annotating several landmarks on a face image is a tedious task , data is rather scarce and the nature of the annotations usually vary a lot between the databases .",19,0,33
dataset/preprocessed/test-data/face_alignment/2,"Because of the scarcity of the data , end - to - end approaches usually rely on learning an intermediate representation , such as edges detection to drive the alignment process .",20,0,32
dataset/preprocessed/test-data/face_alignment/2,"However , these representations are usually ad hoc and do not guarantee to be optimal to address landmark localization tasks .",21,0,21
dataset/preprocessed/test-data/face_alignment/2,"In this paper , we introduce a Deep convolutional Cascade for Face Alignment ( DeCaFA ) .",22,0,17
dataset/preprocessed/test-data/face_alignment/2,"DeCaFA is composed of several stages that each produce landmark - wise attention maps , relatively to heterogeneous annotation markups .",23,0,21
dataset/preprocessed/test-data/face_alignment/2,shows attention maps extracted by the subsequent DeCaFA stages ( horizontally ) and for three different markups ( vertically ) .,24,0,21
dataset/preprocessed/test-data/face_alignment/2,"It illustrates how these attention maps are refined through the successive stages , and how the different prediction tasks can benefit from each other .",25,0,25
dataset/preprocessed/test-data/face_alignment/2,The contributions of this paper are tree - fold :,26,0,10
dataset/preprocessed/test-data/face_alignment/2,"We introduce a fully - convolutional Deep Cascade for Face Alignment ( DeCaFA ) that unifies cascaded regression and end - to - end deep approaches , by using landmark - wise attention maps fused to extract local information around a current landmark estimate .",27,0,45
dataset/preprocessed/test-data/face_alignment/2,"We show that intermediate supervision with increasing weights helps DeCaFA to learn coarse attention maps in its early stages , that are refined in the later stages .",28,0,28
dataset/preprocessed/test-data/face_alignment/2,"Through chaining multiple transfer layers , DeCaFA integrates heterogeneous data annotated with different numbers of landmarks and model the intrinsic relationship between these tasks .",29,0,25
dataset/preprocessed/test-data/face_alignment/2,"We show experimentally that DeCaFA significantly outperforms existing approaches on multiple datasets , inluding the recent WFLW database .",30,0,19
dataset/preprocessed/test-data/face_alignment/2,"Additionally , we highlight how coarsely annotated data helps the network to learn fine landmark alignment even with very few annotated images .",31,0,23
dataset/preprocessed/test-data/face_alignment/2,"Popular examples of cascaded regression methods include SDM : in their pioneering work ,",33,0,14
dataset/preprocessed/test-data/face_alignment/2,Xiong et al show that using simple linear regressors upon SIFT features in a cascaded manner already provides satisfying alignment results .,34,0,22
dataset/preprocessed/test-data/face_alignment/2,LBF [ 14 ] is a refinement that employs randomized decision trees to dramatically speedup feature extraction .,35,0,18
dataset/preprocessed/test-data/face_alignment/2,DAN uses deep networks to learn each cascade stage .,36,0,10
dataset/preprocessed/test-data/face_alignment/2,"However , one downside of these approaches is that the update regressors are not learned jointly in a end - to - end fashion , thus there is no guarantee that the learned feature point alignment sequences might be optimal .",37,0,41
dataset/preprocessed/test-data/face_alignment/2,MDM improves the feature extraction process by sharing the convolutional layer among all steps of the cascade that are performed through a recurrent neural network .,38,0,26
dataset/preprocessed/test-data/face_alignment/2,This results in memory footprint reduction as well as better representation learning and a more optimized landmark trajectory throughout the cascade .,39,0,22
dataset/preprocessed/test-data/face_alignment/2,TCDCN was perhaps the first end - to - end framework that could compete with cascaded regression approaches .,40,0,19
dataset/preprocessed/test-data/face_alignment/2,It relies on supervised pretraining on a wide database of facial attributes .,41,0,13
dataset/preprocessed/test-data/face_alignment/2,"More recently , PCD - CNN uses head pose information to drive the training process .",42,0,16
dataset/preprocessed/test-data/face_alignment/2,CPM + SBR employs landmark registration to regularize training .,43,0,10
dataset/preprocessed/test-data/face_alignment/2,"SAN uses adversarial networks to convert images from different styles to an aggregated style , upon which regression is performed .",44,0,21
dataset/preprocessed/test-data/face_alignment/2,This aggregated style space thus serve as an intermediate representation that is more convenient for training .,45,0,17
dataset/preprocessed/test-data/face_alignment/2,"In the authors propose to use edge map estimation as an intermediate representation to drive the landmark prediction task , as well as to provide a unified representation when images are annotated in terms of different markups , that correspond to different alignment tasks .",46,0,45
dataset/preprocessed/test-data/face_alignment/2,"Finally , DSRN relies on Fourier Embedding and low - rank learning to produce such representation .",47,0,17
dataset/preprocessed/test-data/face_alignment/2,"However , the use of such intermediate representation is usually ad hoc and it is hard to know which one would be all - around better for face alignment .",48,0,30
dataset/preprocessed/test-data/face_alignment/2,"Recently , AAN proposes to use intermediate feature maps as attentional masks to select relevant spatial regions .",49,0,18
dataset/preprocessed/test-data/face_alignment/4,Facial Landmarks Detection by Self - Iterative Regression based Landmarks - Attention Network,2,1,13
dataset/preprocessed/test-data/face_alignment/4,"Cascaded Regression ( CR ) based methods have been proposed to solve facial landmarks detection problem , which learn a series of descent directions by multiple cascaded regressors separately trained in coarse and fine stages .",4,0,36
dataset/preprocessed/test-data/face_alignment/4,They outperform the traditional gradient descent based methods in both accuracy and running speed .,5,0,15
dataset/preprocessed/test-data/face_alignment/4,"However , cascaded regression is not robust enough because each regressor 's training data comes from the output of previous regressor .",6,0,22
dataset/preprocessed/test-data/face_alignment/4,"Moreover , training multiple regressors requires lots of computing resources , especially for deep learning based methods .",7,0,18
dataset/preprocessed/test-data/face_alignment/4,"In this paper , we develop a Self - Iterative Regression ( SIR ) framework to improve the model efficiency .",8,0,21
dataset/preprocessed/test-data/face_alignment/4,"Only one self - iterative regressor is trained to learn the descent directions for samples from coarse stages to fine stages , and parameters are iteratively updated by the same regressor .",9,0,32
dataset/preprocessed/test-data/face_alignment/4,"Specifically , we proposed Landmarks - Attention Network ( LAN ) as our regressor , which concurrently learns features around each landmark and obtains the holistic location increment .",10,0,29
dataset/preprocessed/test-data/face_alignment/4,"By doing so , not only the rest of regressors are removed to simplify the training process , but the number of model parameters is significantly decreased .",11,0,28
dataset/preprocessed/test-data/face_alignment/4,"The experiments demonstrate that with only 3.72 M model parameters , our proposed method achieves the stateof - the - art performance .",12,0,23
dataset/preprocessed/test-data/face_alignment/4,"Facial landmarks detection is one of the most important techniques in face analysis , such as face recognition , facial animation and 3D face reconstruction .",14,0,26
dataset/preprocessed/test-data/face_alignment/4,"It aims to detect the facial landmarks such as eyes , nose and mouth , namely predicting the location parameters of landmarks .",15,0,23
dataset/preprocessed/test-data/face_alignment/4,Researchers usually regard this task as atypical non -linear least squares problem .,16,0,13
dataset/preprocessed/test-data/face_alignment/4,"The Newton 's method and its variants are the traditional gradient based solution , whose convergence rate is quadratic and is guaranteed to converge , provided that the initial estimate is sufficiently close to the minimum .",17,0,37
dataset/preprocessed/test-data/face_alignment/4,"However , when the objective function is not differentiable ( e.g. SIFT ) or the Hessian matrix is not positive definite , the method wo n't works well .",18,0,29
dataset/preprocessed/test-data/face_alignment/4,"In recent years , cascaded regression based methods Copyright c 2018 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",19,0,23
dataset/preprocessed/test-data/face_alignment/4,All rights reserved .,20,0,4
dataset/preprocessed/test-data/face_alignment/4,( a ) Cascaded Regression .,21,0,6
dataset/preprocessed/test-data/face_alignment/4,( b ) Self - Iterative Regression . :,22,0,9
dataset/preprocessed/test-data/face_alignment/4,Facial landmarks detection process of Cascaded Regression ( a ) and Self - Iterative Regression ( b ) .,23,0,19
dataset/preprocessed/test-data/face_alignment/4,"To predict the landmarks ' location parameters , the CR based methods require multiple regressors , while SIR just need one regressor and updates parameters iteratively .",24,0,27
dataset/preprocessed/test-data/face_alignment/4,have been proposed and applied to solve the non-linear least squares problem .,25,0,13
dataset/preprocessed/test-data/face_alignment/4,"They usually train multiple regressors to predict the parameters ' increment sequentially , which outperform the traditional gradient descent based methods in both accuracy and running speed .",26,0,28
dataset/preprocessed/test-data/face_alignment/4,"Moreover , deep learning based cascaded regression methods ) are widely leveraged for this task because of the powerful ability to extract the discriminative feature .",27,0,26
dataset/preprocessed/test-data/face_alignment/4,"However , when applying cascaded regression system , three main problems arise :",28,0,13
dataset/preprocessed/test-data/face_alignment/4,"( 1 ) Each regressor just works well in its local data space , when previous regressor predicts the false descent direction , the final results are very likely to drift away ; ( 2 ) In general , higher accuracy can be obtained by adding more cascaded regressors , while it will increase model storage memory and computing resources ;",29,0,61
dataset/preprocessed/test-data/face_alignment/4,"( 3 ) Subsequent regressors usually can not be activated for training until previous regressors finished their training process , which increases the system complexity .",30,0,26
dataset/preprocessed/test-data/face_alignment/4,"In this paper , we develop a Self - Iterative Regression ( SIR ) framework to solve the above issues .",31,0,21
dataset/preprocessed/test-data/face_alignment/4,"By means of the powerful representation of Convolutional Neural Network ( CNN ) , we only train one regressor to learn the descent directions in coarse and fine stages together .",32,0,31
dataset/preprocessed/test-data/face_alignment/4,"The training data is obtained by random sampling in the parameter space , and in the test - ing process , parameters are updated iteratively by calling the same regressor , which is dubbed Self - Iterative Regression .",33,0,39
dataset/preprocessed/test-data/face_alignment/4,The testing process is illustrated in ( b ) .,34,0,10
dataset/preprocessed/test-data/face_alignment/4,"The experimental results show that for deep learning based method , one regressor achieves comparable performance to state - of the - art multiple cascaded regressors and significantly reduce the training complexity .",35,0,33
dataset/preprocessed/test-data/face_alignment/4,"Moreover , to obtain discriminative landmarks features , we proposed a Landmarks - Attention Network ( LAN ) , which focuses on the appearance around landmarks .",36,0,27
dataset/preprocessed/test-data/face_alignment/4,"It first concurrently extracts local landmarks ' features and then obtains the holistic increment , which significantly reduces the dimension of the final feature layer and the number of model parameters .",37,0,32
dataset/preprocessed/test-data/face_alignment/4,The contributions of this paper are summarized as follows :,38,0,10
dataset/preprocessed/test-data/face_alignment/4,"1 . We propose a novel regression framework called SIR to solve the non-linear least squares problem , which simplifies the cascaded regression framework and obtains stateof - the - art performance in facial landmarks detection task .",39,0,38
dataset/preprocessed/test-data/face_alignment/4,"2 . The Landmarks - Attention Network ( LAN ) is developed to independently learn discriminative features around each landmarks , which significantly reduces the dimension of feature layer and the number of model parameters .",40,0,36
dataset/preprocessed/test-data/face_alignment/4,3 . Experimental results on several publicly available benchmarks demonstrate the effectiveness of the proposed method .,41,0,17
dataset/preprocessed/test-data/face_alignment/4,"In this section , we will review related works in solving nonlinear least squares problems , especially facial landmarks detection problem. , which consists of multiple Stacked Auto - encoder Networks ( SANs ) .",43,0,35
dataset/preprocessed/test-data/face_alignment/4,"The first SAN quickly predicts the preliminary location of landmarks by a low - resolution image , and the subsequent SANs then refine the location with higher and higher resolution .",44,0,31
dataset/preprocessed/test-data/face_alignment/4,"Trigeorgis et al. proposed the Mnemonic Descent Method ( MDM ) , which regards the nonlinear least squares optimization as a dynamic process .",45,0,24
dataset/preprocessed/test-data/face_alignment/4,The Recurrent Neural Network ( RNN ) is introduced to maintain an internal memory unit that accumulates the history information so as to relate the cascaded refinement process .,46,0,29
dataset/preprocessed/test-data/face_alignment/4,Joo et al. proposed a iterative error feedback ( Carreira et al. 2016 ) method to solve the human pose extimation problems .,47,0,23
dataset/preprocessed/test-data/face_alignment/4,"Same with MDM , their training data is generated by previous stages , while ours is obtained by random sampling in coarse stages and fine stages , which simplifies the training process .",48,0,33
dataset/preprocessed/test-data/face_alignment/4,Xiao et al.,49,0,3
dataset/preprocessed/test-data/face_alignment/13,Nonlinear 3D Face Morphable Model,2,0,5
dataset/preprocessed/test-data/face_alignment/13,"As a classic statistical model of 3D facial shape and texture , 3D Morphable Model ( 3 DMM ) is widely used in facial analysis , e.g. , model fitting , image synthesis .",4,0,34
dataset/preprocessed/test-data/face_alignment/13,"Conventional 3 DMM is learned from a set of well - controlled 2 D face images with associated 3 D face scans , and represented by two sets of PCA basis functions .",5,0,33
dataset/preprocessed/test-data/face_alignment/13,"Due to the type and amount of training data , as well as the linear bases , the representation power of 3 DMM can be limited .",6,0,27
dataset/preprocessed/test-data/face_alignment/13,"To address these problems , this paper proposes an innovative framework to learn a nonlinear 3 DMM model from a large set of unconstrained face images , without collecting 3 D face scans .",7,0,34
dataset/preprocessed/test-data/face_alignment/13,"Specifically , given a face image as input , a network encoder estimates the projection , shape and texture parameters .",8,0,21
dataset/preprocessed/test-data/face_alignment/13,"Two decoders serve as the nonlinear 3 DMM to map from the shape and texture parameters to the 3D shape and texture , respectively .",9,0,25
dataset/preprocessed/test-data/face_alignment/13,"With the projection parameter , 3D shape , and texture , a novel analytically - differentiable rendering layer is designed to reconstruct the original input face .",10,0,27
dataset/preprocessed/test-data/face_alignment/13,The entire network is end - to - end trainable with only weak supervision .,11,0,15
dataset/preprocessed/test-data/face_alignment/13,"We demonstrate the superior representation power of our nonlinear 3 DMM over its linear counterpart , and its contribution to face alignment and 3D reconstruction .",12,1,26
dataset/preprocessed/test-data/face_alignment/13,3D Morphable Model ( 3DMM ) is a statistical model of 3 D facial shape and texture in a space where there are explicit correspondences .,15,0,26
dataset/preprocessed/test-data/face_alignment/13,"The morphable model framework provides two key benefits : first , a point - to - point correspondence between the reconstruction and all other models , enabling morphing , and second , modeling underlying transformations between types of faces ( male to female , neutral to smile , etc . ) .",16,0,52
dataset/preprocessed/test-data/face_alignment/13,"3 DMM has been widely applied in numerous areas , such as computer vision , graphics , human behavioral analysis and craniofacial surgery .",17,0,24
dataset/preprocessed/test-data/face_alignment/13,"3 DMM is learnt through supervision by performing dimension reduction , normally Principal Component Anal - Project page : http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html shape / texture , which are trained with 3 D face scans and associated controlled 2D images .",18,0,38
dataset/preprocessed/test-data/face_alignment/13,We propose a nonlinear 3 DMM to model shape / texture via deep neural networks ( DNNs ) .,19,0,19
dataset/preprocessed/test-data/face_alignment/13,"It can be trained from in - the - wild face images without 3 D scans , and also better reconstructs the original images due to the inherent nonlinearity .",20,0,30
dataset/preprocessed/test-data/face_alignment/13,"ysis ( PCA ) , on a training set of face images / scans .",21,0,15
dataset/preprocessed/test-data/face_alignment/13,"To model highly variable 3 D face shapes , a large amount of high - quality 3 D face scans is required .",22,0,23
dataset/preprocessed/test-data/face_alignment/13,"However , this requirement is expensive to fulfill .",23,0,9
dataset/preprocessed/test-data/face_alignment/13,The first 3 DMM was built from scans of 200 subjects with a similar ethnicity / age group .,24,0,19
dataset/preprocessed/test-data/face_alignment/13,"They were also captured in well - controlled conditions , with only neutral expressions .",25,0,15
dataset/preprocessed/test-data/face_alignment/13,"Hence , it is fragile to large variances in the face identity .",26,0,13
dataset/preprocessed/test-data/face_alignment/13,The widely used Basel Face Model ( BFM ) is also built with only 200 subjects in neutral expressions .,27,0,20
dataset/preprocessed/test-data/face_alignment/13,Lack of expression can be compensated using expression bases from FaceWarehouse or BD - 3FE .,28,0,16
dataset/preprocessed/test-data/face_alignment/13,"After more than a decade , almost all models useless than 300 training scans .",29,0,15
dataset/preprocessed/test-data/face_alignment/13,Such a small training set is far from adequate to describe the full variability of human faces .,30,0,18
dataset/preprocessed/test-data/face_alignment/13,"Only recently , Booth et al. spent a significant effort to build 3 DMM from scans of ? 10 , 000 subjects .",31,0,23
dataset/preprocessed/test-data/face_alignment/13,"Second , the texture model of 3 DMM is normally built with a small number of 2 D face images co-captured with 3D scans , under well - controlled conditions .",32,0,31
dataset/preprocessed/test-data/face_alignment/13,"Therefore , such a model is only learnt to represent the facial texture in similar conditions , rather than in - the - wild environments .",33,0,26
dataset/preprocessed/test-data/face_alignment/13,This substantially limits the application scenarios of 3DMM .,34,0,9
dataset/preprocessed/test-data/face_alignment/13,"Finally , the representation power of 3 DMM is limited by not only the size of training set but also its formulation .",35,0,23
dataset/preprocessed/test-data/face_alignment/13,The facial variations are nonlinear in nature .,36,0,8
dataset/preprocessed/test-data/face_alignment/13,"E.g. , the variations in different facial expressions or poses are nonlinear , which violates the linear assumption of PCA - based models .",37,0,24
dataset/preprocessed/test-data/face_alignment/13,"Thus , a PCA model is unable to interpret facial variations well .",38,0,13
dataset/preprocessed/test-data/face_alignment/13,"Given the barrier of 3 DMM in its data , supervision and linear bases , this paper aims to revolutionize the paradigm of learning 3 DMM by answering a fundamental question :",39,0,32
dataset/preprocessed/test-data/face_alignment/13,"Whether and how can we learn a nonlinear 3D Morphable Model of face shape and texture from a set of unconstrained 2 D face images , without collecting 3 D face scans ?",40,0,33
dataset/preprocessed/test-data/face_alignment/13,"If the answer were yes , this would be in sharp contrast to the conventional 3 DMM approach , and remedy all aforementioned limitations .",41,0,25
dataset/preprocessed/test-data/face_alignment/13,"Fortunately , we have developed approaches that offer positive answers to this question .",42,0,14
dataset/preprocessed/test-data/face_alignment/13,"Therefore , the core of this paper is regarding how to learn this new 3 DMM , what is the representation power of the model , and what is the benefit of the model to facial analysis .",43,0,38
dataset/preprocessed/test-data/face_alignment/13,"As shown in , starting with an observation that the linear 3 DMM formulation is equivalent to a single layer network , using a deep network architecture naturally increases the model capacity .",44,0,33
dataset/preprocessed/test-data/face_alignment/13,"Hence , we utilize two network decoders , instead of two PCA spaces , as the shape and texture model components , respectively .",45,0,24
dataset/preprocessed/test-data/face_alignment/13,"With careful consideration of each component , we design different networks for shape and texture : the multi - layer perceptron ( MLP ) for shape and convolutional neural network ( CNN ) for texture .",46,0,36
dataset/preprocessed/test-data/face_alignment/13,Each decoder will take a shape or texture representation as input and output the dense 3 D face or a face texture .,47,0,23
dataset/preprocessed/test-data/face_alignment/13,These two decoders are essentially the nonlinear 3 DMM .,48,0,10
dataset/preprocessed/test-data/face_alignment/13,"Further , we learn the fitting algorithm to our nonlinear 3 DMM , which is formulated as a CNN encoder .",49,0,21
dataset/preprocessed/test-data/data-to-text_generation/3,Pragmatically Informative Text Generation,2,1,4
dataset/preprocessed/test-data/data-to-text_generation/3,We improve the informativeness of models for conditional text generation using techniques from computational pragmatics .,4,0,16
dataset/preprocessed/test-data/data-to-text_generation/3,"These techniques formulate language production as a game between speakers and listeners , in which a speaker should generate output text that a listener can use to correctly identify the original input that the text describes .",5,0,37
dataset/preprocessed/test-data/data-to-text_generation/3,"While such approaches are widely used in cognitive science and grounded language learning , they have received less attention for more standard language generation tasks .",6,0,26
dataset/preprocessed/test-data/data-to-text_generation/3,"We consider two pragmatic modeling methods for text generation : one where pragmatics is imposed by information preservation , and another where pragmatics is imposed by explicit modeling of distractors .",7,0,31
dataset/preprocessed/test-data/data-to-text_generation/3,We find that these methods improve the performance of strong existing systems for abstractive summarization and generation from structured meaning representations .,8,0,22
dataset/preprocessed/test-data/data-to-text_generation/3,Computational approaches to pragmatics cast language generation and interpretation as gametheoretic or Bayesian inference procedures .,10,1,16
dataset/preprocessed/test-data/data-to-text_generation/3,"While such approaches are capable of modeling a variety of pragmatic phenomena , their main application in natural language processing has been to improve the informativeness of generated text in grounded language learning problems .",11,0,35
dataset/preprocessed/test-data/data-to-text_generation/3,"In this paper , we show that pragmatic reasoning can be similarly used to improve performance in more traditional language generation tasks like generation from structured meaning representations ) and summarization .",12,0,32
dataset/preprocessed/test-data/data-to-text_generation/3,"Our work builds on a line of learned Rational Speech Acts ( RSA ) models , in which generated strings are selected to optimize the behav - Human - written A cheap coffee shop in riverside with a 5 out of 5 customer rating is Fitzbillies .",13,0,47
dataset/preprocessed/test-data/data-to-text_generation/3,Fitzbillies is family friendly and serves English food .,14,0,9
dataset/preprocessed/test-data/data-to-text_generation/3,Base sequence - to - sequence model ( S0 ) Fitzbillies is a family friendly coffee shop located near the river .,15,0,22
dataset/preprocessed/test-data/data-to-text_generation/3,Distractor - based pragmatic system ( S D 1 ),16,0,10
dataset/preprocessed/test-data/data-to-text_generation/3,Fitzbillies is a family friendly coffee shop that serves English food .,17,0,12
dataset/preprocessed/test-data/data-to-text_generation/3,It is located in riverside area .,18,0,7
dataset/preprocessed/test-data/data-to-text_generation/3,It has a customer rating of 5 out of 5 and is cheap .,19,0,14
dataset/preprocessed/test-data/data-to-text_generation/3,Reconstructor - based pragmatic system ( S R 1 ),20,0,10
dataset/preprocessed/test-data/data-to-text_generation/3,Fitzbillies is a family friendly coffee shop that serves cheap English food in the riverside area .,21,0,17
dataset/preprocessed/test-data/data-to-text_generation/3,It has a customer rating of 5 out of 5 . :,22,0,12
dataset/preprocessed/test-data/data-to-text_generation/3,Example outputs of our systems on the E2E generation task .,23,0,11
dataset/preprocessed/test-data/data-to-text_generation/3,"While a base sequence - to - sequence model ( S 0 , Sec. 2 ) fails to describe all attributes in the input meaning representation , both of our pragmatic systems ( S R 1 , Sec. 3.1 and SD 1 , Sec. 3.2 ) and the human - written reference do .",24,0,54
dataset/preprocessed/test-data/data-to-text_generation/3,ior of an embedded listener model .,25,0,7
dataset/preprocessed/test-data/data-to-text_generation/3,"The canonical presentation of the RSA framework ( Frank and Goodman , 2012 ) is grounded in reference resolution : models of speakers attempt to describe referents in the presence of distractors , and models of listeners attempt to resolve descriptors to referents .",26,0,44
dataset/preprocessed/test-data/data-to-text_generation/3,"Recent work has extended these models to more complex groundings , including images and trajectories .",27,0,16
dataset/preprocessed/test-data/data-to-text_generation/3,"The techniques used in these settings are similar , and the primary intuition of the RSA framework is preserved : from the speaker 's perspective , a good description is one that picks out , as discriminatively as possible , the content the speaker intends for the listener to identify .",28,0,51
dataset/preprocessed/test-data/data-to-text_generation/3,"Outside of grounding , cognitive modeling , and targeted analysis of linguistic phenomena , rational speech acts models have seen limited application in the natural language processing literature .",29,0,29
dataset/preprocessed/test-data/data-to-text_generation/3,"In this work we show that they can be extended to a distinct class of language generation problems that use as referents structured descriptions of lingustic content , or other natural language texts .",30,0,34
dataset/preprocessed/test-data/data-to-text_generation/3,"In accordance with the maxim of quantity or the Q-principle , pragmatic approaches naturally correct underinformativeness problems observed in state - of - theart language generation systems ( S 0 in ) .",31,0,33
dataset/preprocessed/test-data/data-to-text_generation/3,We present experiments on two language generation tasks : generation from meaning representations and summarization .,32,0,16
dataset/preprocessed/test-data/data-to-text_generation/3,"For each task , we evaluate two models of pragmatics : the reconstructor - based model of and the distractor - based model of .",33,0,25
dataset/preprocessed/test-data/data-to-text_generation/3,"Both models improve performance on both tasks , increasing ROUGE scores by 0.2-0.5 points on the CNN / Daily Mail abstractive summarization dataset and BLEU scores by 2 points on the End - to - End ( E2E ) generation dataset , obtaining new state - of - the - art results .",34,0,53
dataset/preprocessed/test-data/data-to-text_generation/3,"We formulate a conditional generation task as taking an input i from a space of possible inputs I ( e.g. , input sentences for abstractive summarization ; meaning representations for structured generation ) and producing an output o as a sequence of tokens ( o 1 , . . . , o T ) .",36,0,55
dataset/preprocessed/test-data/data-to-text_generation/3,"We build our pragmatic approaches on top of learned base speaker models S 0 , which produce a probability distribution S 0 ( o | i ) over output text for a given input .",37,0,35
dataset/preprocessed/test-data/data-to-text_generation/3,"We focus on two conditional generation tasks where the information in the input context should largely be preserved in the output text , and apply the pragmatic procedures outlined in Sec. 3 to each task .",38,0,36
dataset/preprocessed/test-data/data-to-text_generation/3,"For these S 0 models we use systems from past work thatare strong , but may still be underinformative relative to human reference outputs ( e.g. , ) .",39,0,29
dataset/preprocessed/test-data/data-to-text_generation/3,We evaluate on the E2E task of generation from meaning representations containing restaurant attributes ) .,41,0,16
dataset/preprocessed/test-data/data-to-text_generation/3,"We report the task 's five automatic metrics : BLEU , NIST , METEOR , ROUGE - L and CIDEr .",42,0,21
dataset/preprocessed/test-data/data-to-text_generation/3,compares the performance of our base S 0 and pragmatic models to the baseline T - Gen system and the best previous result from the 20 primary systems evaluated in the E2E challenge .,43,0,34
dataset/preprocessed/test-data/data-to-text_generation/3,"The systems obtaining these results encompass a range of approaches : a template system ) , a neural model , models trained with reinforcement learning , and systems using ensembling and reranking .",44,0,33
dataset/preprocessed/test-data/data-to-text_generation/3,"To ensure that the benefit of the reconstructor - based pragmatic approach , which uses two models , is not due solely to a model combination effect , we also compare to an ensemble of two base models ( S 0 2 ) .",45,0,44
dataset/preprocessed/test-data/data-to-text_generation/3,"This ensemble uses a weighted combination of scores of two independently - trained S 0 models , following Eq. 1 ( with weights tuned on the development data ) .",46,0,30
dataset/preprocessed/test-data/data-to-text_generation/3,"Both of our pragmatic systems improve over the strong baseline S 0 system on all five metrics , with the largest improvements ( 2.1 BLEU , 0.2 NIST , 0.8 METEOR , 1.5 ROUGE - L , and 0.1 CIDEr ) from the S R 1 model .",47,0,48
dataset/preprocessed/test-data/data-to-text_generation/3,"This S R 1 model outperforms the previous best results obtained by any system in the E2E challenge on BLEU , NIST , and CIDEr , with comparable performance on METEOR and ROUGE - L. :",48,0,36
dataset/preprocessed/test-data/data-to-text_generation/3,Test results for the non-anonymized CNN / Daily Mail summarization task .,49,0,12
dataset/preprocessed/test-data/data-to-text_generation/1,A Deep Ensemble Model with Slot Alignment for Sequence - to - Sequence Natural Language Generation,2,1,16
dataset/preprocessed/test-data/data-to-text_generation/1,Natural language generation lies at the core of generative dialogue systems and conversational agents .,4,1,15
dataset/preprocessed/test-data/data-to-text_generation/1,"We describe an ensemble neural language generator , and present several novel methods for data representation and augmentation that yield improved results in our model .",5,0,26
dataset/preprocessed/test-data/data-to-text_generation/1,"We test the model on three datasets in the restaurant , TV and laptop domains , and report both objective and subjective evaluations of our best model .",6,0,28
dataset/preprocessed/test-data/data-to-text_generation/1,"Using a range of automatic metrics , as well as human evaluators , we show that our approach achieves better results than state - of - the - art models on the same datasets .",7,0,35
dataset/preprocessed/test-data/data-to-text_generation/1,"There has recently been a substantial amount of research in natural language processing ( NLP ) in the context of personal assistants , such as Cortana or Alexa .",9,0,29
dataset/preprocessed/test-data/data-to-text_generation/1,"The capabilities of these conversational agents are still fairly limited and lacking in various aspects , one of the most challenging of which is the ability to produce utterances with humanlike coherence and naturalness for many different kinds of content .",10,0,41
dataset/preprocessed/test-data/data-to-text_generation/1,This is the responsibility of the natural language generation ( NLG ) component .,11,0,14
dataset/preprocessed/test-data/data-to-text_generation/1,Our work focuses on language generators whose inputs are structured meaning representations ( MRs ) .,12,0,16
dataset/preprocessed/test-data/data-to-text_generation/1,An MR describes a single dialogue act with a list of key concepts which need to be conveyed to the human user during the dialogue .,13,0,26
dataset/preprocessed/test-data/data-to-text_generation/1,"Each piece of information is represented by a slotvalue pair , where the slot identifies the type of information and the value is the corresponding content .",14,0,27
dataset/preprocessed/test-data/data-to-text_generation/1,"Dialogue act ( DA ) types vary depending on the dialogue manager , ranging from simple ones , such as a goodbye DA with no slots at all , to complex ones , such as an inform DA containing multiple slots with various types of values ( see example in Utt .",15,0,52
dataset/preprocessed/test-data/data-to-text_generation/1,"The Bakers , kid - friendly restaurant , The Golden Curry , offers Japanese cuisine with a moderate price range .",17,0,21
dataset/preprocessed/test-data/data-to-text_generation/1,A natural language generator must produce a syntactically and semantically correct utterance from a given MR .,18,0,17
dataset/preprocessed/test-data/data-to-text_generation/1,"The utterance should express all the information contained in the MR , in a natural and conversational way .",19,0,19
dataset/preprocessed/test-data/data-to-text_generation/1,"In traditional language generator architectures , the assembling of an utterance from an MR is performed in two stages : sentence planning , which enforces semantic correctness and determines the structure of the utterance , and surface realization , which enforces syntactic correctness and produces the final utterance form .",20,0,50
dataset/preprocessed/test-data/data-to-text_generation/1,Earlier work on statistical NLG approaches were typically hybrids of a handcrafted component and a statistical training method .,21,0,19
dataset/preprocessed/test-data/data-to-text_generation/1,"The handcrafted aspects , however , lead to decreased portability and potentially limit the variability of the outputs .",22,0,19
dataset/preprocessed/test-data/data-to-text_generation/1,New corpusbased approaches emerged that used semantically aligned data to train language models that output utterances directly from their MRs .,23,0,21
dataset/preprocessed/test-data/data-to-text_generation/1,"The alignment provides valuable information during training , but the semantic annotation is costly .",24,0,15
dataset/preprocessed/test-data/data-to-text_generation/1,"The most recent methods do not require aligned data and use an end - to - end approach to training , performing sentence planning and surface realization simultaneously .",25,0,29
dataset/preprocessed/test-data/data-to-text_generation/1,"The most successful systems trained on unaligned data use recurrent neural networks ( RNNs ) paired with an encoder - decoder system design , but also other concepts , such as imitation learning .",26,0,34
dataset/preprocessed/test-data/data-to-text_generation/1,"These NLG models , however , typically require greater amount of data for training due to the lack of semantic alignment , and they still have problems producing syntactically and semantically correct output , as well as being limited in naturalness .",27,0,42
dataset/preprocessed/test-data/data-to-text_generation/1,"Here we present a neural ensemble natural language generator , which we train and test on three large unaligned datasets in the restaurant , television , and laptop domains .",28,0,30
dataset/preprocessed/test-data/data-to-text_generation/1,"We explore novel ways to represent the MR inputs , including novel methods for delexicalizing slots and their values , automatic slot alignment , as well as the use of a semantic reranker .",29,0,34
dataset/preprocessed/test-data/data-to-text_generation/1,We use automatic evaluation metrics to show that these methods appreciably improve the performance of our model .,30,0,18
dataset/preprocessed/test-data/data-to-text_generation/1,"On the largest of the datasets , the E2E dataset with nearly 50 K samples , we also demonstrate that our model significantly outperforms the baseline E2E NLG Challenge 1 system in human evaluation .",31,0,35
dataset/preprocessed/test-data/data-to-text_generation/1,"Finally , after augmenting our model with stylistic data selection , subjective evaluations reveal that it can still produce over all better results despite a significantly reduced training set .",32,0,30
dataset/preprocessed/test-data/data-to-text_generation/1,NLG is closely related to machine translation and has similarly benefited from recent rapid development of deep learning methods .,34,0,20
dataset/preprocessed/test-data/data-to-text_generation/1,State - of - the - art NLG systems build thus on deep neural sequenceto - sequence models with an encoder - decoder architecture equipped with an attention mechanism .,35,0,30
dataset/preprocessed/test-data/data-to-text_generation/1,"They typically also rely on slot delexicalization , which allows the model to better generalize to unseen inputs , as exemplified by TGen .",36,0,24
dataset/preprocessed/test-data/data-to-text_generation/1,"However , point out that there are frequent scenarios where delexicalization behaves inadequately ( see Section 5.1 for more details ) , and show that a character - level approach to NLG may avoid the need for delexicalization , at the potential cost of making more semantic omission errors .",37,0,50
dataset/preprocessed/test-data/data-to-text_generation/1,The end - to - end approach to NLG typically requires a mechanism for aligning slots on the output utterances : this allows the model to generate Our work builds upon the successful attentional encoder - decoder framework for sequenceto - sequence learning and expands it through ensembling .,38,0,49
dataset/preprocessed/test-data/data-to-text_generation/1,"We explore the feasibility of a domainindependent slot aligner that could be applied to any dataset , regardless of its size , and beyond the reranking task .",39,0,28
dataset/preprocessed/test-data/data-to-text_generation/1,"We also tackle some challenges caused by delexicalization in order to improve the quality of surface realizations , while retaining the ability of the neural model to generalize .",40,0,29
dataset/preprocessed/test-data/data-to-text_generation/1,We evaluated the models on three datasets from different domains .,42,0,11
dataset/preprocessed/test-data/data-to-text_generation/1,The primary one is the recently released E2E restaurant dataset with 48 K samples .,43,0,15
dataset/preprocessed/test-data/data-to-text_generation/1,"For benchmarking we use the TV dataset and the Laptop dataset with 7 K and 13K samples , respectively .",44,0,20
dataset/preprocessed/test-data/data-to-text_generation/1,"summarizes the proportions of the training , validation , and test sets for each dataset .",45,0,16
dataset/preprocessed/test-data/data-to-text_generation/1,The E2E dataset is by far the largest one available for task - oriented language generation in the restaurant domain .,48,0,21
dataset/preprocessed/test-data/data-to-text_generation/1,The human references were Note that the number of MRs in the E2E dataset was cutoff at 10 K for the sake of visibility of the small differences between other column pairs .,49,0,33
dataset/preprocessed/test-data/data-to-text_generation/5,Step - by - Step : Separating Planning from Realization in Neural Data - to - Text Generation,2,1,18
dataset/preprocessed/test-data/data-to-text_generation/5,"Data - to - text generation can be conceptually divided into two parts : ordering and structuring the information ( planning ) , and generating fluent language describing the information ( realization ) .",5,1,34
dataset/preprocessed/test-data/data-to-text_generation/5,Modern neural generation systems conflate these two steps into a single end - to - end differentiable system .,6,0,19
dataset/preprocessed/test-data/data-to-text_generation/5,"We propose to split the generation process into a symbolic text - planning stage that is faithful to the input , followed by a neural generation stage that focuses only on realization .",7,0,33
dataset/preprocessed/test-data/data-to-text_generation/5,"For training a plan - to - text generator , we present a method for matching reference texts to their corresponding text plans .",8,0,24
dataset/preprocessed/test-data/data-to-text_generation/5,"For inference time , we describe a method for selecting high - quality text plans for new inputs .",9,0,19
dataset/preprocessed/test-data/data-to-text_generation/5,We implement and evaluate our approach on the WebNLG benchmark .,10,0,11
dataset/preprocessed/test-data/data-to-text_generation/5,Our results demonstrate that decoupling text planning from neural realization indeed improves the system 's reliability and adequacy while maintaining fluent output .,11,0,23
dataset/preprocessed/test-data/data-to-text_generation/5,We observe improvements both in BLEU scores and in manual evaluations .,12,0,12
dataset/preprocessed/test-data/data-to-text_generation/5,"Another benefit of our approach is the ability to output diverse realizations of the same input , paving the way to explicit control over the generated text structure .",13,0,29
dataset/preprocessed/test-data/data-to-text_generation/5,"John , birth Place , London John , employer , IBM",14,0,11
dataset/preprocessed/test-data/data-to-text_generation/5,With a possible output : *,15,0,6
dataset/preprocessed/test-data/data-to-text_generation/5,"Consider the task of data to text generation , as exemplified in the WebNLG corpus .",17,0,16
dataset/preprocessed/test-data/data-to-text_generation/5,The system is given a set of RDF triplets describing facts ( entities and relations between them ) and has to produce a fluent text that is faithful to the facts .,18,0,32
dataset/preprocessed/test-data/data-to-text_generation/5,An example of such triplets is :,19,0,7
dataset/preprocessed/test-data/data-to-text_generation/5,"John , who was born in London , works for IBM .",21,0,12
dataset/preprocessed/test-data/data-to-text_generation/5,Other outputs are also possible :,22,0,6
dataset/preprocessed/test-data/data-to-text_generation/5,"2 . John , who works for IBM , was born in London .",23,0,14
dataset/preprocessed/test-data/data-to-text_generation/5,"London is the birthplace of John , who works for IBM .",25,0,12
dataset/preprocessed/test-data/data-to-text_generation/5,"John , who was born in London .",27,0,8
dataset/preprocessed/test-data/data-to-text_generation/5,"These variations result from different ways of structuring the information : choosing which fact to mention first , and in which direction to express each fact .",28,0,27
dataset/preprocessed/test-data/data-to-text_generation/5,"Another choice is to split the text into two different sentences , e.g. ,",29,0,14
dataset/preprocessed/test-data/data-to-text_generation/5,John works for IBM .,30,0,5
dataset/preprocessed/test-data/data-to-text_generation/5,John was born in London .,31,0,6
dataset/preprocessed/test-data/data-to-text_generation/5,"Overall , the choice of fact ordering , entity ordering , and sentence splits for these facts give rise to 12 different structures , each of them putting the focus on somewhat different aspect of the information .",32,0,38
dataset/preprocessed/test-data/data-to-text_generation/5,"Realistic inputs include more than two facts , greatly increasing the number of possibilities .",33,0,15
dataset/preprocessed/test-data/data-to-text_generation/5,Another axis of variation is in how to verbalize the information for a given structure .,34,0,16
dataset/preprocessed/test-data/data-to-text_generation/5,"For example , ( 2 ) can also be verbalized as 2 a .",35,0,14
dataset/preprocessed/test-data/data-to-text_generation/5,John works for IBM and was born in London . and ( 5 ) as : 5 a .,36,0,19
dataset/preprocessed/test-data/data-to-text_generation/5,John is employed by IBM .,37,0,6
dataset/preprocessed/test-data/data-to-text_generation/5,He was born in London .,38,0,6
dataset/preprocessed/test-data/data-to-text_generation/5,We refer to the first set of choices ( how to structure the information ) as text planning and to the second ( how to verbalize a plan ) as plan realization .,39,0,33
dataset/preprocessed/test-data/data-to-text_generation/5,The distinction between planning and realization is at the core of classic natural language generation ( NLG ) works .,40,0,20
dataset/preprocessed/test-data/data-to-text_generation/5,"However , a recent wave of neural NLG systems ignores this distinction and treat the problem as a single end - to - end task of learning to map facts from the input to the output text .",41,0,38
dataset/preprocessed/test-data/data-to-text_generation/5,"These neural systems encode the input facts into an intermediary vector - based representation , which is then decoded into text .",42,0,22
dataset/preprocessed/test-data/data-to-text_generation/5,"While not stated in these terms , the neural system designers hope for the network to take care of both the planning and realization aspect of text generation .",43,0,29
dataset/preprocessed/test-data/data-to-text_generation/5,"A notable exception is the work of , who introduce a neural content - planning module in the end - to - end architecture .",44,0,25
dataset/preprocessed/test-data/data-to-text_generation/5,"While the neural methods achieve impressive levels of output fluency , they also struggle to maintain coherency on longer texts , struggle to produce a coherent order of facts , and are often not faithful to the input facts , either omitting , repeating , hallucinating or changing facts ( the NLG community refers to such errors as errors inadequacy or correctness of the generated text ) .",45,0,68
dataset/preprocessed/test-data/data-to-text_generation/5,"When compared to templatebased methods , the neural systems win in fluency but fall short regarding content selection and faithfulness to the input .",46,0,24
dataset/preprocessed/test-data/data-to-text_generation/5,"Also , they do not allow control over the output 's structure .",47,0,13
dataset/preprocessed/test-data/data-to-text_generation/5,"We speculate that this is due to demanding too much of the network : while the neural system excels at capturing the language details required for fluent realization , they are less well equipped to deal with the higher levels text structuring in a consistent and verifiable manner .",48,0,49
dataset/preprocessed/test-data/data-to-text_generation/5,"Proposal we propose an explicit , symbolic , text planning stage , whose output is fed into a neural generation system .",49,0,22
dataset/preprocessed/test-data/data-to-text_generation/6,Copy Mechanism and Tailored Training for Character - based Data - to - text Generation,2,1,15
dataset/preprocessed/test-data/data-to-text_generation/6,"In the last few years , many different methods have been focusing on using deep recurrent neural networks for natural language generation .",4,1,23
dataset/preprocessed/test-data/data-to-text_generation/6,"The most widely used sequence - to - sequence neural methods are word - based : as such , they need a pre-processing step called delexicalization ( conversely , relexicalization ) to deal with uncommon or unknown words .",5,0,39
dataset/preprocessed/test-data/data-to-text_generation/6,"These forms of processing , however , give rise to models that depend on the vocabulary used and are not completely neural .",6,0,23
dataset/preprocessed/test-data/data-to-text_generation/6,"In this work , we present an end - to - end sequence - to - sequence model with attention mechanism which reads and generates at a character level , no longer requiring delexicalization , tokenization , nor even lowercasing .",7,0,41
dataset/preprocessed/test-data/data-to-text_generation/6,"Moreover , since characters constitute the common "" building blocks "" of every text , it also allows a more general approach to text generation , enabling the possibility to exploit transfer learning for training .",8,1,36
dataset/preprocessed/test-data/data-to-text_generation/6,"These skills are obtained thanks to two major features : ( i ) the possibility to alternate between the standard generation mechanism and a copy one , which allows to directly copy input facts to produce outputs , and ( ii ) the use of an original training pipeline that further improves the quality of the generated texts .",9,0,59
dataset/preprocessed/test-data/data-to-text_generation/6,"We also introduce a new dataset called E2E + , designed to highlight the copying capabilities of character - based models , that is a modified version of the well - known E2E dataset used in the E2E Challenge .",10,0,40
dataset/preprocessed/test-data/data-to-text_generation/6,"We tested our model according to five broadly accepted metrics ( including the widely used bleu ) , showing that it yields competitive performance with respect to both character - based and word - based approaches .",11,0,37
dataset/preprocessed/test-data/data-to-text_generation/6,The ability of recurrent neural networks ( RNNs ) to model sequential data stimulated interest towards deep learning models which face data - to - text generation .,13,0,28
dataset/preprocessed/test-data/data-to-text_generation/6,An interesting application is the generation of descriptions for factual tables that consist of a set of field - value pairs ; an example is shown in .,14,0,28
dataset/preprocessed/test-data/data-to-text_generation/6,We present in this paper an effective end - to - end approach to this task .,15,0,17
dataset/preprocessed/test-data/data-to-text_generation/6,"Sequence - to - sequence frameworks have proved to be very effective in natural language generation ( NLG ) tasks , as well as in machine translation and in language modeling .",16,0,32
dataset/preprocessed/test-data/data-to-text_generation/6,"Usually , data are represented word - by - word both in input and output sequences ; anyways , such schemes ca n't be effective without a special , non-neural delexicalization phase that handles unknown words , such as proper names or foreign words ( see ) .",17,0,48
dataset/preprocessed/test-data/data-to-text_generation/6,"The delexicalization step has the benefit of reducing the dictionary size and , consequently , the data sparsity , but it is affected by various shortcomings .",18,0,27
dataset/preprocessed/test-data/data-to-text_generation/6,"In particular , according to - it needs some reliable mechanism for entity identification , i.e. the recognition of named entities inside text ; - it requires a subsequent "" re-lexicalization "" phase , where the original named entities take back placeholders ' place ; - it can not account for lexical or morphological variations due to the specific entity , such as gender and number agreements , that ca n't be achieved without a clear context awareness .",19,0,79
dataset/preprocessed/test-data/data-to-text_generation/6,"Recently , some strategies have been proposed to solve these issues : and face this problem using a special neural copying mechanism that is quite effective in alleviating the out - of - vocabulary words problem , while tries to extend neural networks with a post-processing phase that copies words as indicated by the model 's output sequence .",20,0,59
dataset/preprocessed/test-data/data-to-text_generation/6,"Some character - level aspects appear as a solution of the issue as well , either as a fallback for rare words , or as subword units .",21,0,28
dataset/preprocessed/test-data/data-to-text_generation/6,"A significantly different approach consists in employing characters instead of words , for input slot - value pairs tokenization as well as for the generation of the final utterances , as done for instance in .",22,0,36
dataset/preprocessed/test-data/data-to-text_generation/6,"In order to give an original contribution to the field , in this paper we present a character - level sequence - to - sequence model with attention mechanism that results in a completely neural end - to - end architecture .",23,0,42
dataset/preprocessed/test-data/data-to-text_generation/6,"In contrast to traditional word - based ones , it does not require delexicalization , tokenization nor lowercasing ; besides , according to our experiments it never hallucinates words , nor duplicates them .",24,0,34
dataset/preprocessed/test-data/data-to-text_generation/6,"As we will see , such an approach achieves rather interesting performance results and produces a vocabulary - free model that is inherently more general , as it does not depend on a specific domain 's set of terms , but rather on a general alphabet .",25,0,47
dataset/preprocessed/test-data/data-to-text_generation/6,"Because of this , it opens up the possibility , not viable when using words , to adapt already trained networks to deal with different datasets .",26,0,27
dataset/preprocessed/test-data/data-to-text_generation/6,"More specifically , our model shows two important features , with respect to the state - of - art architecture proposed by : ( i ) a character - wise copy mechanism , consisting in a soft switch between generation and copy mode , that disengages the model to learn rare and unhelpful self - correspondences , and ( ii ) a peculiar training procedure , which improves the internal representation capabilities , enhancing recall ; it consists in the exchange of encoder and decoder RNNs , ( GRUs As a further original contribution , we also introduce a new dataset , described in section 3.1 , whose particular structure allows to better highlight improvements in copying / recalling abilities with respect to character - based state - of - art approaches .",27,0,133
dataset/preprocessed/test-data/data-to-text_generation/6,"In section 2 , after resuming the main ideas on encoder - decoder methods with attention , we detail our model : section 2.2 is devoted to explaining the copy mechanism while in section 2.3 our peculiar training procedure is presented .",28,0,42
dataset/preprocessed/test-data/data-to-text_generation/6,"Section 3 includes the datasets descriptions , some implementation specifications , the experimental framework and the analysis and evaluation of the achieved results .",29,0,24
dataset/preprocessed/test-data/data-to-text_generation/6,"Finally , in section 4 some conclusions are drawn , outlining future work .",30,0,14
dataset/preprocessed/test-data/data-to-text_generation/6,Summary on Encoder - decoder,32,0,5
dataset/preprocessed/test-data/data-to-text_generation/6,Architectures with Attention,33,0,3
dataset/preprocessed/test-data/data-to-text_generation/6,"The sequence - to - sequence encoder - decoder architecture with attention is represented in figure 1 : on the left , the encoder , a bi-directional RNN , outputs one annotation h j for each input token x j .",34,0,41
dataset/preprocessed/test-data/data-to-text_generation/6,Each vector h j corresponds to the concatenation of the hidden states produced by the backward and forward RNNs .,35,0,20
dataset/preprocessed/test-data/data-to-text_generation/6,"On the right side of the figure , we find the decoder , which produces one state s i for each time step ; on the center of the figure the attention mechanism is shown .",36,0,36
dataset/preprocessed/test-data/data-to-text_generation/6,The main components of the attention mechanism are : ( i ) the alignment model e ij,37,0,17
dataset/preprocessed/test-data/data-to-text_generation/6,"which is parameterized as a feedforward neural network and scores how well input in position j- th and output observed in the i - th time instant match ; T x and Ty are the length of the input and output sequences , respectively .",38,0,45
dataset/preprocessed/test-data/data-to-text_generation/6,( ii ) the attention probability distribution ?,39,0,8
dataset/preprocessed/test-data/data-to-text_generation/6,( e i is the vector whose j- th element is e ij ) ( iii ) the context vector Ci,41,0,21
dataset/preprocessed/test-data/data-to-text_generation/6,weighted sum of the encoder annotations h j .,42,0,9
dataset/preprocessed/test-data/data-to-text_generation/6,"According to , the context vector Ci is the key element for evaluating the conditional probability P ( y i |y 1 , . . . , y i?1 , x ) to output a target token y i , given the previously outputted tokens y 1 , . . . , y i?1 and the input x .",43,0,59
dataset/preprocessed/test-data/data-to-text_generation/6,They in fact express this probability as :,44,0,8
dataset/preprocessed/test-data/data-to-text_generation/6,"where g is a non-linear , potentially multi -layered , function .",45,0,12
dataset/preprocessed/test-data/data-to-text_generation/6,"So doing , the explicit information about y 1 , . . . , y i ?1 and x is replaced with the knowledge of the context Ci and the decoder state s i .",46,0,35
dataset/preprocessed/test-data/data-to-text_generation/6,"The model we present in this paper incorporates two additional mechanisms , detailed in the next sections : a character - wise copy mechanism and a peculiar training procedure based on GRUs switch .",47,0,34
dataset/preprocessed/test-data/data-to-text_generation/6,Learning to Copy,48,0,3
dataset/preprocessed/test-data/data-to-text_generation/6,"On top of the just recalled model , we build a character - based copy mechanism inspired by the Pointer - Generator Network , a word - based model that hybridizes the Bahdanau traditional model and a Pointer Network .",49,0,40
dataset/preprocessed/test-data/data-to-text_generation/0,A Hierarchical Model for Data - to - Text Generation,2,1,10
dataset/preprocessed/test-data/data-to-text_generation/0,"Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as "" data - to - text "" .",4,1,25
dataset/preprocessed/test-data/data-to-text_generation/0,"These structures generally regroup multiple elements , as well as their attributes .",5,0,13
dataset/preprocessed/test-data/data-to-text_generation/0,Most attempts rely on translation encoder - decoder methods which linearize elements into a sequence .,6,0,16
dataset/preprocessed/test-data/data-to-text_generation/0,This however loses most of the structure contained in the data .,7,0,12
dataset/preprocessed/test-data/data-to-text_generation/0,"In this work , we propose to overpass this limitation with a hierarchical model that encodes the data - structure at the element - level and the structure level .",8,0,30
dataset/preprocessed/test-data/data-to-text_generation/0,Evaluations on RotoWire show the effectiveness of our model w.r.t. qualitative and quantitative metrics .,9,0,15
dataset/preprocessed/test-data/data-to-text_generation/0,"Knowledge and / or data is often modeled in a structure , such as indexes , tables , key - value pairs , or triplets .",11,0,26
dataset/preprocessed/test-data/data-to-text_generation/0,"These data , by their nature ( e.g. , raw data or long time - series data ) , are not easily usable by humans ; outlining their crucial need to be synthesized .",12,0,34
dataset/preprocessed/test-data/data-to-text_generation/0,"Recently , numerous works have focused on leveraging structured data in various applications , such as question answering or table retrieval .",13,0,22
dataset/preprocessed/test-data/data-to-text_generation/0,One emerging research field consists in transcribing data - structures into natural language in order to ease their understandablity and their usablity .,14,0,23
dataset/preprocessed/test-data/data-to-text_generation/0,"This field is referred to as "" data - to - text "" and has its place in several application domains ( such as journalism or medical diagnosis ) or wide - audience applications ( such as financial and weather reports , or sport broadcasting ) .",15,0,47
dataset/preprocessed/test-data/data-to-text_generation/0,"As an example , shows a data - structure containing statistics on NBA basketball games , paired with its corresponding journalistic description .",16,0,23
dataset/preprocessed/test-data/data-to-text_generation/0,Designing data - to - text models gives rise to two main challenges :,17,0,14
dataset/preprocessed/test-data/data-to-text_generation/0,1 ) understanding structured data and 2 ) generating associated descriptions .,18,0,12
dataset/preprocessed/test-data/data-to-text_generation/0,Recent datato - text models mostly rely on an encoder - decoder architecture in which the data - structure is first encoded sequentially into a fixed - size vectorial representation by an encoder .,19,0,34
dataset/preprocessed/test-data/data-to-text_generation/0,"Then , a decoder generates words conditioned on this representation .",20,0,11
dataset/preprocessed/test-data/data-to-text_generation/0,"With the introduction of the attention mechanism on one hand , which computes a context focused on important elements from the input at each decoding step and , on the other hand , the copy mechanism to deal with unknown or rare words , these systems produce fluent and domain comprehensive texts .",21,0,53
dataset/preprocessed/test-data/data-to-text_generation/0,"For instance , Roberti et al. train a characterwise encoder - decoder to generate descriptions of restaurants based on their attributes , while Puduppully et al .",22,0,27
dataset/preprocessed/test-data/data-to-text_generation/0,"design a more complex two - step decoder : they first generate a plan of elements to be mentioned , and then condition text generation on this plan .",23,0,29
dataset/preprocessed/test-data/data-to-text_generation/0,"Although previous work yield over all good results , we identify two important caveats , that hinder precision ( i.e. factual mentions ) in the descriptions :",24,0,27
dataset/preprocessed/test-data/data-to-text_generation/0,Linearization of the data - structure .,26,0,7
dataset/preprocessed/test-data/data-to-text_generation/0,"In practice , most works focus on introducing innovating decoding modules , and still represent data as a unique sequence of elements to be encoded .",27,0,26
dataset/preprocessed/test-data/data-to-text_generation/0,"For example , the table from would be linearized to [ ( Hawks , H/ V , H ) , ... ,",28,0,22
dataset/preprocessed/test-data/data-to-text_generation/0,"( Magic , H/V , V ) , ... ] , effectively leading to losing distinction between rows , and therefore entities .",29,0,23
dataset/preprocessed/test-data/data-to-text_generation/0,"To the best of our knowledge , only Liu et al.",30,0,11
dataset/preprocessed/test-data/data-to-text_generation/0,propose encoders constrained by the structure but these approaches are designed for single - entity structures .,31,0,17
dataset/preprocessed/test-data/data-to-text_generation/0,Arbitrary ordering of unordered collections in recurrent networks ( RNN ) .,32,0,12
dataset/preprocessed/test-data/data-to-text_generation/0,"Most data - to - text systems use RNNs as encoders ( such as GRUs or LSTMs ) , these architectures have however some limitations .",33,0,26
dataset/preprocessed/test-data/data-to-text_generation/0,"Indeed , they require in practice their input to be fed sequentially .",34,0,13
dataset/preprocessed/test-data/data-to-text_generation/0,"This way of encoding unordered sequences ( i.e. collections of entities ) implicitly assumes an arbitrary order within the collection which , as demonstrated by Vinyals et al. , significantly impacts the learning performance .",35,0,35
dataset/preprocessed/test-data/data-to-text_generation/0,"To address these shortcomings , we propose a new structured - data encoder assuming that structures should be hierarchically captured .",36,0,21
dataset/preprocessed/test-data/data-to-text_generation/0,"Our contribution focuses on the encoding of the data - structure , thus the decoder is chosen to be a classical module as used in .",37,0,26
dataset/preprocessed/test-data/data-to-text_generation/0,Our contribution is threefold :,38,0,5
dataset/preprocessed/test-data/data-to-text_generation/0,"- We model the general structure of the data using a two - level architecture , first encoding all entities on the basis of their elements , then encoding the data structure on the basis of its entities ; - We introduce the Transformer encoder in data - to - text models to ensure robust encoding of each element / entities in comparison to all others , no matter their initial positioning ; - We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder .",39,0,90
dataset/preprocessed/test-data/data-to-text_generation/0,We report experiments on the RotoWire benchmark which contains around 5 K statistical tables of NBA basketball games paired with humanwritten descriptions .,40,0,23
dataset/preprocessed/test-data/data-to-text_generation/0,Our model is compared to several state - of - the - art models .,41,0,15
dataset/preprocessed/test-data/data-to-text_generation/0,Results show that the proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics .,42,0,20
dataset/preprocessed/test-data/data-to-text_generation/0,"In the following , we first present a state - of - the art of data - to - text literature ( Section 2 ) , and then describe our proposed hierarchical data encoder ( Section 3 ) .",43,0,39
dataset/preprocessed/test-data/data-to-text_generation/0,"The evaluation protocol is presented in Section 4 , followed by the results ( Section 5 ) .",44,0,18
dataset/preprocessed/test-data/data-to-text_generation/0,Section 6 concludes the paper and presents perspectives .,45,0,9
dataset/preprocessed/test-data/data-to-text_generation/0,"Until recently , efforts to bring out semantics from structured - data relied heavily on expert knowledge .",47,0,18
dataset/preprocessed/test-data/data-to-text_generation/0,"For example , in order to better transcribe numerical time series of weather data to a textual forecast , Reiter et al .",48,0,23
dataset/preprocessed/test-data/data-to-text_generation/0,devise complex template schemes in collaboration with weather experts to build a consistent set of data - to - word rules .,49,0,22
dataset/preprocessed/test-data/data-to-text_generation/2,Deep Graph Convolutional Encoders for Structured Data to Text Generation,2,1,10
dataset/preprocessed/test-data/data-to-text_generation/2,Most previous work on neural text generation from graph - structured data relies on standard sequence - to - sequence methods .,4,1,22
dataset/preprocessed/test-data/data-to-text_generation/2,These approaches linearise the input graph to be fed to a recurrent neural network .,5,0,15
dataset/preprocessed/test-data/data-to-text_generation/2,"In this paper , we propose an alternative encoder based on graph convolutional networks that directly exploits the input structure .",6,0,21
dataset/preprocessed/test-data/data-to-text_generation/2,We report results on two graphto - sequence datasets that empirically show the benefits of explicitly encoding the input graph structure .,7,0,22
dataset/preprocessed/test-data/data-to-text_generation/2,Data - to - text generators produce a target natural language text from a source data representation .,10,0,18
dataset/preprocessed/test-data/data-to-text_generation/2,Recent neural generation approaches build on encoder - decoder architectures proposed for machine translation .,11,0,15
dataset/preprocessed/test-data/data-to-text_generation/2,"The source data , differently from the machine translation task , is a structured representation of the content to be conveyed .",12,0,22
dataset/preprocessed/test-data/data-to-text_generation/2,"Generally , it describes attributes and events about entities and relations among them .",13,0,14
dataset/preprocessed/test-data/data-to-text_generation/2,In this work we focus on two generation scenarios where the source data is graph structured .,14,0,17
dataset/preprocessed/test-data/data-to-text_generation/2,"One is the generation of multi-sentence descriptions of Knowledge Base ( KB ) entities from RDF graphs ) , namely the WebNLG task .",15,0,24
dataset/preprocessed/test-data/data-to-text_generation/2,The number of KB relations modelled in this scenario is potentially large and generation involves solving various subtasks ( e.g. lexicalis ation Code and data available at github.com/diegma/graph-2-text .,16,0,29
dataset/preprocessed/test-data/data-to-text_generation/2,2 Resource Description Framework https://www.w3.org/RDF / and aggregation ) .,17,0,10
dataset/preprocessed/test-data/data-to-text_generation/2,Figure ( 1 a ) shows and example of source RDF graph and target natural language description .,18,0,18
dataset/preprocessed/test-data/data-to-text_generation/2,"The other is the linguistic realis ation of the meaning expressed by a source dependency graph , namely the SR11 Deep generation task .",19,0,24
dataset/preprocessed/test-data/data-to-text_generation/2,"In this task , the semantic relations are linguistically motivated and their number is smaller .",20,0,16
dataset/preprocessed/test-data/data-to-text_generation/2,illustrates a source dependency graph and the corresponding target text .,21,0,11
dataset/preprocessed/test-data/data-to-text_generation/2,Most previous work casts the graph structured data to text generation task as a sequenceto - sequence problem .,22,1,19
dataset/preprocessed/test-data/data-to-text_generation/2,They rely on recurrent data encoders with memory and gating mechanisms ( LSTM ; ) .,23,0,16
dataset/preprocessed/test-data/data-to-text_generation/2,Models based on these sequential encoders have shown good results although they do not directly exploit the input structure but rather rely on a separate linearis ation step .,24,0,29
dataset/preprocessed/test-data/data-to-text_generation/2,"In this work , we compare with a model that explicitly encodes structure and is trained end - to - end .",25,0,22
dataset/preprocessed/test-data/data-to-text_generation/2,"Concretely , we use a Graph Convolutional Network ( GCN ; ) as our encoder .",26,0,16
dataset/preprocessed/test-data/data-to-text_generation/2,GCNs area flexible architecture that allows explicit encoding of graph data into neural networks .,27,0,15
dataset/preprocessed/test-data/data-to-text_generation/2,Given their simplicity and expressiveness they have been used to encode dependency syntax and predicate - argument structures in neural machine translation .,28,0,23
dataset/preprocessed/test-data/data-to-text_generation/2,"In contrast to previous work , we do not exploit the sequential information of the input ( i.e. , with an LSTM ) , but we solely rely on a GCN for encoding the source graph structure .",29,0,38
dataset/preprocessed/test-data/data-to-text_generation/2,The main contribution of this work is show - :,30,0,10
dataset/preprocessed/test-data/data-to-text_generation/2,Source RDF graph - target description ( a ) .,31,0,10
dataset/preprocessed/test-data/data-to-text_generation/2,Source dependency graph - target sentence ( b ) .,32,0,10
dataset/preprocessed/test-data/data-to-text_generation/2,ing that explicitly encoding structured data with GCNs is more effective than encoding a linearized version of the structure with LSTMs .,33,0,22
dataset/preprocessed/test-data/data-to-text_generation/2,"We evaluate the GCN - based generator on two graph - tosequence tasks , with different level of source content specification .",34,0,22
dataset/preprocessed/test-data/data-to-text_generation/2,"In both cases , the results we obtain show that GCNs encoders outperforms standard LSTM encoders .",35,0,17
dataset/preprocessed/test-data/data-to-text_generation/2,Graph Convolutional - based Generator,36,0,5
dataset/preprocessed/test-data/data-to-text_generation/2,"Formally , we address the task of text generation from graph - structured data considering as input a directed labeled graph X = ( V , E ) where V is a set of nodes and E is a set of edges between nodes in V .",37,0,47
dataset/preprocessed/test-data/data-to-text_generation/2,The specific semantics of X depends on the task at hand .,38,0,12
dataset/preprocessed/test-data/data-to-text_generation/2,The output Y is a natural language text verbalising the content expressed by X .,39,0,15
dataset/preprocessed/test-data/data-to-text_generation/2,"Our generation model follows the standard attention - based encoder - decoder architecture and predicts Y conditioned on X as P ( Y | X ) = | Y | t=1 P ( y t |y 1:t?1 , X ) .",40,0,41
dataset/preprocessed/test-data/data-to-text_generation/2,Graph Convolutional Encoder,41,0,3
dataset/preprocessed/test-data/data-to-text_generation/2,In order to explicitly encode structural information we adopt graph convolutional networks ( GCNs ) .,42,0,16
dataset/preprocessed/test-data/data-to-text_generation/2,GCNs area variant of graph neural networks ) that has been recently proposed by .,43,0,15
dataset/preprocessed/test-data/data-to-text_generation/2,The goal of GCNs is to calculate the representation of each node in a graph considering the graph structure .,44,0,20
dataset/preprocessed/test-data/data-to-text_generation/2,In this paper we adopt the parametrization proposed by where edge labels and directions are explicitly modeled .,45,0,18
dataset/preprocessed/test-data/data-to-text_generation/2,"Formally , given a directed graph X = ( V , E ) , where V is a set of nodes , and E is a set of edges .",46,0,30
dataset/preprocessed/test-data/data-to-text_generation/2,We represent each node v ?,47,0,6
dataset/preprocessed/test-data/data-to-text_generation/2,V with a feature vector xv ?,48,0,7
dataset/preprocessed/test-data/data-to-text_generation/4,Data - to - Text Generation with Content Selection and Planning,2,1,11
dataset/preprocessed/test-data/data-to-text_generation/4,"Recent advances in data - to - text generation have led to the use of large - scale datasets and neural network models which are trained end - to - end , without explicitly modeling what to say and in what order .",4,0,43
dataset/preprocessed/test-data/data-to-text_generation/4,"In this work , we present a neural network architecture which incorporates content selection and planning without sacrificing end - to - end training .",5,0,25
dataset/preprocessed/test-data/data-to-text_generation/4,We decompose the generation task into two stages .,6,0,9
dataset/preprocessed/test-data/data-to-text_generation/4,"Given a corpus of data records ( paired with descriptive documents ) , we first generate a content plan highlighting which information should be mentioned and in which order and then generate the document while taking the content plan into account .",7,0,42
dataset/preprocessed/test-data/data-to-text_generation/4,Automatic and human - based evaluation experiments show that our model 1 outperforms strong baselines improving the state - of - the - art on the recently released ROTOWIRE dataset .,8,0,31
dataset/preprocessed/test-data/data-to-text_generation/4,Data - to - text generation broadly refers to the task of automatically producing text from non-linguistic input .,10,0,19
dataset/preprocessed/test-data/data-to-text_generation/4,"The input maybe in various forms including data bases of records , spreadsheets , expert system knowledge bases , simulations of physical systems , and soon .",11,0,27
dataset/preprocessed/test-data/data-to-text_generation/4,"shows an example in the form of a data base containing statistics on NBA basketball games , and a corresponding game summary .",12,0,23
dataset/preprocessed/test-data/data-to-text_generation/4,"Traditional methods for data - to - text generation ) implement a pipeline of modules including content planning ( selecting specific content from some input and determining the structure of the output text ) , sentence planning ( determining the structure and lexical content of each sentence ) and surface realization ( converting the sentence plan to a surface string ) .",13,0,62
dataset/preprocessed/test-data/data-to-text_generation/4,"Recent neural generation systems ) do not explicitly model any of these stages , rather they are trained in an end - to - end fashion using the very successful encoder - decoder architecture as their backbone .",14,0,38
dataset/preprocessed/test-data/data-to-text_generation/4,"Despite producing over all fluent text , neural systems have difficulty capturing long - term structure and generating documents more than a few sentences long .",15,0,26
dataset/preprocessed/test-data/data-to-text_generation/4,"show that neural text generation techniques perform poorly at content selection , they struggle to maintain inter-sentential coherence , and more generally a reasonable ordering of the selected facts in the output text .",16,0,34
dataset/preprocessed/test-data/data-to-text_generation/4,Additional challenges include avoiding redundancy and being faithful to the input .,17,0,12
dataset/preprocessed/test-data/data-to-text_generation/4,"Interestingly , comparisons against templatebased methods show that neural techniques do not farewell on metrics of content selection recall and factual output generation ( i.e. , they often hallucinate statements which are not supported by the facts in the data base ) .",18,0,43
dataset/preprocessed/test-data/data-to-text_generation/4,"In this paper , we address these shortcomings by explicitly modeling content selection and planning within a neural data - to - text architecture .",19,0,25
dataset/preprocessed/test-data/data-to-text_generation/4,Our model learns a content plan from the input and conditions on the content plan in order to generate the output document ( see for an illustration ) .,20,0,29
dataset/preprocessed/test-data/data-to-text_generation/4,"An explicit content planning mechanism has at least three advantages for multi-sentence document generation : it represents a high - level organization of the document structure allowing the decoder to concentrate on the easier tasks of sentence planning and surface realization ; it makes the process of data - to - document generation more interpretable by generating an intermediate representation ; and reduces redundancy in the output , since it is less likely for the content plan to contain the same information in multiple places .",21,0,86
dataset/preprocessed/test-data/data-to-text_generation/4,"We train our model end - to - end using neural networks and evaluate its performance on ROTOWIRE , a recently released dataset which contains statistics of NBA basketball games paired with human - written summaries ( see ) .",22,0,40
dataset/preprocessed/test-data/data-to-text_generation/4,Automatic and human evaluation shows that modeling content selection and planning improves generation considerably over competitive baselines .,23,0,18
dataset/preprocessed/test-data/data-to-text_generation/4,The Boston Celtics defeated the host Indiana Pacers 105 - 99 at Bankers Life Fieldhouse on Saturday .,24,0,18
dataset/preprocessed/test-data/data-to-text_generation/4,"In a battle between two injury - riddled teams , the Celtics were able to prevail with a much needed road victory .",25,0,23
dataset/preprocessed/test-data/data-to-text_generation/4,"The key was shooting and defense , as the Celtics outshot the Pacers from the field , from three - point range and from the free - throw line .",26,0,30
dataset/preprocessed/test-data/data-to-text_generation/4,Boston also held Indiana to 42 percent from the field and 22 percent from long distance .,27,0,17
dataset/preprocessed/test-data/data-to-text_generation/4,"The Celtics also won the rebounding and assisting differentials , while tying the Pacers in turnovers .",28,0,17
dataset/preprocessed/test-data/data-to-text_generation/4,"There were 10 ties and 10 lead changes , as this game went down to the final seconds .",29,0,19
dataset/preprocessed/test-data/data-to-text_generation/4,"Boston ( 5 - 4 ) has had to deal with a gluttony of injuries , but they had the fortunate task of playing a team just as injured here .",30,0,31
dataset/preprocessed/test-data/data-to-text_generation/4,"Isaiah Thomas led the team in scoring , totaling 23 points and five assists on 4 - of - 13 shooting .",31,0,22
dataset/preprocessed/test-data/data-to-text_generation/4,He got most of those points by going 14 - of - 15 from the free - throw line .,32,0,20
dataset/preprocessed/test-data/data-to-text_generation/4,"Olynyk got a rare start and finished second on the team with his 16 points , six rebounds and four assists .",34,0,22
dataset/preprocessed/test-data/data-to-text_generation/4,The generation literature provides multiple examples of content selection components developed for various domains which are either hand - built or learned from data .,36,0,25
dataset/preprocessed/test-data/data-to-text_generation/4,"Likewise , creating summaries of sports games has been a topic of interest since the early beginnings of generation systems .",37,0,21
dataset/preprocessed/test-data/data-to-text_generation/4,"Earlier work on content planning has relied on generic planners , based on Rhetorical Structure Theory .",38,0,17
dataset/preprocessed/test-data/data-to-text_generation/4,They defined content planners by analysing the target texts and devising hand - crafted rules .,39,0,16
dataset/preprocessed/test-data/data-to-text_generation/4,studied ordering constraints for content plans and learn a content planner from an aligned corpus of inputs and human outputs .,40,0,21
dataset/preprocessed/test-data/data-to-text_generation/4,A few researchers ) rank content plans according to a ranking function .,41,0,13
dataset/preprocessed/test-data/data-to-text_generation/4,More recent work focuses on end - to - end systems instead of individual components .,42,0,16
dataset/preprocessed/test-data/data-to-text_generation/4,"However , most models make simplistic assumptions such as generation without any content selection or planning or content selection without planning .",43,0,22
dataset/preprocessed/test-data/data-to-text_generation/4,An exception are who incorporate content plans represented as grammar rules operating on the document level .,44,0,17
dataset/preprocessed/test-data/data-to-text_generation/4,"Their approach works reasonably well with weather forecasts , but does not scale easily to larger data bases , with richer vocabularies , and longer text descriptions .",45,0,28
dataset/preprocessed/test-data/data-to-text_generation/4,The model relies on the EM algorithm to learn the weights of the grammar rules which can be very many even when tokens are aligned to data base records as a preprocessing step .,46,0,34
dataset/preprocessed/test-data/data-to-text_generation/4,Our work is closest to recent neural network models which learn generators from data and accompanying text resources .,47,0,19
dataset/preprocessed/test-data/data-to-text_generation/4,Most previous approaches generate from Wikipedia infoboxes focusing either on single sentences or short texts ( Perez-Beltrachini and Lapata 2018 ) .,48,0,22
dataset/preprocessed/test-data/data-to-text_generation/4,"use a neural encoder - decoder model to generate weather forecasts and soccer commentaries , while generate NBA game summaries ( see ) .",49,0,24
dataset/preprocessed/test-data/dependency_parsing/7,Training with Exploration Improves a Greedy Stack LSTM Parser,2,0,9
dataset/preprocessed/test-data/dependency_parsing/7,"We adapt the greedy stack LSTM dependency parser of Dyer et al. ( 2015 ) to support a training - with - exploration procedure using dynamic oracles ( Goldberg and Nivre , 2013 ) instead of assuming an error - free action history .",4,0,44
dataset/preprocessed/test-data/dependency_parsing/7,"This form of training , which accounts for model predictions at training time , improves parsing accuracies .",5,1,18
dataset/preprocessed/test-data/dependency_parsing/7,We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser .,6,0,23
dataset/preprocessed/test-data/dependency_parsing/7,"Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures ; this formalization is known as transitionbased parsing , and is often coupled with a greedy search procedure .",8,1,43
dataset/preprocessed/test-data/dependency_parsing/7,"The literature on transition - based parsing is vast , but all works share in common a classification component that takes into account features of the current parser state 1 and predicts the next action to take conditioned on the state .",9,1,42
dataset/preprocessed/test-data/dependency_parsing/7,The state is of unbounded size .,10,0,7
dataset/preprocessed/test-data/dependency_parsing/7,presented a parser in which the parser 's unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks .,11,0,22
dataset/preprocessed/test-data/dependency_parsing/7,"Coupled with a recursive tree composition function , the feature representation is able to capture information from the entirety of the state , without resorting to locality assumptions that were common in most other transition - based parsers .",12,0,39
dataset/preprocessed/test-data/dependency_parsing/7,"The use of a novel stack LSTM data structure allows the parser to maintain a constant time per-state update , and retain an over all linear parsing time .",13,0,29
dataset/preprocessed/test-data/dependency_parsing/7,"The was trained to maximize the likelihood of gold - standard transition sequences , given words .",14,0,17
dataset/preprocessed/test-data/dependency_parsing/7,"At test time , the parser makes greedy decisions according to the learned model .",15,0,15
dataset/preprocessed/test-data/dependency_parsing/7,"Although this setup obtains very good performance , the training and testing conditions are mismatched in the following way : at training time the historical context of an action is always derived from the gold standard ( i.e. , perfectly correct past actions ) , but at test time , it will be a model prediction .",16,0,57
dataset/preprocessed/test-data/dependency_parsing/7,"In this work , we adapt the training criterion so as to explore parser states drawn not only from the training data , but also from the model as it is being learned .",17,0,34
dataset/preprocessed/test-data/dependency_parsing/7,"To do so , we use the method of to dynamically chose an optimal ( relative to the final attachment accuracy ) action given an imperfect history .",18,0,28
dataset/preprocessed/test-data/dependency_parsing/7,"By interpolating between algorithm states sampled from the model and those sampled from the training data , more robust predictions at test time can be made .",19,0,27
dataset/preprocessed/test-data/dependency_parsing/7,We show that the technique can be used to improve the strong parser of Dyer et al .,20,0,18
dataset/preprocessed/test-data/dependency_parsing/7,Parsing Model and Parameter Learning,21,0,5
dataset/preprocessed/test-data/dependency_parsing/7,Our departure point is the parsing model described by .,22,0,10
dataset/preprocessed/test-data/dependency_parsing/7,"We do not describe the model in detail , and refer the reader to the original work .",23,0,18
dataset/preprocessed/test-data/dependency_parsing/7,"At each stage t of the parsing process , the parser state is encoded into a vector pt , which is used to compute the probability of the parser action at time t as :",24,0,35
dataset/preprocessed/test-data/dependency_parsing/7,"where g z is a column vector representing the ( output ) embedding of the parser action z , and q z is a bias term for action z .",25,0,30
dataset/preprocessed/test-data/dependency_parsing/7,"The set A (S , B ) represents the valid transition actions that maybe taken in the current state .",26,0,20
dataset/preprocessed/test-data/dependency_parsing/7,"Since pt encodes information about all previous decisions made by the parser , the chain rule gives the probability of any valid sequence of parse transitions z conditional on the input :",27,0,32
dataset/preprocessed/test-data/dependency_parsing/7,"The parser is trained to maximize the conditional probability of taking a "" correct "" action at each parsing state .",28,0,21
dataset/preprocessed/test-data/dependency_parsing/7,"The definition of what constitutes a "" correct "" action is the major difference between a static oracle as used by and the dynamic oracle explored here .",29,0,28
dataset/preprocessed/test-data/dependency_parsing/7,"Regardless of the oracle , our training implementation constructs a computation graph ( nodes that represent values , linked by directed edges from each function 's inputs to its outputs ) for the negative log probability for the oracle transition sequence as a function of the current model parameters and uses forward - and backpropagation to obtain the gradients respect to the model parameters .",30,0,65
dataset/preprocessed/test-data/dependency_parsing/7,Training with Static Oracles,31,0,4
dataset/preprocessed/test-data/dependency_parsing/7,"With a static oracle , the training procedure computes a canonical reference series of transitions for each gold parse tree .",32,0,21
dataset/preprocessed/test-data/dependency_parsing/7,"It then runs the parser through this canonical sequence of transitions , while keeping track of the state representation pt at each step t , as well as the distribution over transitions p ( z t | pt ) which is predicted by the current classifier for the state representation .",33,0,51
dataset/preprocessed/test-data/dependency_parsing/7,"Once the end of the sentence is reached , the parameters are updated towards maximizing the likelihood of the reference transition sequence ( Equation 2 ) , which equates to maximizing the probability of the correct transition , p ( z gt | pt ) , at each state along the path .",34,0,53
dataset/preprocessed/test-data/dependency_parsing/7,Training with Dynamic Oracles,35,0,4
dataset/preprocessed/test-data/dependency_parsing/7,"In the static oracle case , the parser is trained to predict the best transition to take at each parsing step , assuming all previous transitions were correct .",36,0,29
dataset/preprocessed/test-data/dependency_parsing/7,"Since the parser is likely to make mistakes at test time and encounter states it has not seen during training , this training criterion is problematic .",37,0,27
dataset/preprocessed/test-data/dependency_parsing/7,"Instead , we would prefer to train the parser to behave optimally even after making a mistake ( under the constraint that it can not backtrack or fix any previous decision ) .",38,0,33
dataset/preprocessed/test-data/dependency_parsing/7,"We thus need to include in the training examples states that result from wrong parsing decisions , together with the optimal transitions to take in these states .",39,0,28
dataset/preprocessed/test-data/dependency_parsing/7,"To this end we reconsider which training examples to show , and what it means to behave optimally on these training examples .",40,0,23
dataset/preprocessed/test-data/dependency_parsing/7,The framework of training with exploration using dynamic oracles suggested by provides answers to these questions .,41,0,17
dataset/preprocessed/test-data/dependency_parsing/7,"While the application of dynamic oracle training is relatively straightforward , some adaptations were needed to accommodate the probabilistic training objective .",42,0,22
dataset/preprocessed/test-data/dependency_parsing/7,These adaptations mostly follow .,43,0,5
dataset/preprocessed/test-data/dependency_parsing/7,Dynamic Oracles .,44,0,3
dataset/preprocessed/test-data/dependency_parsing/7,"A dynamic oracle is the component that , given a gold parse tree , provides the optimal set of possible actions to take for any valid parser state .",45,0,29
dataset/preprocessed/test-data/dependency_parsing/7,"In contrast to static oracles that derive a canonical state sequence for each gold parse tree and say nothing about states that deviate from this canonical path , the dynamic oracle is well defined for states that result from parsing mistakes , and they may produce more than a single gold action for a given state .",46,0,57
dataset/preprocessed/test-data/dependency_parsing/7,"Under the dynamic oracle framework , an action is said to be optimal for a state if the best tree that can be reached after taking the action is no worse ( in terms of accuracy with respect to the gold tree ) than the best tree that could be reached prior to taking that action .",47,0,57
dataset/preprocessed/test-data/dependency_parsing/7,"define the arcdecomposition property of transition systems , and show how to derive efficient dynamic oracles for transition systems thatare arc-decomposable .",48,0,22
dataset/preprocessed/test-data/dependency_parsing/3,From POS tagging to dependency parsing for biomedical event extraction,2,1,10
dataset/preprocessed/test-data/dependency_parsing/3,"Given the importance of relation or event extraction from biomedical research publications to support knowledge capture and synthesis , and the strong dependency of approaches to this information extraction task on syntactic information , it is valuable to understand which approaches to syntactic processing of biomedical text have the highest performance .",5,0,52
dataset/preprocessed/test-data/dependency_parsing/3,"We perform an empirical study comparing state - of - the - art traditional feature - based and neural network - based models for two core natural language processing tasks of part - of - speech ( POS ) tagging and dependency parsing on two benchmark biomedical corpora , GENIA and CRAFT .",6,0,53
dataset/preprocessed/test-data/dependency_parsing/3,"To the best of our knowledge , there is no recent work making such comparisons in the biomedical context ; specifically no detailed analysis of neural models on this data is available .",7,0,33
dataset/preprocessed/test-data/dependency_parsing/3,"Experimental results show that in general , the neural models outperform the feature - based models on two benchmark biomedical corpora GENIA and CRAFT .",8,0,25
dataset/preprocessed/test-data/dependency_parsing/3,"We also perform a task - oriented evaluation to investigate the influences of these models in a downstream application on biomedical event extraction , and show that better intrinsic parsing performance does not always imply better extrinsic event extraction performance .",9,0,41
dataset/preprocessed/test-data/dependency_parsing/3,"We have presented a detailed empirical study comparing traditional feature - based and neural network - based models for POS tagging and dependency parsing in the biomedical context , and also investigated the influence of parser selection for a biomedical event extraction downstream task .",10,0,45
dataset/preprocessed/test-data/dependency_parsing/3,Availability of data and material :,11,0,6
dataset/preprocessed/test-data/dependency_parsing/3,We make the retrained models available at https://github.com/datquocnguyen/BioPosDep.,12,0,8
dataset/preprocessed/test-data/dependency_parsing/3,"The biomedical literature , as captured in the parallel repositories of PubMed ( abstracts ) and PubMed Central ( full text articles ) , is growing at a remarkable rate of over one million publications per year .",14,0,38
dataset/preprocessed/test-data/dependency_parsing/3,Effort to catalog the key research results in these publications demands automation .,15,0,13
dataset/preprocessed/test-data/dependency_parsing/3,Hence extraction of relations and events from the published literature has become a key focus of the biomedical natural language processing community .,16,0,23
dataset/preprocessed/test-data/dependency_parsing/3,"Methods for information extraction typically make use of linguistic information , with a specific emphasis on the value of dependency parses .",17,0,22
dataset/preprocessed/test-data/dependency_parsing/3,"A number of linguistically - annotated resources , notably including the GENIA and CRAFT corpora , have been * Correspondence : dqnguyen@unimelb.edu.au ,",18,0,23
dataset/preprocessed/test-data/dependency_parsing/3,"The University of Melbourne Karin.Verspoor@unimelb.edu.au ,",19,0,6
dataset/preprocessed/test-data/dependency_parsing/3,The University of Melbourne https://www.ncbi.nlm.nih.gov/pubmed,20,0,5
dataset/preprocessed/test-data/dependency_parsing/3,"https://www.ncbi.nlm.nih.gov/pmc produced to support development and evaluation of natural language processing ( NLP ) tools over biomedical publications , based on the observation of the substantive differences between these domain texts and general English texts , as captured in resources such as the Penn Treebank thatare standardly used for development and evaluation of syntactic processing tools .",21,0,57
dataset/preprocessed/test-data/dependency_parsing/3,Recent work on biomedical relation extraction has highlighted the particular importance of syntactic information .,22,0,15
dataset/preprocessed/test-data/dependency_parsing/3,"Despite this , that work , and most other related work , has simply adopted a tool to analyze the syntactic characteristics of the biomedical texts without consideration of the appropriateness of the tool for these texts .",23,0,38
dataset/preprocessed/test-data/dependency_parsing/3,"A commonly used tool is the Stanford CoreNLP dependency parser , although domain - adapted parsers ( e.g. ) are sometimes used .",24,0,23
dataset/preprocessed/test-data/dependency_parsing/3,Prior work on the CRAFT treebank demonstrated substantial variation in the performance of syntactic processing tools for that data .,25,0,20
dataset/preprocessed/test-data/dependency_parsing/3,Given the significant improvements in parsing performance in the last arXiv : 1808.03731v2 [ cs. CL ] 2 Jan 2019,26,0,20
dataset/preprocessed/test-data/dependency_parsing/3,Pre-trained POS tagger Retrained POS tagger,27,0,6
dataset/preprocessed/test-data/dependency_parsing/3,"Event extraction few years , thanks to renewed attention to the problem and exploration of neural methods , it is important to revisit whether the commonly used tools remain the best choices for syntactic analysis of biomedical texts .",30,0,39
dataset/preprocessed/test-data/dependency_parsing/3,"In this paper , we therefore investigate current stateof - the - art ( SOTA ) approaches to dependency parsing as applied to biomedical texts .",31,0,26
dataset/preprocessed/test-data/dependency_parsing/3,"We also present detailed results on the precursor task of POS tagging , since parsing depends heavily on POS tags .",32,0,21
dataset/preprocessed/test-data/dependency_parsing/3,"Finally , we study the impact of parser choice on biomedical event extraction , following the structure of the extrinsic parser evaluation shared task ( EPE 2017 ) for biomedical event extraction .",33,0,33
dataset/preprocessed/test-data/dependency_parsing/3,We find that differences in over all intrinsic parser performance do not consistently explain differences in information extraction performance .,34,0,20
dataset/preprocessed/test-data/dependency_parsing/3,"In this section , we present our empirical approach to evaluate different POS tagging and dependency parsing models on benchmark biomedical corpora .",36,0,23
dataset/preprocessed/test-data/dependency_parsing/3,illustrates our experimental flow .,37,0,5
dataset/preprocessed/test-data/dependency_parsing/3,"In particular , we compare pre-trained and retrained POS taggers , and investigate the effect of these pre-trained and retrained taggers in pre-trained parsing models ( in the first five rows of ) .",38,0,34
dataset/preprocessed/test-data/dependency_parsing/3,We then compare the performance of retrained parsing models to the pre-trained ones ( in the last ten rows of ) .,39,0,22
dataset/preprocessed/test-data/dependency_parsing/3,"Finally , we investigate the influence of pre-trained and retrained parsing models in the biomedical event extraction task ( in ) .",40,0,22
dataset/preprocessed/test-data/dependency_parsing/3,We use two biomedical corpora : GENIA and CRAFT .,42,0,10
dataset/preprocessed/test-data/dependency_parsing/3,"GENIA includes abstracts from PubMed , while CRAFT includes full text publications .",43,0,13
dataset/preprocessed/test-data/dependency_parsing/3,It has been observed that there are substantial linguistic differences between the abstracts and the corresponding full text publications ; hence it is important to consider both contexts when assessing NLP tools in biomedical domain .,44,0,36
dataset/preprocessed/test-data/dependency_parsing/3,"The GENIA corpus contains 18K sentences ( ? 486K words ) from 1,999 Medline abstracts , which are manually annotated following the Penn Treebank ( PTB ) bracketing guidelines .",45,0,30
dataset/preprocessed/test-data/dependency_parsing/3,"On this treebank , we use the training , development and test split from .",46,0,15
dataset/preprocessed/test-data/dependency_parsing/3,We then use the Stanford constituent - to - dependency conversion toolkit ( v3.5.1 ) to generate dependency trees with basic Stanford dependencies .,47,0,24
dataset/preprocessed/test-data/dependency_parsing/3,The CRAFT corpus includes 21 K sentences ( ? 561K words ) from 67 full - text biomedical journal articles .,48,0,21
dataset/preprocessed/test-data/dependency_parsing/3,These sentences are syntactically annotated using an extended PTB tag set .,49,0,12
dataset/preprocessed/test-data/dependency_parsing/1,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,2,1,10
dataset/preprocessed/test-data/dependency_parsing/1,We present a simple and effective scheme for dependency parsing which is based on bidirectional - LSTMs ( BiLSTMs ) .,4,0,21
dataset/preprocessed/test-data/dependency_parsing/1,"Each sentence token is associated with a BiLSTM vector representing the token in its sentential context , and feature vectors are constructed by concatenating a few BiLSTM vectors .",5,0,29
dataset/preprocessed/test-data/dependency_parsing/1,"The BiLSTM is trained jointly with the parser objective , resulting in very effective feature extractors for parsing .",6,0,19
dataset/preprocessed/test-data/dependency_parsing/1,We demonstrate the effectiveness of the approach by applying it to a greedy transition - based parser as well as to a globally optimized graph - based parser .,7,0,29
dataset/preprocessed/test-data/dependency_parsing/1,"The resulting parsers have very simple architectures , and match or surpass the state - of - the - art accuracies on English and Chinese .",8,0,26
dataset/preprocessed/test-data/dependency_parsing/1,"The focus of this paper is on feature representation for dependency parsing , using recent techniques from the neural - networks ( "" deep learning "" ) literature .",10,0,29
dataset/preprocessed/test-data/dependency_parsing/1,Modern approaches to dependency parsing can be broadly categorized into graph - based and transition - based parsers ) .,11,0,20
dataset/preprocessed/test-data/dependency_parsing/1,Graph - based parsers treat parsing as a search - based structured prediction problem in which the goal is learning a scoring function over dependency trees such that the correct tree is scored above all other trees .,12,0,38
dataset/preprocessed/test-data/dependency_parsing/1,"Transition - based parsers treat parsing as a sequence of actions that produce a parse tree , and a classifier is trained to score the possible actions at each stage of the process and guide the parsing process .",13,0,39
dataset/preprocessed/test-data/dependency_parsing/1,"Perhaps the simplest graph - based parsers are arc-factored ( first order ) models , in which the scoring function for a tree decomposes over the individual arcs of the tree .",14,0,32
dataset/preprocessed/test-data/dependency_parsing/1,"More elaborate models look at larger ( overlapping ) parts , requiring more sophisticated inference and training algorithms .",15,0,19
dataset/preprocessed/test-data/dependency_parsing/1,"The basic transition - based parsers work in a greedy manner , performing a series of locally - optimal decisions , and boast very fast parsing speeds .",16,0,28
dataset/preprocessed/test-data/dependency_parsing/1,More advanced transition - based parsers introduce some search into the process using a beam or dynamic programming .,17,0,19
dataset/preprocessed/test-data/dependency_parsing/1,"Regardless of the details of the parsing framework being used , a crucial step in parser design is choosing the right feature function for the underlying statistical model .",18,0,29
dataset/preprocessed/test-data/dependency_parsing/1,"Recent work ( see Section 2.2 for an overview ) attempt to alleviate parts of the feature function design problem by moving from linear to non-linear models , enabling the modeler to focus on a small set of "" core "" features and leaving it up to the machine - learning machinery to come up with good feature combinations .",19,0,60
dataset/preprocessed/test-data/dependency_parsing/1,"However , the need to carefully define a set of core features remains .",20,0,14
dataset/preprocessed/test-data/dependency_parsing/1,"For example , the work of uses 18 different elements in its feature function , while the work of uses 21 different elements .",21,0,24
dataset/preprocessed/test-data/dependency_parsing/1,"Other works , notably and , propose more sophisticated feature representations , in which the feature engineering is replaced with architecture engineering .",22,0,23
dataset/preprocessed/test-data/dependency_parsing/1,"In this work , we suggest an approach which is much simpler in terms of both feature engineering and architecture engineering .",23,0,22
dataset/preprocessed/test-data/dependency_parsing/1,"Our proposal ( Section 3 ) is centered around BiRNNs , and more specifically BiLSTMs , which are strong and trainable sequence models ( see Section 2.3 ) .",24,0,29
dataset/preprocessed/test-data/dependency_parsing/1,"The BiLSTM excels at representing elements in a sequence ( i.e. , words ) together with their contexts , capturing the element and an "" infinite "" window around it .",25,0,31
dataset/preprocessed/test-data/dependency_parsing/1,"We represent each word by its BiLSTM encoding , and use a concatenation of a minimal set of such BiLSTM encodings as our feature function , which is then passed to a non-linear scoring function ( multi - layer perceptron ) .",26,0,42
dataset/preprocessed/test-data/dependency_parsing/1,"Crucially , the BiLSTM is trained with the rest of the parser in order to learn a good feature representation for the parsing problem .",27,0,25
dataset/preprocessed/test-data/dependency_parsing/1,"If we set aside the inherent complexity of the BiLSTM itself and treat it as a black box , our proposal results in a pleasingly simple feature extractor .",28,0,29
dataset/preprocessed/test-data/dependency_parsing/1,"We demonstrate the effectiveness of the approach by using the BiLSTM feature extractor in two parsing architectures , transition - based ( Section 4 ) as well as a graph - based ( Section 5 ) .",29,0,37
dataset/preprocessed/test-data/dependency_parsing/1,"In the graphbased parser , we jointly train a structured - prediction model on top of a BiLSTM , propagating errors from the structured objective all the way back to the BiLSTM feature - encoder .",30,0,36
dataset/preprocessed/test-data/dependency_parsing/1,"To the best of our knowledge , we are the first to perform such end - to - end training of a structured prediction model and a recurrent feature extractor for non-sequential outputs .",31,0,34
dataset/preprocessed/test-data/dependency_parsing/1,"Aside from the novelty of the BiLSTM feature extractor and the end - to - end structured training , we rely on existing models and techniques from the parsing and structured prediction literature .",32,0,34
dataset/preprocessed/test-data/dependency_parsing/1,"We stick to the simplest parsers in each categorygreedy inference for the transition - based architecture , and a first - order , arc -factored model for the graph - based architecture .",33,0,33
dataset/preprocessed/test-data/dependency_parsing/1,"Despite the simplicity of the parsing architectures and the feature functions , we achieve near state - of - the - art parsing accuracies in both English ( 93.1 UAS ) and Chinese ( 86.6 UAS ) , using a first - order parser with two features and while training solely on Treebank data , without relying on semi-supervised signals such as pre-trained word embeddings , word - clusters , or tech - 1 Structured training of sequence tagging models over RNNbased representations was explored by and niques such as tri-training .",34,0,92
dataset/preprocessed/test-data/dependency_parsing/1,"When also including pre-trained word embeddings , we obtain further improvements , with accuracies of 93.9 UAS ( English ) and 87.6 UAS ( Chinese ) for a greedy transition - based parser with 11 features , and 93.6 UAS ( En ) / 87.4 ( Ch ) for a greedy transitionbased parser with 4 features .",35,0,57
dataset/preprocessed/test-data/dependency_parsing/1,Background and Notation,36,0,3
dataset/preprocessed/test-data/dependency_parsing/1,"We use x 1:n to denote a sequence of n vectors x 1 , , x n . F ? ( ) is a function parameterized with parameters ?.",38,0,29
dataset/preprocessed/test-data/dependency_parsing/1,We write FL ( ) as shorthand for F ?,39,0,10
dataset/preprocessed/test-data/dependency_parsing/1,L - an instantiation of F with a specific set of parameters ?,40,0,13
dataset/preprocessed/test-data/dependency_parsing/1,"We use to denote a vector concatenation operation , and v[i ] to denote an indexing operation taking the ith element of a vector v.",42,0,25
dataset/preprocessed/test-data/dependency_parsing/1,Feature Functions in Dependency Parsing,43,0,5
dataset/preprocessed/test-data/dependency_parsing/1,"Traditionally , state - of - the - art parsers rely on linear models over hand - crafted feature functions .",44,0,21
dataset/preprocessed/test-data/dependency_parsing/1,"The feature functions look at core components ( e.g. "" word on top of stack "" , "" leftmost child of the second - totop word on the stack "" , "" distance between the head and the modifier words "" ) , and are comprised of several templates , where each template instantiates a binary indicator function over a conjunction of core elements ( resulting in features of the form "" word on top of stack is X and leftmost child is Y and . . . "" ) .",45,0,91
dataset/preprocessed/test-data/dependency_parsing/1,The design of the feature function - which components to consider and which combinations of components to include - is a major challenge in parser design .,46,0,27
dataset/preprocessed/test-data/dependency_parsing/1,"Once a good feature function is proposed in a paper it is usually adopted in later works , and sometimes tweaked to improve performance .",47,0,25
dataset/preprocessed/test-data/dependency_parsing/1,"Examples of good feature functions are the feature - set proposed by for transitionbased parsing ( including roughly 20 core components and 72 feature templates ) , and the featureset proposed by for graphbased parsing , with the paper listing 18 templates for a first - order parser , while the first order featureextractor in the actual implementation 's code ( MST - Parser 2 ) includes roughly a hundred feature templates .",48,0,73
dataset/preprocessed/test-data/dependency_parsing/1,"The core features in a transition - based parser usually look at information such as the word - identity and part - of - speech ( POS ) tags of a fixed number of words on top of the stack , a fixed number of words on the top of the buffer , the modifiers ( usually leftmost and right - most ) of items on the stack and on the buffer , the number of modifiers of these elements , parents of words on the stack , and the length of the spans spanned by the words on the stack .",49,0,102
dataset/preprocessed/test-data/dependency_parsing/5,Structured Training for Neural Network Transition - Based Parsing,2,1,9
dataset/preprocessed/test-data/dependency_parsing/5,We present structured perceptron training for neural network transition - based dependency parsing .,4,1,14
dataset/preprocessed/test-data/dependency_parsing/5,We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences .,5,0,20
dataset/preprocessed/test-data/dependency_parsing/5,"Given this fixed network representation , we learn a final layer using the structured perceptron with beam - search decoding .",6,0,21
dataset/preprocessed/test-data/dependency_parsing/5,"On the Penn Treebank , our parser reaches 94. 26 % unlabeled and 92.41 % labeled attachment accuracy , which to our knowledge is the best accuracy on Stanford Dependencies to date .",7,0,33
dataset/preprocessed/test-data/dependency_parsing/5,We also provide indepth ablative analysis to determine which aspects of our model provide the largest gains in accuracy .,8,0,20
dataset/preprocessed/test-data/dependency_parsing/5,Syntactic analysis is a central problem in language understanding that has received a tremendous amount of attention .,10,0,18
dataset/preprocessed/test-data/dependency_parsing/5,"Lately , dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages and the efficiency of dependency parsers .",11,1,30
dataset/preprocessed/test-data/dependency_parsing/5,Transition - based parsers have been shown to provide a good balance between efficiency and accuracy .,12,0,17
dataset/preprocessed/test-data/dependency_parsing/5,"In transition - based parsing , sentences are processed in a linear left to right pass ; at each position , the parser needs to choose from a set of possible actions defined by the transition strategy .",13,1,38
dataset/preprocessed/test-data/dependency_parsing/5,"In greedy models , a classifier is used to independently decide which transition to take based on local features of the current parse configuration .",14,0,25
dataset/preprocessed/test-data/dependency_parsing/5,This classifier typically uses hand - engineered features and is trained on individual transitions extracted from the gold transition sequence .,15,0,21
dataset/preprocessed/test-data/dependency_parsing/5,"While extremely fast , these greedy models typically suffer from search errors due to the inability to recover from incorrect decisions .",16,0,22
dataset/preprocessed/test-data/dependency_parsing/5,showed that a beamsearch decoding algorithm utilizing the structured perceptron training algorithm can greatly improve accuracy .,17,0,17
dataset/preprocessed/test-data/dependency_parsing/5,"Nonetheless , significant manual feature engineering was required before transitionbased systems provided competitive accuracy with graph - based parsers , and only by incorporating graph - based scoring functions were able to exceed the accuracy of graph - based approaches .",18,0,41
dataset/preprocessed/test-data/dependency_parsing/5,"In contrast to these carefully hand - tuned approaches , recently presented a neural network version of a greedy transition - based parser .",19,0,24
dataset/preprocessed/test-data/dependency_parsing/5,"In their model , a feedforward neural network with a hidden layer is used to make the transition decisions .",20,0,20
dataset/preprocessed/test-data/dependency_parsing/5,"The hidden layer has the power to learn arbitrary combinations of the atomic inputs , thereby eliminating the need for hand - engineered features .",21,0,25
dataset/preprocessed/test-data/dependency_parsing/5,"Furthermore , because the neural network uses a distributed representation , it is able to model lexical , part - of - speech ( POS ) tag , and arc label similarities in a continuous space .",22,0,37
dataset/preprocessed/test-data/dependency_parsing/5,"However , although their model outperforms its greedy hand - engineered counterparts , it is not competitive with state - of - the - art dependency parsers thatare trained for structured search .",23,0,33
dataset/preprocessed/test-data/dependency_parsing/5,"In this work , we combine the representational power of neural networks with the superior search enabled by structured training and inference , making our parser one of the most accurate dependency parsers to date .",24,0,36
dataset/preprocessed/test-data/dependency_parsing/5,"Training and testing on the Penn Treebank , our transition - based parser achieves 93.99 % unlabeled ( UAS ) / 92.05 % labeled ( LAS ) attachment accuracy , outperforming the 93.22 % UAS / 91.02 % LAS of and 93.27 UAS / 91.19 LAS of .",25,0,48
dataset/preprocessed/test-data/dependency_parsing/5,"In addition , by incorporating unlabeled data into training , we further improve the accuracy of our model to 94.26 % UAS / 92.41 % LAS ( 93.46 % UAS / 91.49 % LAS for our greedy model ) .",26,0,40
dataset/preprocessed/test-data/dependency_parsing/5,"In our approach we start with the basic structure of , but with a deeper architecture and improvements to the optimization procedure .",27,0,23
dataset/preprocessed/test-data/dependency_parsing/5,These modifications ( Section 2 ) increase the performance of the greedy model by as much as 1 % .,28,0,20
dataset/preprocessed/test-data/dependency_parsing/5,"As in prior work , we train the neural network to model the probability of individual parse actions .",29,0,19
dataset/preprocessed/test-data/dependency_parsing/5,"However , we do not use these probabilities directly for prediction .",30,0,12
dataset/preprocessed/test-data/dependency_parsing/5,"Instead , we use the activations from all layers of the neural network as the representation in a structured perceptron model that is trained with beam search and early updates ( Section 3 ) .",31,0,35
dataset/preprocessed/test-data/dependency_parsing/5,"On the Penn Treebank , this structured learning approach significantly improves parsing accuracy by 0.8 % .",32,0,17
dataset/preprocessed/test-data/dependency_parsing/5,An additional contribution of this work is an effective way to leverage unlabeled data .,33,0,15
dataset/preprocessed/test-data/dependency_parsing/5,"Neural networks are known to perform very well in the presence of large amounts of training data ; however , obtaining more expert - annotated parse trees is very expensive .",34,0,31
dataset/preprocessed/test-data/dependency_parsing/5,"To this end , we generate large quantities of high - confidence parse trees by parsing unlabeled data with two different parsers and selecting only the sentences for which the two parsers produced the same trees ( Section 3.3 ) .",35,0,41
dataset/preprocessed/test-data/dependency_parsing/5,"This approach is known as "" tri-training "" and we show that it benefits our neural network parser significantly more than other approaches .",36,0,24
dataset/preprocessed/test-data/dependency_parsing/5,"By adding 10 million automatically parsed tokens to the training data , we improve the accuracy of our parsers by almost ? 1.0 % on web domain data .",37,0,29
dataset/preprocessed/test-data/dependency_parsing/5,We provide an extensive exploration of our model in Section 5 through ablative analysis and other retrospective experiments .,38,0,19
dataset/preprocessed/test-data/dependency_parsing/5,One of the goals of this work is to provide guidance for future refinements and improvements on the architecture and modeling choices we introduce in this paper .,39,0,28
dataset/preprocessed/test-data/dependency_parsing/5,"Finally , we also note that neural network representations have along history in syntactic parsing ; however , like , our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients .",40,0,46
dataset/preprocessed/test-data/dependency_parsing/5,Our work is also not the first to apply structured training to neural networks ( see e.g . Features Extracted early updates ( section 3 ) .,41,0,27
dataset/preprocessed/test-data/dependency_parsing/5,Structured learning reduces bias and significantly improves parsing accuracy by 0.6 % .,42,0,13
dataset/preprocessed/test-data/dependency_parsing/5,"We demonstrate empirically that beam search based on the scores from the neural network does notwork as well , perhaps because of the label bias problem .",43,0,27
dataset/preprocessed/test-data/dependency_parsing/5,A second contribution of this work is an effective way to leverage unlabeled data and other parsers .,44,0,18
dataset/preprocessed/test-data/dependency_parsing/5,Neural networks are known to perform very well in the presence of large amounts of training data .,45,0,18
dataset/preprocessed/test-data/dependency_parsing/5,It is however unlikely that the amount of hand parsed data will increase significantly because of the high cost for syntactic annotations .,46,0,23
dataset/preprocessed/test-data/dependency_parsing/5,To this end we generate large quantities of high - confidence parse trees by parsing an unlabeled corpus and selecting only the sentences on which two different parsers produced the same parse trees .,47,0,34
dataset/preprocessed/test-data/dependency_parsing/5,"This idea comes from tri-training and while applicable to other parsers as well , we show that it benefits neural network parsers more than models with discrete features .",48,0,29
dataset/preprocessed/test-data/dependency_parsing/5,Adding 10 million automatically parsed tokens to the training data improves the accuracy of our parsers further by 0.7 % .,49,0,21
dataset/preprocessed/test-data/dependency_parsing/8,Globally Normalized Transition - Based Neural Networks,2,1,7
dataset/preprocessed/test-data/dependency_parsing/8,"We introduce a globally normalized transition - based neural network model that achieves state - of - the - art part - ofspeech tagging , dependency parsing and sentence compression results .",4,1,32
dataset/preprocessed/test-data/dependency_parsing/8,"Our model is a simple feed - forward neural network that operates on a task - specific transition system , yet achieves comparable or better accuracies than recurrent models .",5,0,30
dataset/preprocessed/test-data/dependency_parsing/8,We discuss the importance of global as opposed to local normalization : a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models .,6,0,36
dataset/preprocessed/test-data/dependency_parsing/8,Neural network approaches have taken the field of natural language processing ( NLP ) by storm .,8,0,17
dataset/preprocessed/test-data/dependency_parsing/8,"In particular , variants of long short - term memory ( LSTM ) networks have produced impressive results on some of the classic NLP tasks such as part - of - speech tagging , syntactic parsing and semantic role labeling .",9,0,41
dataset/preprocessed/test-data/dependency_parsing/8,One might speculate that it is the recurrent nature of these models that enables these results .,10,0,17
dataset/preprocessed/test-data/dependency_parsing/8,"In this work we demonstrate that simple feed - forward networks without any recurrence can achieve comparable or better accuracies than LSTMs , as long as they are globally normalized .",11,0,31
dataset/preprocessed/test-data/dependency_parsing/8,"Our model , described in detail in Section 2 , uses a transition system and feature embeddings as introduced by * On leave from Columbia University ..",12,0,27
dataset/preprocessed/test-data/dependency_parsing/8,"We do not use any recurrence , but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field ( CRF ) objective to overcome the label bias problem that locally normalized models suffer from .",13,0,41
dataset/preprocessed/test-data/dependency_parsing/8,"Since we use beam inference , we approximate the partition function by summing over the elements in the beam , and use early updates .",14,0,25
dataset/preprocessed/test-data/dependency_parsing/8,We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss .,15,0,25
dataset/preprocessed/test-data/dependency_parsing/8,In Section 3 we revisit the label bias problem and the implication that globally normalized models are strictly more expressive than locally normalized models .,16,0,25
dataset/preprocessed/test-data/dependency_parsing/8,"Lookahead features can partially mitigate this discrepancy , but can not fully compensate for it - a point to which we return later .",17,0,24
dataset/preprocessed/test-data/dependency_parsing/8,"To empirically demonstrate the effectiveness of global normalization , we evaluate our model on part - of - speech tagging , syntactic dependency parsing and sentence compression ( Section 4 ) .",18,0,32
dataset/preprocessed/test-data/dependency_parsing/8,"Our model achieves state - of - the - art accuracy on all of these tasks , matching or outperforming LSTMs while being significantly faster .",19,0,26
dataset/preprocessed/test-data/dependency_parsing/8,In particular for dependency parsing on the Wall Street Journal we achieve the best - ever published unlabeled attachment score of 94.61 % .,20,0,24
dataset/preprocessed/test-data/dependency_parsing/8,"As discussed in more detail in Section 5 , we also outperform previous structured training approaches used for neural network transitionbased parsing .",21,0,23
dataset/preprocessed/test-data/dependency_parsing/8,"Our ablation experiments show that we outperform and because we do global backpropagation training of all model parameters , while they fix the neural network parameters when training the global part of their model .",22,0,35
dataset/preprocessed/test-data/dependency_parsing/8,We also outperform despite using a smaller beam .,23,0,9
dataset/preprocessed/test-data/dependency_parsing/8,"To shed additional light on the label bias problem in practice , we provide a sentence compression example where the local model completely fails .",24,0,25
dataset/preprocessed/test-data/dependency_parsing/8,"We then demonstrate that a globally normalized parsing model without any lookahead features is almost as accurate as our best model , while a locally normalized model loses more than 10 % absolute in accuracy because it can not effectively incorporate evidence as it becomes available .",25,0,47
dataset/preprocessed/test-data/dependency_parsing/8,"Finally , we provide an open - source implementation of our method , called SyntaxNet , 1 which we have integrated into the popular TensorFlow 2 framework .",26,0,28
dataset/preprocessed/test-data/dependency_parsing/8,"We also provide a pre-trained , state - of - the art English dependency parser called "" Parsey McParseface , "" which we tuned for a balance of speed , simplicity , and accuracy .",27,0,35
dataset/preprocessed/test-data/dependency_parsing/8,"At its core , our model is an incremental transitionbased parser .",29,0,12
dataset/preprocessed/test-data/dependency_parsing/8,To apply it to different tasks we only need to adjust the transition system and the input features .,30,0,19
dataset/preprocessed/test-data/dependency_parsing/8,"Given an input x , most often a sentence , we define :",32,0,13
dataset/preprocessed/test-data/dependency_parsing/8,A set of states S ( x ) .,33,0,9
dataset/preprocessed/test-data/dependency_parsing/8,A special start state s ? S ( x ) .,34,0,11
dataset/preprocessed/test-data/dependency_parsing/8,"A set of allowed decisions A ( s , x ) for all s ?",35,0,15
dataset/preprocessed/test-data/dependency_parsing/8,"S ( x ) . A transition function t( s , d , x ) returning a new state s ?",36,0,21
dataset/preprocessed/test-data/dependency_parsing/8,for any decision d ?,37,0,5
dataset/preprocessed/test-data/dependency_parsing/8,"A ( s , x ) .",38,0,7
dataset/preprocessed/test-data/dependency_parsing/8,We will use a function ?,39,0,6
dataset/preprocessed/test-data/dependency_parsing/8,to compute the score of decision din state s for input x .,40,0,13
dataset/preprocessed/test-data/dependency_parsing/8,The vector ?,41,0,3
dataset/preprocessed/test-data/dependency_parsing/8,"contains the model parameters and we assume that ?( s , d , x ; ? ) is differentiable with respect to ?.",42,0,23
dataset/preprocessed/test-data/dependency_parsing/8,"In this section , for brevity , we will drop the dependence of x in the functions given above , simply writing S , A ( s ) , t( s , d ) , and ?.",43,0,37
dataset/preprocessed/test-data/dependency_parsing/8,Throughout this work we will use transition systems in which all complete structures for the same input x have the same number of decisions n ( x ) ( or n for brevity ) .,44,0,35
dataset/preprocessed/test-data/dependency_parsing/8,"In dependency parsing for example , this is true for both the arc-standard and arc-eager transition systems , where for a sentence x of length m , the number of deci-sions for any complete parse is n ( x ) = 2 m. 3",45,0,44
dataset/preprocessed/test-data/dependency_parsing/8,"A complete structure is then a sequence of decision / state pairs ( s 1 , d 1 ) . . . ( s n , d n ) such that s 1 = s , d i ?",46,0,39
dataset/preprocessed/test-data/dependency_parsing/8,"S (s i ) for i = 1 . . . n , and s i + 1 = t ( s i , d i ) .",47,0,28
dataset/preprocessed/test-data/dependency_parsing/8,We use the notation d 1:j to refer to a decision sequence d 1 . . . d j .,48,0,20
dataset/preprocessed/test-data/dependency_parsing/8,"We assume that there is a one - to - one mapping between decision sequences d 1:j?1 and states s j : that is , we essentially assume that a state encodes the entire history of decisions .",49,0,38
dataset/preprocessed/test-data/dependency_parsing/6,DEEP BIAFFINE ATTENTION FOR NEURAL DEPENDENCY PARSING,2,1,7
dataset/preprocessed/test-data/dependency_parsing/6,This paper builds off recent work from Kiperwasser & Goldberg ( 2016 ) using neural attention in a simple graph - based dependency parser .,4,0,25
dataset/preprocessed/test-data/dependency_parsing/6,"We use a larger but more thoroughly regularized parser than other recent BiLSTM - based approaches , with biaffine classifiers to predict arcs and labels .",5,0,26
dataset/preprocessed/test-data/dependency_parsing/6,"Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages , achieving 95.7 % UAS and 94.1 % LAS on the most popular English PTB dataset .",6,0,38
dataset/preprocessed/test-data/dependency_parsing/6,"This makes it the highest - performing graph - based parser on this benchmarkoutperforming Kiperwasser & Goldberg ( 2016 ) by 1.8 % and 2.2 % - and comparable to the highest performing transition - based parser ( Kuncoro et al. , 2016 ) , which achieves 95.8 % UAS and 94.6 % LAS .",7,0,55
dataset/preprocessed/test-data/dependency_parsing/6,"We also show which hyperparameter choices had a significant effect on parsing accuracy , allowing us to achieve large gains over other graph - based approaches .",8,0,27
dataset/preprocessed/test-data/dependency_parsing/6,"Dependency parsers - which annotate sentences in a way designed to be easy for humans and computers alike to understand - have been found to be extremely useful for a sizable number of NLP tasks , especially those involving natural language understanding in someway .",10,0,45
dataset/preprocessed/test-data/dependency_parsing/6,"However , frequent incorrect parses can severely inhibit final performance , so improving the quality of dependency parsers is needed for the improvement and success of these downstream tasks .",11,0,30
dataset/preprocessed/test-data/dependency_parsing/6,The current state - of - the - art transition - based neural dependency parser substantially outperforms many much simpler neural graph - based parsers .,12,0,26
dataset/preprocessed/test-data/dependency_parsing/6,"We modify the neural graphbased approach first proposed by in a few ways to achieve competitive performance : we build a network that 's larger but uses more regularization ; we replace the traditional MLP - based attention mechanism and affine label classifier with biaffine ones ; and rather than using the top recurrent states of the LSTM in the biaffine transformations , we first put them through MLP operations that reduce their dimensionality .",13,0,75
dataset/preprocessed/test-data/dependency_parsing/6,"Furthermore , we compare models trained with different architectures and hyperparameters to motivate our approach empirically .",14,0,17
dataset/preprocessed/test-data/dependency_parsing/6,The resulting parser maintains most of the simplicity of neural graph - based approaches while approaching the performance of the SOTA transition - based one .,15,0,26
dataset/preprocessed/test-data/dependency_parsing/6,BACKGROUND AND RELATED WORK,16,0,4
dataset/preprocessed/test-data/dependency_parsing/6,"Transition - based parsers - such as shift - reduce parsers - parse sentences from left to right , maintaining a "" buffer "" of words that have not yet been parsed and a "" stack "" of words whose head has not been seen or whose dependents have not all been fully parsed .",17,0,55
dataset/preprocessed/test-data/dependency_parsing/6,"At each step , transition - based parsers can access and manipulate the stack and buffer and assign arcs from one word to another .",18,0,25
dataset/preprocessed/test-data/dependency_parsing/6,"One can then train any multi-class machine learning classifier on features extracted from the stack , buffer , and previous arc actions in order to predict the next action .",19,0,30
dataset/preprocessed/test-data/dependency_parsing/6,Chen & Manning ( 2014 ) make the first successful attempt at incorporating deep learning into a transition - based dependency parser .,20,0,23
dataset/preprocessed/test-data/dependency_parsing/6,"At each step , the ( feedforward ) network assigns a probability to each action the parser can take based on word , tag , and label embeddings from certain words root / ROOT Casey / NNP hugged / VBD",21,0,40
dataset/preprocessed/test-data/dependency_parsing/6,Kim / NNP root nsubj dobj :,22,0,7
dataset/preprocessed/test-data/dependency_parsing/6,"A dependency tree parse for Casey hugged Kim , including part - of - speech tags and a special root token .",23,0,22
dataset/preprocessed/test-data/dependency_parsing/6,Directed edges ( or arcs ) with labels ( or relations ) connect the verb to the root and the arguments to the verb head .,24,0,26
dataset/preprocessed/test-data/dependency_parsing/6,on the stack and buffer .,25,0,6
dataset/preprocessed/test-data/dependency_parsing/6,"A number of other researchers have attempted to address some limitations of Chen & Manning 's Chen & Manning parser by augmenting it with additional complexity : and augment it with a beam search and a conditional random field loss objective to allow the parser to "" undo "" previous actions once it finds evidence that they may have been incorrect ; and and instead use LSTMs to represent the stack and buffer , getting state - of - the - art performance by building in a way of composing parsed phrases together .",26,0,94
dataset/preprocessed/test-data/dependency_parsing/6,Transition - based parsing processes a sentence sequentially to buildup a parse tree one arc at a time .,27,0,19
dataset/preprocessed/test-data/dependency_parsing/6,"Consequently , these parsers do n't use machine learning for directly predicting edges ; they use it for predicting the operations of the transition algorithm .",28,0,26
dataset/preprocessed/test-data/dependency_parsing/6,"Graph - based parsers , by contrast , use machine learning to assign a weight or probability to each possible edge and then construct a maximum spaning tree ( MST ) from these weighted edges .",29,0,36
dataset/preprocessed/test-data/dependency_parsing/6,present a neural graph - based parser ( in addition to a transition - based one ) that uses the same kind of attention mechanism as for machine translation .,30,0,30
dataset/preprocessed/test-data/dependency_parsing/6,"In Kiperwasser & Goldberg 's 2016 model , the ( bidirectional ) LSTM 's recurrent output vector for each word is concatenated with each possible head 's recurrent vector , and the result is used as input to an MLP that scores each resulting arc .",31,0,46
dataset/preprocessed/test-data/dependency_parsing/6,The predicted tree structure at training time is the one where each word depends on its highestscoring head .,32,0,19
dataset/preprocessed/test-data/dependency_parsing/6,"Labels are generated analogously , with each word 's recurrent output vector and it s gold or predicted head word 's recurrent vector being used in a multi -class MLP .",33,0,31
dataset/preprocessed/test-data/dependency_parsing/6,"Similarly , include a graph - based dependency parser in their multi -task neural model .",34,0,16
dataset/preprocessed/test-data/dependency_parsing/6,"In addition to training the model with multiple distinct objectives , they replace the traditional MLP - based attention mechanism that use with a bilinear one ( but still using an MLP label classifier ) .",35,0,36
dataset/preprocessed/test-data/dependency_parsing/6,This makes it analogous to Luong et al. 's 2015 proposed attention mechanism for neural machine translation .,36,0,18
dataset/preprocessed/test-data/dependency_parsing/6,"likewise propose a graph - based neural dependency parser , but in a way that attempts to circumvent the limitation of other neural graph - based parsers being unable to condition the scores of each possible arc on previous parsing decisions .",37,0,42
dataset/preprocessed/test-data/dependency_parsing/6,"In addition to having one bidirectional recurrent network that computes a recurrent hidden vector for each word , they have additional , unidirectional recurrent networks ( leftto - right and right - to - left ) that keep track of the probabilities of each previous arc , and use these together to predict the scores for the next arc .",38,0,60
dataset/preprocessed/test-data/dependency_parsing/6,PROPOSED DEPENDENCY PARSER,39,0,3
dataset/preprocessed/test-data/dependency_parsing/6,DEEP BIAFFINE ATTENTION,40,0,3
dataset/preprocessed/test-data/dependency_parsing/6,"We make a few modifications to the graph - based architectures of , , and , shown in : we use biaffine attention instead of bilinear or traditional MLP - based attention ; we use a biaffine dependency label classifier ; and we apply dimension - reducing MLPs to each recurrent output vector r i before applying the biaffine transformation .",41,0,61
dataset/preprocessed/test-data/dependency_parsing/6,"The choice of biaffine rather than bilinear or MLP mechanisms makes the classifiers in our model analogous to traditional affine classifiers , which use an affine transformation over a single LSTM output state r i ( or other vector input ) to predict the vector of scores s i for all classes ( 1 ) .",43,0,56
dataset/preprocessed/test-data/dependency_parsing/6,We can think of the proposed biaffine attention mechanism as being a traditional affine,44,0,14
dataset/preprocessed/test-data/dependency_parsing/6,"Figure 2 : BiLSTM with deep biaffine attention to score each possible head for each dependent , applied to the sentence "" Casey hugged Kim "" .",45,0,27
dataset/preprocessed/test-data/dependency_parsing/6,We reverse the order of the biaffine transformation here for clarity .,46,0,12
dataset/preprocessed/test-data/dependency_parsing/6,"classifier , but using a ( d d ) linear transformation of the stacked LSTM output RU ( 1 ) in place of the weight matrix W and a ( d 1 ) transformation Ru ( 2 ) for the bias term b.",47,0,43
dataset/preprocessed/test-data/dependency_parsing/6,"In addition to being arguably simpler than the MLP - based approach ( involving one bilinear layer rather than two linear layers and a nonlinearity ) , this has the conceptual advantage of directly modeling both the prior probability of a word j receiving any dependents in the term r ? j u ( 2 ) and the likelihood of j receiving a specific dependent i in the term r ? j U ( 1 ) r i .",48,0,79
dataset/preprocessed/test-data/dependency_parsing/6,"Analogously , we also use a biaffine classifier to predict dependency labels given the gold or predicted heady i ( 3 ) .",49,0,23
dataset/preprocessed/test-data/dependency_parsing/0,An improved neural network model for joint POS tagging and dependency parsing,2,1,12
dataset/preprocessed/test-data/dependency_parsing/0,We propose a novel neural network model for joint part - of - speech ( POS ) tagging and dependency parsing .,4,1,22
dataset/preprocessed/test-data/dependency_parsing/0,"Our model extends the well - known BIST graph - based dependency parser ( Kiperwasser and Goldberg , 2016 ) by incorporating a BiLSTM - based tagging component to produce automatically predicted POS tags for the parser .",5,0,38
dataset/preprocessed/test-data/dependency_parsing/0,"On the benchmark English Penn treebank , our model obtains strong UAS and LAS scores at 94.51 % and 92.87 % , respectively , producing 1.5 + % absolute improvements to the BIST graph - based parser , and also obtaining a state - of - the - art POS tagging accuracy at 97.97 % .",6,0,56
dataset/preprocessed/test-data/dependency_parsing/0,"Furthermore , experimental results on parsing 61 "" big "" Universal Dependencies treebanks from raw texts show that our model outperforms the baseline UDPipe ( Straka and Strakov , 2017 ) with 0.8 % higher average POS tagging score and 3.6 % higher average LAS score .",7,0,47
dataset/preprocessed/test-data/dependency_parsing/0,"In addition , with our model , we also obtain state - of - the - art downstream task scores for biomedical event extraction and opinion analysis applications .",8,0,29
dataset/preprocessed/test-data/dependency_parsing/0,Our code is available together with all pretrained models at : https://github.com/datquocnguyen/jPTDP .,9,0,13
dataset/preprocessed/test-data/dependency_parsing/0,"Dependency parsing - a key research topic in natural language processing ( NLP ) in the last decade ) - has also been demonstrated to be extremely useful in many applications such as relation extraction , semantic parsing and machine translation ) .",11,1,43
dataset/preprocessed/test-data/dependency_parsing/0,"In general , dependency parsing models can be categorized as graph - based and transition - based .",12,0,18
dataset/preprocessed/test-data/dependency_parsing/0,"Most traditional graph - or transition - based models define a set of core and combined features , while recent stateof - the - art models propose neural network architectures to handle feature - engineering .",13,0,36
dataset/preprocessed/test-data/dependency_parsing/0,Most traditional and neural network - based parsing models use automatically predicted POS tags as essential features .,14,0,18
dataset/preprocessed/test-data/dependency_parsing/0,"However , POS taggers are not perfect , resulting in error propagation problems .",15,0,14
dataset/preprocessed/test-data/dependency_parsing/0,"Some work has attempted to avoid using POS tags for dependency parsing , however , to achieve the strongest parsing scores these methods still require automatically assigned POS tags .",16,0,30
dataset/preprocessed/test-data/dependency_parsing/0,"Alternatively , joint POS tagging and dependency parsing has also attracted a lot of attention in NLP community as it could help improve both tagging and parsing results over independent modeling .",17,0,32
dataset/preprocessed/test-data/dependency_parsing/0,"In this paper , we present a novel neural network - based model for jointly learning POS tagging and dependency paring .",18,0,22
dataset/preprocessed/test-data/dependency_parsing/0,Our joint model extends the well - known BIST graph - based dependency parser with an additional lower - level BiLSTM - based tagging component .,19,0,26
dataset/preprocessed/test-data/dependency_parsing/0,"In particular , this tagging component generates predicted POS tags for the parser component .",20,0,15
dataset/preprocessed/test-data/dependency_parsing/0,duces a 1.5 + % absolute improvement over the BIST graph - based parser with a strong UAS score of 94.51 % and LAS score of 92.87 % ; and also obtaining a state - of - the - art POS tagging accuracy of 97.97 % .,21,0,47
dataset/preprocessed/test-data/dependency_parsing/0,"In addition , multilingual parsing experiments from raw texts on 61 "" big "" Universal Dependencies treebanks show that our model outperforms the baseline UDPipe with 0.8 % higher average POS tagging score , 3.1 % higher UAS and 3.6 % higher LAS .",22,0,44
dataset/preprocessed/test-data/dependency_parsing/0,"Furthermore , experimental results on downstream task applications show that our joint model helps produce state - of - the - art scores for biomedical event extraction and opinion analysis .",23,0,31
dataset/preprocessed/test-data/dependency_parsing/0,Our joint model,24,0,3
dataset/preprocessed/test-data/dependency_parsing/0,This section presents our model for joint POS tagging and graph - based dependency parsing .,25,0,16
dataset/preprocessed/test-data/dependency_parsing/0,illustrates the architecture of our joint model which can be viewed as a two - component mixture of a tagging component and a parsing component .,26,0,26
dataset/preprocessed/test-data/dependency_parsing/0,"Given word tokens in an input sentence , the tagging component uses a BiLSTM to learn "" latent "" feature vectors representing these word tokens .",27,0,26
dataset/preprocessed/test-data/dependency_parsing/0,Then the tagging component feeds these feature vectors into a multilayer perceptron with one hidden layer ( MLP ) to predict POS tags .,28,0,24
dataset/preprocessed/test-data/dependency_parsing/0,"The parsing component then uses another BiLSTM to learn another set of latent feature representations , based on both the input word tokens and the predicted POS tags .",29,0,29
dataset/preprocessed/test-data/dependency_parsing/0,These latent feature representations are fed into a MLP to decode dependency arcs and another MLP to label the predicted dependency arcs .,30,0,23
dataset/preprocessed/test-data/dependency_parsing/0,Word vector representation,31,0,3
dataset/preprocessed/test-data/dependency_parsing/0,"Given an input sentence s consisting of n word tokens w 1 , w 2 , ... , w n , we represent each i th word w i in s by a vector e i .",32,0,37
dataset/preprocessed/test-data/dependency_parsing/0,We obtain e i by concatenating word embedding e ( W ) w i and character - level word embedding e ( C ) w i :,33,0,27
dataset/preprocessed/test-data/dependency_parsing/0,"Here , each word type win the training data is represented by a real - valued word embedding e ( W ) w .",34,0,24
dataset/preprocessed/test-data/dependency_parsing/0,"Given the word type w consisting of k characters w = c 1 c 2 ... c k where each j th character in w is represented by a character embedding c j , we use a sequence BiLSTM ( BiLSTM seq ) to learn its character - level vector representation .",35,0,52
dataset/preprocessed/test-data/dependency_parsing/0,"The input to BiLSTM seq is the sequence of k character embeddings c 1 :k , and the output is a concatenation of outputs of a forward LSTM ( LSTM f ) reading the input in its regular order and a reverse LSTM ( LSTM r ) reading the input in reverse :",36,0,53
dataset/preprocessed/test-data/dependency_parsing/0,"We feed the sequence of vectors e 1:n with an additional context position index i into another BiL - STM ( BiLSTM pos ) , resulting in latent feature vectors v",38,0,31
dataset/preprocessed/test-data/dependency_parsing/0,We use a MLP with softmax output ( MLP pos ) on top of the BiLSTM pos to predict POS tag of each word in s .,39,0,27
dataset/preprocessed/test-data/dependency_parsing/0,The number of nodes in the output layer of this MLP pos is the number of POS tags .,40,0,19
dataset/preprocessed/test-data/dependency_parsing/0,"Given v ( pos ) i , we compute an output vector as :",41,0,14
dataset/preprocessed/test-data/dependency_parsing/0,Based on output vectors ?,42,0,5
dataset/preprocessed/test-data/dependency_parsing/0,"i , we then compute the cross - entropy objective loss L POS ( t , t ) , in whicht and tare the sequence of predicted POS tags and sequence of gold POS tags of words in the input sentence s , respectively .",43,0,45
dataset/preprocessed/test-data/dependency_parsing/0,"Our tagging component thus can be viewed as a simplified version of the POS tagging model proposed by , without their additional auxiliary loss for rare words .",44,0,28
dataset/preprocessed/test-data/dependency_parsing/0,"Assume that p 1 , p 2 , ... , p n are the predicted POS tags produced by the tagging component for the input words .",46,0,27
dataset/preprocessed/test-data/dependency_parsing/0,We represent each i th predicted POS tag by a vector embedding e ( P ) pi .,47,0,18
dataset/preprocessed/test-data/dependency_parsing/0,We then create a sequence of vectors x 1:n in which each x i is produced by concatenating the POS tag embedding e ( P ) pi and the word vector representation e i :,48,0,35
dataset/preprocessed/test-data/dependency_parsing/0,"We feed the sequence of vectors x 1:n with an additional index i into a BiLSTM ( BiLSTM dep ) , resulting in latent feature vectors vi as follows :",49,0,30
dataset/preprocessed/test-data/dependency_parsing/2,Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser,2,0,11
dataset/preprocessed/test-data/dependency_parsing/2,We introduce two first - order graph - based dependency parsers achieving a new state of the art .,4,0,19
dataset/preprocessed/test-data/dependency_parsing/2,The first is a consensus parser built from an ensemble of independently trained greedy LSTM transition - based parsers with different random initializations .,5,0,24
dataset/preprocessed/test-data/dependency_parsing/2,We cast this approach as minimum Bayes risk decoding ( under the Hamming cost ) and argue that weaker consensus within the ensemble is a useful signal of difficulty or ambiguity .,6,0,32
dataset/preprocessed/test-data/dependency_parsing/2,"The second parser is a "" distillation "" of the ensemble into a single model .",7,0,16
dataset/preprocessed/test-data/dependency_parsing/2,"We train the distillation parser using a structured hinge loss objective with a novel cost that incorporates ensemble uncertainty estimates for each possible attachment , thereby avoiding the intractable crossentropy computations required by applying standard distillation objectives to problems with structured outputs .",8,0,43
dataset/preprocessed/test-data/dependency_parsing/2,"The first - order distillation parser matches or surpasses the state of the art on English , Chinese , and German .",9,0,22
dataset/preprocessed/test-data/dependency_parsing/2,"Neural network dependency parsers achieve state of the art performance , but training them involves gradient descent on non-convex objectives , which is unstable with respect to initial parameter values .",11,0,31
dataset/preprocessed/test-data/dependency_parsing/2,"For some tasks , an ensemble of neural networks from different random initializations has been found to improve performance over individual models .",12,0,23
dataset/preprocessed/test-data/dependency_parsing/2,"In 3 , we apply this idea to build a firstorder graph - based ( FOG ) ensemble parser ) that seeks consensus among 20 randomly - initialized stack LSTM parsers , achieving nearly the best - reported performance on the standard Penn Treebank Stanford dependencies task ( 94.51 UAS , 92.70 LAS ) .",13,0,55
dataset/preprocessed/test-data/dependency_parsing/2,"We give a probabilistic interpretation to the ensemble parser ( with a minor modification ) , viewing it as an instance of minimum Bayes risk inference .",14,0,27
dataset/preprocessed/test-data/dependency_parsing/2,We propose that dis agreements among the ensemble 's members maybe taken as a signal that an attachment decision is difficult or ambiguous .,15,0,24
dataset/preprocessed/test-data/dependency_parsing/2,"Ensemble parsing is not a practical solution , however , since an ensemble of N parsers requires",16,0,17
dataset/preprocessed/test-data/dependency_parsing/2,"N times as much computation , plus the runtime of finding consensus .",17,0,13
dataset/preprocessed/test-data/dependency_parsing/2,"We address this issue in 5 by distilling the ensemble into a single FOG parser with discriminative training by defining a new cost function , inspired by the notion of "" soft targets "" .",18,0,35
dataset/preprocessed/test-data/dependency_parsing/2,"The essential idea is to derive the cost of each possible attachment from the ensemble 's division of votes , and use this cost in discriminative learning .",19,0,28
dataset/preprocessed/test-data/dependency_parsing/2,"The application of distilliation to structured prediction is , to our knowledge , new , as is the idea of empirically estimating cost functions .",20,0,25
dataset/preprocessed/test-data/dependency_parsing/2,"The distilled model performs almost as well as the ensemble consensus and much better than ( i ) a strong LSTM FOG parser trained using the conventional Hamming cost function , ( ii ) recently published strong , and ( iii ) many higher - order graph - based parsers .",21,0,51
dataset/preprocessed/test-data/dependency_parsing/2,"It represents a new state of the art for graphbased dependency parsing for English , Chinese , and German .",22,1,20
dataset/preprocessed/test-data/dependency_parsing/2,The code to reproduce our results is publicly available .,23,0,10
dataset/preprocessed/test-data/dependency_parsing/2,Notation and Definitions,25,0,3
dataset/preprocessed/test-data/dependency_parsing/2,"Let x = x 1 , . . . , x n denote an n -length sentence .",26,0,18
dataset/preprocessed/test-data/dependency_parsing/2,"A dependency parse for x , denoted y , is a set of tuples , where h is the index of ahead , m the index of a modifier , and a dependency label ( or relation type ) .",27,0,40
dataset/preprocessed/test-data/dependency_parsing/2,Most dependency parsers are constrained to return y that form a directed tree .,28,0,14
dataset/preprocessed/test-data/dependency_parsing/2,"A first - order graph - based ( FOG ; also known as "" arc-factored "" ) dependency parser exactly solve ?",29,0,22
dataset/preprocessed/test-data/dependency_parsing/2,"where T ( x ) is the set of directed trees over x , and sis a local scoring function that considers only a single dependency arc at a time .",30,0,31
dataset/preprocessed/test-data/dependency_parsing/2,"( We suppress dependency labels ; there are various ways to incorporate them , discussed later . )",31,0,18
dataset/preprocessed/test-data/dependency_parsing/2,"To define s , used hand - engineered features of the surrounding and in - between context of x hand x m ; more recently , Kiperwasser and Goldberg ( 2016 ) used a bidirectional LSTM followed by a single hidden layer with non-linearity .",32,0,45
dataset/preprocessed/test-data/dependency_parsing/2,"The exact solution to Eq. 1 can be found using a minimum ( directed ) spanning tree algorithm or , under a projectivity constraint , a dynamic programming algorithm , in O ( n 2 ) or O ( n 3 ) runtime , respectively .",33,0,46
dataset/preprocessed/test-data/dependency_parsing/2,We refer to parsing with a minimum spanning tree algorithm as MST parsing .,34,0,14
dataset/preprocessed/test-data/dependency_parsing/2,"An alternative that runs in linear time is transition - based parsing , which recasts parsing as a sequence of actions that manipulate auxiliary data structures to incrementally build a parse tree .",35,0,33
dataset/preprocessed/test-data/dependency_parsing/2,Such parsers can return a solution in a faster O ( n ) asymptotic runtime .,36,0,16
dataset/preprocessed/test-data/dependency_parsing/2,"Unlike FOG parsers , transition - based parsers allow the use of scoring functions with history - based features , so that attachment decisions can interact more freely ; the best performing parser at the time of this writing employ neural networks .",37,0,43
dataset/preprocessed/test-data/dependency_parsing/2,"Let h y ( m ) denote the parent of x min y ( using a special null symbol when m is the root of the tree ) , and h y ( m ) denotes the parent of x min the predicted tree y .",38,0,46
dataset/preprocessed/test-data/dependency_parsing/2,"Given two dependency parses of the same sentence , y and y , the Hamming cost is",39,0,17
dataset/preprocessed/test-data/dependency_parsing/2,"This cost underlies the standard dependency parsing evaluation scores ( unlabeled and labeled attachment scores , henceforth UAS and LAS ) .",40,0,22
dataset/preprocessed/test-data/dependency_parsing/2,"More generally , a cost function C maps pairs of parses for the same sentence to non-negative values interpreted as the cost of mistaking one for the other , and a firstorder cost function ( FOC ) is one that decomposes by attachments , like the Hamming cost .",41,0,49
dataset/preprocessed/test-data/dependency_parsing/2,"Given a cost function C and a probabilistic model that defines p ( y | x ) , minimum Bayes risk ( MBR ) decoding is defined b ?",42,0,29
dataset/preprocessed/test-data/dependency_parsing/2,"Under the Hamming cost , MBR parsing equates algorithmically to FOG parsing with s ( h , m , x ) = p ( ( h , m ) ?",43,0,30
dataset/preprocessed/test-data/dependency_parsing/2,"Y | x ) , the posterior marginal of the attachment under p.",44,0,13
dataset/preprocessed/test-data/dependency_parsing/2,This is shown by linearity of expectation ; see also .,45,0,11
dataset/preprocessed/test-data/dependency_parsing/2,"Apart from MBR decoding , cost functions are also used for discriminative training of a parser .",46,0,17
dataset/preprocessed/test-data/dependency_parsing/2,"For example , suppose we seek to estimate the parameters ?",47,0,11
dataset/preprocessed/test-data/dependency_parsing/2,of scoring function S ? .,48,0,6
dataset/preprocessed/test-data/dependency_parsing/2,One approach is to minimize the structured hinge loss of a training dataset D with respect to ?:,49,0,18
dataset/preprocessed/test-data/dependency_parsing/4,Stack - Pointer Networks for Dependency Parsing,2,1,7
dataset/preprocessed/test-data/dependency_parsing/4,We introduce a novel architecture for dependency parsing : stack - pointer networks ( STACKPTR ) .,4,0,17
dataset/preprocessed/test-data/dependency_parsing/4,"Combining pointer networks ( Vinyals et al. , 2015 ) with an internal stack , the proposed model first reads and encodes the whole sentence , then builds the dependency tree top - down ( from root - to - leaf ) in a depth - first fashion .",5,0,49
dataset/preprocessed/test-data/dependency_parsing/4,The stack tracks the status of the depthfirst search and the pointer networks select one child for the word at the top of the stack at each step .,6,0,29
dataset/preprocessed/test-data/dependency_parsing/4,"The STACKPTR parser benefits from the information of the whole sentence and all previously derived subtree structures , and removes the leftto - right restriction in classical transitionbased parsers .",7,0,30
dataset/preprocessed/test-data/dependency_parsing/4,"Yet , the number of steps for building any ( including non-projective ) parse tree is linear in the length of the sentence just as other transition - based parsers , yielding an efficient decoding algorithm with O ( n 2 ) time complexity .",8,0,45
dataset/preprocessed/test-data/dependency_parsing/4,"We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas , and achieve state - of - theart performance on 21 of them .",9,0,29
dataset/preprocessed/test-data/dependency_parsing/4,"Dependency parsing , which predicts the existence and type of linguistic dependency relations between words , is a first step towards deep language understanding .",11,0,25
dataset/preprocessed/test-data/dependency_parsing/4,"Its importance is widely recognized in the natural language processing ( NLP ) community , with it benefiting a wide range of NLP applications , such as coreference resolution Work done while at Carnegie Mellon University .",12,0,37
dataset/preprocessed/test-data/dependency_parsing/4,"2016 ) , sentiment analysis , machine translation , information extraction , word sense dis ambiguation , and low - resource languages processing .",13,0,24
dataset/preprocessed/test-data/dependency_parsing/4,"There are two dominant approaches to dependency parsing : local and greedy transitionbased algorithms , and the globally optimized graph - based algorithms .",14,0,24
dataset/preprocessed/test-data/dependency_parsing/4,Transition - based dependency parsers read words sequentially ( commonly from left - to - right ) and build dependency trees incrementally by making series of multiple choice decisions .,15,0,30
dataset/preprocessed/test-data/dependency_parsing/4,The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence .,16,0,29
dataset/preprocessed/test-data/dependency_parsing/4,"The challenge , however , is that the decision made at each step is based on local information , leading to error propagation and worse performance compared to graph - based parsers on root and long dependencies .",17,0,38
dataset/preprocessed/test-data/dependency_parsing/4,Previous studies have explored solutions to address this challenge .,18,0,10
dataset/preprocessed/test-data/dependency_parsing/4,Stack LSTMs are capable of learning representations of the parser state thatare sensitive to the complete contents of the parser 's state .,19,0,23
dataset/preprocessed/test-data/dependency_parsing/4,proposed a globally normalized transition model to replace the locally normalized classifier .,20,0,13
dataset/preprocessed/test-data/dependency_parsing/4,"However , the parsing accuracy is still behind state - of - the - art graph - based parsers .",21,0,20
dataset/preprocessed/test-data/dependency_parsing/4,"Graph - based dependency parsers , on the other hand , learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring tree .",22,0,36
dataset/preprocessed/test-data/dependency_parsing/4,"Incorporating this global search algorithm with distributed representations learned from neural networks , neural graph - based parsers have achieved the state - of - the - art accuracies on a number of treebanks in different languages .",23,0,38
dataset/preprocessed/test-data/dependency_parsing/4,"Nevertheless , these models , while accurate , are usually slow ( e.g. decoding is O ( n 3 ) time complexity for first - order models and higher polynomials for higherorder models ) .",24,0,35
dataset/preprocessed/test-data/dependency_parsing/4,"In this paper , we propose a novel neural network architecture for dependency parsing , stackpointer networks ( STACKPTR ) .",25,0,21
dataset/preprocessed/test-data/dependency_parsing/4,"STACKPTR is a transition - based architecture , with the corresponding asymptotic efficiency , but still maintains a global view of the sentence that proves essential for achieving competitive accuracy .",26,0,31
dataset/preprocessed/test-data/dependency_parsing/4,"Our STACKPTR parser has a pointer network as its backbone , and is equipped with an internal stack to maintain the order of head words in tree structures .",27,0,29
dataset/preprocessed/test-data/dependency_parsing/4,"The STACKPTR parser performs parsing in an incremental , topdown , depth - first fashion ; at each step , it generates an arc by assigning a child for the headword at the top of the internal stack .",28,0,39
dataset/preprocessed/test-data/dependency_parsing/4,"This architecture makes it possible to capture information from the whole sentence and all the previously derived subtrees , while maintaining a number of parsing steps linear in the sentence length .",29,0,32
dataset/preprocessed/test-data/dependency_parsing/4,"We evaluate our parser on 29 treebanks across 20 languages and different dependency annotation schemas , and achieve state - of - the - art performance on 21 of them .",30,0,31
dataset/preprocessed/test-data/dependency_parsing/4,The contributions of this work are summarized as follows :,31,0,10
dataset/preprocessed/test-data/dependency_parsing/4,"( i ) We propose a neural network architecture for dependency parsing that is simple , effective , and efficient .",32,0,21
dataset/preprocessed/test-data/dependency_parsing/4,( ii ) Empirical evaluations on benchmark datasets over 20 languages show that our method achieves state - of - the - art performance on 21 different treebanks 1 . ( iii ) Comprehensive error analysis is conducted to compare the proposed method to a strong graph - based baseline using biaffine attention .,33,0,54
dataset/preprocessed/test-data/dependency_parsing/4,"We first briefly describe the task of dependency parsing , setup the notation , and review Pointer Networks .",35,0,19
dataset/preprocessed/test-data/dependency_parsing/4,Dependency Parsing and Notations,36,0,4
dataset/preprocessed/test-data/dependency_parsing/4,Dependency trees represent syntactic relationships between words in the sentences through labeled directed edges between head words and their dependents .,37,0,21
dataset/preprocessed/test-data/dependency_parsing/4,"shows a dependency tree for the sentence , "" But there were no buyers "" .",38,0,16
dataset/preprocessed/test-data/dependency_parsing/4,"In this paper , we will use the following notation :",39,0,11
dataset/preprocessed/test-data/dependency_parsing/4,"Input : x = {w 1 , . . . , w n } represents a generic sentence , where w i is the ith word .",40,0,27
dataset/preprocessed/test-data/dependency_parsing/4,"Output : y = {p 1 , p 2 , , pk } represents a generic ( possibly non-projective ) dependency tree , where each path pi = $ , w i , 1 , w i , 2 , , w i , l i is a sequence of words from the root to a leaf .",41,0,58
dataset/preprocessed/test-data/dependency_parsing/4,""" $ "" is an universal virtual root that is added to each tree .",42,0,15
dataset/preprocessed/test-data/dependency_parsing/4,Stack : ?,43,0,3
dataset/preprocessed/test-data/dependency_parsing/4,"denotes a stack configuration , which is a sequence of words .",44,0,12
dataset/preprocessed/test-data/dependency_parsing/4,We use ?|w to represent a stack configuration that pushes word w into the stack ?.,45,0,16
dataset/preprocessed/test-data/dependency_parsing/4,Children : ch ( w i ) denotes the list of all the children ( modifiers ) of word w i .,46,0,22
dataset/preprocessed/test-data/dependency_parsing/4,Pointer Networks ( PTR - NET ) area variety of neural network capable of learning the conditional probability of an output sequence with elements thatare discrete tokens corresponding to positions in an input sequence .,48,0,35
dataset/preprocessed/test-data/dependency_parsing/4,This model can not be trivially expressed by standard sequence - to - sequence networks due to the variable number of input positions in each sentence .,49,0,27
dataset/preprocessed/test-data/entity_linking/7,Semi-supervised Word Sense Disambiguation with Neural Models,2,1,7
dataset/preprocessed/test-data/entity_linking/7,Determining the intended sense of words in text - word sense disambiguation ( WSD ) - is a longstanding problem in natural language processing .,4,1,25
dataset/preprocessed/test-data/entity_linking/7,"Recently , researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms .",5,1,23
dataset/preprocessed/test-data/entity_linking/7,"However , a simple average or concatenation of word vectors for each word in a text loses the sequential and syntactic information of the text .",6,0,26
dataset/preprocessed/test-data/entity_linking/7,"In this paper , we study WSD with a sequence learning neural net , LSTM , to better capture the sequential and syntactic patterns of the text .",7,0,28
dataset/preprocessed/test-data/entity_linking/7,"To alleviate the lack of training data in all - words WSD , we employ the same LSTM in a semi-supervised label propagation classifier .",8,0,25
dataset/preprocessed/test-data/entity_linking/7,"We demonstrate state - of - the - art results , especially on verbs .",9,0,15
dataset/preprocessed/test-data/entity_linking/7,This work is licensed under a Creative Commons Attribution 4.0 International License .,10,0,13
dataset/preprocessed/test-data/entity_linking/7,License details :,11,0,3
dataset/preprocessed/test-data/entity_linking/7,Word sense disambiguation ( WSD ) is a long - standing problem in natural language processing ( NLP ) with broad applications .,13,0,23
dataset/preprocessed/test-data/entity_linking/7,"Supervised , unsupervised , and knowledge - based approaches have been studied for WSD .",14,0,15
dataset/preprocessed/test-data/entity_linking/7,"However , for all - words WSD , where all words in a corpus need to be annotated with word senses , it has proven extremely challenging to beat the strong baseline , which always assigns the most frequent sense of a word without considering the context .",15,0,48
dataset/preprocessed/test-data/entity_linking/7,"Given the good performance of published supervised WSD systems when provided with significant training data on specific words , it appears lack of sufficient labeled training data for large vocabularies is the central problem .",16,0,35
dataset/preprocessed/test-data/entity_linking/7,One way to leverage unlabeled data is to train a neural network language model ( NNLM ) on the data .,17,0,21
dataset/preprocessed/test-data/entity_linking/7,Word embeddings extracted from such a NNLM ( often Word2 Vec ) can be incorporated as features into a WSD algorithm .,18,0,22
dataset/preprocessed/test-data/entity_linking/7,show that this can substantially improve WSD performance and indeed that competitive performance can be attained using word embeddings alone .,19,0,21
dataset/preprocessed/test-data/entity_linking/7,"In this paper , we describe two novel WSD algorithms .",20,0,11
dataset/preprocessed/test-data/entity_linking/7,The first is based on a Long Short Term Memory ( LSTM ) ) .,21,0,15
dataset/preprocessed/test-data/entity_linking/7,"Since this model is able to take into account word order when classifying , it performs significantly better than an algorithm based on a continuous bag of words model ( Word2 vec ) , especially on verbs .",22,0,38
dataset/preprocessed/test-data/entity_linking/7,We then present a semi-supervised algorithm which uses label propagation to label unlabeled sentences based on their similarity to labeled ones .,23,0,22
dataset/preprocessed/test-data/entity_linking/7,"This allows us to better estimate the distribution of word senses , obtaining more accurate decision boundaries and higher classification accuracy .",24,0,22
dataset/preprocessed/test-data/entity_linking/7,The best performance was achieved by using an LSTM language model with label propagation .,25,0,15
dataset/preprocessed/test-data/entity_linking/7,Our algorithm achieves state - of - art performance on many SemEval all - words tasks .,26,0,17
dataset/preprocessed/test-data/entity_linking/7,It also outperforms the most -frequent - sense and Word2 Vec baselines by 10 % ( see Section 5.2 for details ) .,27,0,23
dataset/preprocessed/test-data/entity_linking/7,"The development of large lexical resources , such as WordNet and BabelNet , has enabled knowledge - based algorithms which show promising results on allwords prediction tasks .",29,0,28
dataset/preprocessed/test-data/entity_linking/7,"WSD algorithms based on supervised learning are generally believed to perform better than knowledgebased WSD algorithms , but they need large training sets to perform well .",30,0,27
dataset/preprocessed/test-data/entity_linking/7,Acquiring large training sets is costly .,31,0,7
dataset/preprocessed/test-data/entity_linking/7,"In this paper , we show that a supervised WSD algorithm can perform well with ?",32,0,16
dataset/preprocessed/test-data/entity_linking/7,20 training examples per sense .,33,0,6
dataset/preprocessed/test-data/entity_linking/7,"In the past few years , much progress has been made on using neural networks to learn word embeddings , to construct language models , perform sentiment analysis , machine translation and many other NLP applications .",34,0,37
dataset/preprocessed/test-data/entity_linking/7,A number of different ways have been studied for using word embeddings in WSD .,35,0,15
dataset/preprocessed/test-data/entity_linking/7,There are some common elements :,36,0,6
dataset/preprocessed/test-data/entity_linking/7,Context embeddings .,37,0,3
dataset/preprocessed/test-data/entity_linking/7,"Given a window of text w n?k , ... , w n , ... , w n+k surrounding a focus word w n ( whose label is either known in the case of example sentences or to be determined in the case of classification ) , an embedding for the context is computed as a concatenation or weighted sum of the embeddings of the words w i , i = n.",38,0,71
dataset/preprocessed/test-data/entity_linking/7,Context embeddings of various kinds are used in both and .,39,0,11
dataset/preprocessed/test-data/entity_linking/7,Sense embeddings .,40,0,3
dataset/preprocessed/test-data/entity_linking/7,Embeddings are computed for each word sense in the word sense inventory ( e.g. WordNet ) .,41,0,17
dataset/preprocessed/test-data/entity_linking/7,"In , equations are derived relating embeddings for word senses with embeddings for undisambiguated words .",42,0,16
dataset/preprocessed/test-data/entity_linking/7,The equations are solved to compute the sense embeddings .,43,0,10
dataset/preprocessed/test-data/entity_linking/7,"In , sense embeddings are computed first as weighted sums of the embeddings of words in the WordNet gloss for each sense .",44,0,23
dataset/preprocessed/test-data/entity_linking/7,"These are used in an initial bootstrapping WSD phase , and then refined by a neural network which is trained on this bootstrap data . Embeddings as SVM features .",45,0,30
dataset/preprocessed/test-data/entity_linking/7,"Context embeddings , or features computed by combining context embeddings with sense embeddings , can be used as additional features in a supervised WSD system e.g. the SVMbased IMS .",46,0,30
dataset/preprocessed/test-data/entity_linking/7,Indeed found that using embeddings as the only features in IMS gave competitive WSD performance .,47,0,16
dataset/preprocessed/test-data/entity_linking/7,Nearest neighbor classifier .,48,0,4
dataset/preprocessed/test-data/entity_linking/7,"Another way to perform classification is to find the word sense whose sense embedding is closest , as measured by cosine similarity , to the embedding of the classification context .",49,0,31
dataset/preprocessed/test-data/entity_linking/3,Does BERT Make Any Sense ? Interpretable Word Sense Disambiguation with Contextualized Embeddings,2,1,13
dataset/preprocessed/test-data/entity_linking/3,"Contextualized word embeddings ( CWE ) such as provided by ELMo ( Peters et al. , 2018 ) , Flair NLP ( Akbik et al. , 2018 ) , or BERT ( Devlin et al. , 2019 area major recent innovation in NLP .",4,0,44
dataset/preprocessed/test-data/entity_linking/3,CWEs provide semantic vector representations of words depending on their respective context .,5,0,13
dataset/preprocessed/test-data/entity_linking/3,"Their advantage over static word embeddings has been shown fora number of tasks , such as text classification , sequence tagging , or machine translation .",6,0,26
dataset/preprocessed/test-data/entity_linking/3,"Since vectors of the same word type can vary depending on the respective context , they implicitly provide a model for word sense disambiguation ( WSD ) .",7,1,28
dataset/preprocessed/test-data/entity_linking/3,We introduce a simple but effective approach to WSD using a nearest neighbor classification on CWEs .,8,1,17
dataset/preprocessed/test-data/entity_linking/3,We compare the performance of different CWE models for the task and can report improvements above the current state of the art for two standard WSD benchmark datasets .,9,0,29
dataset/preprocessed/test-data/entity_linking/3,"We further show that the pre-trained BERT model is able to place polysemic words into distinct ' sense ' regions of the embedding space , while ELMo and Flair NLP do not seem to possess this ability .",10,0,38
dataset/preprocessed/test-data/entity_linking/3,Synonymy and Polysemy of Word Representations,11,0,6
dataset/preprocessed/test-data/entity_linking/3,"Lexical semantics is characterized by a high degree of polysemy , i.e. the meaning of a word changes depending on the context in which it is currently used .",12,0,29
dataset/preprocessed/test-data/entity_linking/3,Word Sense Disambiguation ( WSD ) is the task to identify the correct sense of the usage of a word from a ( usually ) fixed inventory of sense identifiers .,13,0,31
dataset/preprocessed/test-data/entity_linking/3,"For the English language , WordNet is the most commonly used sense inventory providing more than 200K word - sense pairs .",14,0,22
dataset/preprocessed/test-data/entity_linking/3,"To train and evaluate WSD systems , a number of shared task datasets have been published in the SemEval workshop series .",15,0,22
dataset/preprocessed/test-data/entity_linking/3,"In the lexical sample task , a training set and a test set is provided .",16,0,16
dataset/preprocessed/test-data/entity_linking/3,The relatively large data contains one sense - annotated word per training / test instance .,17,0,16
dataset/preprocessed/test-data/entity_linking/3,The all - words task only provides a small number of documents as test data where each ambiguous word is annotated with its sense .,18,0,25
dataset/preprocessed/test-data/entity_linking/3,"To facilitate the comparison of WSD systems , some efforts have been made to provide a comprehensive evaluation framework , and to unify all publicly available datasets for the English language .",19,0,32
dataset/preprocessed/test-data/entity_linking/3,"WSD systems can be distinguished into three types - knowledge - based , supervised , and semisupervised approaches .",20,0,19
dataset/preprocessed/test-data/entity_linking/3,"Knowledge - based systems utilize language resources such as dictionaries , thesauri and knowledge graphs to infer senses .",21,0,19
dataset/preprocessed/test-data/entity_linking/3,Supervised approaches train a machine classifier to predict a sense given the target word and its context based on an annotated training data set .,22,0,25
dataset/preprocessed/test-data/entity_linking/3,Semisupervised approaches extend manually created training sets by large corpora of unlabeled data to improve WSD performance .,23,0,18
dataset/preprocessed/test-data/entity_linking/3,All approaches rely on someway of context representation to predict the correct sense .,24,0,14
dataset/preprocessed/test-data/entity_linking/3,"Context is typically modeled via dictionary resources linked with senses , or as some feature vector obtained from a machine learning model .",25,0,23
dataset/preprocessed/test-data/entity_linking/3,A fundamental assumption in structuralist linguistics is the distinction between signifier and signified as introduced by Ferdinand de Saussure in the early 20 th century .,26,0,26
dataset/preprocessed/test-data/entity_linking/3,"Computational linguistic approaches , when using character strings as the only representatives for word meaning , implicitly assume identity between signifier and signified .",27,0,24
dataset/preprocessed/test-data/entity_linking/3,Different word senses are simply collapsed into the same string representation .,28,0,12
dataset/preprocessed/test-data/entity_linking/3,"In this respect , word counting and dictionary - based approaches to analyze natural language texts have been criticized as pre-Saussurean .",29,0,22
dataset/preprocessed/test-data/entity_linking/3,"In contrast , the distributional hypothesis not only states that meaning is dependent on context .",30,0,16
dataset/preprocessed/test-data/entity_linking/3,It also states that words occurring in the same contexts tend to have a similar meaning .,31,0,17
dataset/preprocessed/test-data/entity_linking/3,"Hence , a more elegant way of representing meaning has been introduced by using the contexts of a word as an intermediate semantic representation that mediates between signifier and signified .",32,0,31
dataset/preprocessed/test-data/entity_linking/3,"For this , explicit vector representations , such as TF - IDF , or latent vector representations , with reduced dimensionality , have been widely used .",33,0,27
dataset/preprocessed/test-data/entity_linking/3,Latent vector representations of words are commonly called word embeddings .,34,0,11
dataset/preprocessed/test-data/entity_linking/3,"They are fixed length vector representations , which are supposed to encode semantic properties .",35,0,15
dataset/preprocessed/test-data/entity_linking/3,"The seminal neural word embedding model , for instance , can be trained efficiently on billions of sentence contexts to obtain semantic vectors , one for each word type in the vocabulary .",36,0,33
dataset/preprocessed/test-data/entity_linking/3,It allows synonymous terms to have similar vector representations that can be used for modeling virtually any downstream NLP task .,37,0,21
dataset/preprocessed/test-data/entity_linking/3,"Still , a polysemic term is represented by one single vector only , which represents all of its different senses in a collapsed fashion .",38,0,25
dataset/preprocessed/test-data/entity_linking/3,"To capture polysemy as well , the idea of word embeddings has been extended to encode word sense embeddings .",39,0,20
dataset/preprocessed/test-data/entity_linking/3,first introduced a neural model to learn multiple embeddings for one word depending on different senses .,40,0,17
dataset/preprocessed/test-data/entity_linking/3,"The number of senses can be defined by a given parameter , or derived automatically in a non-paramentric version of the model .",41,0,23
dataset/preprocessed/test-data/entity_linking/3,"However , employing sense embeddings in any downstream NLP task requires a reliable WSD system in an earlier stage to decide how to choose the appropriate embedding from the sense inventory .",42,0,32
dataset/preprocessed/test-data/entity_linking/3,Recent efforts to capture polysemy for word embeddings give upon the idea of a fixed word sense inventory .,43,0,19
dataset/preprocessed/test-data/entity_linking/3,"Contextualized word embeddings ( CWE ) do not only create one vector representation for each type in the vocabulary , they also they produce distinct vectors for each token in a given context .",44,0,34
dataset/preprocessed/test-data/entity_linking/3,The contextualized vector representation is supposed to represent word meaning and context information .,45,0,14
dataset/preprocessed/test-data/entity_linking/3,This enables downstream tasks to actually distinguish the two levels of the signifier and the signified allowing for more realistic modeling of natural language .,46,0,25
dataset/preprocessed/test-data/entity_linking/3,The advantage of such contextually embedded token representations compared to static word embeddings has been shown fora number of tasks such as text classification and sequence tagging .,47,0,28
dataset/preprocessed/test-data/entity_linking/3,We show that CWEs can be utilized directly to approach the WSD task due to their nature of providing distinct vector representations for the same token depending on its context .,49,0,31
dataset/preprocessed/test-data/entity_linking/16,One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data,2,1,13
dataset/preprocessed/test-data/entity_linking/16,"Due to recent technical and scientific advances , we have a wealth of information hidden in unstructured text data such as offline / online narratives , research articles , and clinical reports .",4,0,33
dataset/preprocessed/test-data/entity_linking/16,"To mine these data properly , attributable to their innate ambiguity , a Word Sense Disambiguation ( WSD ) algorithm can avoid numbers of difficulties in Natural Language Processing ( NLP ) pipeline .",5,1,34
dataset/preprocessed/test-data/entity_linking/16,"However , considering a large number of ambiguous words in one language or technical domain , we may encounter limiting constraints for proper deployment of existing WSD models .",6,1,29
dataset/preprocessed/test-data/entity_linking/16,This paper attempts to address the problem of oneclassifier - per - one - word WSD algorithms by proposing a single Bidirectional Long Short - Term Memory ( BLSTM ) network which by considering senses and context sequences works on all ambiguous words collectively .,7,0,45
dataset/preprocessed/test-data/entity_linking/16,"Evaluated on SensEval - 3 benchmark , we show the result of our model is comparable with top - performing WSD algorithms .",8,0,23
dataset/preprocessed/test-data/entity_linking/16,We also discuss how applying additional modifications alleviates the model fault and the need for more training data .,9,0,19
dataset/preprocessed/test-data/entity_linking/16,"Word Sense Disambiguation ( WSD ) is an important problem in Natural Language Processing ( NLP ) , both in its own right and as a steppingstone to other advanced tasks in the NLP pipeline , applications such as machine translation and question answering .",11,0,45
dataset/preprocessed/test-data/entity_linking/16,"WSD specifically deals with identifying the correct sense of a word , among a set of given candidate senses for that word , when it is presented in a brief narrative ( surrounding text ) which is generally referred to as context .",12,0,43
dataset/preprocessed/test-data/entity_linking/16,Consider the ambiguous word ' cold '.,13,0,7
dataset/preprocessed/test-data/entity_linking/16,"In the sentence "" He started to give me a cold shoulder after that experiment "" , the possible senses for cold can be cold temperature ( S1 ) , a cold sensation ( S2 ) , common cold ( S3 ) , or a negative emotional reaction ( S4 ) .",14,0,52
dataset/preprocessed/test-data/entity_linking/16,"Therefore , the ambiguous word cold is specified along with the sense set { S1 , S2 , S3 , S4 } and our goal is to identify the correct sense S4 ( as the closest meaning ) for this specific occurrence of cold after considering - the semantic and the syntactic information of - its context .",15,0,58
dataset/preprocessed/test-data/entity_linking/16,"In this effort , we develop our supervised WSD model that leverages a Bidirectional Long Short - Term Memory ( BLSTM ) network .",16,0,24
dataset/preprocessed/test-data/entity_linking/16,"This network works with neural sense vectors ( i.e. sense embeddings ) , which are learned during model training , and employs neural word vectors ( i.e. word embeddings ) , which are learned through an unsupervised deep learning approach called GloVe ( Global Vectors for word representation ) for the context words .",17,0,54
dataset/preprocessed/test-data/entity_linking/16,"By evaluating our onemodel - fits - all WSD network over the public gold standard dataset of SensEval - 3 , we demonstrate that the accuracy of our model in terms of F- measure is comparable with the state - of - the - art WSD algorithms '.",18,0,48
dataset/preprocessed/test-data/entity_linking/16,We outline the organization of the rest of the paper as follows .,19,0,13
dataset/preprocessed/test-data/entity_linking/16,"In Section 2 , we briefly explore earlier efforts in WSD and discuss recent approaches that incorporate deep neural networks and word embeddings .",20,0,24
dataset/preprocessed/test-data/entity_linking/16,Our main model that employs BLSTM with the sense and word embeddings is detailed in Section 3 .,21,0,18
dataset/preprocessed/test-data/entity_linking/16,We then present our experiments and results in Section 4 supported by a discussion on how to avoid some drawbacks of the current model in order to achieve higher accuracies and demand less number of training data which is desirable .,22,0,41
dataset/preprocessed/test-data/entity_linking/16,"Finally , in Section 5 , we conclude with some future research directions for the construction of sense embeddings as well as applications of such model in other domains such as biomedicine .",23,0,33
dataset/preprocessed/test-data/entity_linking/16,Background and Related Work,24,0,4
dataset/preprocessed/test-data/entity_linking/16,"Generally , there are three categories of WSD algorithms : supervised , knowledgebased , and unsupervised .",25,0,17
dataset/preprocessed/test-data/entity_linking/16,Supervised algorithms consist of automatically inducing classification models or rules from labeled examples .,26,0,14
dataset/preprocessed/test-data/entity_linking/16,Knowledge - based WSD approaches are dependent on manually created lexical resources such as WordNet and the Unified Medical Language System 4 ( UMLS ) .,27,0,26
dataset/preprocessed/test-data/entity_linking/16,Unsupervised algorithms may employ topic modeling - based methods to disambiguate when the senses are known ahead of time .,28,0,20
dataset/preprocessed/test-data/entity_linking/16,For a thorough survey of WSD algorithms refer to Navigli .,29,0,11
dataset/preprocessed/test-data/entity_linking/16,Neural Embeddings for WSD,30,0,4
dataset/preprocessed/test-data/entity_linking/16,"In the past few years , there has been an increasing interest in training neural word embeddings from large unlabeled corpora using neural networks .",31,0,25
dataset/preprocessed/test-data/entity_linking/16,"Word embeddings are typically represented as a dense real - valued low dimensional matrix W ( i.e. a lookup table ) of size d v , where dis the predefined embedding dimension and v is the vocabulary size .",32,0,39
dataset/preprocessed/test-data/entity_linking/16,Each column of the matrix is an embedding vector associated with a word in the vocabulary and each row of the matrix represents a latent feature .,33,0,27
dataset/preprocessed/test-data/entity_linking/16,These vectors can subsequently be used to initialize the input layer of a neural network or some other NLP model .,34,0,21
dataset/preprocessed/test-data/entity_linking/16,GloVe is one of the existing unsupervised learning algorithms for obtaining these vector representations of the words in which training is performed on aggregated global word - word co-occurrence statistics from a corpus .,35,0,34
dataset/preprocessed/test-data/entity_linking/16,"Besides word embeddings , recently , computation of sense embeddings has gained the attention of numerous studies as well .",36,0,20
dataset/preprocessed/test-data/entity_linking/16,"For example ,",37,0,3
dataset/preprocessed/test-data/entity_linking/16,Chen et al .,38,0,4
dataset/preprocessed/test-data/entity_linking/16,adapted neural word embeddings to compute different sense embeddings ( of the same word ) and showed competitive performance on the SemEval - 2007 data .,39,0,26
dataset/preprocessed/test-data/entity_linking/16,"Long Short - Term Memory ( LSTM ) , introduced by Hochreiter and Schmidhuber ( 1997 ) , is a gated recurrent neural network ( RNN ) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs .",41,0,44
dataset/preprocessed/test-data/entity_linking/16,"Unlike feedforward neural networks , RNNs have cyclic connections making them powerful for modeling sequences .",42,0,16
dataset/preprocessed/test-data/entity_linking/16,A Bidirectional LSTM is made up of two reversed unidirectional LSTMs .,43,0,12
dataset/preprocessed/test-data/entity_linking/16,"For WSD this means we are able to encode information of both preceding and succeeding words within context of an ambiguous word , which is necessary to correctly classify its sense .",44,0,32
dataset/preprocessed/test-data/entity_linking/16,One Single BLSTM network for WSD,45,0,6
dataset/preprocessed/test-data/entity_linking/16,"Given a document and the position of a target word , our model computes a probability distribution over possible senses related to that word .",46,0,25
dataset/preprocessed/test-data/entity_linking/16,"The architecture of our model , depicted in , consist of 6 layers which area sigmoid layer ( at the top ) , a fully - connected layer , a concatenation layer , a BLSTM layer , a cosine layer , and a sense and word embeddings layer ( on the bottom ) .",47,0,54
dataset/preprocessed/test-data/entity_linking/16,"In contrast to other supervised neural WSD networks in which generally a softmax layer - with across entropy or hinge loss - is parameterized by the context words and selects the corresponding weight matrix and bias vector for each ambiguous word 's senses , our network shares parameters overall words ' senses .",48,0,53
dataset/preprocessed/test-data/entity_linking/16,"While remaining computationally efficient , this structure aims to encode statistical information across different words enabling the network to select the true sense ( or even a proper word ) in a blank space within a context .",49,0,38
dataset/preprocessed/test-data/entity_linking/10,Deep contextualized word representations,2,1,4
dataset/preprocessed/test-data/entity_linking/10,"We introduce a new type of deep contextualized word representation that models both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",4,1,48
dataset/preprocessed/test-data/entity_linking/10,"Our word vectors are learned functions of the internal states of a deep bidirectional language model ( biLM ) , which is pretrained on a large text corpus .",5,0,29
dataset/preprocessed/test-data/entity_linking/10,"We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems , including question answering , textual entailment and sentiment analysis .",6,0,36
dataset/preprocessed/test-data/entity_linking/10,"We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial , allowing downstream models to mix different types of semi-supervision signals .",7,0,29
dataset/preprocessed/test-data/entity_linking/10,Pre-trained word representations are a key component in many neural language understanding models .,9,1,14
dataset/preprocessed/test-data/entity_linking/10,"However , learning high quality representations can be challenging .",10,0,10
dataset/preprocessed/test-data/entity_linking/10,"They should ideally model both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",11,0,40
dataset/preprocessed/test-data/entity_linking/10,"In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .",12,0,49
dataset/preprocessed/test-data/entity_linking/10,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,13,0,26
dataset/preprocessed/test-data/entity_linking/10,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,14,0,28
dataset/preprocessed/test-data/entity_linking/10,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .",15,0,16
dataset/preprocessed/test-data/entity_linking/10,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .",16,0,32
dataset/preprocessed/test-data/entity_linking/10,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .",17,0,33
dataset/preprocessed/test-data/entity_linking/10,Combining the internal states in this manner allows for very rich word representations .,18,0,14
dataset/preprocessed/test-data/entity_linking/10,"Using intrinsic evaluations , we show that the higher - level LSTM states capture context - dependent aspects of word meaning ( e.g. , they can be used without modification to perform well on supervised word sense disambiguation tasks ) while lowerlevel states model aspects of syntax ( e.g. , they can be used to do part - of - speech tagging ) .",19,0,64
dataset/preprocessed/test-data/entity_linking/10,"Simultaneously exposing all of these signals is highly beneficial , allowing the learned models select the types of semi-supervision that are most useful for each end task .",20,0,28
dataset/preprocessed/test-data/entity_linking/10,Extensive experiments demonstrate that ELMo representations work extremely well in practice .,21,0,12
dataset/preprocessed/test-data/entity_linking/10,"We first show that they can be easily added to existing models for six diverse and challenging language understanding problems , including textual entailment , question answering and sentiment analysis .",22,0,31
dataset/preprocessed/test-data/entity_linking/10,"The addition of ELMo representations alone significantly improves the state of the art in every case , including up to 20 % relative error reductions .",23,0,26
dataset/preprocessed/test-data/entity_linking/10,"For tasks where direct comparisons are possible , ELMo outperforms CoVe , which computes contextualized representations using a neural machine translation encoder .",24,0,23
dataset/preprocessed/test-data/entity_linking/10,"Finally , an analysis of both ELMo and CoVe reveals that deep representations outperform those derived from just the top layer of an LSTM .",25,0,25
dataset/preprocessed/test-data/entity_linking/10,"Our trained models and code are publicly available , and we expect that ELMo will provide similar gains for many other NLP problems .",26,0,24
dataset/preprocessed/test-data/entity_linking/10,"2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text , pretrained word vectors are a standard component of most state - of the - art NLP architectures , including for question answering , textual entailment and semantic role labeling .",28,0,51
dataset/preprocessed/test-data/entity_linking/10,"However , these approaches for learning word vectors only allow a single contextindependent representation for each word .",29,0,18
dataset/preprocessed/test-data/entity_linking/10,"Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information ( e.g. , or learning separate vectors for each word sense ( e.g. , .",30,0,34
dataset/preprocessed/test-data/entity_linking/10,"Our approach also benefits from subword units through the use of character convolutions , and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes .",31,0,32
dataset/preprocessed/test-data/entity_linking/10,Other recent work has also focused on learning context - dependent representations .,32,0,13
dataset/preprocessed/test-data/entity_linking/10,context2vec uses a bidirectional Long Short Term Memory ( LSTM ; to encode the context around a pivot word .,33,0,20
dataset/preprocessed/test-data/entity_linking/10,Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation ( MT ) system or an unsupervised language model .,34,0,37
dataset/preprocessed/test-data/entity_linking/10,"Both of these approaches benefit from large datasets , although the MT approach is limited by the size of parallel corpora .",35,0,22
dataset/preprocessed/test-data/entity_linking/10,"In this paper , we take full advantage of access to plentiful monolingual data , and train our biLM on a corpus with approximately 30 million sentences .",36,0,28
dataset/preprocessed/test-data/entity_linking/10,"We also generalize these approaches to deep contextual representations , which we show work well across abroad range of diverse NLP tasks .",37,0,23
dataset/preprocessed/test-data/entity_linking/10,Previous work has also shown that different layers of deep biRNNs encode different types of information .,39,0,17
dataset/preprocessed/test-data/entity_linking/10,"For example , introducing multi-task syntactic supervision ( e.g. , part - of - speech tags ) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing or CCG super tagging .",40,0,42
dataset/preprocessed/test-data/entity_linking/10,"In an RNN - based encoder - decoder machine translation system , showed that the representations learned at the first layer in a 2 layer LSTM encoder are better at predicting POS tags then second layer .",41,0,37
dataset/preprocessed/test-data/entity_linking/10,"Finally , the top layer of an LSTM for encoding word context has been shown to learn representations of word sense .",42,0,22
dataset/preprocessed/test-data/entity_linking/10,"We show that similar signals are also induced by the modified language model objective of our ELMo representations , and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision .",43,0,39
dataset/preprocessed/test-data/entity_linking/10,and pretrain encoder - decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision .,44,0,21
dataset/preprocessed/test-data/entity_linking/10,"In contrast , after pretraining the biLM with unlabeled data , we fix the weights and add additional taskspecific model capacity , allowing us to leverage large , rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model .",45,0,46
dataset/preprocessed/test-data/entity_linking/10,Dai and Le,46,0,3
dataset/preprocessed/test-data/entity_linking/10,Bidirectional language models,47,0,3
dataset/preprocessed/test-data/entity_linking/10,"Given a sequence of N tokens , ( t 1 , t 2 , ... , t N ) , a forward language model computes the probability of the sequence by modeling the probability of to - ken t k given the history ( t 1 , ... , t k 1 ) :",48,0,54
dataset/preprocessed/test-data/entity_linking/10,Recent state - of - the - art neural language models compute a context - independent token representation x LM k ( via token embeddings or a CNN over characters ) then pass it through L layers of forward LSTMs .,49,0,41
dataset/preprocessed/test-data/entity_linking/9,Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation,2,1,13
dataset/preprocessed/test-data/entity_linking/9,"Named Entity Disambiguation ( NED ) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",4,1,36
dataset/preprocessed/test-data/entity_linking/9,"In this paper , we propose a novel embedding method specifically designed for NED .",5,1,15
dataset/preprocessed/test-data/entity_linking/9,The proposed method jointly maps words and entities into the same continuous vector space .,6,0,15
dataset/preprocessed/test-data/entity_linking/9,We extend the skip - gram model by using two models .,7,0,12
dataset/preprocessed/test-data/entity_linking/9,"The KB graph model learns the relatedness of entities using the link structure of the KB , whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words .",8,0,50
dataset/preprocessed/test-data/entity_linking/9,"By combining contexts based on the proposed embedding with standard NED features , we achieved state - of - theart accuracy of 93.1 % on the standard CoNLL dataset and 85.2 % on the TAC 2010 dataset .",9,0,38
dataset/preprocessed/test-data/entity_linking/9,"Named Entity Disambiguation ( NED ) is the task of resolving ambiguous mentions of entities to their referent entities in a knowledge base ( KB ) ( e.g. , Wikipedia ) .",11,0,32
dataset/preprocessed/test-data/entity_linking/9,"NED has lately been extensively studied ) and used as a fundamental component in numerous tasks , such as information extraction , knowledge base population , and semantic search .",12,0,30
dataset/preprocessed/test-data/entity_linking/9,We use Wikipedia as KB in this paper .,13,0,9
dataset/preprocessed/test-data/entity_linking/9,The main difficulty in NED is ambiguity in the meaning of entity mentions .,14,0,14
dataset/preprocessed/test-data/entity_linking/9,"For example , the mention "" Washington "" in a document can refer to various entities , such as the state , or the capital of the US , the actor Denzel Washington , the first US president George Washington , and soon .",15,0,44
dataset/preprocessed/test-data/entity_linking/9,"In order to resolve these ambiguous mentions into references to the correct entities , early approaches focused on modeling textual context , such as the similarity between contextual words and encyclopedic descriptions of a candidate entity .",16,0,37
dataset/preprocessed/test-data/entity_linking/9,"Most state - of - theart methods use more sophisticated global approaches , where all mentions in a document are simultaneously disambiguated based on global coherence among disambiguation decisions .",17,0,30
dataset/preprocessed/test-data/entity_linking/9,Word embedding methods are also becoming increasingly popular .,18,0,9
dataset/preprocessed/test-data/entity_linking/9,"These involve learning continuous vector representations of words from large , unstructured text corpora .",19,0,15
dataset/preprocessed/test-data/entity_linking/9,The vectors are designed to capture the semantic similarity of words when similar words are placed near one another in a relatively low - dimensional vector space .,20,0,28
dataset/preprocessed/test-data/entity_linking/9,"In this paper , we propose a method to construct a novel embedding that jointly maps words and entities into the same continuous vector space .",21,0,26
dataset/preprocessed/test-data/entity_linking/9,"In this model , similar words and entities are placed close to one another in a vector space .",22,0,19
dataset/preprocessed/test-data/entity_linking/9,"Hence , we can measure the similarity between any pair of items ( i.e. , words , entities , and a word and an entity ) by simply computing their cosine similarity .",23,0,33
dataset/preprocessed/test-data/entity_linking/9,"This enables us to easily measure the contextual information for NED , such as the similarity between a context word and a candidate entity , and the relatedness of entities required to model coherence .",24,0,35
dataset/preprocessed/test-data/entity_linking/9,"Our model is based on the skip - gram model , a recently proposed embedding model that learns to predict each context word given the target word .",25,0,28
dataset/preprocessed/test-data/entity_linking/9,Our model consists of the following three models based on the skip - gram model :,26,0,16
dataset/preprocessed/test-data/entity_linking/9,"1 ) the conventional skip - gram model that learns to predict neighboring words given the target word in text corpora , 2 ) the KB graph model that learns to estimate neighboring entities given the target entity in the link graph of the KB , and 3 ) the anchor context model that learns to predict neighboring words given the target entity using anchors and their context words in the KB .",27,0,73
dataset/preprocessed/test-data/entity_linking/9,"By jointly optimizing these models , our method simultaneously learns the embedding of words and entities .",28,0,17
dataset/preprocessed/test-data/entity_linking/9,"Based on our proposed embedding , we also develop a straightforward NED method that computes two contexts using the proposed embedding : textual context similarity , and coherence .",29,0,29
dataset/preprocessed/test-data/entity_linking/9,Textual context similarity is measured according to vector similarity between an entity and words in a document .,30,0,18
dataset/preprocessed/test-data/entity_linking/9,Coherence is measured based on the relatedness between the target entity and other entities in a document .,31,0,18
dataset/preprocessed/test-data/entity_linking/9,"Our NED method combines these contexts with several standard features ( e.g. , prior probability ) using supervised machine learning .",32,0,21
dataset/preprocessed/test-data/entity_linking/9,We tested the proposed method using two standard NED datasets : the CoNLL dataset and the TAC 2010 dataset .,33,0,20
dataset/preprocessed/test-data/entity_linking/9,Experimental results revealed that our method outperforms state - of - the - art methods on both datasets by significant margins .,34,0,22
dataset/preprocessed/test-data/entity_linking/9,"Moreover , we conducted experiments to separately assess the quality of the vector representation of entities using an entity relatedness dataset , and discovered that our method successfully learns the quality representations of entities .",35,0,35
dataset/preprocessed/test-data/entity_linking/9,Joint Embedding of Words and Entities,36,0,6
dataset/preprocessed/test-data/entity_linking/9,"In this section , we first describe the conventional skip - gram model for learning word embedding .",37,0,18
dataset/preprocessed/test-data/entity_linking/9,We then explain our method to construct an embedding that jointly maps words and entities into the same continuous d-dimensional vector space .,38,0,23
dataset/preprocessed/test-data/entity_linking/9,We extend the skip - gram model by adding the KB graph model and the anchor context model .,39,0,19
dataset/preprocessed/test-data/entity_linking/9,Skip- gram Model for Word Similarity,40,0,6
dataset/preprocessed/test-data/entity_linking/9,The training objective of the skip - gram model is to find word representations that are useful to predict context words given the target word .,41,0,26
dataset/preprocessed/test-data/entity_linking/9,"Formally , given a sequence of T words w 1 , w 2 , ... , w T , the model aims to maximize the following objective function :",42,0,29
dataset/preprocessed/test-data/entity_linking/9,"where c is the size of the context window , wt denotes the target word , and w t +j is its context word .",43,0,25
dataset/preprocessed/test-data/entity_linking/9,The conditional probability P ( w t+j |w t ) is computed using the following softmax function :,44,0,18
dataset/preprocessed/test-data/entity_linking/9,"Wis a set containing all words in the vocabulary , and V w ?",46,0,14
dataset/preprocessed/test-data/entity_linking/9,Rd and U w ?,47,0,5
dataset/preprocessed/test-data/entity_linking/9,"Rd denote the vectors of word win matrices V and U , respectively .",48,0,14
dataset/preprocessed/test-data/entity_linking/9,"The skip - gram model is trained to optimize the above function L w , and V are used as the resulting vector representations of words .",49,0,27
dataset/preprocessed/test-data/entity_linking/1,Pre-training of Deep Contextualized Embeddings of Words and Entities for Named Entity Disambiguation,2,1,13
dataset/preprocessed/test-data/entity_linking/1,"Deep contextualized embeddings trained using unsupervised language modeling ( e.g. , ELMo and BERT ) are successful in a wide range of NLP tasks .",4,0,25
dataset/preprocessed/test-data/entity_linking/1,"In this paper , we propose a new contextualized embedding model of words and entities for named entity disambiguation ( NED ) .",5,1,23
dataset/preprocessed/test-data/entity_linking/1,Our model is based on the bidirectional transformer encoder and produces contextualized embeddings for words and entities in the input text .,6,0,22
dataset/preprocessed/test-data/entity_linking/1,The embeddings are trained using a new masked entity prediction task that aims to train the model by predicting randomly masked entities in entityannotated texts .,7,0,26
dataset/preprocessed/test-data/entity_linking/1,We trained the model using entity - annotated texts obtained from Wikipedia .,8,0,13
dataset/preprocessed/test-data/entity_linking/1,We evaluated our model by addressing NED using a simple NED model based on the trained contextualized embeddings .,9,1,19
dataset/preprocessed/test-data/entity_linking/1,"As a result , we achieved stateof - the - art or competitive results on several standard NED datasets .",10,0,20
dataset/preprocessed/test-data/entity_linking/1,Named entity disambiguation ( NED ) refers to the task of assigning entity mentions in a text to corresponding entries in a knowledge base ( KB ) .,12,0,28
dataset/preprocessed/test-data/entity_linking/1,"This task is challenging owing to the ambiguity between entity names ( e.g. , "" World Cup "" ) and the entities they refer to ( e.g. , FIFA World Cup and Rugby World Cup ) .",13,0,37
dataset/preprocessed/test-data/entity_linking/1,"Deep contextualized word embedding models , e.g. , ELMo and BERT , have recently achieved stateof - the - art results on many tasks .",14,0,25
dataset/preprocessed/test-data/entity_linking/1,"Unlike conventional word embedding models that assign a single , fixed embedding per word , these models produce a contextualized embedding for each word in the input text using a pretrained neural network encoder .",15,0,35
dataset/preprocessed/test-data/entity_linking/1,"The encoder can be a recurrent neural network or transformer , and is usually trained using an unsupervised objective based on language modeling .",16,0,24
dataset/preprocessed/test-data/entity_linking/1,"For instance , proposed Masked Language Model ( MLM ) , which aims to train the embeddings by predicting randomly masked words in the text .",17,0,26
dataset/preprocessed/test-data/entity_linking/1,"In this paper , we describe a new contextualized embedding model for words and entities for NED .",18,0,18
dataset/preprocessed/test-data/entity_linking/1,"Following , the proposed model is based on the bidirectional transformer encoder .",19,0,13
dataset/preprocessed/test-data/entity_linking/1,"It takes a sequence of words and entities in the input text , and produces a contextualized embedding for each word and entity .",20,0,24
dataset/preprocessed/test-data/entity_linking/1,"Inspired by MLM , we propose masked entity prediction , a new task that aims to train the embedding model by predicting randomly masked entities based on words and non-masked entities in the input text .",21,0,36
dataset/preprocessed/test-data/entity_linking/1,We trained the model using texts and their entity annotations retrieved from Wikipedia .,22,0,14
dataset/preprocessed/test-data/entity_linking/1,We evaluated the proposed model by addressing NED using an NED model based on trained contextualized embeddings .,23,0,18
dataset/preprocessed/test-data/entity_linking/1,The NED model addresses the task by capturing word - based and entity - based contextual information using the trained contextualized embeddings .,24,0,23
dataset/preprocessed/test-data/entity_linking/1,"As a result , we achieved state - of - the - art or competitive results on various standard NED datasets .",25,0,22
dataset/preprocessed/test-data/entity_linking/1,We will release our code and trained embeddings for further research .,26,0,12
dataset/preprocessed/test-data/entity_linking/1,Background and Related Work,27,0,4
dataset/preprocessed/test-data/entity_linking/1,Neural network - based approaches have recently achieved strong results on NED .,28,0,13
dataset/preprocessed/test-data/entity_linking/1,"A key component of these approaches is an embedding model of words and entities trained using a large knowledge base ( e.g. , Wikipedia ) .",29,0,26
dataset/preprocessed/test-data/entity_linking/1,Such embedding models enable us to design NED models that capture the contextual information required to address NED .,30,0,19
dataset/preprocessed/test-data/entity_linking/1,"These models are typically based on conventional word embedding models ( e.g. , skip - gram ) that assign a fixed embedding to each word and entity .",31,0,28
dataset/preprocessed/test-data/entity_linking/1,"In this study , we aim to test the effectiveness of the pretrained contextualized embeddings for NED .",32,0,18
dataset/preprocessed/test-data/entity_linking/1,Embeddings of Words and Entities,34,0,5
dataset/preprocessed/test-data/entity_linking/1,"In this section , we introduce our contextualized embedding model for words and entities .",35,0,15
dataset/preprocessed/test-data/entity_linking/1,shows the architecture of the proposed model .,36,0,8
dataset/preprocessed/test-data/entity_linking/1,Our model adopts a multi - layer bidirectional transformer encoder 1 with input representations described later in this section .,37,0,20
dataset/preprocessed/test-data/entity_linking/1,"Given a sequence of tokens consisting of words and entities , the model first represents the sequence as a sequence of input embeddings , one for each token , and then the model generates a contextualized output embedding for each token .",38,0,42
dataset/preprocessed/test-data/entity_linking/1,Both input and output embeddings have H dimensions .,39,0,9
dataset/preprocessed/test-data/entity_linking/1,"Hereafter , we denote the number of words and that of entities in the vocabulary of our model by V wand V e , respectively .",40,0,26
dataset/preprocessed/test-data/entity_linking/1,"Similar to the approach adopted in , the input representation of a given token ( i.e. , word or entity ) is constructed by summing the following three embeddings of H dimensions :",42,0,33
dataset/preprocessed/test-data/entity_linking/1,Token embedding is the embedding of the corresponding token .,43,0,10
dataset/preprocessed/test-data/entity_linking/1,The matrices of the word and entity token embeddings are represented as A ? R VwH and B ?,44,0,19
dataset/preprocessed/test-data/entity_linking/1,"R VeH , respectively .",45,0,5
dataset/preprocessed/test-data/entity_linking/1,"Token type embedding represents the type of token , namely word type ( denoted by C word ) or entity type ( denoted by C entity ) .",46,0,28
dataset/preprocessed/test-data/entity_linking/1,Position embedding represents the position of the token in a word sequence .,47,0,13
dataset/preprocessed/test-data/entity_linking/1,"A word and an entity appearing at i -th position in the sequence are represented as Di and E i , respectively .",48,0,23
dataset/preprocessed/test-data/entity_linking/1,"If an entity name contains multiple words , we compute its position embedding by averaging the embeddings of the corresponding positions ( e.g. , New York City in ) .",49,0,30
dataset/preprocessed/test-data/entity_linking/5,Improving the Coverage and the Generalization Ability of Neural Word Sense Disambiguation through Hypernymy and Hyponymy Relationships,2,1,17
dataset/preprocessed/test-data/entity_linking/5,"In Word Sense Disambiguation ( WSD ) , the predominant approach generally involves a supervised system trained on sense annotated corpora .",4,1,22
dataset/preprocessed/test-data/entity_linking/5,The limited quantity of such corpora however restricts the coverage and the performance of these systems .,5,0,17
dataset/preprocessed/test-data/entity_linking/5,"In this article , we propose anew method that solves these issues by taking advantage of the knowledge present in WordNet , and especially the hypernymy and hyponymy relationships between synsets , in order to reduce the number of different sense tags that are necessary to disambiguate all words of the lexical database .",6,0,54
dataset/preprocessed/test-data/entity_linking/5,"Our method leads to state of the art results on most WSD evaluation tasks , while improving the coverage of supervised systems , reducing the training time and the size of the models , without additional training data .",7,1,39
dataset/preprocessed/test-data/entity_linking/5,"In addition , we exhibit results that significantly outperform the state of the art when our method is combined with an ensembling technique and the addition of the WordNet Gloss Tagged as training corpus .",8,0,35
dataset/preprocessed/test-data/entity_linking/5,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .",10,0,34
dataset/preprocessed/test-data/entity_linking/5,"Various approaches have been proposed to achieve WSD , and they are generally ordered by the type and the quantity of resources they use :",11,0,25
dataset/preprocessed/test-data/entity_linking/5,"Knowledge - based methods rely on dictionaries , lexical databases , thesauri or knowledge graphs as main resources , and use algorithms such as lexical similarity measures or graph - based measures .",12,0,33
dataset/preprocessed/test-data/entity_linking/5,"Supervised methods , on the other hand , exploit sense annotated corpora as training instances that can be used by a multiclass classifier such as SVM , or more recently by a neural network .",13,0,35
dataset/preprocessed/test-data/entity_linking/5,Semi-supervised methods generally use unannotated data to artificially increase the quantity of sense annotated data and hence improve supervised methods .,14,0,21
dataset/preprocessed/test-data/entity_linking/5,Supervised methods are by far the most predominant as they generally offer the best results in evaluation campaigns ( for instance ) .,15,0,23
dataset/preprocessed/test-data/entity_linking/5,"State of the art classifiers used to combine a set of specific features such as the parts of speech tags of surrounding words , local collocations and pretrained word embeddings , but they are now replaced by recurrent neural networks which learn their own representation of words .",16,0,48
dataset/preprocessed/test-data/entity_linking/5,One of the major bottleneck of supervised systems is the restricted quantity of manually sense annotated corpora .,17,0,18
dataset/preprocessed/test-data/entity_linking/5,"Indeed , while the lexical database WordNet , the sense inventory of reference used inmost works on WSD , contains more than 200 000 different word - sense pairs 1 , the SemCor , the corpus which is used the most in the training of supervised systems , only represents approximately 34 000 of them .",18,0,56
dataset/preprocessed/test-data/entity_linking/5,"Many works try to leverage this problem by creating new sense annotated corpora , either automatically , semi-automatically , or through crowdsourcing , but in this work , the idea is to solve this issue by taking advantage of one of the multiple semantic relationships between senses included in WordNet : the hypernymy and hyponymy relationships .",19,0,57
dataset/preprocessed/test-data/entity_linking/5,Our method is based on three observations :,20,0,8
dataset/preprocessed/test-data/entity_linking/5,"1 . A sense , its hypernym and it s hyponyms share a common idea or concept , but on different levels of abstraction .",21,0,25
dataset/preprocessed/test-data/entity_linking/5,"In general , a word can be disambiguated using the hypernyms of its senses , and not necessarily the senses themselves .",23,0,22
dataset/preprocessed/test-data/entity_linking/5,"Consequently , we do not need to know every sense of WordNet to disambiguate all words of WordNet .",25,0,19
dataset/preprocessed/test-data/entity_linking/5,We propose a method for reducing the vocabulary of senses of Word Net by selecting the minimal set of senses required for differentiating the meaning of every word .,27,0,29
dataset/preprocessed/test-data/entity_linking/5,"By using this technique , and converting the sense tags present in sense annotated corpora to the most generalized sense possible , we are able to greatly improve the coverage and the generalization ability of supervised systems .",28,0,38
dataset/preprocessed/test-data/entity_linking/5,"We start by presenting the state of the art of supervised neural architectures for word sense disambiguation , then we describe our new method for sense vocabulary reduction .",29,0,29
dataset/preprocessed/test-data/entity_linking/5,Our method is then evaluated by measuring its contribution to a state of the art neural WSD system evaluated on classic WSD evaluation campaigns .,30,0,25
dataset/preprocessed/test-data/entity_linking/5,The code for using our system or reproducing our results is available at the following URL : https://github.com/getalp/disambiguate,31,0,18
dataset/preprocessed/test-data/entity_linking/5,Neural Word Sense Disambiguation,32,0,4
dataset/preprocessed/test-data/entity_linking/5,"The neural approaches for WSD fall into two categories : approaches based on a neural model that learns to classify a sense directly , and approaches based on a neural language model that learns to predict a word , and is then used to find the closest sense to the predicted word .",33,0,53
dataset/preprocessed/test-data/entity_linking/5,Language Model Based WSD,34,0,4
dataset/preprocessed/test-data/entity_linking/5,"The core of these approaches is a powerful neural language model able to predict a word with consideration for the words surrounding it , thanks to a recurrent neural network trained on a massive quantity of unannotated data .",35,0,39
dataset/preprocessed/test-data/entity_linking/5,The main works that implement these kind of model are and .,36,0,12
dataset/preprocessed/test-data/entity_linking/5,"Once the language model is trained , its predictions are used to produce sense vectors as the average of the word vectors predicted by the language model in the places where the words are sense annotated .",37,0,37
dataset/preprocessed/test-data/entity_linking/5,"At test time , the language model is used to predict a vector according to the surrounding context , and the sense closest to the predicted vector is assigned to each word .",38,0,33
dataset/preprocessed/test-data/entity_linking/5,These systems have the advantage of bypassing the problem of the lack of sense annotated data by concentrating the power of abstraction offered by recurrent neural networks on a good quality language model trained in an unsupervised manner .,39,0,39
dataset/preprocessed/test-data/entity_linking/5,"However , sense annotated corpora are still indispensable to construct the sense vectors , and the quantity of data needed for training the language model ( 100 billion tokens for , 2 billion tokens for ) makes these systems more difficult to train than those relying on sense annotated data only .",40,0,52
dataset/preprocessed/test-data/entity_linking/5,Classification Based WSD,41,0,3
dataset/preprocessed/test-data/entity_linking/5,"In these systems , the main neural network directly classifies and attributes a sense to each input word .",42,0,19
dataset/preprocessed/test-data/entity_linking/5,"Sense annotations are simply seen as tags put on every word , like a POS - tagging task for instance .",43,0,21
dataset/preprocessed/test-data/entity_linking/5,"These models are more similar to classical supervised models such as , except that the input features are not manually selected , but trained as part of the neural network ( using pre-trained word embeddings or not ) .",44,0,39
dataset/preprocessed/test-data/entity_linking/5,"In addition , we can distinguish two separate branches of these types of neural networks :",45,0,16
dataset/preprocessed/test-data/entity_linking/5,"Those in which we have several distinct and small neural networks ( or classifiers ) for every different word in the dictionary ) , each of them being able to manage a particular word and its particular senses .",47,0,39
dataset/preprocessed/test-data/entity_linking/5,"For instance , one of the classifiers is specialized into choosing between the four possible senses of the noun "" mouse "" .",48,0,23
dataset/preprocessed/test-data/entity_linking/5,"This type of approaches is particularly fitted for the lexical sample tasks , where a small and finite set of very ambiguous words have to be sense annotated in several contexts , but it can also be used in all - words word sense disambiguation tasks .",49,0,47
dataset/preprocessed/test-data/entity_linking/15,Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories,2,1,14
dataset/preprocessed/test-data/entity_linking/15,The first stage of every knowledge base question answering approach is to link entities in the input question .,4,0,19
dataset/preprocessed/test-data/entity_linking/15,We investigate entity linking in the context of a question answering task and present a jointly optimized neural architecture for entity mention detection and entity disambiguation that models the surrounding context on different levels of granularity .,5,0,37
dataset/preprocessed/test-data/entity_linking/15,We use the Wikidata knowledge base and available question answering datasets to create benchmarks for entity linking on question answering data .,6,0,22
dataset/preprocessed/test-data/entity_linking/15,"Our approach outperforms the previous state - of - the - art system on this data , resulting in an average 8 % improvement of the final score .",7,0,29
dataset/preprocessed/test-data/entity_linking/15,We further demonstrate that our model delivers a strong performance across different entity categories .,8,0,15
dataset/preprocessed/test-data/entity_linking/15,Knowledge base question answering ( QA ) requires a precise modeling of the question semantics through the entities and relations available in the knowledge base ( KB ) in order to retrieve the correct answer .,10,0,36
dataset/preprocessed/test-data/entity_linking/15,"The first stage for every QA approach is entity linking ( EL ) , that is the identification of entity mentions in the question and linking them to entities in KB .",11,1,32
dataset/preprocessed/test-data/entity_linking/15,"In , two entity mentions are detected and linked to the knowledge base referents .",12,0,15
dataset/preprocessed/test-data/entity_linking/15,This step is crucial for QA since the correct answer must be connected via some path over KB to the entities mentioned in the question .,13,0,26
dataset/preprocessed/test-data/entity_linking/15,The state - of - the - art QA systems usually rely on off - the - shelf EL systems to extract entities from the question .,14,1,27
dataset/preprocessed/test-data/entity_linking/15,Multiple EL systems are freely available and can be readily applied what are taylor swift 's albums ?,15,0,18
dataset/preprocessed/test-data/entity_linking/15,"Taylor Swift Q462 album Q24951125 Red , 1989 , etc. :",16,0,11
dataset/preprocessed/test-data/entity_linking/15,"An example question from a QA dataset that shows the correct entity mentions and their relationship with the correct answer to the question , Qxxx stands fora knowledge base identifier for question answering ( e.g. DBPedia Spotlight 1 , AIDA 2 ) .",17,0,43
dataset/preprocessed/test-data/entity_linking/15,"However , these systems have certain drawbacks in the QA setting : they are targeted at long well - formed documents , such as news texts , and are less suited for typically short and noisy question data .",18,0,39
dataset/preprocessed/test-data/entity_linking/15,"Other EL systems focus on noisy data ( e.g. S - MART , , but are not openly available and hence limited in their usage and application .",19,0,28
dataset/preprocessed/test-data/entity_linking/15,Multiple error analyses of QA systems point to entity linking as a major external source of error .,20,0,18
dataset/preprocessed/test-data/entity_linking/15,"The QA datasets are normally collected from the web and contain very noisy and diverse data , which poses a number of challenges for EL .",23,0,26
dataset/preprocessed/test-data/entity_linking/15,"First , many common features used in EL systems , such as capitalization , are not meaningful on noisy data .",24,0,21
dataset/preprocessed/test-data/entity_linking/15,"Moreover , a question is a short text snippet that does not contain broader context that is helpful for entity disambiguation .",25,0,22
dataset/preprocessed/test-data/entity_linking/15,The QA data also features many entities of various categories and differs in this respect from the Twitter datasets that are often used to evaluate EL systems .,26,0,28
dataset/preprocessed/test-data/entity_linking/15,"In this paper , we present an approach that tackles the challenges listed above : we perform entity mention detection and entity disambiguation jointly in a single neural model that makes the whole process end - to - end differentiable .",27,0,41
dataset/preprocessed/test-data/entity_linking/15,"This ensures that any token n-gram can be considered as a potential entity mention , which is important to be able to link entities of different categories , such as movie titles and organization names .",28,0,36
dataset/preprocessed/test-data/entity_linking/15,"To overcome the noise in the data , we automatically learn features over a set of contexts of different granularity levels .",29,0,22
dataset/preprocessed/test-data/entity_linking/15,Each level of granularity is handled by a separate component of the model .,30,0,14
dataset/preprocessed/test-data/entity_linking/15,"A token - level component extracts higher - level features from the whole question context , whereas a character - level component builds lower - level features for the candidate n-gram .",31,0,32
dataset/preprocessed/test-data/entity_linking/15,"Simultaneously , we extract features from the knowledge base context of the candidate entity : character - level features are extracted for the entity label and higher - level features are produced based on the entities surrounding the candidate entity in the knowledge graph .",32,0,45
dataset/preprocessed/test-data/entity_linking/15,This information is aggregated and used to predict whether the n-gram is an entity mention and to what entity it should be linked .,33,0,24
dataset/preprocessed/test-data/entity_linking/15,The two main contributions of our work are :,35,0,9
dataset/preprocessed/test-data/entity_linking/15,( i ) We construct two datasets to evaluate EL for QA and present a set of strong baselines : the existing EL systems that were used as a building block for QA before and a model that uses manual features from the previous work on noisy data .,36,0,49
dataset/preprocessed/test-data/entity_linking/15,( ii ) We design and implement an entity linking system that models contexts of variable granularity to detect and disambiguate entity mentions .,37,0,24
dataset/preprocessed/test-data/entity_linking/15,"To the best of our knowledge , we are the first to present a unified end - to - end neural model for entity linking for noisy data that operates on different context levels and does not rely on manual features .",38,0,42
dataset/preprocessed/test-data/entity_linking/15,Our architecture addresses the challenges of entity linking on question answering data and outperforms state - of - the - art EL systems .,39,0,24
dataset/preprocessed/test-data/entity_linking/15,Code and datasets,40,0,3
dataset/preprocessed/test-data/entity_linking/15,Our system can be applied on any QA dataset .,41,0,10
dataset/preprocessed/test-data/entity_linking/15,The complete code as well as the scripts that produce the evaluation data can be found here : https://github.com/UKPLab/ starsem2018-entity-linking.,42,0,20
dataset/preprocessed/test-data/entity_linking/15,"Several benchmarks exist for EL on Wikipedia texts and news articles , such as ACE and CoNLL - YAGO .",43,0,20
dataset/preprocessed/test-data/entity_linking/15,"These datasets contain multi-sentence documents and largely cover three types of entities : Location , Person and Organization .",44,0,19
dataset/preprocessed/test-data/entity_linking/15,"These types are commonly recognized by named entity recognition systems , such as Stanford NER Tool .",45,0,17
dataset/preprocessed/test-data/entity_linking/15,"Therefore in this scenario , an EL system can solely focus on entity disambiguation .",46,0,15
dataset/preprocessed/test-data/entity_linking/15,"In the recent years , EL on Twitter data has emerged as a branch of entity linking research .",47,0,19
dataset/preprocessed/test-data/entity_linking/15,"In particular , EL on tweets was the central task of the NEEL shared task from 2014 to 2016 .",48,0,20
dataset/preprocessed/test-data/entity_linking/15,Tweet s share some of the challenges with QA data : in both cases the input data is short and noisy .,49,0,22
dataset/preprocessed/test-data/entity_linking/8,LEARNING CROSS - CONTEXT ENTITY REPRESENTA - TIONS FROM TEXT,2,0,10
dataset/preprocessed/test-data/entity_linking/8,Work done as a Google AI Resident,3,0,7
dataset/preprocessed/test-data/entity_linking/8,"Language modeling tasks , in which words , or word - pieces , are predicted on the basis of a local context , have been very effective for learning word embeddings and context dependent representations of phrases .",5,1,38
dataset/preprocessed/test-data/entity_linking/8,"Motivated by the observation that efforts to code world knowledge into machine readable knowledge bases or human readable encyclopedias tend to be entity - centric , we investigate the use of a fill - in - the - blank task to learn context independent representations of entities from the text contexts in which those entities were mentioned .",6,0,58
dataset/preprocessed/test-data/entity_linking/8,"We show that large scale training of neural models allows us to learn high quality entity representations , and we demonstrate successful results on four domains : ( 1 ) existing entity - level typing benchmarks , including a 64 % error reduction over previous work on TypeNet ; ( 2 ) a novel few - shot category reconstruction task ; ( 3 ) existing entity linking benchmarks , where we match the state - of - the - art on CoNLL - Aida without linking - specific features and obtain a score of 89.8 % on TAC - KBP 2010 without using any alias table , external knowledge base or in domain training data and ( 4 ) answering trivia questions , which uniquely identify entities .",7,0,128
dataset/preprocessed/test-data/entity_linking/8,"Our global entity representations encode fine - grained type categories , such as Scottish footballers , and can answer trivia questions such as Who was the last inmate of Spandau jail in Berlin ?",8,0,34
dataset/preprocessed/test-data/entity_linking/8,A long term goal of artificial intelligence has been the development and population of an entitycentric representation of human knowledge .,10,0,21
dataset/preprocessed/test-data/entity_linking/8,Efforts have been made to create the knowledge representation with knowledge engineers or crowdsourcers .,11,0,15
dataset/preprocessed/test-data/entity_linking/8,"However , these methods have relied heavily on human definitions of their ontologies , which are both limited in scope and brittle in nature .",12,0,25
dataset/preprocessed/test-data/entity_linking/8,"Conversely , due to recent advances in deep learning , we can now learn robust general purpose representations of words and contextualized phrases directly from large textual corpora .",13,0,29
dataset/preprocessed/test-data/entity_linking/8,"In particular , we observe that existing methods of building contextualized phrase representations capture a significant amount of local semantic context .",14,0,22
dataset/preprocessed/test-data/entity_linking/8,"We hypothesize that by learning an entity encoder which aggregates all of the textual contexts in which an entity is seen , we should be able to extract and condense general purpose knowledge about that entity .",15,0,37
dataset/preprocessed/test-data/entity_linking/8,Consider the following contexts in which an entity mention has been replaced a [ MASK ] :,16,0,17
dataset/preprocessed/test-data/entity_linking/8,". . . the second woman in space , 19 years after .",17,0,13
dataset/preprocessed/test-data/entity_linking/8,". . . , a Russian factory worker , was the first woman in space . . . . . . , the first woman in space , entered politics . . . .",18,0,34
dataset/preprocessed/test-data/entity_linking/8,"As readers , we understand that first woman in space is a unique identifier , and we are able to fill in the blank unambiguously .",19,0,26
dataset/preprocessed/test-data/entity_linking/8,"The central hypothesis of this paper is that , by matching entities to the contexts in which they are mentioned , we should be able to build a representation for Valentina Tereshkova that encodes the fact that she was the first woman in space , that she was a politician , etc. and that we should be able to use these representations across a wide variety of downstream entity - centric tasks .",20,0,73
dataset/preprocessed/test-data/entity_linking/8,"We present RELIC ( Representations of Entities Learned in Context ) , a table of independent entity embeddings that have been trained to match fixed length vector representations of the textual context in which those entities have been seen .",21,0,40
dataset/preprocessed/test-data/entity_linking/8,"We apply RELIC to entity typing ( mapping each entity to its properties in an external , curated , ontology ) ; entity linking ( identifying which entity is referred to by a textual context ) , and trivia question answering ( retrieving the entity that best answers a question ) .",22,0,52
dataset/preprocessed/test-data/entity_linking/8,"Through these experiments , we show that :",23,0,8
dataset/preprocessed/test-data/entity_linking/8,RELIC accurately captures categorical information encoded by human experts in the Freebase and Wikipedia category hierarchies .,24,0,17
dataset/preprocessed/test-data/entity_linking/8,"We demonstrate significant improvements over previous work on established benchmarks , including a 64 % error reduction in the TypeNet low data setting .",25,0,24
dataset/preprocessed/test-data/entity_linking/8,We also show that given just a few exemplar entities of a given category such as Scottish footballers we can use RELIC to recover the remaining entities of that category with good precision .,26,0,34
dataset/preprocessed/test-data/entity_linking/8,Using RELIC for entity linking can match state - of - the - art approaches that make use of non-local and non-linguistic information about entities .,27,0,26
dataset/preprocessed/test-data/entity_linking/8,"On the CoNLL - Aida benchmark , RELIC achieves a 94.9 % accuracy , matching the state - of - the - art of , despite not using any entity linking - specific features .",28,0,35
dataset/preprocessed/test-data/entity_linking/8,"On the TAC - KBP 2010 benchmark RELIC achieves 89.8 % accuracy , just behind the top ranked system , which makes use of external knowledge bases , alias tables , and taskspecific hand - engineered features .",29,0,38
dataset/preprocessed/test-data/entity_linking/8,"RELIC learns better representations of entity properties if it is trained to match just the contexts in which entities are mentioned , and not the surface form of the mention itself .",30,0,32
dataset/preprocessed/test-data/entity_linking/8,"For entity linking , the opposite is true .",31,0,9
dataset/preprocessed/test-data/entity_linking/8,"We can treat the RELIC embedding matrix as a store of knowledge , and retrieve answers to questions through nearest neighbor search .",32,0,23
dataset/preprocessed/test-data/entity_linking/8,We show that this approach correctly answers 51 % of the questions in the TriviaQA reading comprehension task despite not using the task 's evidence text at inference time .,33,0,30
dataset/preprocessed/test-data/entity_linking/8,"The questions answered correctly by RELIC are surprisingly complex , such as Who was the last inmate of Spandau jail in Berlin ?",34,0,23
dataset/preprocessed/test-data/entity_linking/8,The most widely studied entity - level task is entity linking - mapping each entity mention onto a unique entity identifier .,37,0,22
dataset/preprocessed/test-data/entity_linking/8,"The Wikification task , in particular , is similar to the work presented in this paper , as it requires systems to map mentions to the Wikipedia pages describing the entities mentioned .",38,0,33
dataset/preprocessed/test-data/entity_linking/8,"There is significant previous work that makes use of neural context and entity encoders in downstream entity linking systems , but that previous work focuses solely on discriminating between entities that match a given mention according to an external alias table .",39,0,42
dataset/preprocessed/test-data/entity_linking/8,Here we go further in investigating the degree to which RELIC can capture world knowledge about entities .,40,0,18
dataset/preprocessed/test-data/entity_linking/8,Mention - level entity typing,41,0,5
dataset/preprocessed/test-data/entity_linking/8,Another well studied task is mention - level entity typing ( e.g. .,42,0,13
dataset/preprocessed/test-data/entity_linking/8,"In this task , entities are labeled with types that are supported by the immediate textual context .",43,0,18
dataset/preprocessed/test-data/entity_linking/8,"For example , given the sentence ' Michelle Obama attended her book signing ' , Michelle Obama should be assigned the type author but not lawyer .",44,0,27
dataset/preprocessed/test-data/entity_linking/8,"Subsequently , mention - level entity typing systems make use of contextualized representations of the entity mention , rather than the global entity representations that we focus on here .",45,0,30
dataset/preprocessed/test-data/entity_linking/8,Entity - level typing,46,0,4
dataset/preprocessed/test-data/entity_linking/8,"An alternative notion of entity typing is entity - level typing , where each entity should be associated with all of the types supported by a corpus .",47,0,28
dataset/preprocessed/test-data/entity_linking/8,"and introduce entity - level typing tasks , which we describe more in Section 5.2 .",48,0,16
dataset/preprocessed/test-data/entity_linking/8,"Entity - level typing is an important task in information extraction , since most common ontologies make use of entity type systems .",49,0,23
dataset/preprocessed/test-data/entity_linking/6,Incorporating Glosses into Neural Word Sense Disambiguation,2,1,7
dataset/preprocessed/test-data/entity_linking/6,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,4,1,20
dataset/preprocessed/test-data/entity_linking/6,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .,5,1,21
dataset/preprocessed/test-data/entity_linking/6,"However , previous neural networks for WSD always rely on massive labeled data ( context ) , ignoring lexical resources like glosses ( sense definitions ) .",6,0,27
dataset/preprocessed/test-data/entity_linking/6,"In this paper , we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge .",7,0,32
dataset/preprocessed/test-data/entity_linking/6,"Therefore , we propose GAS : a gloss - augmented WSD neural network which jointly encodes the context and glosses of the target word .",8,0,25
dataset/preprocessed/test-data/entity_linking/6,"GAS models the semantic relationship between the context and the gloss in an improved memory network framework , which breaks the barriers of the previous supervised methods and knowledge - based methods .",9,0,33
dataset/preprocessed/test-data/entity_linking/6,We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information .,10,0,21
dataset/preprocessed/test-data/entity_linking/6,The experimental results show that our model outperforms the state - of - theart systems on several English all - words WSD datasets 1 .,11,0,25
dataset/preprocessed/test-data/entity_linking/6,Word Sense Disambiguation ( WSD ) is a fundamental task and long - standing challenge in Natural Language Processing ( NLP ) .,13,0,23
dataset/preprocessed/test-data/entity_linking/6,There are several lines of research on WSD .,14,0,9
dataset/preprocessed/test-data/entity_linking/6,Knowledge - based methods focus on exploiting lexical resources to infer the senses of word in the context .,15,0,19
dataset/preprocessed/test-data/entity_linking/6,Supervised methods usually train multiple classifiers with manual designed features .,16,0,11
dataset/preprocessed/test-data/entity_linking/6,"Although supervised methods can achieve the state - of - the - art performance , there are still two major challenges .",17,0,22
dataset/preprocessed/test-data/entity_linking/6,"Firstly , supervised methods usually train a dedicated classifier for each word individually ( often called word expert ) .",18,0,20
dataset/preprocessed/test-data/entity_linking/6,So it can not easily scale up to all - words WSD task which requires to disambiguate all the polysemous word in texts,19,0,23
dataset/preprocessed/test-data/entity_linking/6,"Recent neural - based methods solve this problem by building a unified model for all the polysemous words , but they still ca n't beat the best word expert system .",21,0,31
dataset/preprocessed/test-data/entity_linking/6,"Secondly , all the neural - based methods always only consider the local context of the target word , ignoring the lexical resources like which are widely used in the knowledge - based methods .",22,0,35
dataset/preprocessed/test-data/entity_linking/6,"The gloss , which extensionally defines a word sense meaning , plays a key role in the well - known Lesk algorithm .",23,0,23
dataset/preprocessed/test-data/entity_linking/6,"Recent studies have shown that enriching gloss information through its semantic relations can greatly improve the accuracy of To this end , our goal is to incorporate the gloss information into a unified neural network for all of the polysemous words .",24,0,42
dataset/preprocessed/test-data/entity_linking/6,We further consider extending the original gloss through its semantic relations in our framework .,25,0,15
dataset/preprocessed/test-data/entity_linking/6,"As shown in , the glosses of hypernyms and hyponyms can enrich the original gloss information as well as help to build better a sense representation .",26,0,27
dataset/preprocessed/test-data/entity_linking/6,"Therefore , we integrate not only the original gloss but also :",27,0,12
dataset/preprocessed/test-data/entity_linking/6,"The hypernym ( green node ) and hyponyms ( blue nodes ) for the 2nd sense bed 2 of bed , which means a plot of ground in which plants are growing , rather than the bed for sleeping in .",28,0,41
dataset/preprocessed/test-data/entity_linking/6,"The figure shows that bed 2 is a kind of plot 2 , and bed 2 includes flowerbed 1 , seedbed 1 , etc .",29,0,25
dataset/preprocessed/test-data/entity_linking/6,the related glosses of hypernyms and hyponyms into the neural network .,30,0,12
dataset/preprocessed/test-data/entity_linking/6,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .",31,0,27
dataset/preprocessed/test-data/entity_linking/6,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,32,0,26
dataset/preprocessed/test-data/entity_linking/6,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .",33,0,33
dataset/preprocessed/test-data/entity_linking/6,The main contributions of this paper are listed as follows :,34,0,11
dataset/preprocessed/test-data/entity_linking/6,"To the best of our knowledge , our model is the first to incorporate the glosses into an end - to - end neural WSD model .",35,0,27
dataset/preprocessed/test-data/entity_linking/6,"In this way , our model can benefit from not only massive labeled data but also rich lexical knowledge .",36,0,20
dataset/preprocessed/test-data/entity_linking/6,"In order to model semantic relationship of context and glosses , we propose a glossaugmented neural network ( GAS ) in an improved memory network paradigm .",37,0,27
dataset/preprocessed/test-data/entity_linking/6,We further expand the gloss through its semantic relations to enrich the gloss information and better infer the context .,38,0,20
dataset/preprocessed/test-data/entity_linking/6,We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet .,39,0,23
dataset/preprocessed/test-data/entity_linking/6,The experimental results on several English all - words WSD benchmark datasets show that our model outperforms the state - of - theart systems .,40,0,25
dataset/preprocessed/test-data/entity_linking/6,"Knowledge - based , supervised and neural - based methods have already been applied to WSD task ) .",42,0,19
dataset/preprocessed/test-data/entity_linking/6,Knowledge - based WSD methods mainly exploit two kinds of knowledge to disambiguate polysemous words :,43,0,16
dataset/preprocessed/test-data/entity_linking/6,"1 ) The gloss , which defines a word sense meaning , is mainly used in Lesk algorithm and its variants .",44,0,22
dataset/preprocessed/test-data/entity_linking/6,"2 ) The structure of the semantic network , whose nodes are synsets 3 and edges are semantic relations , is mainly used in graph - based algorithms .",45,0,29
dataset/preprocessed/test-data/entity_linking/6,Supervised methods usually involve each target word as a separate classification problem ( often called word expert ) and train classifiers based on manual designed features .,46,0,27
dataset/preprocessed/test-data/entity_linking/6,"Although word expert supervised WSD methods perform best in terms of accuray , they are less flexible than knowledge - based methods in the allwords WSD task .",47,0,28
dataset/preprocessed/test-data/entity_linking/6,"To deal with this problem , recent neural - based methods aim to build a unified classifier which shares parameters among all the polysemous words .",48,0,26
dataset/preprocessed/test-data/entity_linking/6,leverages the bidirectional long short - term memory network which shares model parameters among all the polysemous words .,49,0,19
dataset/preprocessed/test-data/entity_linking/11,Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation,2,1,14
dataset/preprocessed/test-data/entity_linking/11,"In this article , we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation , by exploiting the semantic relationships between senses such as synonymy , hypernymy and hyponymy , in order to compress the sense vocabulary of Princeton WordNet , and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database .",4,0,73
dataset/preprocessed/test-data/entity_linking/11,"We propose two different methods that greatly reduce the size of neural WSD models , with the benefit of improving their coverage without additional training data , and without impacting their precision .",5,1,33
dataset/preprocessed/test-data/entity_linking/11,"In addition to our methods , we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks .",6,1,37
dataset/preprocessed/test-data/entity_linking/11,"Word Sense Disambiguation ( WSD ) is a task which aims to clarify a text by assigning to each of its words the most suitable sense labels , given a predefined sense inventory .",8,1,34
dataset/preprocessed/test-data/entity_linking/11,"Various approaches have been proposed to achieve WSD : Knowledge - based methods rely on dictionaries , lexical databases , thesauri or knowledge graphs as primary resources , and use algorithms such as lexical similarity measures or graph - based measures .",9,0,42
dataset/preprocessed/test-data/entity_linking/11,"Supervised methods , on the other hand , exploit sense annotated corpora as training instances fora classifier such as SVM , or more recently by a neural network .",10,0,29
dataset/preprocessed/test-data/entity_linking/11,"Finally , unsupervised methods automatically iden - tify the different senses of words from unannotated or parallel corpora ( e.g. ) .",11,0,22
dataset/preprocessed/test-data/entity_linking/11,Supervised methods are by far the most predominant as they generally offer the best results in evaluation campaigns ( for instance ) .,12,0,23
dataset/preprocessed/test-data/entity_linking/11,"State of the art classifiers used to combine specific features such as the parts of speech and the lemmas of surrounding words , but they are now replaced by neural networks which learn their own representation of words .",13,0,39
dataset/preprocessed/test-data/entity_linking/11,One major bottleneck of supervised systems is the restricted quantity of manually sense annotated corpora :,14,0,16
dataset/preprocessed/test-data/entity_linking/11,"In the annotated corpus SemCor , the largest manually sense annotated corpus available , words are annotated with 33 760 different sense keys , which corresponds to only approximately 16 % of the sense inventory of WordNet , the lexical database of reference widely used in WSD .",15,0,48
dataset/preprocessed/test-data/entity_linking/11,"Many works try to leverage this problem by creating new sense annotated corpora , either automatically , semi-automatically , or through crowdsourcing .",16,0,23
dataset/preprocessed/test-data/entity_linking/11,"In this work , the idea is to solve this issue by taking advantage of the semantic relationships between senses included in WordNet , such as the hypernymy , the hyponymy , the meronymy , the antonymy , etc .",17,0,40
dataset/preprocessed/test-data/entity_linking/11,"Our method is based on the observation that a sense and its closest related senses ( it s hypernym or it s hyponyms for instance ) all share a common idea or concept , and so a word can sometimes be disambiguated using only related concepts .",18,0,47
dataset/preprocessed/test-data/entity_linking/11,"Consequently , we do not need to know every sense of WordNet to disambiguate all words of WordNet .",19,0,19
dataset/preprocessed/test-data/entity_linking/11,"For instance , let us consider the word "" mouse "" and two of its senses which are the computer mouse and the animal mouse .",20,0,26
dataset/preprocessed/test-data/entity_linking/11,"We only need to know the notions of "" animal "" and "" electronic de-vice "" to distinguish them , and all notions that are more specialized such as "" rodent "" or "" mammal "" are therefore superfluous .",21,0,40
dataset/preprocessed/test-data/entity_linking/11,"By grouping them , we can benefit from all other instances of electronic devices or animals in a training corpus , even if they do not mention the word "" mouse "" .",22,0,33
dataset/preprocessed/test-data/entity_linking/11,"Contributions : In this paper , we hypothesize that only a subset of WordNet senses could be considered to disambiguate all words of the lexical database .",23,0,27
dataset/preprocessed/test-data/entity_linking/11,"Therefore , we propose two different methods for building this subset and we call them sense vocabulary compression methods .",24,0,20
dataset/preprocessed/test-data/entity_linking/11,"By using these techniques , we are able to greatly improve the coverage of supervised WSD systems , nearly eliminating the need fora backoff strategy that is currently used inmost systems when dealing with a word which has never been observed in the training data .",25,0,46
dataset/preprocessed/test-data/entity_linking/11,"We evaluate our method on a state of the art WSD neural network , based on pretrained contextualized word vector representations , and we present results that significantly outperform the state of the art on every standard WSD evaluation task .",26,0,41
dataset/preprocessed/test-data/entity_linking/11,"Finally , we provide a documented tool for training and evaluating neural WSD models , as well as our best pretrained model in a dedicated GitHub repository 1 .",27,0,29
dataset/preprocessed/test-data/entity_linking/11,"In WSD , several recent advances have been made in the creation of new neural architectures for supervised models and the integration of knowledge into these systems .",29,0,28
dataset/preprocessed/test-data/entity_linking/11,Multiple works also exploit the idea of grouping together related senses .,30,0,12
dataset/preprocessed/test-data/entity_linking/11,"In this section , we give an overview of these works .",31,0,12
dataset/preprocessed/test-data/entity_linking/11,WSD Based on a Language Model,32,0,6
dataset/preprocessed/test-data/entity_linking/11,"In this type of approach , that has been initiated by and reimplemented by , the central component is a neural language model able to predict a word with consideration for the words surrounding it , thanks to a recurrent neural network trained on a massive quantity of unannotated data .",33,0,51
dataset/preprocessed/test-data/entity_linking/11,"Once the language model is trained , it is used to produce sense vectors that result from averaging the word vectors predicted by the language model at all positions of words annotated with the given sense .",34,0,37
dataset/preprocessed/test-data/entity_linking/11,"At test time , the language model is used to predict a vector according to the surrounding context , 1 https://github.com/getalp/disambiguate and the sense closest to the predicted vector is assigned to each word .",35,0,35
dataset/preprocessed/test-data/entity_linking/11,These systems have the advantage of bypassing the problem of the lack of sense annotated data by concentrating the power of abstraction offered by recurrent neural networks on a good quality language model trained in an unsupervised manner .,36,0,39
dataset/preprocessed/test-data/entity_linking/11,"However , sense annotated corpora are still indispensable to contruct the sense vectors .",37,0,14
dataset/preprocessed/test-data/entity_linking/11,WSD Based on a Softmax Classifier,38,0,6
dataset/preprocessed/test-data/entity_linking/11,"In these systems , the main neural network directly classifies and attributes a sense to each input word through a probability distribution computed by a softmax function .",39,0,28
dataset/preprocessed/test-data/entity_linking/11,"Sense annotations are simply seen as tags put on every word , like a POS - tagging task for instance .",40,0,21
dataset/preprocessed/test-data/entity_linking/11,We can distinguish two separate branches of these types of neural networks :,41,0,13
dataset/preprocessed/test-data/entity_linking/11,"1 . Those in which we have several distinct and token - specific neural networks ( or classifiers ) for every different word in the dictionary ) , each of them being able to manage a particular word and its particular senses .",42,0,43
dataset/preprocessed/test-data/entity_linking/11,"For instance , one of the classifiers is specialized in choosing between the four possible senses of the noun "" mouse "" .",43,0,23
dataset/preprocessed/test-data/entity_linking/11,"This type of approach is particularly fitted for the lexical sample tasks , where a small and finite set of very ambiguous words have to be sense annotated in several contexts , but it can also be used in all - words word sense disambiguation tasks .",44,0,47
dataset/preprocessed/test-data/entity_linking/11,Those in which we have a larger and general neural network that is able to manage all different words and assign a sense in the set of all existing sense in the dictionary used .,46,0,35
dataset/preprocessed/test-data/entity_linking/11,"The advantage of the first branch of approaches is that in order to disambiguate a word , limiting our choice to one of its possible senses is computationally much easier than searching through all the senses of all words .",47,0,40
dataset/preprocessed/test-data/entity_linking/11,"To put things in perspective , the average number of senses of polysemous words in WordNet is approximately 3 , whereas the total number of senses considering all words is 206 941 .",48,0,33
dataset/preprocessed/test-data/entity_linking/11,"The second approach , however , has an interesting property : all senses reside in the same vector space and hence share features in the hidden layers of the network .",49,0,31
dataset/preprocessed/test-data/entity_linking/14,Knowledge - based Word Sense Disambiguation using Topic Models,2,1,9
dataset/preprocessed/test-data/entity_linking/14,Disambiguation is an open problem in Natural Language Processing which is particularly challenging and useful in the unsupervised setting where all the words in any given text need to be disambiguated without using any labeled data .,4,1,37
dataset/preprocessed/test-data/entity_linking/14,Typically WSD systems use the sentence or a small window of words around the target word as the context for disambiguation because their computational complexity scales exponentially with the size of the context .,5,1,34
dataset/preprocessed/test-data/entity_linking/14,"In this paper , we leverage the formalism of topic model to design a WSD system that scales linearly with the number of words in the context .",6,0,28
dataset/preprocessed/test-data/entity_linking/14,"As a result , our system is able to utilize the whole document as the context for a word to be disambiguated .",7,0,23
dataset/preprocessed/test-data/entity_linking/14,The proposed method is a variant of Latent Dirichlet Allocation in which the topic proportions for a document are replaced by synset proportions .,8,0,24
dataset/preprocessed/test-data/entity_linking/14,We further utilize the information in the WordNet by assigning a non-uniform prior to synset distribution over words and a logistic - normal prior for document distribution over synsets .,9,0,30
dataset/preprocessed/test-data/entity_linking/14,"We evaluate the proposed method on Senseval - 2 , Senseval - 3 , SemEval - 2007 , SemEval-2013 and SemEval - 2015 English All - Word WSD datasets and show that it outperforms the state - of - the - art unsupervised knowledge - based WSD system by a significant margin .",10,0,53
dataset/preprocessed/test-data/entity_linking/14,Word Sense Disambiguation ( WSD ) is the task of mapping an ambiguous word in a given context to its correct meaning .,12,1,23
dataset/preprocessed/test-data/entity_linking/14,"WSD is an important problem in natural language processing ( NLP ) , both in its own right and as a steppingstone to more advanced tasks such as machine translation , information extraction and retrieval , and question answering .",13,0,40
dataset/preprocessed/test-data/entity_linking/14,"WSD , being AI - complete ( Navigli 2009 ) , is still an open problem after over two decades of research .",14,0,23
dataset/preprocessed/test-data/entity_linking/14,"Following Navigli ( 2009 ) , we can roughly distinguish between supervised and knowledge - based ( unsupervised ) approaches .",15,0,21
dataset/preprocessed/test-data/entity_linking/14,Supervised methods require senseannotated training data and are suitable for lexical sample WSD tasks where systems are required to disambiguate a restricted set of target words .,16,0,27
dataset/preprocessed/test-data/entity_linking/14,"However , the performance of supervised systems is limited in the all - word WSD tasks as labeled data for the full lexicon is sparse and difficult to obtain .",17,0,30
dataset/preprocessed/test-data/entity_linking/14,"As the all - word WSD task is more challenging and has , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",18,0,24
dataset/preprocessed/test-data/entity_linking/14,All rights reserved .,19,0,4
dataset/preprocessed/test-data/entity_linking/14,"more practical applications , there has been significant interest in developing unsupervised knowledge - based systems .",20,0,17
dataset/preprocessed/test-data/entity_linking/14,These systems only require an external knowledge source ( such as WordNet ) but no labeled training data .,21,0,19
dataset/preprocessed/test-data/entity_linking/14,"In this paper , we propose a novel knowledge - based WSD algorithm for the all - word WSD task , which utilizes the whole document as the context for a word , rather than just the current sentence used by most WSD systems .",22,0,45
dataset/preprocessed/test-data/entity_linking/14,"In order to model the whole document for WSD , we leverage the formalism of topic models , especially Latent Dirichlet Allocation ( LDA ) .",23,0,26
dataset/preprocessed/test-data/entity_linking/14,Our method is a variant of LDA in which the topic proportions for a document are replaced by synset proportions for a document .,24,0,24
dataset/preprocessed/test-data/entity_linking/14,We use a non-uniform prior for the synset distribution over words to model the frequency of words within a synset .,25,0,21
dataset/preprocessed/test-data/entity_linking/14,"Furthermore , we also model the relationships between synsets by using a logisticnormal prior for drawing the synset proportions of the document .",26,0,23
dataset/preprocessed/test-data/entity_linking/14,"This makes our model similar to the correlated topic model , with the difference that our priors are not learned but fixed .",27,0,23
dataset/preprocessed/test-data/entity_linking/14,"In particular , the values of these priors are determined using the knowledge from WordNet .",28,0,16
dataset/preprocessed/test-data/entity_linking/14,"We evaluate our system on a set of five benchmark datasets , show that the proposed model outperforms stateof - the - art knowledge - based WSD system .",29,0,29
dataset/preprocessed/test-data/entity_linking/14,Lesk ) is a classical knowledge - based WSD algorithm which disambiguates a word by selecting a sense whose definition overlaps the most with the words in its context .,31,0,30
dataset/preprocessed/test-data/entity_linking/14,Many subsequent knowledge - based systems are based on the Lesk algorithm .,32,0,13
dataset/preprocessed/test-data/entity_linking/14,Lesk by utilizing the definitions of words in the context and weighing the words by term frequency - inverse document frequency ( tf - idf ) .,34,0,27
dataset/preprocessed/test-data/entity_linking/14,Lesk by using word embeddings to calculate the similarity between sense definitions and words in the context .,36,0,18
dataset/preprocessed/test-data/entity_linking/14,The above methods only use the words in the context for disambiguating the target word .,37,0,16
dataset/preprocessed/test-data/entity_linking/14,"However , Chaplot , show that sense of a word depends on not just the words in the context but also on their senses .",38,0,25
dataset/preprocessed/test-data/entity_linking/14,"Since the senses of the words in the context are also unknown , they need to be optimized jointly .",39,0,20
dataset/preprocessed/test-data/entity_linking/14,"In the past decade , many graph - based unsupervised WSD methods have been developed which typically leverage the underlying structure of Lexical Knowledge Base such as Word - Net and apply well - known graph - based techniques to efficiently select the best possible combination of senses in the context .",40,0,52
dataset/preprocessed/test-data/entity_linking/14,and build a subgraph of the entire lexicon containing vertices useful for disambiguation and then use graph connectivity measures to determine the most appropriate senses .,41,0,26
dataset/preprocessed/test-data/entity_linking/14,"and construct a sentence - wise graph , where , for each word every possible sense forms a vertex .",42,0,20
dataset/preprocessed/test-data/entity_linking/14,Then graph - based iterative ranking and centrality algorithms are applied to find most probable sense .,43,0,17
dataset/preprocessed/test-data/entity_linking/14,"More recently , presented an unsupervised WSD approach based on personalized page rank over the graphs generated using Word Net .",44,0,21
dataset/preprocessed/test-data/entity_linking/14,The graph is created by adding content words to the Word - Net graph and connecting them to the synsets in which they appear in as strings .,45,0,28
dataset/preprocessed/test-data/entity_linking/14,"Then , the Personalized PageRank ( PPR ) algorithm is used to compute relative weights of the synsets according to their relative structural importance and consequently , for each content word , the synset with the highest PPR weight is chosen as the correct sense .",46,0,46
dataset/preprocessed/test-data/entity_linking/14,"Chaplot , Bhattacharyya , and Paranjape ( 2015 ) present a graph - based unsupervised WSD system which maximizes the total joint probability of all the senses in the context by modeling the WSD problem as a Markov Random Field constructed using the WordNet and a dependency parser and using a Maximum A Posteriori ( MAP ) Query for inference .",47,0,61
dataset/preprocessed/test-data/entity_linking/14,Babelfy is another graph - based approach which unifies WSD and Entity Linking .,48,0,14
dataset/preprocessed/test-data/entity_linking/14,"It performs WSD by performing random walks with restart over BabelNet , which is a semantic network integrating WordNet with various knowledge resources .",49,0,24
dataset/preprocessed/test-data/entity_linking/0,Deep Joint Entity Disambiguation with Local Neural Attention,2,1,8
dataset/preprocessed/test-data/entity_linking/0,"We propose a novel deep learning model for joint document - level entity disambiguation , which leverages learned neural representations .",4,0,21
dataset/preprocessed/test-data/entity_linking/0,"Key components are entity embeddings , a neural attention mechanism over local context windows , and a differentiable joint inference stage for disambiguation .",5,0,24
dataset/preprocessed/test-data/entity_linking/0,Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention - entity maps .,6,0,23
dataset/preprocessed/test-data/entity_linking/0,Extensive experiments show that we are able to obtain competitive or stateof - the - art accuracy at moderate computational costs .,7,0,22
dataset/preprocessed/test-data/entity_linking/0,Entity disambiguation ( ED ) is an important stage in text understanding which automatically resolves references to entities in a given knowledge base ( KB ) .,9,1,27
dataset/preprocessed/test-data/entity_linking/0,This task is challenging due to the inherent ambiguity between surface form mentions such as names and the entities they refer to .,10,0,23
dataset/preprocessed/test-data/entity_linking/0,This many - to - many ambiguity can often be captured partially by name- entity co-occurrence counts extracted from entity - linked corpora .,11,0,24
dataset/preprocessed/test-data/entity_linking/0,"ED research has largely focused on two types of contextual information for disambiguation : local information based on words that occur in a context window around an entity mention , and , global information , exploiting document - level coherence of the referenced entities .",12,1,45
dataset/preprocessed/test-data/entity_linking/0,"Many stateof - the - art methods aim to combine the benefits of both , which is also the philosophy we follow in this paper .",13,0,26
dataset/preprocessed/test-data/entity_linking/0,What is specific to our approach is that we use embeddings of entities as a common representation to assess local as well as global evidence .,14,0,26
dataset/preprocessed/test-data/entity_linking/0,"In recent years , many text and language understanding tasks have been advanced by neural network architectures .",15,0,18
dataset/preprocessed/test-data/entity_linking/0,"However , despite recent work , competitive ED systems still largely employ manually designed features .",16,0,16
dataset/preprocessed/test-data/entity_linking/0,Such features often rely on domain knowledge and may fail to capture all relevant statistical dependencies and interactions .,17,0,19
dataset/preprocessed/test-data/entity_linking/0,The explicit goal of our work is to use deep learning in order to learn basic features and their combinations from scratch .,18,0,23
dataset/preprocessed/test-data/entity_linking/0,"To the best of our knowledge , our approach is the first to carryout this program with full rigor .",19,0,20
dataset/preprocessed/test-data/entity_linking/0,Contributions and Related Work,20,0,4
dataset/preprocessed/test-data/entity_linking/0,"There is avast prior research on entity disambiguation , highlighted by .",21,0,12
dataset/preprocessed/test-data/entity_linking/0,We will focus hereon a discussion of our main contributions in relation to prior work .,22,0,16
dataset/preprocessed/test-data/entity_linking/0,Entity Embeddings .,23,0,3
dataset/preprocessed/test-data/entity_linking/0,"We have developed a simple , yet effective method to embed entities and words in a common vector space .",24,0,20
dataset/preprocessed/test-data/entity_linking/0,"This follows the popular line of work on word embeddings , e.g. , which was recently extended to entities and ED by .",25,0,23
dataset/preprocessed/test-data/entity_linking/0,"In contrast to the above methods that require data about entity - entity co-occurrences which often suffers from sparsity , we rather bootstrap entity embeddings from their canonical entity pages and local context of their hyperlink annotations .",26,0,38
dataset/preprocessed/test-data/entity_linking/0,This allows for more efficient training and alleviates the need to compile co-linking statistics .,27,0,15
dataset/preprocessed/test-data/entity_linking/0,"These vector representations area key component to avoid hand - engineered features , multiple disambiguation steps , or the need for additional ad hoc heuristics when solving the ED task .",28,0,31
dataset/preprocessed/test-data/entity_linking/0,Context Attention .,29,0,3
dataset/preprocessed/test-data/entity_linking/0,We present a novel attention mechanism for local ED .,30,0,10
dataset/preprocessed/test-data/entity_linking/0,"Inspired by mem-ory networks of and insights of , our model deploys attention to select words that are informative for the disambiguation decision .",31,0,24
dataset/preprocessed/test-data/entity_linking/0,A learned combination of the resulting context - based entity scores and a mention - entity prior yields the final local scores .,32,0,23
dataset/preprocessed/test-data/entity_linking/0,"Our local model achieves better accuracy than the local probabilistic model of , as well as the feature - engineered local model of .",33,0,24
dataset/preprocessed/test-data/entity_linking/0,"As an added benefit , our model has a smaller memory footprint and it 's very fast for both training and testing .",34,0,23
dataset/preprocessed/test-data/entity_linking/0,There have been other deep learning approaches to define local context models for ED .,35,0,15
dataset/preprocessed/test-data/entity_linking/0,"For instance use convolutional neural networks ( CNNs ) and stacked denoising auto - encoders , respectively , to learn representations of textual documents and canonical entity pages .",36,0,29
dataset/preprocessed/test-data/entity_linking/0,Entities for each mention are locally scored based on cosine similarity with the respective document embedding .,37,0,17
dataset/preprocessed/test-data/entity_linking/0,"Ina similar local setting , embed mentions , their immediate contexts and their candidate entities using word embeddings and CNNs .",38,0,21
dataset/preprocessed/test-data/entity_linking/0,"However , their entity representations are restrictively built from entity titles and entity categories only .",39,0,16
dataset/preprocessed/test-data/entity_linking/0,"Unfortunately , the above models are rather ' blackbox ' ( as opposed to ours which reveals the attention focus ) and were never extended to perform joint document disambiguation .",40,0,31
dataset/preprocessed/test-data/entity_linking/0,Collective Disambiguation .,41,0,3
dataset/preprocessed/test-data/entity_linking/0,"Last , a novel deep learning architecture for global ED is proposed .",42,0,13
dataset/preprocessed/test-data/entity_linking/0,"Mentions in a document are resolved jointly , using a conditional random field with parametrized potentials .",43,0,17
dataset/preprocessed/test-data/entity_linking/0,We suggest to learn the latter by casting loopy belief propagation ( LBP ) as a rolled - out deep network .,44,0,22
dataset/preprocessed/test-data/entity_linking/0,"This is inspired by similar approaches in computer vision , e.g. , and allows us to backpropagate through the ( truncated ) message passing , thereby optimizing the CRF potentials to work well in conjunction with the inference scheme .",45,0,40
dataset/preprocessed/test-data/entity_linking/0,Our model is thus trained end - to - end with the exception of the pre-trained word and entity embeddings .,46,0,21
dataset/preprocessed/test-data/entity_linking/0,"Previous work has investigated different approximation techniques , including : random graph walks , personalized PageRank , intermention voting , graph pruning , integer linear programming , or ranking SVMs .",47,0,31
dataset/preprocessed/test-data/entity_linking/0,Mostly connected to our approach is where LBP is used for inference ( but not learning ) in a probabilistic graphical model and where a single round of message passing with attention is performed .,48,0,35
dataset/preprocessed/test-data/entity_linking/0,"To our knowledge , we are one of the first to investigate differentiable message passing for NLP problems .",49,0,19
dataset/preprocessed/test-data/entity_linking/12,Incorporating Glosses into Neural Word Sense Disambiguation,2,1,7
dataset/preprocessed/test-data/entity_linking/12,Word Sense Disambiguation ( WSD ) aims to identify the correct meaning of polysemous words in the particular context .,4,1,20
dataset/preprocessed/test-data/entity_linking/12,Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge - based methods .,5,1,21
dataset/preprocessed/test-data/entity_linking/12,"However , previous neural networks for WSD always rely on massive labeled data ( context ) , ignoring lexical resources like glosses ( sense definitions ) .",6,0,27
dataset/preprocessed/test-data/entity_linking/12,"In this paper , we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge .",7,0,32
dataset/preprocessed/test-data/entity_linking/12,"Therefore , we propose GAS : a gloss - augmented WSD neural network which jointly encodes the context and glosses of the target word .",8,0,25
dataset/preprocessed/test-data/entity_linking/12,"GAS models the semantic relationship between the context and the gloss in an improved memory network framework , which breaks the barriers of the previous supervised methods and knowledge - based methods .",9,0,33
dataset/preprocessed/test-data/entity_linking/12,We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information .,10,0,21
dataset/preprocessed/test-data/entity_linking/12,The experimental results show that our model outperforms the state - of - theart systems on several English all - words WSD datasets .,11,0,24
dataset/preprocessed/test-data/entity_linking/12,Word Sense Disambiguation ( WSD ) is a fundamental task and long - standing challenge in Natural Language Processing ( NLP ) .,13,0,23
dataset/preprocessed/test-data/entity_linking/12,There are several lines of research on WSD .,14,0,9
dataset/preprocessed/test-data/entity_linking/12,Knowledge - based methods focus on exploiting lexical resources to infer the senses of word in the context .,15,0,19
dataset/preprocessed/test-data/entity_linking/12,Supervised methods usually train multiple classifiers with manual designed features .,16,0,11
dataset/preprocessed/test-data/entity_linking/12,"Although supervised methods can achieve the state - of - the - art performance , there are still two major challenges .",17,0,22
dataset/preprocessed/test-data/entity_linking/12,"Firstly , supervised methods usually train a dedicated classifier for each word individually ( often called word expert ) .",18,0,20
dataset/preprocessed/test-data/entity_linking/12,So it can not easily scale up to all - words WSD task which requires to disambiguate all the polysemous word in texts,19,0,23
dataset/preprocessed/test-data/entity_linking/12,"Recent neural - based methods solve this problem by building a unified model for all the polysemous words , but they still ca n't beat the best word expert system .",21,0,31
dataset/preprocessed/test-data/entity_linking/12,"Secondly , all the neural - based methods always only consider the local context of the target word , ignoring the lexical resources like which are widely used in the knowledge - based methods .",22,0,35
dataset/preprocessed/test-data/entity_linking/12,"The gloss , which extensionally defines a word sense meaning , plays a key role in the well - known Lesk algorithm .",23,0,23
dataset/preprocessed/test-data/entity_linking/12,"Recent studies have shown that enriching gloss information through its semantic relations can greatly improve the accuracy of To this end , our goal is to incorporate the gloss information into a unified neural network for all of the polysemous words .",24,0,42
dataset/preprocessed/test-data/entity_linking/12,We further consider extending the original gloss through its semantic relations in our framework .,25,0,15
dataset/preprocessed/test-data/entity_linking/12,"As shown in , the glosses of hypernyms and hyponyms can enrich the original gloss information as well as help to build better a sense representation .",26,0,27
dataset/preprocessed/test-data/entity_linking/12,"Therefore , we integrate not only the original gloss but also the related glosses of hypernyms and hyponyms into the neural network .",27,0,23
dataset/preprocessed/test-data/entity_linking/12,"The hypernym ( green node ) and hyponyms ( blue nodes ) for the 2nd sense bed 2 of bed , which means a plot of ground in which plants are growing , rather than the bed for sleeping in .",28,0,41
dataset/preprocessed/test-data/entity_linking/12,"The figure shows that bed 2 is a kind of plot 2 , and bed 2 includes flowerbed 1 , seedbed 1 , etc .",29,0,25
dataset/preprocessed/test-data/entity_linking/12,"In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .",30,0,27
dataset/preprocessed/test-data/entity_linking/12,GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module .,31,0,26
dataset/preprocessed/test-data/entity_linking/12,"In order to measure the inner relationship between glosses and context more accurately , we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms .",32,0,33
dataset/preprocessed/test-data/entity_linking/12,The main contributions of this paper are listed as follows :,33,0,11
dataset/preprocessed/test-data/entity_linking/12,"To the best of our knowledge , our model is the first to incorporate the glosses into an end - to - end neural WSD model .",34,0,27
dataset/preprocessed/test-data/entity_linking/12,"In this way , our model can benefit from not only massive labeled data but also rich lexical knowledge .",35,0,20
dataset/preprocessed/test-data/entity_linking/12,"In order to model semantic relationship of context and glosses , we propose a glossaugmented neural network ( GAS ) in an improved memory network paradigm .",36,0,27
dataset/preprocessed/test-data/entity_linking/12,We further expand the gloss through its semantic relations to enrich the gloss information and better infer the context .,37,0,20
dataset/preprocessed/test-data/entity_linking/12,We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet .,38,0,23
dataset/preprocessed/test-data/entity_linking/12,The experimental results on several English all - words WSD benchmark datasets show that our model outperforms the state - of - theart systems .,39,0,25
dataset/preprocessed/test-data/entity_linking/12,"Knowledge - based , supervised and neural - based methods have already been applied to WSD task ) .",41,0,19
dataset/preprocessed/test-data/entity_linking/12,Knowledge - based WSD methods mainly exploit two kinds of knowledge to disambiguate polysemous words :,42,0,16
dataset/preprocessed/test-data/entity_linking/12,"1 ) The gloss , which defines a word sense meaning , is mainly used in Lesk algorithm and its variants .",43,0,22
dataset/preprocessed/test-data/entity_linking/12,"2 ) The structure of the semantic network , whose nodes are synsets 2 and edges are semantic relations , is mainly used in graph - based algorithms .",44,0,29
dataset/preprocessed/test-data/entity_linking/12,Supervised methods usually involve each target word as a separate classification problem ( often called word expert ) and train classifiers based on manual designed features .,45,0,27
dataset/preprocessed/test-data/entity_linking/12,"Although word expert supervised WSD methods perform best in terms of accuray , they are less flexible than knowledge - based methods in the allwords WSD task .",46,0,28
dataset/preprocessed/test-data/entity_linking/12,"To deal with this problem , recent neural - based methods aim to build a unified classifier which shares parameters among all the polysemous words .",47,0,26
dataset/preprocessed/test-data/entity_linking/12,leverages the bidirectional long short - term memory network which shares model parameters among all the polysemous words .,48,0,19
dataset/preprocessed/test-data/entity_linking/12,transfers the WSD problem into a neural sequence labeling task .,49,0,11
dataset/preprocessed/test-data/entity_linking/2,Neural Sequence Learning Models for Word Sense Disambiguation,2,1,8
dataset/preprocessed/test-data/entity_linking/2,Word Sense Disambiguation models exist in many flavors .,4,0,9
dataset/preprocessed/test-data/entity_linking/2,"Even though supervised ones tend to perform best in terms of accuracy , they often lose ground to more flexible knowledge - based solutions , which do not require training by a word expert for every disambiguation target .",5,0,39
dataset/preprocessed/test-data/entity_linking/2,"To bridge this gap we adopt a different perspective and rely on sequence learning to frame the disambiguation problem : we propose and study in depth a series of end - to - end neural architectures directly tailored to the task , from bidirectional Long Short - Term Memory to encoder - decoder models .",6,0,55
dataset/preprocessed/test-data/entity_linking/2,"Our extensive evaluation over standard benchmarks and in multiple languages shows that sequence learning enables more versatile all - words models that consistently lead to state - of - the - art results , even against word experts with engineered features .",7,0,42
dataset/preprocessed/test-data/entity_linking/2,"As one of the long - standing challenges in Natural Language Processing ( NLP ) , Word Sense Disambiguation , WSD ) has received considerable attention over recent years .",9,1,30
dataset/preprocessed/test-data/entity_linking/2,"Indeed , by dealing with lexical ambiguity an effective WSD model brings numerous benefits to a variety of downstream tasks and applications , from Information Retrieval and Extraction to Machine Translation .",10,0,32
dataset/preprocessed/test-data/entity_linking/2,"Recently , WSD has also been leveraged to build continuous vector representations for word senses .",11,0,16
dataset/preprocessed/test-data/entity_linking/2,"Inasmuch as WSD is described as the task of associating words in context with the most suitable entries in a pre-defined sense inventory , the majority of WSD approaches to date can be grouped into two main categories : supervised ( or semisupervised ) and knowledge - based .",12,0,49
dataset/preprocessed/test-data/entity_linking/2,"Supervised models have been shown to consistently outperform knowledge - based ones in all standard benchmarks , at the expense , however , of harder training and limited flexibility .",13,0,30
dataset/preprocessed/test-data/entity_linking/2,"First of all , obtaining reliable sense - annotated corpora is highly expensive and especially difficult when non-expert annotators are involved ( de Lacalle and , and as a consequence approaches based on unlabeled data and semisupervised learning are emerging .",14,0,41
dataset/preprocessed/test-data/entity_linking/2,"Apart from the shortage of training data , a crucial limitation of current supervised approaches is that a dedicated classifier ( word expert ) needs to be trained for every target lemma , making them less flexible and hampering their use within endto - end applications .",15,0,47
dataset/preprocessed/test-data/entity_linking/2,"In contrast , knowledge - based systems do not require sense - annotated data and often draw upon the structural properties of lexicosemantic resources .",16,0,25
dataset/preprocessed/test-data/entity_linking/2,"Such systems construct a model based only on the underlying resource , which is then able to handle multiple target words at the same time and disambiguate them jointly , whereas word experts are forced to treat each disambiguation target in isolation .",17,0,43
dataset/preprocessed/test-data/entity_linking/2,"In this paper our focus is on supervised WSD , but we depart from previous approaches and adopt a different perspective on the task : instead of framing a separate classification problem for each given word , we aim at modeling the joint disambiguation of the target text as a whole in terms of a sequence labeling problem .",18,0,59
dataset/preprocessed/test-data/entity_linking/2,"From this standpoint , WSD amounts to translating a sequence of words into a sequence of potentially sense - tagged tokens .",19,0,22
dataset/preprocessed/test-data/entity_linking/2,"With this in mind , we design , analyze and compare experimentally various neural architectures of different complexities , ranging from a single bidirectional Long Short - Term Memory to a sequence - tosequence approach .",20,0,36
dataset/preprocessed/test-data/entity_linking/2,"Each architecture reflects a particular way of modeling the disambiguation problem , but they all share some key features that set them apart from previous supervised approaches to WSD : they are trained end - to - end from sense - annotated text to sense labels , and learn a single all - words model from the training data , without fine tuning or explicit engineering of local features .",21,0,70
dataset/preprocessed/test-data/entity_linking/2,The contributions of this paper are twofold .,22,0,8
dataset/preprocessed/test-data/entity_linking/2,"First , we show that neural sequence learning represents a novel and effective alternative to the traditional way of modeling supervised WSD , enabling a single all - words model to compete with a pool of word experts and achieve state - of - the - art results , while also being easier to train , arguably more versatile to use within downstream applications , and directly adaptable to different languages without requiring additional sense - annotated data ( as we show in Section 6.2 ) ; second , we carryout an extensive experimental evaluation where we compare various neural architectures designed for the task ( and somehow left underinvestigated in previous literature ) , exploring different configurations and training procedures , and analyzing their strengths and weaknesses on all the standard benchmarks for all - words WSD .",23,0,139
dataset/preprocessed/test-data/entity_linking/2,The literature on WSD is broad and comprehensive : new models are continuously being developed and tested over a wide variety of standard benchmarks .,25,0,25
dataset/preprocessed/test-data/entity_linking/2,"Moreover , the field has been explored in depth from different angles by means of extensive empirical studies and evaluation frameworks ( Pilehvar and .",26,0,25
dataset/preprocessed/test-data/entity_linking/2,"As regards supervised WSD , traditional approaches are generally based on extracting local features from the words surrounding the target , and then training a classifier for each target lemma .",27,0,31
dataset/preprocessed/test-data/entity_linking/2,"In their latest developments , these models include more complex features based on word embeddings .",28,0,16
dataset/preprocessed/test-data/entity_linking/2,The recent upsurge of neural networks has also contributed to fueling WSD research : rely on a powerful neural language model to obtain a latent representation for the whole sentence containing a target word w ; their instance - based system then compares that representation with those of example sentences annotated with the candidate meanings of w .,29,0,58
dataset/preprocessed/test-data/entity_linking/2,"Similarly , Context2 Vec makes use of a bidirectional LSTM architecture trained on an unlabeled corpus and learns a context vector for each sense annotation in the training data .",30,0,30
dataset/preprocessed/test-data/entity_linking/2,"Finally , present a supervised classifier based on bidirectional LSTM for the lexical sample task .",31,0,16
dataset/preprocessed/test-data/entity_linking/2,All these contributions have shown that supervised neural models can achieve state - of - the - art performances without taking advantage of external resources or language - specific features .,32,0,31
dataset/preprocessed/test-data/entity_linking/2,"However , they all consider each target word as a separate classification problem and , to the best of our knowledge , very few attempts have been made to disambiguate a text jointly using sequence learning .",33,0,37
dataset/preprocessed/test-data/entity_linking/2,"Sequence learning , especially using LSTM , has become a well - established standard in numerous NLP tasks .",34,0,19
dataset/preprocessed/test-data/entity_linking/2,"In particular , sequence - to - sequence models have grown increasingly popular and are used extensively in , e.g. , Machine Translation , Sentence Representation , Syntactic Parsing , Conversation Modeling , Morphological Inflection and Text Summarization .",35,0,39
dataset/preprocessed/test-data/entity_linking/2,"In line with this trend , we focus on the ( so far unexplored ) context of supervised WSD , and investigate state - of - the - art all - words approaches that are based on neural sequence learning and capable of disambiguating all target content words within an input text , a key feature in several knowledge - based approaches .",36,0,63
dataset/preprocessed/test-data/entity_linking/2,y 3 y 2 y 1 y 4 y 5 : Bidirectional LSTM sequence labeling architecture for WSD ( 2 hidden layers ) .,37,0,24
dataset/preprocessed/test-data/entity_linking/2,We use the notation of Navigli ( 2009 ) for word senses : w i p is the i - th sense of w with part of speech p.,38,0,29
dataset/preprocessed/test-data/entity_linking/2,Sequence Learning for Word Sense Disambiguation,39,0,6
dataset/preprocessed/test-data/entity_linking/2,In this section we define WSD in terms of a sequence learning problem .,40,0,14
dataset/preprocessed/test-data/entity_linking/2,"While in its classical formulation ) WSD is viewed as a classification problem fora given word win context , with word senses of w being the class labels , here we consider a variable - length sequence of input symbols x = x 1 , ... , x T and we aim at predicting a sequence of output symbols y = y 1 , ... , y T .",41,0,69
dataset/preprocessed/test-data/entity_linking/2,1 Input symbols are word tokens drawn from a given vocabulary V .,42,0,13
dataset/preprocessed/test-data/entity_linking/2,"2 Output symbols are either drawn from a pre-defined sense inventory S ( if the corresponding input symbols are open - class content words , i.e. , nouns , verbs , adjectives or adverbs ) , or from the same input vocabulary V ( e.g. , if the corresponding input symbols are function words , like prepositions or determiners ) .",43,0,61
dataset/preprocessed/test-data/entity_linking/2,"Hence , we can define a WSD model in terms of a function that maps sequences of symbols",44,0,18
dataset/preprocessed/test-data/entity_linking/2,"Here all - words WSD is no longer broken down into a series of distinct and separate classification tasks ( one per target word ) but rather treated directly at the sequence level , with a single model handling all disambiguation decisions .",45,0,43
dataset/preprocessed/test-data/entity_linking/2,"In what follows , we describe three different models for accomplishing this : a traditional LSTMbased model ( Section 3.1 ) , a variant that incorporates an attention mechanism ( Section 3.2 ) , and an encoder - decoder architecture ( Section 3.3 ) .",46,0,45
dataset/preprocessed/test-data/entity_linking/2,"In general x and y might have different lengths , e.g. , if x contains a multi-word expression ( European Union ) which is mapped to a unique sense identifier ( European Union 1 n ) .",47,0,37
dataset/preprocessed/test-data/entity_linking/2,2 V generalizes traditional vocabularies used in WSD and includes both word lemmas and inflected forms .,48,0,17
dataset/preprocessed/test-data/entity_linking/4,Learning Distributed Representations of Texts and Entities from Knowledge Base,2,1,10
dataset/preprocessed/test-data/entity_linking/4,We describe a neural network model that jointly learns distributed representations of texts and knowledge base ( KB ) entities .,4,0,21
dataset/preprocessed/test-data/entity_linking/4,"Given a text in the KB , we train our proposed model to predict entities that are relevant to the text .",5,0,22
dataset/preprocessed/test-data/entity_linking/4,Our model is designed to be generic with the ability to address various NLP tasks with ease .,6,0,18
dataset/preprocessed/test-data/entity_linking/4,We train the model using a large corpus of texts and their entity annotations extracted from Wikipedia .,7,0,18
dataset/preprocessed/test-data/entity_linking/4,"We evaluated the model on three important NLP tasks ( i.e. , sentence textual similarity , entity linking , and factoid question answering ) involving both unsupervised and supervised settings .",8,0,31
dataset/preprocessed/test-data/entity_linking/4,"As a result , we achieved state - of - the - art results on all three of these tasks .",9,0,21
dataset/preprocessed/test-data/entity_linking/4,Our code and trained models are publicly available for further academic research .,10,0,13
dataset/preprocessed/test-data/entity_linking/4,Our model Skip - gram Europe Eastern Europe ( 0.67 ) Western Europe ( 0.66 ) Central Europe ( 0.64 ) Asia ( 0.64 ) North America ( 0.64 ) Asia ( 0.85 ) Western Europe ( 0.78 ) North America ( 0.76 ) Central Europe ( 0.75 ) Americas ( 0.73 ) Golf Golf course ( 0.76 ) PGA Tour ( 0.74 ) LPGA ( 0.74 ) Professional golfer ( 0.73 ) U.S. Open ( 0.71 ) Tennis ( 0.74 ) LPGA ( 0.72 ) PGA Tour ( 0.69 ) Golf course ( 0.68 ) Nicklaus Design ( 0.66 ) Tea Coffee ( 0.82 ) Green tea ( 0.81 ),12,0,111
dataset/preprocessed/test-data/entity_linking/4,Black tea ( 0.80 ) Camellia sinensis ( 0.78 ) Spice ( 0.76 ) Coffee ( 0.78 ) Green tea ( 0.76 ) Black tea ( 0.75 ) Camellia sinensis ( 0.74 ) Spice ( 0.73 ) Smartphone Tablet computer ( 0.93 ) Mobile device ( 0.89 ) Personal digital assistant ( 0.88 ) Android ( operating system ) ( 0.86 ) iPhone ( 0.85 ) Tablet computer ( 0.91 ) Personal digital assistant ( 0.84 ) Mobile device ( 0.84 ) Android ( operating system ) ( 0.82 ) Feature phone ( 0.82 ) Scarlett Johansson Kirsten Dunst ( 0.85 ) Anne Hathaway ( 0.85 ) Cameron Diaz ( 0.85 ) Natalie Portman ( 0.85 ) Jessica Biel ( 0.84 ) Anne Hathaway ( 0.79 ) Natalie Portman ( 0.78 ) Kirsten Dunst ( 0.78 ) Cameron Diaz ( 0.78 ),13,0,142
dataset/preprocessed/test-data/entity_linking/4,Kate Beckinsale ( 0.77 ) The Lord of the Rings The Hobbit ( 0.85 ) J. R. R. Tolkien ( 0.84 ) The Silmarillion ( 0.81 ),14,0,27
dataset/preprocessed/test-data/entity_linking/4,The Fellowship of the Ring ( 0.80 ),15,0,8
dataset/preprocessed/test-data/entity_linking/4,The Lord of the Rings ( film series ) ( 0.78 ) The Hobbit ( 0.77 ) J. R. R. Tolkien ( 0.76 ) The Silmarillion ( 0.71 ),16,0,29
dataset/preprocessed/test-data/entity_linking/4,The Fellowship of the Ring ( 0.70 ) Elvish languages ( 0.69 ) Table,17,0,14
dataset/preprocessed/test-data/entity_linking/4,7 : Examples of top five similar entities with their cosine similarities in our learned entity representations with those of the skip - gram model .,18,0,26
dataset/preprocessed/test-data/entity_linking/4,"Methods capable of learning distributed representations of arbitrary - length texts ( i.e. , fixed - length continuous vectors that encode the semantics of texts ) , such as sentences and paragraphs , have recently attracted considerable attention ( Le and .",20,0,42
dataset/preprocessed/test-data/entity_linking/4,These methods aim to learn generic representations that are useful across domains similar to word embedding methods such as Word2vec and Glo Ve .,21,0,24
dataset/preprocessed/test-data/entity_linking/4,Another interesting approach is learning distributed representations of entities in a knowledge 1 https://github.com/studio-ousia/ntee base ( KB ) such as Wikipedia and Freebase .,22,0,24
dataset/preprocessed/test-data/entity_linking/4,These methods encode information of entities in the KB into a continuous vector space .,23,0,15
dataset/preprocessed/test-data/entity_linking/4,"They are shown to be effective for various KB - related tasks such as entity search , entity linking , and link prediction .",24,0,24
dataset/preprocessed/test-data/entity_linking/4,"In this paper , we describe a novel method to bridge these two different approaches .",25,0,16
dataset/preprocessed/test-data/entity_linking/4,"In particular , we propose Neural Text - Entity Encoder ( NTEE ) , a neural network model to jointly learn distributed representations of texts ( i.e. , sentences and paragraphs ) and KB entities .",26,1,36
dataset/preprocessed/test-data/entity_linking/4,"For every text in the KB , our model aims to predict its relevant entities , and places the text and the relevant entities close to each other in a continuous vector space .",27,0,34
dataset/preprocessed/test-data/entity_linking/4,We use humanedited entity annotations obtained from Wikipedia ( see ) as supervised data of relevant entities to the texts containing these annotations .,28,0,24
dataset/preprocessed/test-data/entity_linking/4,"Note that , KB entities have been conventionally used to model semantics of texts .",29,0,15
dataset/preprocessed/test-data/entity_linking/4,"A representative example is Explicit Semantic Analysis ( ESA ) , which represents the semantics of a text using a sparse vector space , where each dimension corresponds to the relevance score of the text to each entity .",30,0,39
dataset/preprocessed/test-data/entity_linking/4,"Essentially , ESA shows that text can be accurately represented using a small set of its relevant entities .",31,0,19
dataset/preprocessed/test-data/entity_linking/4,Based on 2 Entity annotations in Wikipedia can be viewed as supervised data of relevant entities because Wikipedia instructs its contributors to create annotations only where they are relevant in its manual :,32,0,33
dataset/preprocessed/test-data/entity_linking/4,"wiki/Wikipedia:Manual_of_Style ar Xiv:1705.02494v3 [ cs.CL ] 7 Nov 2017 this fact , we hypothesize that we can use the annotations of relevant entities as the supervised data of learning text representations .",34,0,32
dataset/preprocessed/test-data/entity_linking/4,"Furthermore , we also consider that placing texts and entities into the same vector space enables us to easily compute the similarity between texts and entities , which can be beneficial for various KB - related tasks .",35,0,38
dataset/preprocessed/test-data/entity_linking/4,"In order to test this hypothesis , we conduct three experiments involving both the unsupervised and the supervised tasks .",36,0,20
dataset/preprocessed/test-data/entity_linking/4,"First , we use standard semantic textual similarity datasets to evaluate the quality of the learned text representations of our method in an unsupervised fashion .",37,0,26
dataset/preprocessed/test-data/entity_linking/4,"As a result , our method clearly outperformed the state - of - the - art methods .",38,0,18
dataset/preprocessed/test-data/entity_linking/4,"Furthermore , to test the effectiveness of our method to perform KB - related tasks , we address the following two important problems in the supervised setting : entity linking ( EL ) and factoid question answering ( QA ) .",39,0,41
dataset/preprocessed/test-data/entity_linking/4,"In both tasks , we adopt a simple multi -layer perceptron ( MLP ) classifier with the learned representations as features .",40,0,22
dataset/preprocessed/test-data/entity_linking/4,"We tested our method using two standard datasets ( i.e. , CoNLL 2003 and TAC 2010 ) for the EL task and a popular factoid QA dataset based on the quiz bowl quiz game for the factoid QA task .",41,0,40
dataset/preprocessed/test-data/entity_linking/4,"As a result , our method outperformed recent state - of - the - art methods on both the EL and the factoid QA tasks .",42,0,26
dataset/preprocessed/test-data/entity_linking/4,"Additionally , there have also been proposed methods that map words and entities into the same continuous vector space .",43,0,20
dataset/preprocessed/test-data/entity_linking/4,"Our work differs from these works because we aim to map texts ( i.e. , sentences and paragraphs ) and entities into the same vector space .",44,0,27
dataset/preprocessed/test-data/entity_linking/4,Our contributions are summarized as follows :,45,0,7
dataset/preprocessed/test-data/entity_linking/4,We propose a neural network model that jointly learns vector representations of texts and KB entities .,46,0,17
dataset/preprocessed/test-data/entity_linking/4,We train the model using a large amount of entity annotations extracted directly from Wikipedia .,47,0,16
dataset/preprocessed/test-data/entity_linking/4,We demonstrate that our proposed representations are surprisingly effective for various NLP tasks .,48,0,14
dataset/preprocessed/test-data/entity_linking/4,"In particular , we apply the proposed model to three different NLP tasks , namely semantic textual similarity , entity linking , and factoid question answering , and achieve stateof - the - art results on all three tasks .",49,0,40
dataset/preprocessed/test-data/entity_linking/13,Word Sense Disambiguation using a Bidirectional LSTM,2,1,7
dataset/preprocessed/test-data/entity_linking/13,"In this paper we present a clean , yet effective , model for word sense disambiguation .",4,0,17
dataset/preprocessed/test-data/entity_linking/13,Our approach leverage a bidirectional long short - term memory network which is shared between all words .,5,0,18
dataset/preprocessed/test-data/entity_linking/13,This enables the model to share statistical strength and to scale well with vocabulary size .,6,0,16
dataset/preprocessed/test-data/entity_linking/13,"The model is trained end - to - end , directly from the raw text to sense labels , and makes effective use of word order .",7,0,27
dataset/preprocessed/test-data/entity_linking/13,"We evaluate our approach on two standard datasets , using identical hyperparameter settings , which are in turn tuned on a third set of held out data .",8,0,28
dataset/preprocessed/test-data/entity_linking/13,"We employ no external resources ( e.g. knowledge graphs , part - of - speech tagging , etc ) , language specific features , or hand crafted rules , but still achieve statistically equivalent results to the best state - of - the - art systems , that employ no such limitations .",9,0,53
dataset/preprocessed/test-data/entity_linking/13,Words are in general ambiguous and can have several related or unrelated meanings depending on context .,12,0,17
dataset/preprocessed/test-data/entity_linking/13,"For instance , the word rock can refer to both a stone and a music genre , but in the sentence "" Without the guitar , there would be no rock music "" the sense of rock is no longer ambiguous .",13,0,42
dataset/preprocessed/test-data/entity_linking/13,"The task of assigning a word token in a text , e.g. rock , to a well defined word sense in a lexicon is called word sense disambiguation ( WSD ) .",14,1,32
dataset/preprocessed/test-data/entity_linking/13,From the rock example above it is easy to see that the context surrounding the word is what disambiguates the sense .,15,0,22
dataset/preprocessed/test-data/entity_linking/13,"However , it may not be so obvious that this is a difficult task .",16,0,15
dataset/preprocessed/test-data/entity_linking/13,"To see this , consider instead the phrase "" Solid rock "" where changing the order of words completely changes the meaning , or "" Hard rock crushes heavy metal "" where individual words seem to indicate stone but together they actually define the word token as music .",17,0,49
dataset/preprocessed/test-data/entity_linking/13,"With this in mind , our thesis is that to do WSD well we need to go beyond bag of words and into the territory of sequence modeling .",18,0,29
dataset/preprocessed/test-data/entity_linking/13,"Improved WSD would be beneficial to many natural language processing ( NLP ) problems , e.g. machine translation , information Retrieval , information Extraction , and sense aware word representations .",19,1,31
dataset/preprocessed/test-data/entity_linking/13,"However , though much progress has been made in the area , many current WSD systems suffer from one or two of the following deficits .",20,0,26
dataset/preprocessed/test-data/entity_linking/13,( 1 ) Disregarding the order of words in the context which can lead to problems as described above .,21,0,20
dataset/preprocessed/test-data/entity_linking/13,"( 2 ) Relying on complicated and potentially language specific hand crafted features and resources , which is a big problem particularly for resource poor languages .",22,0,27
dataset/preprocessed/test-data/entity_linking/13,"We aim to mitigate these problems by ( 1 ) modeling the sequence of words surrounding the target word , and ( 2 ) refrain from using any hand crafted features or external resources and instead represent the words using real valued vector representation , i.e. word embeddings .",23,0,49
dataset/preprocessed/test-data/entity_linking/13,Using word embeddings has previously been shown to improve WSD .,24,0,11
dataset/preprocessed/test-data/entity_linking/13,"However , these works did not consider the order of words or their operational effect on each other .",25,0,19
dataset/preprocessed/test-data/entity_linking/13,The main contributions of this work include :,26,0,8
dataset/preprocessed/test-data/entity_linking/13,"A purely learned approach to WSD that achieves results on par with state - of - the - art resource heavy systems , employing e.g. knowledge graphs , parsers , part - of - speech tagging , etc .",27,0,39
dataset/preprocessed/test-data/entity_linking/13,Parameter sharing between different word types to make more efficient use of labeled data and make full vocabulary scaling plausible without the number of parameters exploding .,28,0,27
dataset/preprocessed/test-data/entity_linking/13,Empirical evidence that highlights the importance of word order for WSD .,29,0,12
dataset/preprocessed/test-data/entity_linking/13,"A WSD system that , by using no explicit window , is allowed to combine local and global information when deducing the sense .",30,0,24
dataset/preprocessed/test-data/entity_linking/13,In this section we introduce the most important underlying techniques for our proposed model .,32,0,15
dataset/preprocessed/test-data/entity_linking/13,Long short - term memory ( LSTM ) is a gated type of recurrent neural network ( RNN ) .,34,0,20
dataset/preprocessed/test-data/entity_linking/13,LSTMs were introduced by to enable RNNs to better capture long term dependencies when used to model sequences .,35,0,19
dataset/preprocessed/test-data/entity_linking/13,This is achieved by letting the model copy the state between timesteps without forcing the state through a non-linearity .,36,0,20
dataset/preprocessed/test-data/entity_linking/13,The flow of information is instead regulated using multiplicative gates which preserves the gradient better than e.g. the logistic function .,37,0,21
dataset/preprocessed/test-data/entity_linking/13,"The bidirectional variant of LSTM , ( BLSTM ) is an adaptation of the LSTM where the state at each time step consist of the state of two LSTMs , one going left and one going right .",38,0,38
dataset/preprocessed/test-data/entity_linking/13,"For WSD this means that the state has information about both preceding words and succeeding words , which in many cases are absolutely necessary to correctly classify the sense .",39,0,30
dataset/preprocessed/test-data/entity_linking/13,Word embeddings by GloVe,40,0,4
dataset/preprocessed/test-data/entity_linking/13,Word embeddings is away to represent words as real valued vectors in a semantically meaningful space .,41,0,17
dataset/preprocessed/test-data/entity_linking/13,"Global Vectors for Word Representation ( GloVe ) , introduced by Pennington et al. is a hybrid approach to embedding words that combine a log - linear model , made popular by , with counting based co-occurrence statistics to more efficiently capture global statistics .",42,0,45
dataset/preprocessed/test-data/entity_linking/13,"Word embeddings are trained in an unsupervised fashion , typically on large amounts of data , and is able to capture fine grained semantic and syntactic information about words .",43,0,30
dataset/preprocessed/test-data/entity_linking/13,These vectors can subsequently be used to initialize the input layer of a neural network or some other NLP model .,44,0,21
dataset/preprocessed/test-data/entity_linking/13,"Given a document and the position of the target word , i.e. the word to disambiguate , the model computes a probability distribution over the possible senses corresponding to that word .",46,0,32
dataset/preprocessed/test-data/entity_linking/13,"The architecture of the model , depicted in , consist of a softmax layer , a hidden layer , and a BLSTM .",47,0,23
dataset/preprocessed/test-data/entity_linking/13,See Section 2.1 for more details regarding the BLSTM .,48,0,10
dataset/preprocessed/test-data/entity_linking/13,"The BLSTM and the hidden layer share parameters overall word types and senses , while the softmax is parameterized byword type and selects the corresponding weight matrix and bias vector for each word type respectively .",49,0,36
dataset/preprocessed/test-data/coreference_resolution/7,Coreference Resolution with Entity Equalization,2,1,5
dataset/preprocessed/test-data/coreference_resolution/7,"A key challenge in coreference resolution is to capture properties of entity clusters , and use those in the resolution process .",4,0,22
dataset/preprocessed/test-data/coreference_resolution/7,"Here we provide a simple and effective approach for achieving this , via an "" Entity Equalization "" mechanism .",5,0,20
dataset/preprocessed/test-data/coreference_resolution/7,The Equalization approach represents each mention in a cluster via an approximation of the sum of all mentions in the cluster .,6,0,22
dataset/preprocessed/test-data/coreference_resolution/7,"We show how this can be done in a fully differentiable end - to - end manner , thus enabling high - order inferences in the resolution process .",7,0,29
dataset/preprocessed/test-data/coreference_resolution/7,"Our approach , which also employs BERT embeddings , results in new stateof - the - art results on the CoNLL - 2012 coreference resolution task , improving average F1 by 3.6 % .",8,0,34
dataset/preprocessed/test-data/coreference_resolution/7,Coreference resolution is the task of grouping mentions into entities .,11,0,11
dataset/preprocessed/test-data/coreference_resolution/7,A key challenge in this task is that information about an entity is spread across multiple mentions .,12,0,18
dataset/preprocessed/test-data/coreference_resolution/7,"Thus , deciding whether to assign a given mention to a candidate entity could require entity - level information that needs to be aggregated from all mentions .",13,0,28
dataset/preprocessed/test-data/coreference_resolution/7,Most coreference resolution systems rely on pairwise scoring of entity mentions .,14,0,12
dataset/preprocessed/test-data/coreference_resolution/7,As such they are prone to missing global entity information .,15,0,11
dataset/preprocessed/test-data/coreference_resolution/7,"The problem of entity - level representation ( also referred to as high - order coreference models ) has attracted considerable interest recently , with methods ranging from imitation learning to iterative refinement .",16,0,34
dataset/preprocessed/test-data/coreference_resolution/7,"Specifically , tackled this problem by iteratively averaging the antecedents of each mention to create mention representations thatare more "" global "" ( i.e. , reflect information about the entity to which the mention refers ) .",17,0,37
dataset/preprocessed/test-data/coreference_resolution/7,"Here we propose an approach that provides an entity - level representation in a simple and intuitive manner , and also facilitates end - to - end optimization .",18,0,29
dataset/preprocessed/test-data/coreference_resolution/7,"Our "" Entity Equalization "" approach posits that each entity should be represented via the sum of its corresponding mention representations .",19,0,22
dataset/preprocessed/test-data/coreference_resolution/7,"It is not immediately obvious how to perform this equalization , which relies on the entity - to- mention mapping , but we provide a natural smoothed representation of this mapping , and demonstrate how to use it for equalization .",20,0,41
dataset/preprocessed/test-data/coreference_resolution/7,"Now that each mention contains information about all its corresponding entities , we can use a standard pairwise scoring model , and this model will be able to use global entity - level information .",21,0,35
dataset/preprocessed/test-data/coreference_resolution/7,"Similar to recent coreference models , our approach uses contextual embeddings as input mention representations .",22,0,16
dataset/preprocessed/test-data/coreference_resolution/7,"While previous approaches employed the ELMo model , we propose to use BERT embeddings , motivated by the impressive empirical performance of BERT on other tasks .",23,0,27
dataset/preprocessed/test-data/coreference_resolution/7,It is challenging to apply BERT to the coreference resolution setting because BERT is limited to a fixed sequence length which is shorter than most coreference resolution documents .,24,0,29
dataset/preprocessed/test-data/coreference_resolution/7,We show that this can be done by using BERT in a fully convolutional manner .,25,0,16
dataset/preprocessed/test-data/coreference_resolution/7,"Our work is the first to use BERT for the task of coreference resolution , and we demonstrate that this results in significant improvement over current state - of - the - art .",26,0,34
dataset/preprocessed/test-data/coreference_resolution/7,"In summary , our contributions are : a.",27,0,8
dataset/preprocessed/test-data/coreference_resolution/7,A simple and intuitive approach for entity - level representation via the notion of Entity - Equalization .,28,0,18
dataset/preprocessed/test-data/coreference_resolution/7,The first use of BERT embeddings in coreferenceresolution .,30,0,9
dataset/preprocessed/test-data/coreference_resolution/7,"c. New state - of - the - art performance on the CoNLL - 2012 coreference resolution task , improving over previous F1 performance by 3.6 % .",31,0,28
dataset/preprocessed/test-data/coreference_resolution/7,"Following , we cast the coreference resolution task as finding a set of antecedent assignments y i for each span i in the document .",33,0,25
dataset/preprocessed/test-data/coreference_resolution/7,"The set of possible values for each y i is Y ( i ) = { , 1 , . . . , i ? 1 } , a dummy antecedent and all preceding spans .",34,0,36
dataset/preprocessed/test-data/coreference_resolution/7,"Non-dummy antecedents represent coreference links between i and y i , whereas indicates that the span is either not a mention , or is a first mention in a newly formed cluster .",35,0,33
dataset/preprocessed/test-data/coreference_resolution/7,"Whenever a new cluster is formed it receives a new index , and every mention with y i = receives the index of its antecedents .",36,0,26
dataset/preprocessed/test-data/coreference_resolution/7,Thus the process results in clusters of coreferent entities .,37,0,10
dataset/preprocessed/test-data/coreference_resolution/7,We briefly describe the baseline model ) which we will later augment with Entity - Equalization and BERT features .,39,0,20
dataset/preprocessed/test-data/coreference_resolution/7,"Let s ( i , j ) denote a pairwise score between two spans i and j.",40,0,17
dataset/preprocessed/test-data/coreference_resolution/7,"Next , for each span i define the distribution P ( y i ) over antecedents :",41,0,17
dataset/preprocessed/test-data/coreference_resolution/7,The score is a function of the span representations defined as follows .,42,0,13
dataset/preprocessed/test-data/coreference_resolution/7,For each span i let g i ?,43,0,8
dataset/preprocessed/test-data/coreference_resolution/7,Rd denote its corresponding representation vector ( see for more details about the model architecture ) .,44,0,17
dataset/preprocessed/test-data/coreference_resolution/7,"computes the antecedent score s ( i , j ) = f s ( g i , g j ) as a pairwise function of the span representations , i.e. not directly incorporating any information about the entities to which they might belong .",45,0,44
dataset/preprocessed/test-data/coreference_resolution/7,"improved upon this model by "" refining "" the span representations as follows .",46,0,14
dataset/preprocessed/test-data/coreference_resolution/7,The expected antecedent representation a i of each span i is computed by using the current antecedent distribution P ( y i ) as an attention mechanism :,47,0,28
dataset/preprocessed/test-data/coreference_resolution/7,The current span representation g i is then updated via interpolation with its expected antecedent representation a i :,48,0,19
dataset/preprocessed/test-data/coreference_resolution/7,"where f i = ff ( g i , a i ) is a learned gate vector .",49,0,18
dataset/preprocessed/test-data/coreference_resolution/3,Higher - order Coreference Resolution with Coarse - to - fine Inference,2,1,12
dataset/preprocessed/test-data/coreference_resolution/3,We introduce a fully differentiable approximation to higher - order inference for coreference resolution .,4,0,15
dataset/preprocessed/test-data/coreference_resolution/3,Our approach uses the antecedent distribution from a span - ranking architecture as an attention mechanism to iteratively refine span representations .,5,0,22
dataset/preprocessed/test-data/coreference_resolution/3,This enables the model to softly consider multiple hops in the predicted clusters .,6,0,14
dataset/preprocessed/test-data/coreference_resolution/3,"To alleviate the computational cost of this iterative process , we introduce a coarse - to - fine approach that incorporates a less accurate but more efficient bilinear factor , enabling more aggressive pruning without hurting accuracy .",7,0,38
dataset/preprocessed/test-data/coreference_resolution/3,"Compared to the existing state - of - the - art span - ranking approach , our model significantly improves accuracy on the English OntoNotes benchmark , while being far more computationally efficient .",8,0,34
dataset/preprocessed/test-data/coreference_resolution/3,"Recent coreference resolution systems have heavily relied on first order models , where only pairs of entity mentions are scored by the model .",10,0,24
dataset/preprocessed/test-data/coreference_resolution/3,These models are computationally efficient and scalable to long documents .,11,0,11
dataset/preprocessed/test-data/coreference_resolution/3,"However , because they make independent decisions about coreference links , they are susceptible to predicting clusters thatare locally consistent but globally inconsistent .",12,0,24
dataset/preprocessed/test-data/coreference_resolution/3,shows an example from that illustrates this failure case .,13,0,10
dataset/preprocessed/test-data/coreference_resolution/3,"The plurality of [ you ] is underspecified , making it locally compatible with both [ I ] and [ all of you ] , while the full cluster would have mixed plurality , resulting in global inconsistency .",14,0,39
dataset/preprocessed/test-data/coreference_resolution/3,We introduce an approximation of higher - order inference that uses the span - ranking architecture from in an iterative manner .,15,0,22
dataset/preprocessed/test-data/coreference_resolution/3,"At each iteration , the antecedent distribution is used as an attention mechanism to optionally update existing span representations , enabling later corefer - Speaker 1 : U m and think that is what 's - Go ahead Linda .",16,0,40
dataset/preprocessed/test-data/coreference_resolution/3,Speaker 2 : Welland uh thanks goes to and to the media to help us ...,17,0,16
dataset/preprocessed/test-data/coreference_resolution/3,So our hat is off to [ all of you ] as well .,18,0,14
dataset/preprocessed/test-data/coreference_resolution/3,"To alleviate computational challenges from this higher - order inference , we also propose a coarseto - fine approach that is learned with a single endto - end objective .",19,0,30
dataset/preprocessed/test-data/coreference_resolution/3,We introduce a less accurate but more efficient coarse factor in the pairwise scoring function .,20,0,16
dataset/preprocessed/test-data/coreference_resolution/3,This additional factor enables an extra pruning step during inference that reduces the number of antecedents considered by the more accurate but inefficient fine factor .,21,0,26
dataset/preprocessed/test-data/coreference_resolution/3,"Intuitively , the model cheaply computes a rough sketch of likely antecedents before applying a more expensive scoring function .",22,0,20
dataset/preprocessed/test-data/coreference_resolution/3,Our experiments show that both of the above contributions improve the performance of coreference resolution on the English OntoNotes benchmark .,23,0,21
dataset/preprocessed/test-data/coreference_resolution/3,"We observe a significant increase in average F1 with a second - order model , but returns quickly diminish with a third - order model .",24,0,26
dataset/preprocessed/test-data/coreference_resolution/3,"Additionally , our analysis shows that the coarse - to - fine approach makes the model performance relatively insensitive to more aggressive antecedent pruning , compared to the distance - based heuristic pruning from previous work .",25,0,37
dataset/preprocessed/test-data/coreference_resolution/3,"We formulate the coreference resolution task as a set of antecedent assignments y i for each of span i in the given document , following .",28,0,26
dataset/preprocessed/test-data/coreference_resolution/3,"The set of possible assignments for each y i is Y ( i ) = {? , 1 , . . . , i ? 1 } , a dummy antecedent ?",29,0,32
dataset/preprocessed/test-data/coreference_resolution/3,and all preceding spans .,30,0,5
dataset/preprocessed/test-data/coreference_resolution/3,Non-dummy antecedents represent coreference links between i and y i .,31,0,11
dataset/preprocessed/test-data/coreference_resolution/3,The dummy antecedent ?,32,0,4
dataset/preprocessed/test-data/coreference_resolution/3,represents two possible scenarios : ( 1 ) the span is not an entity mention or ( 2 ) the span is an entity mention but it is not coreferent with any previous span .,33,0,35
dataset/preprocessed/test-data/coreference_resolution/3,"These decisions implicitly define a final clustering , which can be recovered by grouping together all spans thatare connected by the set of antecedent predictions .",34,0,26
dataset/preprocessed/test-data/coreference_resolution/3,"We describe the baseline model , which we will improve to address the modeling and computational limitations discussed previously .",36,0,20
dataset/preprocessed/test-data/coreference_resolution/3,The goal is to learn a distribution P ( y i ) over antecedents for each span i :,37,0,19
dataset/preprocessed/test-data/coreference_resolution/3,"where s ( i , j ) is a pairwise score for a coreference link between span i and span j.",38,0,21
dataset/preprocessed/test-data/coreference_resolution/3,"The baseline model includes three factors for this pairwise coreference score : ( 1 ) s m ( i ) , whether span i is a mention , ( 2 ) s m ( j ) , whether span j is a mention , and ( 3 ) s a ( i , j ) whether j is an antecedent of i:",39,0,62
dataset/preprocessed/test-data/coreference_resolution/3,"In the special case of the dummy antecedent , the score s ( i , ? ) is instead fixed to 0 .",40,0,23
dataset/preprocessed/test-data/coreference_resolution/3,A common component used throughout the model is the vector representations g i for each possible span i .,41,0,19
dataset/preprocessed/test-data/coreference_resolution/3,These are computed via bidirectional LSTMs ) that learn context - dependent boundary and head representations .,42,0,17
dataset/preprocessed/test-data/coreference_resolution/3,The scoring functions s m and s a take these span representations as input :,43,0,15
dataset/preprocessed/test-data/coreference_resolution/3,"where denotes element - wise multiplication , FFNN denotes a feed - forward neural network , and the antecedent scoring function s a ( i , j ) includes explicit element - wise similarity of each span g i g j and a feature vector ?( i , j ) encoding speaker and genre information from the metadata and the distance between the two spans .",44,0,66
dataset/preprocessed/test-data/coreference_resolution/3,The model above is factored to enable a twostage beam search .,45,0,12
dataset/preprocessed/test-data/coreference_resolution/3,A beam of up to M potential mentions is computed ( where M is proportional to the document length ) based on the spans with the highest mention scores s m ( i ) .,46,0,35
dataset/preprocessed/test-data/coreference_resolution/3,Pairwise coreference scores are only computed between surviving mentions during both training and inference .,47,0,15
dataset/preprocessed/test-data/coreference_resolution/3,"Given supervision of gold coreference clusters , the model is learned by optimizing the marginal log - likelihood of the possibly correct antecedents .",48,0,24
dataset/preprocessed/test-data/coreference_resolution/3,This marginalization is required since the best antecedent for each span is a latent variable .,49,0,16
dataset/preprocessed/test-data/coreference_resolution/9,BERT for Coreference Resolution : Baselines and Analysis,2,1,8
dataset/preprocessed/test-data/coreference_resolution/9,"We apply BERT to coreference resolution , achieving strong improvements on the OntoNotes ( + 3.9 F1 ) and GAP ( + 11.5 F1 ) benchmarks .",4,0,27
dataset/preprocessed/test-data/coreference_resolution/9,"A qualitative analysis of model predictions indicates that , compared to ELMo and BERT - base , BERT - large is particularly better at distinguishing between related but distinct entities ( e.g. , President and CEO ) .",5,0,38
dataset/preprocessed/test-data/coreference_resolution/9,"However , there is still room for improvement in modeling document - level context , conversations , and mention paraphrasing .",6,0,21
dataset/preprocessed/test-data/coreference_resolution/9,Our code and models are publicly available 1 .,7,0,9
dataset/preprocessed/test-data/coreference_resolution/9,"Recent BERT - based models have reported dramatic gains on multiple semantic benchmarks including question - answering , natural language inference , and named entity recognition .",9,0,27
dataset/preprocessed/test-data/coreference_resolution/9,"Apart from better bidirectional reasoning , one of BERT 's major improvements over previous methods is passage - level training , 2 which allows it to better model longer sequences .",10,0,31
dataset/preprocessed/test-data/coreference_resolution/9,"We fine - tune BERT to coreference resolution , achieving strong improvements on the GAP and benchmarks .",11,0,18
dataset/preprocessed/test-data/coreference_resolution/9,We present two ways of extending the c 2f - coref model in .,12,0,14
dataset/preprocessed/test-data/coreference_resolution/9,The independent variant uses non-overlapping segments each of which acts as an independent instance for BERT .,13,0,17
dataset/preprocessed/test-data/coreference_resolution/9,The overlap variant splits the document into overlapping segments so as to provide the model with context beyond 512 tokens .,14,0,21
dataset/preprocessed/test-data/coreference_resolution/9,BERT - large improves over ELMo - based c 2f - coref 3.9 % on OntoNotes and 11.5 % on GAP ( both absolute ) .,15,0,26
dataset/preprocessed/test-data/coreference_resolution/9,"2 Each BERT training example consists of around 512 word pieces , while ELMo is trained on single sentences .",17,0,20
dataset/preprocessed/test-data/coreference_resolution/9,"A qualitative analysis of BERT and ELMobased models suggests that BERT - large ( unlike BERT - base ) is remarkably better at distinguishing between related yet distinct entities or concepts ( e.g. , Repulse Bay and Victoria Harbor ) .",18,0,41
dataset/preprocessed/test-data/coreference_resolution/9,"However , both models often struggle to resolve coreferences for cases that require world knowledge ( e.g. , the developing story and the scandal ) .",19,0,26
dataset/preprocessed/test-data/coreference_resolution/9,"Likewise , modeling pronouns remains difficult , especially in conversations .",20,0,11
dataset/preprocessed/test-data/coreference_resolution/9,We also find that BERT - large benefits from using longer context windows ( 384 word pieces ) while BERT - base performs better with shorter contexts ( 128 word pieces ) .,21,0,33
dataset/preprocessed/test-data/coreference_resolution/9,"Yet , both variants perform much worse with longer context windows ( 512 tokens ) in spite of being trained on 512 - size contexts .",22,0,26
dataset/preprocessed/test-data/coreference_resolution/9,"Moreover , the overlap variant , which artificially extends the context window beyond 512 tokens provides no improvement .",23,0,19
dataset/preprocessed/test-data/coreference_resolution/9,This indicates that using larger context windows for pretraining might not translate into effective long - range features for a downstream task .,24,0,23
dataset/preprocessed/test-data/coreference_resolution/9,"Larger models also exacerbate the memory - intensive nature of span representations , which have driven recent improvements in coreference resolution .",25,0,22
dataset/preprocessed/test-data/coreference_resolution/9,"Together , these observations suggest that there is still room for improvement in modeling document - level context , conversations , and mention paraphrasing .",26,0,25
dataset/preprocessed/test-data/coreference_resolution/9,"For our experiments , we use the higher - order coreference model in which is the current state of the art for the English OntoNotes dataset .",28,0,27
dataset/preprocessed/test-data/coreference_resolution/9,We refer to this as c 2 f - coref in the paper .,29,0,14
dataset/preprocessed/test-data/coreference_resolution/9,Overview of c2f- coref,30,0,4
dataset/preprocessed/test-data/coreference_resolution/9,"For each mention span x , the model learns a distribution P ( ) over possible antecedent spans Y :",31,0,20
dataset/preprocessed/test-data/coreference_resolution/9,"The scoring function s ( x , y ) between spans x and y uses fixed - length span representations , g x and g y to represent its inputs .",32,0,31
dataset/preprocessed/test-data/coreference_resolution/9,These consist of a concatenation of three vectors : the two LSTM states of the span endpoints and an attention vector computed over the span tokens .,33,0,27
dataset/preprocessed/test-data/coreference_resolution/9,"It computes the score s ( x , y ) by the mention score of x ( i.e. how likely is the span x to be a mention ) , the mention score of y , and the joint compatibility score of x and y ( i.e. assuming they are both mentions , how likely are x and y to refer to the same entity ) .",34,0,67
dataset/preprocessed/test-data/coreference_resolution/9,The components are computed as follows :,35,0,7
dataset/preprocessed/test-data/coreference_resolution/9,"where FFNN ( ) represents a feedforward neural network and ?( x , y) represents speaker and metadata features .",36,0,20
dataset/preprocessed/test-data/coreference_resolution/9,These span representations are later refined using antecedent distribution from a spanranking architecture as an attention mechanism .,37,0,18
dataset/preprocessed/test-data/coreference_resolution/9,We replace the entire LSTM - based encoder ( with ELMo and GloVe embeddings as input ) in c2fcoref with the BERT transformer .,39,0,24
dataset/preprocessed/test-data/coreference_resolution/9,We treat the first and last word - pieces ( concatenated with the attended version of all word pieces in the span ) as span representations .,40,0,27
dataset/preprocessed/test-data/coreference_resolution/9,"Documents are split into segments of max segment len , which we treat as a hyperparameter .",41,0,17
dataset/preprocessed/test-data/coreference_resolution/9,We experiment with two variants of splitting :,42,0,8
dataset/preprocessed/test-data/coreference_resolution/9,The independent variant uses nonoverlapping segments each of which acts as an independent instance for BERT .,44,0,17
dataset/preprocessed/test-data/coreference_resolution/9,The representation for each token is limited to the set of words that lie in its segment .,45,0,18
dataset/preprocessed/test-data/coreference_resolution/9,"As BERT is trained on sequences of at most 512 word pieces , this variant has limited encoding capacity especially for tokens that lie at the start or end of their segments .",46,0,33
dataset/preprocessed/test-data/coreference_resolution/9,The overlap variant splits the document into overlapping segments by creating a Tsized segment after every T / 2 tokens .,48,0,21
dataset/preprocessed/test-data/coreference_resolution/9,"These segments are then passed onto the BERT encoder independently , and the final token representation is derived by element - wise interpolation of representations from both overlapping segments .",49,0,30
dataset/preprocessed/test-data/coreference_resolution/1,End - to - end Deep Reinforcement Learning Based Coreference Resolution,2,1,11
dataset/preprocessed/test-data/coreference_resolution/1,Recent neural network models have significantly advanced the task of coreference resolution .,4,0,13
dataset/preprocessed/test-data/coreference_resolution/1,"However , current neural coreference models are typically trained with heuristic loss functions thatare computed over a sequence of local decisions .",5,0,22
dataset/preprocessed/test-data/coreference_resolution/1,"In this paper , we introduce an end - to - end reinforcement learning based coreference resolution model to directly optimize coreference evaluation metrics .",6,0,25
dataset/preprocessed/test-data/coreference_resolution/1,"Specifically , we modify the state - of - the - art higherorder mention ranking approach in Lee et al .",7,0,21
dataset/preprocessed/test-data/coreference_resolution/1,( 2018 ) to a reinforced policy gradient model by incorporating the reward associated with a sequence of coreference linking actions .,8,0,22
dataset/preprocessed/test-data/coreference_resolution/1,"Furthermore , we introduce maximum entropy regularization for adequate exploration to prevent the model from prematurely converging to a bad local optimum .",9,0,23
dataset/preprocessed/test-data/coreference_resolution/1,Our proposed model achieves new state - of - the - art performance on the English OntoNotes v5.0 benchmark .,10,0,20
dataset/preprocessed/test-data/coreference_resolution/1,"Coreference resolution is one of the most fundamental tasks in natural language processing ( NLP ) , which has a significant impact on many downstream applications including information extraction ) , question answering , and entity linking .",12,0,38
dataset/preprocessed/test-data/coreference_resolution/1,"Given an input text , coreference resolution aims to identify and group all the mentions that refer to the same entity .",13,0,22
dataset/preprocessed/test-data/coreference_resolution/1,"In recent years , deep neural network models for coreference resolution have been prevalent .",14,0,15
dataset/preprocessed/test-data/coreference_resolution/1,"These models , however , either assumed mentions were given and only developed a coreference linking model or built a pipeline system to detect mention first then resolved coreferences .",15,0,30
dataset/preprocessed/test-data/coreference_resolution/1,"In either case , they depend on hand - crafted fea -tures and syntactic parsers that may not generalize well or may even propagate errors .",16,0,26
dataset/preprocessed/test-data/coreference_resolution/1,"To avoid the cascading errors of pipeline systems , recent NLP researchers have developed endto - end approaches , which directly consider all text spans , jointly identify entity mentions and cluster them .",17,0,34
dataset/preprocessed/test-data/coreference_resolution/1,The core of those end - to - end models are vector embeddings to represent text spans in the document and scoring functions to compute the mention scores for text spans and antecedent scores for pairs of spans .,18,0,39
dataset/preprocessed/test-data/coreference_resolution/1,"Depending on how the span embeddings are computed , the end - to - end coreference models could be further divided into first order methods or higher order methods .",19,0,30
dataset/preprocessed/test-data/coreference_resolution/1,"Although recent end - to - end neural coreference models have advanced the state - of - the - art performance for coreference resolution , they are still trained with heuristic loss functions and make a sequence of local decisions for each pair of mentions .",20,0,46
dataset/preprocessed/test-data/coreference_resolution/1,"However as studied in ; , most coreference resolution evaluation measures are not accessible over local decisions , but can only be known until all other decisions have been made .",21,0,31
dataset/preprocessed/test-data/coreference_resolution/1,"Therefore , the next key research question is how to integrate and directly optimize coreference evaluation metrics in an end - to - end manner .",22,0,26
dataset/preprocessed/test-data/coreference_resolution/1,"In this paper , we propose a goal - directed endto - end deep reinforcement learning framework to resolve coreference as shown in .",23,0,24
dataset/preprocessed/test-data/coreference_resolution/1,"Specifically , we leverage the neural architecture in as our policy network , which includes learning span representation , scoring potential entity mentions , and generating a probability distribution over all possible coreference linking actions from the current mention to its antecedents .",24,0,43
dataset/preprocessed/test-data/coreference_resolution/1,"Once a sequence of linking actions are made , our reward function is used to measure how good the generated coreference clusters are , which is directly related to coreference evaluation metrics .",25,0,33
dataset/preprocessed/test-data/coreference_resolution/1,"Besides , we introduce an entropy regularization term to encourage exploration and prevent the policy from prematurely converging to a bad local optimum .",26,0,24
dataset/preprocessed/test-data/coreference_resolution/1,"Finally , we update the regularized policy network parameters based on the rewards associated with sequences of sampled actions , which are computed on the whole input document .",27,0,29
dataset/preprocessed/test-data/coreference_resolution/1,We evaluate our end - to - end reinforced coreference resolution model on the English OntoNotes v5.0 benchmark .,28,0,19
dataset/preprocessed/test-data/coreference_resolution/1,"Our model achieves the new state - of - the - art F1 - score of 73.8 % , which outperforms previous best - published result ( 73.0 % ) of with statistical significance .",29,0,35
dataset/preprocessed/test-data/coreference_resolution/1,Closely related to our work are the end - to - end coreference models developed by and .,31,0,18
dataset/preprocessed/test-data/coreference_resolution/1,"Different from previous pipeline approaches , neural networks to learn mention representations and calculate mention and antecedent scores without using syntactic parsers .",32,0,23
dataset/preprocessed/test-data/coreference_resolution/1,"However , their models optimize a heuristic loss based on local decisions rather than the actual coreference evaluation metrics , while our reinforcement model directly optimizes the evaluation metrics based on the rewards calculated from sequences of actions .",33,0,39
dataset/preprocessed/test-data/coreference_resolution/1,"Our work is also inspired by and , which resolve coreferences with reinforcement learning techniques .",34,0,16
dataset/preprocessed/test-data/coreference_resolution/1,"They view the mention - ranking model as an agent taking a series of actions , where each action links each mention to a candidate antecedent .",35,0,27
dataset/preprocessed/test-data/coreference_resolution/1,They also use pretraining for initialization .,36,0,7
dataset/preprocessed/test-data/coreference_resolution/1,"Nevertheless , their models assume mentions are given while our work is end - to - end .",37,0,18
dataset/preprocessed/test-data/coreference_resolution/1,"Furthermore , we add entropy regularization to encourage more exploration ) and prevent our model from prematurely converging to a sub-optimal ( or bad ) local optimum .",38,0,28
dataset/preprocessed/test-data/coreference_resolution/1,"Given a document , the task of end - to - end coreference resolution aims to identify a set of mention clusters , each of which refers to the same entity .",41,0,32
dataset/preprocessed/test-data/coreference_resolution/1,"Following , we formulate the task as a sequence of linking decisions for each span i to the set of its possible antecedents , denoted as Y ( i ) = { , 1 , , i ? 1 } , a dummy antecedent and all preceding spans .",42,0,49
dataset/preprocessed/test-data/coreference_resolution/1,"In particular , the use of dummy antecedent for a span is to handle two possible scenarios : ( i ) the span is not an entity mention or ( ii ) the span is an entity mention but it is not coreferent with any previous spans .",43,0,48
dataset/preprocessed/test-data/coreference_resolution/1,The final coreference clusters can be recovered with a backtracking step on the antecedent predictions .,44,0,16
dataset/preprocessed/test-data/coreference_resolution/1,Figure 2 illustrates a demonstration of our iterative coreference resolution model on a document .,46,0,15
dataset/preprocessed/test-data/coreference_resolution/1,"Given a document , our model first identifies top scored mentions , and then conducts a sequence of actions a 1:T = {a 1 , a 2 , , a T } over them , where T is the number of mentions and each action at assigns mention t to a candidate antecedent",47,0,53
dataset/preprocessed/test-data/coreference_resolution/1,"Once our model has finished all the actions , it observes a reward R ( a 1:T ) .",48,0,19
dataset/preprocessed/test-data/coreference_resolution/1,The calculated gradients are then propagated to update model parameters .,49,0,11
dataset/preprocessed/test-data/coreference_resolution/5,Learning Global Features for Coreference Resolution,2,1,6
dataset/preprocessed/test-data/coreference_resolution/5,There is compelling evidence that coreference prediction would benefit from modeling global information about entity - clusters .,4,1,18
dataset/preprocessed/test-data/coreference_resolution/5,"Yet , state - of - the - art performance can be achieved with systems treating each mention prediction independently , which we attribute to the inherent difficulty of crafting informative clusterlevel features .",5,0,34
dataset/preprocessed/test-data/coreference_resolution/5,"We instead propose to use recurrent neural networks ( RNNs ) to learn latent , global representations of entity clusters directly from their mentions .",6,0,25
dataset/preprocessed/test-data/coreference_resolution/5,"We show that such representations are especially useful for the prediction of pronominal mentions , and can be incorporated into an end - to - end coreference system that outperforms the state of the art without requiring any additional search .",7,0,41
dataset/preprocessed/test-data/coreference_resolution/5,: um and [ I ] 1 think that is what 's - Go ahead [ Linda ] 2 .,8,0,20
dataset/preprocessed/test-data/coreference_resolution/5,LW : Welland uh thanks goes to [ you ] 1 and to [ the media ] 3 to help [ us ] 4 ... So [ our ] 4 hat is off to all of [ you ] 5 as well .,9,0,43
dataset/preprocessed/test-data/coreference_resolution/5,"While structured , non-local coreference models would seem to hold promise for avoiding many common coreference errors ( as discussed further in Section 3 ) , the results of employing such models in practice are decidedly mixed , and state - of - the - art results can be obtained using a completely local , mention - ranking system .",11,0,60
dataset/preprocessed/test-data/coreference_resolution/5,"In this work , we posit that global context is indeed necessary for further improvements in coreference resolution , but argue that informative cluster , rather than mention , level features are very difficult to devise , limiting their effectiveness .",12,0,41
dataset/preprocessed/test-data/coreference_resolution/5,"Accordingly , we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network ( shown in Section 4 ) .",13,0,27
dataset/preprocessed/test-data/coreference_resolution/5,"Our model has no manually defined cluster features , but instead learns a global representation from the individual mentions present in each cluster .",14,0,24
dataset/preprocessed/test-data/coreference_resolution/5,We incorporate these representations into a mention - ranking style coreference system .,15,0,13
dataset/preprocessed/test-data/coreference_resolution/5,"The entire model , including the recurrent neural network and the mention - ranking sub-system , is trained end - to - end on the coreference task .",16,0,28
dataset/preprocessed/test-data/coreference_resolution/5,"We train the model as a local classifier with fixed context ( that is , as a history - based model ) .",17,0,23
dataset/preprocessed/test-data/coreference_resolution/5,"As such , unlike several recent approaches , which may require complicated inference during training , we are able to train our model in much the same way as a vanilla mentionranking model .",18,0,34
dataset/preprocessed/test-data/coreference_resolution/5,Experiments compare the use of learned global features to several strong baseline systems for coreference resolution .,19,0,17
dataset/preprocessed/test-data/coreference_resolution/5,"We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions , which remain a persistent source of errors for modern coreference systems .",20,0,31
dataset/preprocessed/test-data/coreference_resolution/5,"Our final system improves over 0.8 points in CoNLL score over the current state of the art , and the improvement is statistically significant on all three CoNLL metrics .",21,0,30
dataset/preprocessed/test-data/coreference_resolution/5,Background and Notation,22,0,3
dataset/preprocessed/test-data/coreference_resolution/5,Coreference resolution is fundamentally a clustering task .,23,0,8
dataset/preprocessed/test-data/coreference_resolution/5,"Given a sequence ( x n ) N n=1 of ( intra-document ) mentions - that is , syntactic units that can refer or be referred to - coreference resolution involves partitioning ( x n ) into a sequence of clusters ( X ( m ) )",24,0,47
dataset/preprocessed/test-data/coreference_resolution/5,M m= 1 such that all the mentions in any particular cluster X ( m ) refer to the same underlying entity .,25,0,23
dataset/preprocessed/test-data/coreference_resolution/5,"Since the mentions within a particular cluster maybe ordered linearly by their appearance in the document , 1 we will use the notation X ( m ) j to refer to the j'th mention in the m'th cluster .",26,0,39
dataset/preprocessed/test-data/coreference_resolution/5,"A valid clustering places each mention in exactly one cluster , and so we may represent a clustering with a vector z ?",27,0,23
dataset/preprocessed/test-data/coreference_resolution/5,"{ 1 , . . . , M } N , where z n = miff x n is a member of X .",28,0,24
dataset/preprocessed/test-data/coreference_resolution/5,Coreference systems attempt to find the best clustering z * ?,29,0,11
dataset/preprocessed/test-data/coreference_resolution/5,"Z under some scoring function , with Z the set of valid clusterings .",30,0,14
dataset/preprocessed/test-data/coreference_resolution/5,"One strategy to avoid the computational intractability associated with predicting an entire clustering z is to instead predict a single antecedent for each mention x n ; because x n may not be anaphoric ( and therefore have no antecedents ) , a "" dummy "" antecedent may also be predicted .",31,0,52
dataset/preprocessed/test-data/coreference_resolution/5,"The aforementioned strategy is adopted by "" mention - ranking "" systems , which , formally , predict an antecedent ? ?",32,0,22
dataset/preprocessed/test-data/coreference_resolution/5,"Y ( x n ) for each mention x n , where Y ( x n ) = { 1 , . . . , n ? 1 , }.",33,0,30
dataset/preprocessed/test-data/coreference_resolution/5,"Through transitivity , these decisions induce a clustering over the document .",34,0,12
dataset/preprocessed/test-data/coreference_resolution/5,"Mention - ranking systems make their antecedent predictions with a local scoring function f ( x n , y) defined for any mention x n and any antecedent y ?",35,0,30
dataset/preprocessed/test-data/coreference_resolution/5,Y ( x n ) .,36,0,6
dataset/preprocessed/test-data/coreference_resolution/5,"While such a scoring function clearly ignores much structural information , the mentionranking approach has been attractive for at least two reasons .",37,0,23
dataset/preprocessed/test-data/coreference_resolution/5,"First , inference is relatively simple and efficient , requiring only a left - to - right pass through a document 's mentions during which a mention 's antecedents ( as well as ) are scored and the highest scoring antecedent is predicted .",38,0,44
dataset/preprocessed/test-data/coreference_resolution/5,"Second , from a linguistic modeling perspective , mention - ranking models learn a scoring function that requires a mention x n to be compatible with only one of its coreferent antecedents .",39,0,33
dataset/preprocessed/test-data/coreference_resolution/5,"This contrasts with mention - pair models ( e.g. , ) , which score all pairs of mentions in a cluster , as well as with certain cluster - based models ( see discussion in ) .",40,0,37
dataset/preprocessed/test-data/coreference_resolution/5,"Modeling each mention as having a single antecedent is particularly advantageous for pronominal mentions , which we might like to model as linking to a single nominal or proper antecedent , for example , but not necessarily to all other coreferent mentions .",41,0,43
dataset/preprocessed/test-data/coreference_resolution/5,"Accordingly , in this paper we attempt to maintain the inferential simplicity and modeling benefits of mention ranking , while allowing the model to utilize global , structural information relating to z in making its predictions .",42,0,37
dataset/preprocessed/test-data/coreference_resolution/5,"We therefore investigate objective functions of the form arg max y 1 , ... ,y N N n= 1 f ( x n , y n ) + g ( x n , y n , z 1:n?1 ) , where g is a global function that , in making predictions for x n , may examine ( features of ) the clustering z 1:n?",43,0,65
dataset/preprocessed/test-data/coreference_resolution/5,1 induced by the antecedent predictions made through y n?1 .,44,0,11
dataset/preprocessed/test-data/coreference_resolution/5,The Role of Global Features,45,0,5
dataset/preprocessed/test-data/coreference_resolution/5,Here we motivate the use of global features for coreference resolution by focusing on the issues that may arise when resolving pronominal mentions in a purely local way .,46,0,29
dataset/preprocessed/test-data/coreference_resolution/5,See and for more general motivation for using global models .,47,0,11
dataset/preprocessed/test-data/coreference_resolution/5,Recent empirical work has shown that the resolution of pronominal mentions accounts for a substantial percentage of the total errors made by modern mention - ranking systems .,49,0,28
dataset/preprocessed/test-data/coreference_resolution/8,End - to - end Neural Coreference Resolution,2,1,8
dataset/preprocessed/test-data/coreference_resolution/8,We introduce the first end - to - end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector .,4,1,31
dataset/preprocessed/test-data/coreference_resolution/8,The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each .,5,0,24
dataset/preprocessed/test-data/coreference_resolution/8,The model computes span embeddings that combine context - dependent boundary representations with a headfinding attention mechanism .,6,0,18
dataset/preprocessed/test-data/coreference_resolution/8,It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions .,7,0,26
dataset/preprocessed/test-data/coreference_resolution/8,"Experiments demonstrate state - of - the - art performance , with again of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5 - model ensemble , despite the fact that this is the first approach to be successfully trained with no external resources .",8,0,49
dataset/preprocessed/test-data/coreference_resolution/8,We present the first state - of - the - art neural coreference resolution model that is learned end - toend given only gold mention clusters .,10,0,27
dataset/preprocessed/test-data/coreference_resolution/8,"All recent coreference models , including neural approaches that achieved impressive performance gains , rely on syntactic parsers , both for headword features and as the input to carefully handengineered mention proposal algorithms .",11,0,34
dataset/preprocessed/test-data/coreference_resolution/8,"We demonstrate for the first time that these resources are not required , and in fact performance can be improved significantly without them , by training an end - to - end neural model that jointly learns which spans are entity mentions and how to best cluster them .",12,0,49
dataset/preprocessed/test-data/coreference_resolution/8,Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters .,13,0,28
dataset/preprocessed/test-data/coreference_resolution/8,"It includes a span - ranking model that decides , for each span , which of the previous spans ( if any ) is a good antecedent .",14,0,28
dataset/preprocessed/test-data/coreference_resolution/8,"At the core of our model are vector embeddings representing spans of text in the document , which combine context - dependent boundary representations with a head - finding attention mechanism over the span .",15,0,35
dataset/preprocessed/test-data/coreference_resolution/8,"The attention component is inspired by parser - derived head - word matching features from previous systems , but is less susceptible to cascading errors .",16,0,26
dataset/preprocessed/test-data/coreference_resolution/8,"In our analyses , we show empirically that these learned attention weights correlate strongly with traditional headedness definitions .",17,0,19
dataset/preprocessed/test-data/coreference_resolution/8,"Scoring all span pairs in our end - to - end model is impractical , since the complexity would be quartic in the document length .",18,0,26
dataset/preprocessed/test-data/coreference_resolution/8,"Therefore we factor the model over unary mention scores and pairwise antecedent scores , both of which are simple functions of the learned span embedding .",19,0,26
dataset/preprocessed/test-data/coreference_resolution/8,"The unary mention scores are used to prune the space of spans and antecedents , to aggressively reduce the number of pairwise computations .",20,0,24
dataset/preprocessed/test-data/coreference_resolution/8,Our final approach outperforms existing models by 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5 - model ensemble .,21,0,24
dataset/preprocessed/test-data/coreference_resolution/8,"It is not only accurate , but also relatively interpretable .",22,0,11
dataset/preprocessed/test-data/coreference_resolution/8,"The model factors , for example , directly indicate whether an absent coreference link is due to low mention scores ( for either span ) or a low score from the mention ranking component .",23,0,35
dataset/preprocessed/test-data/coreference_resolution/8,The head - finding attention mechanism also reveals which mentioninternal words contribute most to coreference decisions .,24,0,17
dataset/preprocessed/test-data/coreference_resolution/8,"We leverage this over all interpretability to do detailed quantitative and qualitative analyses , providing insights into the strengths and weak - nesses of the approach .",25,0,27
dataset/preprocessed/test-data/coreference_resolution/8,Machine learning methods have along history in coreference resolution ( see for a detailed survey ) .,27,0,17
dataset/preprocessed/test-data/coreference_resolution/8,"However , the learning problem is challenging and , until very recently , handengineered systems built on top of automatically produced parse trees outperformed all learning approaches .",28,0,28
dataset/preprocessed/test-data/coreference_resolution/8,"showed that highly lexical learning approaches reverse this trend , and more recent neural models have achieved significant performance gains .",29,0,21
dataset/preprocessed/test-data/coreference_resolution/8,"However , all of these models use parsers for head features and include highly engineered mention proposal algorithms .",30,0,19
dataset/preprocessed/test-data/coreference_resolution/8,1 Such pipelined systems suffer from two major drawbacks : ( 1 ) parsing mistakes can introduce cascading errors and ( 2 ) many of the handengineered rules do not generalize to new languages .,31,0,35
dataset/preprocessed/test-data/coreference_resolution/8,A non-pipelined system that jointly models mention detection and coreference resolution was first proposed by .,32,0,16
dataset/preprocessed/test-data/coreference_resolution/8,They introduce a search - based system that predicts the coreference structure in a left - to - right transition system that can incorporate global features .,33,0,27
dataset/preprocessed/test-data/coreference_resolution/8,"In contrast , our approach performs well while making much stronger independence assumptions , enabling straightforward inference .",34,0,18
dataset/preprocessed/test-data/coreference_resolution/8,"More generally , a wide variety of approaches for learning coreference models have been proposed .",35,0,16
dataset/preprocessed/test-data/coreference_resolution/8,"They can typically be categorized as ( 1 ) mention - pair classifiers , ( 2 ) entity - level models , ( 3 ) latent - tree models , or ( 4 ) mention - ranking models .",36,0,39
dataset/preprocessed/test-data/coreference_resolution/8,"Our span - ranking approach is most similar to mention ranking , but we reason over a larger space by jointly detecting mentions and predicting coreference .",37,0,27
dataset/preprocessed/test-data/coreference_resolution/8,We formulate the task of end - to - end coreference resolution as a set of decisions for every possible span in the document .,39,0,25
dataset/preprocessed/test-data/coreference_resolution/8,The input is a document D containing T words along with metadata such as speaker and genre information .,40,0,19
dataset/preprocessed/test-data/coreference_resolution/8,Let N = T ( T + 1 ) 2 be the number of possible text spans in D .,41,0,20
dataset/preprocessed/test-data/coreference_resolution/8,"Denote the start and end indices of a span i in D respectively by and END ( i ) , for 1 ?",42,0,23
dataset/preprocessed/test-data/coreference_resolution/8,We assume an ordering of the spans based on START ( i ) ; spans with the same start index are ordered by END ( i ) .,45,0,28
dataset/preprocessed/test-data/coreference_resolution/8,The task is to assign to each span i an antecedent y i .,46,0,14
dataset/preprocessed/test-data/coreference_resolution/8,"The set of possible assignments for each y i is Y ( i ) = {? , 1 , . . . , i ? 1 } , a dummy antecedent ?",47,0,32
dataset/preprocessed/test-data/coreference_resolution/8,and all preceding spans .,48,0,5
dataset/preprocessed/test-data/coreference_resolution/8,"True antecedents of span i , i.e. span j such that 1 ? j ? i ?",49,0,17
dataset/preprocessed/test-data/coreference_resolution/6,Learning Word Representations with Cross - Sentence Dependency for End - to - End Co -reference Resolution,2,1,17
dataset/preprocessed/test-data/coreference_resolution/6,"In this work , we present a word embedding model that learns cross - sentence dependency for improving end - to - end co-reference resolution ( E2E - CR ) .",4,1,31
dataset/preprocessed/test-data/coreference_resolution/6,"While the traditional E2E - CR model generates word representations by running long short - term memory ( LSTM ) recurrent neural networks on each sentence of an input article or conversation separately , we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency .",5,1,49
dataset/preprocessed/test-data/coreference_resolution/6,Both sentence linking strategies enable the LSTMs to make use of valuable information from context sentences while calculating the representation of the current input word .,6,0,26
dataset/preprocessed/test-data/coreference_resolution/6,"With this approach , the LSTMs learn word embeddings considering knowledge not only from the current sentence but also from the entire input document .",7,0,25
dataset/preprocessed/test-data/coreference_resolution/6,"Experiments show that learning cross - sentence dependency enriches information contained by the word representations , and improves the performance of the co-reference resolution model compared with our baseline .",8,0,30
dataset/preprocessed/test-data/coreference_resolution/6,Co-reference resolution requires models to cluster mentions that refer to the same physical entities .,10,1,15
dataset/preprocessed/test-data/coreference_resolution/6,The models based on neural networks typically require different levels of semantic representations of input sentences .,11,0,17
dataset/preprocessed/test-data/coreference_resolution/6,"The models usually need to calculate the representations of word spans , or mentions , given pre-trained character and wordlevel embeddings before predicting antecedents .",12,0,25
dataset/preprocessed/test-data/coreference_resolution/6,"The mention - level embeddings are used to make coreference decisions , typically by scoring mention pairs and making links .",13,0,21
dataset/preprocessed/test-data/coreference_resolution/6,Long short - term memories ( LSTMs ) are often used to encode the syntactic and semantic information of input sentences .,14,0,22
dataset/preprocessed/test-data/coreference_resolution/6,Articles and conversations include more than one sentences .,15,0,9
dataset/preprocessed/test-data/coreference_resolution/6,"Considering the accuracy and efficiency of co-reference resolution models , the encoder LSTM usually processes input sentences separately as a batch .",16,0,22
dataset/preprocessed/test-data/coreference_resolution/6,"The dis advantage of this method is that the models do not consider the dependency among words from different sentences , which plays a significant role in word representation learning and co-reference predicting .",17,0,34
dataset/preprocessed/test-data/coreference_resolution/6,"For example , pronouns are often linked to entities mentioned in other sentences , while their initial word vectors lack dependency information .",18,0,23
dataset/preprocessed/test-data/coreference_resolution/6,"As a result , a word representation model can not learn an informative embedding of a pronoun without considering cross - sentence dependency in this case .",19,0,27
dataset/preprocessed/test-data/coreference_resolution/6,It is also problematic if we encode the input document considering cross - sentence dependency and treat the entire document as one sentence .,20,0,24
dataset/preprocessed/test-data/coreference_resolution/6,An input article or conversation can be too long for a single LSTM cell to memorize .,21,0,17
dataset/preprocessed/test-data/coreference_resolution/6,"If the LSTM updates itself for too many steps , gradients will vanish or explode , and the coreference resolution model will be very difficult to optimize .",22,0,28
dataset/preprocessed/test-data/coreference_resolution/6,Regarding the entire input corpus as one sequence instead of a batch also significantly increases the time complexity of the model .,23,0,22
dataset/preprocessed/test-data/coreference_resolution/6,"To solve the problem that traditional LSTM encoders , which treat the input sentences as a batch , lack an ability to capture cross - sentence dependency , and to avoid the time complexity and difficulties of training the model concatenating all input sentences , we propose a cross - sentence encoder for end - to - end co-reference ( E2E - CR ) .",24,0,65
dataset/preprocessed/test-data/coreference_resolution/6,"Borrowing the idea of an external memory module from , an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model .",25,0,30
dataset/preprocessed/test-data/coreference_resolution/6,"With this context memory block , the proposed model is able to encode input sentences as a batch , and also calculate the representations of input words by taking both target sentences and context sentences into consideration .",26,0,38
dataset/preprocessed/test-data/coreference_resolution/6,Experiments showed that this approach improved the performance of co-reference resolution models .,27,0,13
dataset/preprocessed/test-data/coreference_resolution/6,2 Related Work,28,0,3
dataset/preprocessed/test-data/coreference_resolution/6,Co- reference Resolution,29,0,3
dataset/preprocessed/test-data/coreference_resolution/6,A popular method of co-reference resolution is mention ranking .,30,0,10
dataset/preprocessed/test-data/coreference_resolution/6,"Reading each mention , the model calculates coreference scores for all antecedent mentions , and picks the mention with the highest positive score to be its co-reference .",31,0,28
dataset/preprocessed/test-data/coreference_resolution/6,Many recent works are based on this approach .,32,0,9
dataset/preprocessed/test-data/coreference_resolution/6,designed a set of feature templates to improve the mention - ranking model .,33,0,14
dataset/preprocessed/test-data/coreference_resolution/6,proposed a mention - ranking model by jointly learning mention heads and co-references .,34,0,14
dataset/preprocessed/test-data/coreference_resolution/6,proposed a reinforcement learning framework for the mention ranking approach .,35,0,11
dataset/preprocessed/test-data/coreference_resolution/6,"Based on similar ideas but without using parsing features , the authors of proposed the current state - of - the - art model which uses neural networks to embed mentions and calculate mention and antecedent scores .",36,0,38
dataset/preprocessed/test-data/coreference_resolution/6,applied ELMo embeddings to improve within - sentence dependency modeling and word representation learning .,37,0,15
dataset/preprocessed/test-data/coreference_resolution/6,and proposed models using global entity - level features .,38,0,10
dataset/preprocessed/test-data/coreference_resolution/6,Language Representation Learning,39,0,3
dataset/preprocessed/test-data/coreference_resolution/6,Distributed word embeddings has been used as the basic unit of language representation for over a decade .,40,0,18
dataset/preprocessed/test-data/coreference_resolution/6,"Pre-trained word embeddings , for example GloVe and Skip - Gram are widely used as the input of natural language processing models .",41,0,23
dataset/preprocessed/test-data/coreference_resolution/6,Long short - term memory ( LSTM ) networks are widely used for sentence modeling .,42,0,16
dataset/preprocessed/test-data/coreference_resolution/6,A single - layer LSTM network was applied in the previous state - of - theart co-reference model to generate word and mention representations .,43,0,25
dataset/preprocessed/test-data/coreference_resolution/6,"To capture dependency of longer distances , proposed a recurrent model that outputs hidden states by skipping input tokens .",44,0,20
dataset/preprocessed/test-data/coreference_resolution/6,"Recently , memory networks have been applied in language modeling .",45,0,11
dataset/preprocessed/test-data/coreference_resolution/6,"Applying an attention mechanism on memory cells , memory networks allow the model to focus on significant words or segments for classification and generation tasks .",46,0,26
dataset/preprocessed/test-data/coreference_resolution/6,Previous works have shown that applying memory blocks in LSTMs also improves longdistance dependency extraction .,47,0,16
dataset/preprocessed/test-data/coreference_resolution/6,Learning Cross - Sentence dependency,48,0,5
dataset/preprocessed/test-data/coreference_resolution/6,"To improve the word representation learning model for better co-reference resolution performance , we propose two word representation models that learn cross - sentence dependency .",49,0,26
dataset/preprocessed/test-data/coreference_resolution/0,Improving Coreference Resolution by Learning Entity - Level Distributed Representations,2,1,10
dataset/preprocessed/test-data/coreference_resolution/0,A long - standing challenge in coreference resolution has been the incorporation of entity - level information - features defined over clusters of mentions instead of mention pairs .,4,0,29
dataset/preprocessed/test-data/coreference_resolution/0,We present a neural network based coreference system that produces high - dimensional vector representations for pairs of coreference clusters .,5,0,21
dataset/preprocessed/test-data/coreference_resolution/0,"Using these representations , our system learns when combining clusters is desirable .",6,0,13
dataset/preprocessed/test-data/coreference_resolution/0,We train the system with a learning - to - search algorithm that teaches it which local decisions ( cluster merges ) will lead to a high - scoring final coreference partition .,7,0,33
dataset/preprocessed/test-data/coreference_resolution/0,The system substantially outperforms the current state - of - the - art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand - engineered features .,8,0,34
dataset/preprocessed/test-data/coreference_resolution/0,"Coreference resolution , the task of identifying which mentions in a text refer to the same realworld entity , is fundamentally a clustering problem .",10,0,25
dataset/preprocessed/test-data/coreference_resolution/0,"However , many recent state - of - the - art coreference systems operate solely by linking pairs of mentions together .",11,0,22
dataset/preprocessed/test-data/coreference_resolution/0,"An alternative approach is to use agglomerative clustering , treating each mention as a singleton cluster at the outset and then repeatedly merging clusters of mentions deemed to be referring to the same entity .",12,0,35
dataset/preprocessed/test-data/coreference_resolution/0,"Such systems can take advantage of entity - level information , i.e. , features between clusters of mentions instead of between just two mentions .",13,0,25
dataset/preprocessed/test-data/coreference_resolution/0,"As an example for why this is useful , it is clear that the clusters { Bill Clinton } and { Clinton , she } are not referring to the same entity , but it is ambiguous whether the pair of mentions Bill Clinton and Clinton are coreferent .",14,0,49
dataset/preprocessed/test-data/coreference_resolution/0,Previous work has incorporated entity - level information through features that capture hard constraints like having gender or number agreement between clusters .,15,0,23
dataset/preprocessed/test-data/coreference_resolution/0,"In this work , we instead train a deep neural network to build distributed representations of pairs of coreference clusters .",16,0,21
dataset/preprocessed/test-data/coreference_resolution/0,"This captures entity - level information with a large number of learned , continuous features instead of a small number of hand - crafted categorical ones .",17,0,27
dataset/preprocessed/test-data/coreference_resolution/0,"Using the cluster - pair representations , our network learns when combining two coreference clusters is desirable .",18,0,18
dataset/preprocessed/test-data/coreference_resolution/0,"At test time it builds up coreference clusters incrementally , starting with each mention in its own cluster and then merging a pair of clusters each step .",19,0,28
dataset/preprocessed/test-data/coreference_resolution/0,It makes these decisions with a novel easy - first cluster - ranking procedure that combines the strengths of cluster - ranking ( Rahman and and easy - first coreference algorithms .,20,0,32
dataset/preprocessed/test-data/coreference_resolution/0,Training incremental coreference systems is challenging because the coreference decisions facing a model depend on previous decisions it has already made .,21,0,22
dataset/preprocessed/test-data/coreference_resolution/0,We address this by using a learning - to - search algorithm inspired by SEARN to train our neural network .,22,0,21
dataset/preprocessed/test-data/coreference_resolution/0,This approach allows the model to learn which action ( a cluster merge ) available from the current state ( a partially completed coreference clustering ) will eventually lead to a high - scoring coreference partition .,23,0,37
dataset/preprocessed/test-data/coreference_resolution/0,"Our system uses little manual feature engineering , which means it is easily extended to multiple languages .",24,0,18
dataset/preprocessed/test-data/coreference_resolution/0,We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task dataset .,25,0,18
dataset/preprocessed/test-data/coreference_resolution/0,The cluster - ranking model significantly outperforms a mention - ranking model that does not use entity - level information .,26,0,21
dataset/preprocessed/test-data/coreference_resolution/0,We also show that using an easy - first strategy improves the performance of the cluster - ranking model .,27,0,20
dataset/preprocessed/test-data/coreference_resolution/0,"Our final system achieves CoNLL F 1 scores of 65.29 for English and 63.66 for Chinese , substantially outperforming other state - of - the - art systems .",28,0,29
dataset/preprocessed/test-data/coreference_resolution/0,Our cluster - ranking model is a single neural network that learns which coreference cluster merges are desirable .,31,0,19
dataset/preprocessed/test-data/coreference_resolution/0,"However , it is helpful to think of the network as being composed of distinct subnetworks .",32,0,17
dataset/preprocessed/test-data/coreference_resolution/0,The mention - pair encoder produces distributed representations for pairs of mentions by passing relevant features through a feedforward neural network .,33,0,22
dataset/preprocessed/test-data/coreference_resolution/0,"The cluster - pair encoder produces distributed representations for pairs of clusters by applying a pooling operation over the representations of relevant mention pairs , i.e. , pairs where one mention is in each cluster .",34,0,36
dataset/preprocessed/test-data/coreference_resolution/0,The clusterranking model then scores pairs of clusters by passing their representations through a single neural network layer .,35,0,19
dataset/preprocessed/test-data/coreference_resolution/0,We also train a mention - ranking model that scores pairs of mentions by passing their representations through a single neural network layer .,36,0,24
dataset/preprocessed/test-data/coreference_resolution/0,"Its parameters are used to initialize the clusterranking model , and the scores it produces are used to prune which candidate cluster merges the cluster - ranking model considers , allowing the cluster - ranking model to run much faster .",37,0,41
dataset/preprocessed/test-data/coreference_resolution/0,The system architecture is summarized in .,38,0,7
dataset/preprocessed/test-data/coreference_resolution/0,Mention - Pair Encoder,39,0,4
dataset/preprocessed/test-data/coreference_resolution/0,"Given a mention m and candidate antecedent a , the mention - pair encoder produces a distributed representation of the pair rm ( a , m ) ?",40,0,28
dataset/preprocessed/test-data/coreference_resolution/0,"Rd with a feedforward neural network , which is shown in .",41,0,12
dataset/preprocessed/test-data/coreference_resolution/0,"The candidate antecedent maybe any mention that occurs before min the document or NA , indicating that m has no antecedent .",42,0,22
dataset/preprocessed/test-data/coreference_resolution/0,"We also experimented with models based on Long Short - Term Memory recurrent neural networks ( Hochreiter and Schmidhuber , 1997 ) , but found these to perform slightly worse when used in an end - to - end coreference system due to heavy overfitting to the training data .",43,0,50
dataset/preprocessed/test-data/coreference_resolution/0,Input Layer .,44,0,3
dataset/preprocessed/test-data/coreference_resolution/0,"For each mention , the model extracts various words and groups of words thatare fed into the neural network .",45,0,20
dataset/preprocessed/test-data/coreference_resolution/0,Each word is represented by a vector w i ?,46,0,10
dataset/preprocessed/test-data/coreference_resolution/0,R dw .,47,0,3
dataset/preprocessed/test-data/coreference_resolution/0,Each group of words is represented by the average of the vectors of each word in the group .,48,0,19
dataset/preprocessed/test-data/coreference_resolution/0,"For each mention and pair of mentions , a small number of binary features and distance features are also extracted .",49,0,21
dataset/preprocessed/test-data/coreference_resolution/2,Deep Reinforcement Learning for Mention - Ranking Coreference Models,2,0,9
dataset/preprocessed/test-data/coreference_resolution/2,Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning .,4,1,15
dataset/preprocessed/test-data/coreference_resolution/2,In this paper we instead apply reinforcement learning to directly optimize a neural mention - ranking model for coreference evaluation metrics .,5,0,22
dataset/preprocessed/test-data/coreference_resolution/2,We experiment with two approaches : the REINFORCE policy gradient algorithm and a rewardrescaled max - margin objective .,6,0,19
dataset/preprocessed/test-data/coreference_resolution/2,"We find the latter to be more effective , resulting in significant improvements over the current state - of the - art on the English and Chinese portions of the CoNLL 2012 Shared Task .",7,0,35
dataset/preprocessed/test-data/coreference_resolution/2,1 Code and trained models are available at https,8,0,9
dataset/preprocessed/test-data/coreference_resolution/2,"Coreference resolution systems typically operate by making sequences of local decisions ( e.g. , adding a coreference link between two mentions ) .",10,0,23
dataset/preprocessed/test-data/coreference_resolution/2,"However , most measures of coreference resolution performance do not decompose over local decisions , which means the utility of a particular decision is not known until all other decisions have been made .",11,0,34
dataset/preprocessed/test-data/coreference_resolution/2,"Due to this difficulty , coreference systems are usually trained with loss functions that heuristically define the goodness of a particular coreference decision .",12,0,24
dataset/preprocessed/test-data/coreference_resolution/2,These losses contain hyperparameters thatare carefully selected to ensure the model performs well according to coreference evaluation metrics .,13,0,19
dataset/preprocessed/test-data/coreference_resolution/2,"This complicates training , especially across different languages and datasets where systems may work best with different settings of the hyperparameters .",14,0,22
dataset/preprocessed/test-data/coreference_resolution/2,"To address this , we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics .",15,0,23
dataset/preprocessed/test-data/coreference_resolution/2,"In particular , we modify the max-margin coreference objective proposed by by incorporating the reward associated with each coreference decision into the loss 's slack rescaling .",16,0,27
dataset/preprocessed/test-data/coreference_resolution/2,We also test the REINFORCE policy gradient algorithm .,17,0,9
dataset/preprocessed/test-data/coreference_resolution/2,Our model is a neural mention - ranking model .,18,0,10
dataset/preprocessed/test-data/coreference_resolution/2,Mention - ranking models score pairs of mentions for their likelihood of coreference rather than comparing partial coreference clusters .,19,0,20
dataset/preprocessed/test-data/coreference_resolution/2,Hence they operate in a simple setting where coreference decisions are made independently .,20,0,14
dataset/preprocessed/test-data/coreference_resolution/2,"Although they are less expressive than entity - centric approaches to coreference ( e.g. , Haghighi and Klein , 2010 ) , mention - ranking models are fast , scalable , and simple to train , causing them to be the dominant approach to coreference in recent years .",21,0,49
dataset/preprocessed/test-data/coreference_resolution/2,Having independent actions is particularly useful when applying reinforcement learning because it means a particular action 's effect on the final reward can be computed efficiently .,22,0,27
dataset/preprocessed/test-data/coreference_resolution/2,We evaluate the models on the English and Chinese portions of the CoNLL 2012 Shared Task .,23,0,17
dataset/preprocessed/test-data/coreference_resolution/2,The REINFORCE algorithm is competitive with a heuristic loss function while the reward - rescaled objective significantly outperforms both 1 .,24,0,21
dataset/preprocessed/test-data/coreference_resolution/2,We attribute this to reward rescaling being well suited for a ranking task due to its max - margin loss as well as benefiting from directly optimizing for coreference metrics .,25,0,31
dataset/preprocessed/test-data/coreference_resolution/2,"Error analysis shows that using the reward - rescaling loss results in a similar number of mistakes as the heuristic loss , but the mistakes tend to be less severe .",26,0,31
dataset/preprocessed/test-data/coreference_resolution/2,Neural Mention - Ranking Model,27,0,5
dataset/preprocessed/test-data/coreference_resolution/2,"We use the neural mention - ranking model described in , which we briefly go over in this section .",28,0,20
dataset/preprocessed/test-data/coreference_resolution/2,"Given a mention m and candidate antecedent c , the mention - ranking model produces a score for the pair s ( c , m ) indicating their compatibility for coreference with a feedforward neural network .",29,0,37
dataset/preprocessed/test-data/coreference_resolution/2,"The candidate antecedent maybe any mention that occurs before min the document or NA , indicating that m has no antecedent .",30,0,22
dataset/preprocessed/test-data/coreference_resolution/2,Input Layer .,31,0,3
dataset/preprocessed/test-data/coreference_resolution/2,"For each mention , the model extracts various words ( e.g. , the mention 's head word ) and groups of words ( e.g. , all words in the mention 's sentence ) thatare fed into the neural network .",32,0,40
dataset/preprocessed/test-data/coreference_resolution/2,Each word is represented by a vector w i ?,33,0,10
dataset/preprocessed/test-data/coreference_resolution/2,R dw .,34,0,3
dataset/preprocessed/test-data/coreference_resolution/2,Each group of words is represented by the average of the vectors of each word in the group .,35,0,19
dataset/preprocessed/test-data/coreference_resolution/2,"In addition to the embeddings , a small number of additional features are used , including distance , string matching , and speaker identification features .",36,0,26
dataset/preprocessed/test-data/coreference_resolution/2,See for the full set of features and an ablation study .,37,0,12
dataset/preprocessed/test-data/coreference_resolution/2,"These features are concatenated to produce an Idimensional vector h 0 , the input to the neural network .",38,0,19
dataset/preprocessed/test-data/coreference_resolution/2,"If c = NA , features defined over pairs of mentions are not included .",39,0,15
dataset/preprocessed/test-data/coreference_resolution/2,"For this case , we train a separate network with an identical architecture to the pair network except for the input layer to produce anaphoricity scores .",40,0,27
dataset/preprocessed/test-data/coreference_resolution/2,Hidden Layers .,41,0,3
dataset/preprocessed/test-data/coreference_resolution/2,The input gets passed through three hidden layers of rectified linear ( ReLU ) units .,42,0,16
dataset/preprocessed/test-data/coreference_resolution/2,Each unit in a hidden layer is fully connected to the previous layer :,43,0,14
dataset/preprocessed/test-data/coreference_resolution/2,Scoring Layer .,44,0,3
dataset/preprocessed/test-data/coreference_resolution/2,The final layer is a fully connected layer of size 1 :,45,0,12
dataset/preprocessed/test-data/coreference_resolution/2,where W 4 is a 1 M 3 weight matrix .,46,0,11
dataset/preprocessed/test-data/coreference_resolution/2,"At test time , the mention - ranking model links each mention with its highest scoring candidate antecedent .",47,0,19
dataset/preprocessed/test-data/coreference_resolution/2,Mention - ranking models are typically trained with heuristic loss functions thatare tuned via hyperparameters .,49,0,16
dataset/preprocessed/test-data/coreference_resolution/4,A Mention - Ranking Model for Abstract Anaphora Resolution,2,1,9
dataset/preprocessed/test-data/coreference_resolution/4,"Resolving abstract anaphora is an important , but difficult task for text understanding .",4,0,14
dataset/preprocessed/test-data/coreference_resolution/4,"Yet , with recent advances in representation learning this task becomes a more tangible aim .",5,0,16
dataset/preprocessed/test-data/coreference_resolution/4,A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its ( typically non-nominal ) antecedent .,6,0,28
dataset/preprocessed/test-data/coreference_resolution/4,We propose a mention - ranking model that learns how abstract anaphors relate to their antecedents with an LSTM - Siamese Net .,7,0,23
dataset/preprocessed/test-data/coreference_resolution/4,We overcome the lack of training data by generating artificial anaphoric sentenceantecedent pairs .,8,0,14
dataset/preprocessed/test-data/coreference_resolution/4,Our model outperforms state - of - the - art results on shell noun resolution .,9,0,16
dataset/preprocessed/test-data/coreference_resolution/4,We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus .,10,0,16
dataset/preprocessed/test-data/coreference_resolution/4,This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders .,11,0,22
dataset/preprocessed/test-data/coreference_resolution/4,"We found model variants that outperform the baselines for nominal anaphors , without training on individual anaphor data , but still lag behind for pronominal anaphors .",12,0,27
dataset/preprocessed/test-data/coreference_resolution/4,Our model selects syntactically plausible candidates and - if disregarding syntax - discriminates candidates using deeper features .,13,0,18
dataset/preprocessed/test-data/coreference_resolution/4,"Current research in anaphora ( or coreference ) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real Leo Born , Juri Opitz and Anette Frank contributed equally to this work .",15,1,38
dataset/preprocessed/test-data/coreference_resolution/4,"world , which is arguably the most frequently occurring type .",16,0,11
dataset/preprocessed/test-data/coreference_resolution/4,"Distinct from these are diverse types of abstract anaphora ( AA ) where reference is made to propositions , facts , events or properties .",17,0,25
dataset/preprocessed/test-data/coreference_resolution/4,An example is given in ( 1 ) below .,18,0,10
dataset/preprocessed/test-data/coreference_resolution/4,"While recent approaches address the resolution of selected abstract shell nouns ( Kolhatkar and Hirst , 2014 ) , we aim to resolve a wide range of abstract anaphors , such as the NP this trend in ( 1 ) , as well as pronominal anaphors ( this , that , or it ) .",19,0,55
dataset/preprocessed/test-data/coreference_resolution/4,"Henceforth , we refer to a sentence that contains an abstract anaphor as the anaphoric sentence ( AnaphS ) , and to a constituent that the anaphor refers to as the antecedent ( Antec ) ( cf. ( 1 ) ) .",20,0,42
dataset/preprocessed/test-data/coreference_resolution/4,"( 1 ) Ever-more powerful desktop computers , designed with one or more microprocessors as their "" brains "" , are expected to increasingly take on functions carried out by more expensive minicomputers and mainframes .",21,0,36
dataset/preprocessed/test-data/coreference_resolution/4,""" [ Antec The guys that make traditional hardware are really being obsoleted by microprocessor - based machines ] "" , said Mr. Benton .",22,0,25
dataset/preprocessed/test-data/coreference_resolution/4,"As a result of this trend AA , longtime powerhouses HP , IBM and Digital Equipment Corp. are scrambling to counterattack with microprocessor - based systems of their own . ]",24,0,31
dataset/preprocessed/test-data/coreference_resolution/4,A major obstacle for solving this task is the lack of sufficient amounts of annotated training data .,25,0,18
dataset/preprocessed/test-data/coreference_resolution/4,We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types .,26,0,20
dataset/preprocessed/test-data/coreference_resolution/4,"This enables us to use neural methods which have shown great success in related tasks : coreference resolution ( Clark and Manning , 2016 a ) , textual entailment , learning textual similarity , and discourse relation sense classification .",27,0,40
dataset/preprocessed/test-data/coreference_resolution/4,"Our model is inspired by the mention - ranking model for coreference resolution and combines it with a Siamese Net , for learning similarity between sentences .",28,0,27
dataset/preprocessed/test-data/coreference_resolution/4,"Given an anaphoric sentence ( AntecS in ( 1 ) ) and a candidate antecedent ( any constituent in a given context , e.g. being obsoleted by microprocessor - based machines in ( 1 ) ) , the LSTM - Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space .",29,0,56
dataset/preprocessed/test-data/coreference_resolution/4,These representations are combined into a joint representation used to calculate a score that characterizes the relation between them .,30,0,20
dataset/preprocessed/test-data/coreference_resolution/4,The learned score is used to select the highest - scoring antecedent candidate for the given anaphoric sentence and hence its anaphor .,31,0,23
dataset/preprocessed/test-data/coreference_resolution/4,We consider one anaphor at a time and provide the embedding of the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphorsimilar to the encoding proposed by for individuating multiply occurring predicates in SRL .,32,0,48
dataset/preprocessed/test-data/coreference_resolution/4,With deeper inspection we show that the model learns a relation between the anaphor in the anaphoric sentence and its antecedent .,33,0,22
dataset/preprocessed/test-data/coreference_resolution/4,displays our architecture .,34,0,4
dataset/preprocessed/test-data/coreference_resolution/4,"In contrast to other work , our method for generating training data is not confined to specific types of anaphora such as shell nouns or anaphoric connectives .",35,0,28
dataset/preprocessed/test-data/coreference_resolution/4,It produces large amounts of instances and is easily adaptable to other languages .,36,0,14
dataset/preprocessed/test-data/coreference_resolution/4,"This enables us to build a robust , knowledge - lean model for abstract anaphora resolution that easily extends to multiple languages .",37,0,23
dataset/preprocessed/test-data/coreference_resolution/4,We evaluate our model on the shell noun resolution dataset of and show that it outperforms their state - of - the - art results .,38,0,26
dataset/preprocessed/test-data/coreference_resolution/4,"Moreover , we report results of the model ( trained on our newly constructed dataset ) on unrestricted abstract anaphora instances from the ARRAU corpus .",39,0,26
dataset/preprocessed/test-data/coreference_resolution/4,To our knowledge this provides the first state - of - the - art benchmark on this data subset .,40,0,20
dataset/preprocessed/test-data/coreference_resolution/4,Our Tensor Flow 2 implementation of the model and scripts for data extraction are available at : https://github.com/amarasovic / neural-abstract-anaphora.,41,0,20
dataset/preprocessed/test-data/coreference_resolution/4,Related and prior work,42,0,4
dataset/preprocessed/test-data/coreference_resolution/4,"Abstract anaphora has been extensively studied in linguistics and shown to exhibit specific properties in terms of semantic antecedent types , their degrees of abstractness , and general dis - 2 course properties .",43,0,34
dataset/preprocessed/test-data/coreference_resolution/4,"In contrast to nominal anaphora , abstract anaphora is difficult to resolve , given that agreement and lexical match features are not applicable .",44,0,24
dataset/preprocessed/test-data/coreference_resolution/4,"Annotation of abstract anaphora is also difficult for humans , and thus , only few smaller - scale corpora have been constructed .",45,0,23
dataset/preprocessed/test-data/coreference_resolution/4,We evaluate our models on a subset of the AR - RAU corpus ) that contains abstract anaphors and the shell noun corpus used in .,46,0,26
dataset/preprocessed/test-data/coreference_resolution/4,We are not aware of other freely available abstract anaphora datasets .,47,0,12
dataset/preprocessed/test-data/coreference_resolution/4,Little work exists for the automatic resolution of abstract anaphora .,48,0,11
dataset/preprocessed/test-data/coreference_resolution/4,"Early work ( Eckert and has focused on spoken language , which exhibits specific properties .",49,0,16
dataset/preprocessed/test-data/face_detection/7,Recurrent Scale Approximation for Object Detection in CNN,2,1,8
dataset/preprocessed/test-data/face_detection/7,"Since convolutional neural network ( CNN ) lacks an inherent mechanism to handle large scale variations , we always need to compute feature maps multiple times for multiscale object detection , which has the bottleneck of computational cost in practice .",4,1,41
dataset/preprocessed/test-data/face_detection/7,"To address this , we devise a recurrent scale approximation ( RSA ) to compute feature map once only , and only through this map can we approximate the rest maps on other levels .",5,0,35
dataset/preprocessed/test-data/face_detection/7,"At the core of RSA is the recursive rolling out mechanism : given an initial map at a particular scale , it generates the prediction at a smaller scale that is half the size of input .",6,0,37
dataset/preprocessed/test-data/face_detection/7,"To further increase efficiency and accuracy , we ( a ) : design a scale - forecast network to globally predict potential scales in the image since there is no need to compute maps on all levels of the pyramid .",7,0,41
dataset/preprocessed/test-data/face_detection/7,( b ) : propose a landmark retracing network ( LRN ) to trace back locations of the regressed landmarks and generate a confidence score for each landmark ; LRN can effectively alleviate false positives caused by the accumulated error in RSA .,8,0,43
dataset/preprocessed/test-data/face_detection/7,The whole system can be trained end - to - end in a unified CNN framework .,9,0,17
dataset/preprocessed/test-data/face_detection/7,Experiments demonstrate that our proposed algorithm is superior against state - of - the - art methods on face detection benchmarks and achieves comparable results for generic proposal generation .,10,0,30
dataset/preprocessed/test-data/face_detection/7,The source code of our system is available .,11,0,9
dataset/preprocessed/test-data/face_detection/7,Object detection is one of the most important tasks in computer vision .,14,0,13
dataset/preprocessed/test-data/face_detection/7,The convolutional neural network ( CNN ) based approaches have been widely applied in object detection and recognition with promising performance .,15,0,22
dataset/preprocessed/test-data/face_detection/7,"To localize objects at arbitrary scales and locations in an image , we need to han -",16,0,17
dataset/preprocessed/test-data/face_detection/7,"Our codes and annotations mentioned in Sec.4.1 can be accessed at github.com/sciencefans/RSA-for-object-detection dle the variations caused by appearance , location and scale .",17,0,23
dataset/preprocessed/test-data/face_detection/7,"Most of the appearance variations can now be handled in CNN , benefiting from the invariance property of convolution and pooling operations .",18,0,23
dataset/preprocessed/test-data/face_detection/7,"The location variations can be naturally solved via sliding windows , which can be efficiently incorporated into CNN in a fully convolutional manner .",19,0,24
dataset/preprocessed/test-data/face_detection/7,"However , CNN itself does not have an inherent mechanism to handle the scale variations .",20,0,16
dataset/preprocessed/test-data/face_detection/7,"The scale problem is often addressed via two ways , namely , multi-shot by single - scale detector and single - shot by multi-scale detector .",21,0,26
dataset/preprocessed/test-data/face_detection/7,"The first way , as shown in , handles objects of different scales independently by resizing the input into different scales and then forwarding the resized images multiple times for detection .",22,0,32
dataset/preprocessed/test-data/face_detection/7,"Models in such a philosophy probably have the highest recall as long as the sampling of scales is dense enough , but they suffer from high computation cost and more false positives .",23,0,33
dataset/preprocessed/test-data/face_detection/7,"The second way , as depicted in , forwards the image only once and then directly regresses objects at multiple scales .",24,0,22
dataset/preprocessed/test-data/face_detection/7,Such a scheme takes the scale variation as a black box .,25,0,12
dataset/preprocessed/test-data/face_detection/7,"Although more parameters and complex structures would improve the performance , the spirit of direct regression still has limitations in real - time applications , for example in face detection , the size of faces can vary from 20 20 to 1920 1080 .",26,0,44
dataset/preprocessed/test-data/face_detection/7,"To handle the scale variation in a CNN - based detection system in terms of both efficiency and accuracy , we are inspired by the fast feature pyramid work proposed by Dollr et al. , where a detection system using hand - crafted features is designed for pedestrian detection .",27,0,50
dataset/preprocessed/test-data/face_detection/7,It is found that image gradients across scales can be predicted based on natural image statistics .,28,0,17
dataset/preprocessed/test-data/face_detection/7,They showed that dense feature pyramids can be efficiently constructed on top of coarsely sampled feature pyramids .,29,0,18
dataset/preprocessed/test-data/face_detection/7,"In this paper , we extend the spirit of fast feature pyramid to CNN and go a few steps further .",30,0,21
dataset/preprocessed/test-data/face_detection/7,"Our solution to the feature pyramid in CNN descends from the observations of modern CNN - based detectors , including Faster - RCNN , R - FCN , SSD , YOLO and STN , where feature maps are first computed and the detection results are decoded from the maps afterwards .",31,0,51
dataset/preprocessed/test-data/face_detection/7,"However , the computation cost of generating feature maps becomes a bottleneck for methods using multi-scale testing and it seems not to be a neat solution to the scale variation problem .",32,0,32
dataset/preprocessed/test-data/face_detection/7,"To this end , our philosophy of designing an elegant detection system is that we calculate the feature pyramid once only , and only through that pyramid can we approximate the rest feature pyramids at other scales .",33,0,38
dataset/preprocessed/test-data/face_detection/7,The intuition is illustrated in .,34,0,6
dataset/preprocessed/test-data/face_detection/7,"In this work , we propose a recurrent scale approximation ( RSA , see ) unit to achieve the goal aforementioned .",35,0,22
dataset/preprocessed/test-data/face_detection/7,The RSA unit is designed to be plugged at some specific depths in a network and to be fed with an initial feature map at the largest scale .,36,0,29
dataset/preprocessed/test-data/face_detection/7,The unit convolves the input in a recurrent manner to generate the prediction of the feature map that is half the size of the input .,37,0,26
dataset/preprocessed/test-data/face_detection/7,Such a scheme could feed the network with input at one scale only and approximate the rest features at smaller scales through a learnable RSA unit - a balance considering both efficiency and accuracy .,38,0,35
dataset/preprocessed/test-data/face_detection/7,We propose two more schemes to further save the computational budget and improve the detection performance under the RSA framework .,39,0,21
dataset/preprocessed/test-data/face_detection/7,The first is a scale - forecast network to globally predict potential scales for a novel image and we compute feature pyramids for just a certain set of scales based on the prediction .,40,0,34
dataset/preprocessed/test-data/face_detection/7,"There are only a few scales of objects appearing in the image and hence most of the feature pyramids correspond to the background , indicating a redundancy if maps on all levels are computed .",41,0,35
dataset/preprocessed/test-data/face_detection/7,The second is a landmark retracing network that retraces the location of the regressed landmarks in the preceding layers and generates a confidence score for each landmark based on the landmark feature set .,42,0,34
dataset/preprocessed/test-data/face_detection/7,The final score of identifying a face within an anchor is thereby revised by the LRN network .,43,0,18
dataset/preprocessed/test-data/face_detection/7,Such a design alleviates false positives caused by the accumulated error in the RSA unit .,44,0,16
dataset/preprocessed/test-data/face_detection/7,The pipeline of our proposed algorithm is shown in .,45,0,10
dataset/preprocessed/test-data/face_detection/7,The three components can be incorporated into a unified CNN framework and trained end - to - end .,46,0,19
dataset/preprocessed/test-data/face_detection/7,Experiments show that our approach is superior to other state - of the - art methods in face detection and achieves reasonable results for object detection .,47,0,27
dataset/preprocessed/test-data/face_detection/7,"To sum up , our contributions in this work are as follows :",48,0,13
dataset/preprocessed/test-data/face_detection/7,"1 ) We prove that deep CNN features for an image can be approximated from different scales using a portable recurrent unit ( RSA ) , which fully leverages efficiency and accuracy .",49,0,33
dataset/preprocessed/test-data/face_detection/3,A Fast and Accurate Unconstrained Face Detector,2,0,7
dataset/preprocessed/test-data/face_detection/3,"We propose a method to address challenges in unconstrained face detection , such as arbitrary pose variations and occlusions .",4,1,20
dataset/preprocessed/test-data/face_detection/3,"First , a new image feature called Normalized Pixel Difference ( NPD ) is proposed .",5,0,16
dataset/preprocessed/test-data/face_detection/3,"NPD feature is computed as the difference to sum ratio between two pixel values , inspired by the Weber Fraction in experimental psychology .",6,0,24
dataset/preprocessed/test-data/face_detection/3,"The new feature is scale invariant , bounded , and is able to reconstruct the original image .",7,0,18
dataset/preprocessed/test-data/face_detection/3,"Second , we propose a deep quadratic tree to learn the optimal subset of NPD features and their combinations , so that complex face manifolds can be partitioned by the learned rules .",8,0,33
dataset/preprocessed/test-data/face_detection/3,"This way , only a single soft - cascade classifier is needed to handle unconstrained face detection .",9,0,18
dataset/preprocessed/test-data/face_detection/3,"Furthermore , we show that the NPD features can be efficiently obtained from a lookup table , and the detection template can be easily scaled , making the proposed face detector very fast .",10,0,34
dataset/preprocessed/test-data/face_detection/3,"Experimental results on three public face datasets ( FDDB , GENKI , and CMU - MIT ) show that the proposed method achieves state - of - the - art performance in detecting unconstrained faces with arbitrary pose variations and occlusions in cluttered scenes .",11,0,45
dataset/preprocessed/test-data/face_detection/3,The objective of face detection is to find and locate faces in an image .,13,0,15
dataset/preprocessed/test-data/face_detection/3,It is the first step in automatic face recognition applications .,14,1,11
dataset/preprocessed/test-data/face_detection/3,Face detection has been well studied for frontal and near frontal faces .,15,0,13
dataset/preprocessed/test-data/face_detection/3,"The Viola and Jones ' face detector is the most well known face detection algorithm , which is based on Haar - like features and cascade AdaBoost classifier .",16,0,29
dataset/preprocessed/test-data/face_detection/3,"However , in unconstrained scenes such as faces in a crowd , state - of - the - art face detectors fail to perform well due to large pose variations , illumination variations , occlusions , expression variations , out - of - focus blur , and low image resolution .",17,0,51
dataset/preprocessed/test-data/face_detection/3,"For example , the Viola - Jones face detector fails to detect most of the face images in the Face Detection Data set and Benchmark ( FDDB ) database ( examples shown in ) due to the difficulties mentioned above .",18,0,41
dataset/preprocessed/test-data/face_detection/3,"In this paper , we refer to face detection with arbitrary facial variations as the unconstrained face detection problem .",19,1,20
dataset/preprocessed/test-data/face_detection/3,We are interested in face detection in unconstrained scenarios such as video surveillance or images captured by hand - held devices .,20,0,22
dataset/preprocessed/test-data/face_detection/3,"Numerous face detection methods have been developed following Viola and Jones ' work , mainly focusing on extracting different types of features and developing different cascade structures .",21,0,28
dataset/preprocessed/test-data/face_detection/3,"A variety of complex features , , , , , , , , .",22,0,14
dataset/preprocessed/test-data/face_detection/3,"Face images annotated ( red ellipses ) in the FDDB database. , have been proposed to replace the Haarlike features used in .",23,0,23
dataset/preprocessed/test-data/face_detection/3,"While these methods can improve the face detection performance to some extent , they generate a very large number ( hundreds of thousands ) of features and the resulting systems take too much time to train .",24,0,37
dataset/preprocessed/test-data/face_detection/3,"Another development in face detection has been to learn different cascade structures for multiview face detection , such as parallel cascade , pyramid architecture , and Width - First - Search ( WFS ) tree .",25,0,36
dataset/preprocessed/test-data/face_detection/3,All these methods need to learn one cascade classifier for each specific facial view ( or view range ) .,26,0,20
dataset/preprocessed/test-data/face_detection/3,"In unconstrained scenarios , however , it is not easy to define all possible views of a face , and the computational cost increases with an increasing number of classifiers in complex cascade structures .",27,0,35
dataset/preprocessed/test-data/face_detection/3,"Moreover , these approaches require manual labeling of face pose in each training image .",28,0,15
dataset/preprocessed/test-data/face_detection/3,"While some of the available methods , , can handle multiview faces , they are notable ar Xiv:1408.1656v3 [ cs.CV ] 7 Sep 2015 to simultaneously consider other challenges such as occlusion .",29,0,33
dataset/preprocessed/test-data/face_detection/3,"In fact , since these methods require partitioning multiview data into known poses , occlusion is not easy to handle in this way .",30,0,24
dataset/preprocessed/test-data/face_detection/3,"On the other hand , while several studies addressed face detection under occlusion , , , , , they constrained themselves to detect only frontal faces under occlusion .",31,0,29
dataset/preprocessed/test-data/face_detection/3,"As discussed in , a robust face detection algorithm should be effective under arbitrary variations in pose and occlusion , which remains an unresolved challenging problem .",32,0,27
dataset/preprocessed/test-data/face_detection/3,"In this paper , we are interested in developing effective features and robust classifiers for unconstrained face detection with arbitrary facial variation .",33,0,23
dataset/preprocessed/test-data/face_detection/3,"First , we propose a simple pixel - level feature , called the Normalized Pixel Difference ( NPD ) .",34,0,20
dataset/preprocessed/test-data/face_detection/3,"An NPD is computed as the ratio of the difference between any two pixel intensity values to the sum of their values , in the same form as the Weber Fraction in experimental psychology .",35,0,35
dataset/preprocessed/test-data/face_detection/3,"The NPD feature has several desirable properties , such as scale invariance , boundedness , and ability to reconstruct the original image .",36,0,23
dataset/preprocessed/test-data/face_detection/3,"we further show that NPD features can be obtained from a lookup table , and the resulting face detection template can be easily scaled for multiscale face detection .",37,0,29
dataset/preprocessed/test-data/face_detection/3,"Secondly , we propose a deep quadratic tree learning method and construct a single soft - cascade AdaBoost classifier to handle complex face manifolds and arbitrary pose and occlusion conditions .",38,0,31
dataset/preprocessed/test-data/face_detection/3,"While individual NPD features may have "" weak "" discriminative ability , our work indicates that a subset of NPD features can be optimally learned and combined to construct more discriminative features in a deep quadratic tree .",39,0,38
dataset/preprocessed/test-data/face_detection/3,"In this way , different types of faces can be automatically divided into different leaves of a tree classifier , and the complex face manifold in a high dimensional space can be partitioned in the learning process .",40,0,38
dataset/preprocessed/test-data/face_detection/3,"This is the "" divide and conquer "" strategy to tackle unconstrained face detection in a single classifier , without pre-labeling of views in the training set of face images .",41,0,31
dataset/preprocessed/test-data/face_detection/3,"The resulting face detector is robust to variations in pose , occlusion , and illumination , as well as to blur and low image resolution .",42,0,26
dataset/preprocessed/test-data/face_detection/3,The novelty of this work is summarized as follows :,43,0,10
dataset/preprocessed/test-data/face_detection/3,"A new type of feature , called NPD is proposed , which is efficient to compute and has several desirable properties , including scale invariance , boundedness , and enabling reconstruction of the original image .",44,0,36
dataset/preprocessed/test-data/face_detection/3,A deep quadratic tree learner is proposed to learn and combine an optimal subset of NPD features to boost their discriminability .,45,0,22
dataset/preprocessed/test-data/face_detection/3,"In this way , only a single soft - cascade AdaBoost classifier is needed to handle unconstrained faces with occlusions and arbitrary viewpoints , without pose labeling or clustering in the training stage .",46,0,34
dataset/preprocessed/test-data/face_detection/3,The advantages of the proposed approach include :,47,0,8
dataset/preprocessed/test-data/face_detection/3,"The NPD feature evaluation is extremely fast , requiring a single memory access using a lookup table .",48,0,18
dataset/preprocessed/test-data/face_detection/3,Multiscale face detection can be easily achieved by applying pre-scaled detection templates .,49,0,13
dataset/preprocessed/test-data/face_detection/16,"HyperFace : A Deep Multi-task Learning Framework for Face Detection , Landmark Localization , Pose Estimation , and Gender Recognition",2,1,20
dataset/preprocessed/test-data/face_detection/16,"We present an algorithm for simultaneous face detection , landmarks localization , pose estimation and gender recognition using deep convolutional neural networks ( CNN ) .",4,0,26
dataset/preprocessed/test-data/face_detection/16,"The proposed method called , HyperFace , fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features .",5,0,32
dataset/preprocessed/test-data/face_detection/16,It exploits the synergy among the tasks which boosts up their individual performances .,6,0,14
dataset/preprocessed/test-data/face_detection/16,"Additionally , we propose two variants of HyperFace :",7,0,9
dataset/preprocessed/test-data/face_detection/16,"( 1 ) HyperFace - ResNet that builds on the ResNet - 101 model and achieves significant improvement in performance , and ( 2 ) Fast - HyperFace that uses a high recall fast face detector for generating region proposals to improve the speed of the algorithm .",8,0,48
dataset/preprocessed/test-data/face_detection/16,Extensive experiments show that the proposed models are able to capture both global and local information in faces and performs significantly better than many competitive algorithms for each of these four tasks .,9,0,33
dataset/preprocessed/test-data/face_detection/16,"D ETECTION and analysis of faces is a challenging problem in computer vision , and has been actively researched for applications such as face verification , face tracking , person identification , etc .",11,0,34
dataset/preprocessed/test-data/face_detection/16,"Although recent methods based on deep Convolutional Neural Networks ( CNN ) have achieved remarkable results for the face detection task , , , it is still difficult to obtain facial landmark locations , head pose estimates and gender information from face images containing extreme poses , illumination and resolution variations .",12,0,52
dataset/preprocessed/test-data/face_detection/16,"The tasks of face detection , landmark localization , pose estimation and gender classification have generally been solved as separate problems .",13,0,22
dataset/preprocessed/test-data/face_detection/16,"Recently , it has been shown that learning correlated tasks simultaneously can boost the performance of individual tasks , , .",14,0,21
dataset/preprocessed/test-data/face_detection/16,"In this paper , we present a novel framework based on CNNs for simultaneous face detection , facial landmarks localization , head pose estimation and gender recognition from a given image ( see ) .",15,0,35
dataset/preprocessed/test-data/face_detection/16,We design a CNN architecture to learn common features for these tasks and exploit the synergy among them .,16,0,19
dataset/preprocessed/test-data/face_detection/16,We exploit the fact that information contained in features is hierarchically distributed throughout the network as demonstrated in .,17,0,19
dataset/preprocessed/test-data/face_detection/16,"Lower layers respond to edges and corners , and hence contain better localization properties .",18,0,15
dataset/preprocessed/test-data/face_detection/16,They are more suitable for learning landmarks localization and pose estimation tasks .,19,0,13
dataset/preprocessed/test-data/face_detection/16,"R. On the other hand , deeper layers are class - specific and suitable for learning complex tasks such as face detection and gender recognition .",20,0,26
dataset/preprocessed/test-data/face_detection/16,It is evident that we need to make use of all the intermediate layers of a deep CNN in order to train different tasks under consideration .,21,0,27
dataset/preprocessed/test-data/face_detection/16,We refer the set of intermediate layer features as hyperfeatures .,22,0,11
dataset/preprocessed/test-data/face_detection/16,We borrow this term from which uses it to denote a stack of local histograms for multilevel image coding .,23,0,20
dataset/preprocessed/test-data/face_detection/16,"Since a CNN architecture contains multiple layers with hundreds of feature maps in each layer , the overall dimension of hyperfeatures is too large to be efficient for learning multiple tasks .",24,0,32
dataset/preprocessed/test-data/face_detection/16,"Moreover , the hyperfeatures must be associated in away that they efficiently encode the arXiv:1603.01249v3 [ cs. CV ] 6 Dec 2017 features common to the multiple tasks .",25,0,29
dataset/preprocessed/test-data/face_detection/16,This can be handled using feature fusion techniques .,26,0,9
dataset/preprocessed/test-data/face_detection/16,Features fusion aims to transform the features to a common subspace where they can be combined linearly or non-linearly .,27,0,20
dataset/preprocessed/test-data/face_detection/16,Recent advances in deep learning have shown that CNNs are capable of estimating an arbitrary complex function .,28,0,18
dataset/preprocessed/test-data/face_detection/16,"Hence , we construct a separate fusion - CNN to fuse the hyperfeatures .",29,0,14
dataset/preprocessed/test-data/face_detection/16,"In order to learn the tasks , we train them simultaneously using multiple loss functions .",30,0,16
dataset/preprocessed/test-data/face_detection/16,"In this way , the features get better at understanding faces , which leads to improvements in the performances of individual tasks .",31,0,23
dataset/preprocessed/test-data/face_detection/16,The deep CNN combined with the fusion - CNN can be learned together in an end -toend fashion .,32,0,19
dataset/preprocessed/test-data/face_detection/16,"We also study the performance of face detection , landmarks localization , pose estimation and gender recognition tasks using off - the - shelf Region - based CNN ( R - CNN ) approach .",33,0,35
dataset/preprocessed/test-data/face_detection/16,"Although R - CNN for face detection has been explored in DP2 MFD , we provide a comprehensive study of all these tasks based on R - CNN .",34,0,29
dataset/preprocessed/test-data/face_detection/16,"Furthermore , we study the multitask approach without fusing the intermediate layers of CNN .",35,0,15
dataset/preprocessed/test-data/face_detection/16,Detailed experiments show that the multi-task learning method performs better than methods based on individual learning .,36,0,17
dataset/preprocessed/test-data/face_detection/16,Fusing the intermediate layer features provides additional performance boost .,37,0,10
dataset/preprocessed/test-data/face_detection/16,This paper makes the following contributions .,38,0,7
dataset/preprocessed/test-data/face_detection/16,"Region Proposals ( IRP ) and Landmarks - based Non - Maximum Suppression ( L - NMS ) , which leverage the multi-task information obtained from the CNN to improve the overall performance .",39,0,34
dataset/preprocessed/test-data/face_detection/16,3 ) We study the performance of R - CNN - based approaches for individual tasks and the multi-task approach without intermediate layer fusion .,40,0,25
dataset/preprocessed/test-data/face_detection/16,4 ) We achieve significant improvement in performance on challenging unconstrained datasets for all of these four tasks .,41,0,19
dataset/preprocessed/test-data/face_detection/16,This paper is organized as follows .,42,0,7
dataset/preprocessed/test-data/face_detection/16,Section 2 reviews related work .,43,0,6
dataset/preprocessed/test-data/face_detection/16,Section 3 describes the proposed HyperFace framework in detail .,44,0,10
dataset/preprocessed/test-data/face_detection/16,"Section 4 describes the implementation of R - CNN , Multitask Face and HF - ResNet approaches .",45,0,18
dataset/preprocessed/test-data/face_detection/16,Section 5 provides the results of HyperFace and HF - ResNet along with R - CNN baselines on challenging datasets .,46,0,21
dataset/preprocessed/test-data/face_detection/16,"Finally , Section 6 concludes the paper with a brief summary and discussion .",47,0,14
dataset/preprocessed/test-data/face_detection/16,Multi - Task Learning :,49,0,5
dataset/preprocessed/test-data/face_detection/10,DSFD : Dual Shot Face Detector,2,0,6
dataset/preprocessed/test-data/face_detection/10,"In this paper , we propose a novel face detection network with three novel contributions that address three key aspects of face detection , including better feature learning , progressive loss design and anchor assign based data augmentation , respectively .",4,1,41
dataset/preprocessed/test-data/face_detection/10,"First , we propose a Feature Enhance Module ( FEM ) for enhancing the original feature maps to extend the single shot detector to dual shot detector .",5,0,28
dataset/preprocessed/test-data/face_detection/10,"Second , we adopt Progressive Anchor Loss ( PAL ) computed by two different sets of anchors to effectively facilitate the features .",6,0,23
dataset/preprocessed/test-data/face_detection/10,"Third , we use an Improved Anchor Matching ( IAM ) by integrating novel anchor assign strategy into data aug -",7,0,21
dataset/preprocessed/test-data/face_detection/10,"Face detection is a fundamental step for various facial applications , like face alignment , parsing , recognition , and verification .",9,0,22
dataset/preprocessed/test-data/face_detection/10,"As the pioneering work for face detection , Viola - Jones adopts AdaBoost algorithm with hand - crafted features , which are now replaced by deeply learned features from the convolutional neural network ( CNN ) that achieves great progress .",10,0,41
dataset/preprocessed/test-data/face_detection/10,"Although the CNN based face detectors have being extensively studied , detecting faces with high degree of variability in scale , pose , occlusion , expression , appearance and illumination in real - world scenarios remains a challenge .",11,0,39
dataset/preprocessed/test-data/face_detection/10,Previous state - of - the - art face detectors can be roughly divided into two categories .,12,0,18
dataset/preprocessed/test-data/face_detection/10,The first one is mainly based on the Region Proposal Network ( RPN ) adopted in Faster RCNN and employs two stage detection schemes .,13,0,25
dataset/preprocessed/test-data/face_detection/10,RPN is trained end - to - end and generates highquality region proposals which are further refined by Fast R - CNN detector .,14,0,24
dataset/preprocessed/test-data/face_detection/10,"The other one is Single Shot Detector ( SSD ) based one - stage methods , which get rid of RPN , and directly predict the bounding boxes and confidence .",15,0,31
dataset/preprocessed/test-data/face_detection/10,"Recently , one - stage face detection framework has attracted more attention due to its higher inference efficiency and straightforward system deployment .",16,0,23
dataset/preprocessed/test-data/face_detection/10,"Despite the progress achieved by the above methods , there are still some problems existed in three aspects :",17,0,19
dataset/preprocessed/test-data/face_detection/10,Feature learning Feature extraction part is essential fora face detector .,18,0,11
dataset/preprocessed/test-data/face_detection/10,"Currently , Feature Pyramid Network ( FPN ) is widely used in state - of - the - art face detectors for rich features .",19,0,25
dataset/preprocessed/test-data/face_detection/10,"However , FPN just aggregates hierarchical feature maps between high and low - level output layers , which does not consider the current layer 's information , and the context relationship between anchors is ignored .",20,0,36
dataset/preprocessed/test-data/face_detection/10,The conventional loss functions used in object detection include a regression loss for the face region and a classification loss for identifying if a face is detected or not .,22,0,30
dataset/preprocessed/test-data/face_detection/10,"To further address the class imbalance problem , Lin et al.",23,0,11
dataset/preprocessed/test-data/face_detection/10,propose Focal Loss to focus training on a sparse set of hard examples .,24,0,14
dataset/preprocessed/test-data/face_detection/10,"To use all original and enhanced features , propose Hierarchical Loss to effectively learn the network .",25,0,17
dataset/preprocessed/test-data/face_detection/10,"However , the above loss functions do not consider progressive learning ability of feature maps in both of different levels and shots .",26,0,23
dataset/preprocessed/test-data/face_detection/10,"Anchor matching Basically , pre-set anchors for each feature map are generated by regularly tiling a collection of boxes with different scales and aspect ratios on the image .",27,0,29
dataset/preprocessed/test-data/face_detection/10,Some works analyze a series of reasonable anchor scales and anchor compensation strategy to increase positive anchors .,28,0,18
dataset/preprocessed/test-data/face_detection/10,"However , such strategy ignores random sampling in data augmentation , which still causes imbalance between positive and negative anchors .",29,0,21
dataset/preprocessed/test-data/face_detection/10,"In this paper , we propose three novel techniques to address the above three issues , respectively .",30,0,18
dataset/preprocessed/test-data/face_detection/10,"First , we introduce a Feature Enhance Module ( FEM ) to enhance the discriminability and robustness of the features , which combines the advantages of the FPN in PyramidBox and Receptive Field Block ( RFB ) in RFBNet .",31,0,40
dataset/preprocessed/test-data/face_detection/10,"Second , motivated by the hierarchical loss and pyramid anchor in PyramidBox , we design Progressive Anchor Loss ( PAL ) that uses progressive anchor sizes for not only different levels , but also different shots .",32,0,37
dataset/preprocessed/test-data/face_detection/10,"Specifically , we assign smaller anchor sizes in the first shot , and use larger sizes in the second shot .",33,0,21
dataset/preprocessed/test-data/face_detection/10,"Third , we propose Improved Anchor Matching ( IAM ) , which integrates anchor partition strategy and anchor-based data augmentation to better match anchors and ground truth faces , and thus provides better initialization for the regressor .",34,0,38
dataset/preprocessed/test-data/face_detection/10,The three aspects are complementary so that these techniques can work together to further improve the performance .,35,0,18
dataset/preprocessed/test-data/face_detection/10,"Besides , since these techniques are all related to two - stream design , we name the proposed network as Dual Shot Face Detector ( DSFD ) .",36,0,28
dataset/preprocessed/test-data/face_detection/10,"shows the effectiveness of DSFD on various variations , especially on extreme small faces or heavily occluded faces .",37,0,19
dataset/preprocessed/test-data/face_detection/10,"In summary , the main contributions of this paper include :",38,0,11
dataset/preprocessed/test-data/face_detection/10,A novel Feature Enhance Module to utilize different level information and thus obtain more discriminability and robustness features .,39,0,19
dataset/preprocessed/test-data/face_detection/10,Auxiliary supervisions introduced in early layers via a set of smaller anchors to effectively facilitate the features .,40,0,18
dataset/preprocessed/test-data/face_detection/10,An improved anchor matching strategy to match anchors and ground truth faces as far as possible to provide better initialization for the regressor .,41,0,24
dataset/preprocessed/test-data/face_detection/10,Comprehensive experiments conducted on popular benchmarks FDDB and WIDER FACE to demonstrate the superiority of our proposed DSFD network compared with the state - of - the - art methods .,42,0,31
dataset/preprocessed/test-data/face_detection/10,We review the prior works from three perspectives .,44,0,9
dataset/preprocessed/test-data/face_detection/10,"Feature Learning Early works on face detection mainly rely on hand - crafted features , such as Harr - like features , control point set , edge orientation histograms .",45,0,30
dataset/preprocessed/test-data/face_detection/10,"However , hand - crafted features design is lack of guidance .",46,0,12
dataset/preprocessed/test-data/face_detection/10,"With the great progress of deep learning , handcrafted features have been replaced by Convolutional Neural Networks ( CNN ) .",47,0,21
dataset/preprocessed/test-data/face_detection/10,"For example , Overfeat , Cascade - CNN , MTCNN adopt CNN as a sliding window detector on image pyramid to build feature pyramid .",48,0,25
dataset/preprocessed/test-data/face_detection/10,"However , using an image pyramid is slow and memory inefficient .",49,0,12
dataset/preprocessed/test-data/face_detection/21,Supervised Transformer Network for Efficient Face Detection,2,1,7
dataset/preprocessed/test-data/face_detection/21,Large pose variations remain to be a challenge that confronts real - word face detection .,4,1,16
dataset/preprocessed/test-data/face_detection/21,"We propose a new cascaded Convolutional Neural Network , dubbed the name Supervised Transformer Network , to address this challenge .",5,0,21
dataset/preprocessed/test-data/face_detection/21,"The first stage is a multi-task Region Proposal Network ( RPN ) , which simultaneously predicts candidate face regions along with associated facial landmarks .",6,0,25
dataset/preprocessed/test-data/face_detection/21,The candidate regions are then warped by mapping the detected facial landmarks to their canonical positions to better normalize the face patterns .,7,0,23
dataset/preprocessed/test-data/face_detection/21,"The second stage , which is a RCNN , then verifies if the warped candidate regions are valid faces or not .",8,0,22
dataset/preprocessed/test-data/face_detection/21,"We conduct end - to - end learning of the cascaded network , including optimizing the canonical positions of the facial landmarks .",9,0,23
dataset/preprocessed/test-data/face_detection/21,This supervised learning of the transformations automatically selects the best scale to differentiate face / non - face patterns .,10,0,20
dataset/preprocessed/test-data/face_detection/21,"By combining feature maps from both stages of the network , we achieve state - of - the - art detection accuracies on several public benchmarks .",11,0,27
dataset/preprocessed/test-data/face_detection/21,"For real - time performance , we run the cascaded network only on regions of interests produced from a boosting cascade face detector .",12,0,24
dataset/preprocessed/test-data/face_detection/21,Our detector runs at 30 FPS on a single CPU core fora VGA - resolution image .,13,0,17
dataset/preprocessed/test-data/face_detection/21,"Among the various factors that confront real - world face detection , large pose variations remain to be a big challenge .",15,0,22
dataset/preprocessed/test-data/face_detection/21,"For example , the seminal Viola - Jones detector works well for near - frontal faces , but become much less effective for faces in poses that are far from frontal views , due to the weakness of the Haar features on non-frontal faces .",16,0,45
dataset/preprocessed/test-data/face_detection/21,There were abundant works attempted to tackle with large pose variations under the regime of the boosting cascade advocated by Viola and Jones .,17,0,24
dataset/preprocessed/test-data/face_detection/21,Most of them adopt a divide - and - conquer strategy to build a multi -view face detector .,18,0,19
dataset/preprocessed/test-data/face_detection/21,Some works proposed to train a detector cascade for each view and combine their results of all detectors at the test time .,19,0,23
dataset/preprocessed/test-data/face_detection/21,Some other works proposed to first estimate the face pose and then run the cascade of the corresponding face pose to verify the detection .,20,0,25
dataset/preprocessed/test-data/face_detection/21,"The complexity of the former approach increases with the number of pose categories , while the accuracy of the latter is prone to the mistakes of pose estimation .",21,0,29
dataset/preprocessed/test-data/face_detection/21,Part - based model offers an alternative solution .,22,0,9
dataset/preprocessed/test-data/face_detection/21,"These detectors are flexible and robust to both pose variation and partial occlusion , since they can reliably detect the faces based on some confident part detections .",23,0,28
dataset/preprocessed/test-data/face_detection/21,"However , these methods always require the target face to be large and clear , which is essential to reliably model the parts .",24,0,24
dataset/preprocessed/test-data/face_detection/21,"Other works approach to this issue by using more sophisticated invariant features other than Haar wavelets , e.g. , HOG , SIFT , multiple channel features , and high - level CNN features .",25,0,34
dataset/preprocessed/test-data/face_detection/21,"Besides these model - based methods ,",26,0,7
dataset/preprocessed/test-data/face_detection/21,"Shen et al. proposed to use an exemplar - based method to detect faces by image retrieval , which achieved state - of - the - art detection accuracy .",27,0,30
dataset/preprocessed/test-data/face_detection/21,It has been shown in recent years that a face detector trained end - to - end using DNN can significantly outperforms previous methods .,28,0,25
dataset/preprocessed/test-data/face_detection/21,"However , to effectively handle the different variations , especially pose variations , it often requires a DNN with lots of parameters , inducing high computational cost .",29,0,28
dataset/preprocessed/test-data/face_detection/21,"To address the conflicting challenge , Li et al. proposed a cascade DNN architecture at multiple resolutions .",30,0,18
dataset/preprocessed/test-data/face_detection/21,"It quickly rejects the background regions in the low resolution stages , and carefully evaluates the challenging candidates in the high resolution stage .",31,0,24
dataset/preprocessed/test-data/face_detection/21,"However , the set of DNNs in Li et al. are trained sequentially , instead of end - to - end , which may not be desirable .",32,0,28
dataset/preprocessed/test-data/face_detection/21,"In contrast , we propose a new cascade Convolutional Neural Network that is trained end - to - end .",33,0,20
dataset/preprocessed/test-data/face_detection/21,"The first stage is a multi-task Region Proposal Network ( RPN ) , which simultaneously proposes candidate face regions along with associated facial landmarks .",34,0,25
dataset/preprocessed/test-data/face_detection/21,"Inspired by Chen et al. , we jointly conduct face detection and face alignment , since face alignment is helpful to distinguish faces / non - faces patterns .",35,0,29
dataset/preprocessed/test-data/face_detection/21,"Different from Li et al. , this network is calculated on the original resolution to better leverage more discriminative information .",36,0,21
dataset/preprocessed/test-data/face_detection/21,"The alignment step warps each candidate face region to a canonical pose , which maps the facial landmarks into a set of canonical positions .",37,0,25
dataset/preprocessed/test-data/face_detection/21,"The aligned candidate face region is then fed into the second - stage network , a RCNN , for further verification .",38,0,22
dataset/preprocessed/test-data/face_detection/21,Note we only keep the K face candidate regions with top responses in a local neighborhood from the RPN .,39,0,20
dataset/preprocessed/test-data/face_detection/21,"In other words , those Non-top K regions are suppressed .",40,0,11
dataset/preprocessed/test-data/face_detection/21,This helps increase detection recall .,41,0,6
dataset/preprocessed/test-data/face_detection/21,"Inspired by previous work , which revealed that joint features from different spatial resolutions or scales will improve accuracy .",42,0,20
dataset/preprocessed/test-data/face_detection/21,"We concatenate the feature maps from the two cascaded networks together to form an architecture that is trained end - to - end , as shown in .",43,0,28
dataset/preprocessed/test-data/face_detection/21,"Note in the learning process , we treat the set of canonical positions also as parameters , which are learnt in the end - to - end learning process .",44,0,30
dataset/preprocessed/test-data/face_detection/21,Note that the canonical positions of the facial landmarks in the aligned face image and the predicted facial landmarks in the candidate face region jointly defines the transform from the candidate face region .,45,0,34
dataset/preprocessed/test-data/face_detection/21,"In the end - to - end training , the training of the first - stage RPN to predict facial landmarks is also supervised by annotated facial landmarks in each true face regions .",46,0,34
dataset/preprocessed/test-data/face_detection/21,We hence call our network a Supervised Transformer Network .,47,0,10
dataset/preprocessed/test-data/face_detection/21,"These two characteristics differentiate our model from the Spatial Transformer Network Transformer Network conducts regression on the transformation parameters directly , and b ) it is only supervised by the final recognition objective .",48,0,34
dataset/preprocessed/test-data/face_detection/21,The proposed Supervised Transformer Network can efficiently run on the G - PU .,49,0,14
dataset/preprocessed/test-data/face_detection/9,Finding Tiny Faces,2,1,3
dataset/preprocessed/test-data/face_detection/9,"We describe a detector that can find around 800 faces out of the reportedly 1000 present , by making use of novel characterizations of scale , resolution , and context to find small objects .",5,0,35
dataset/preprocessed/test-data/face_detection/9,Detector confidence is given by the colorbar on the right : can you confidently identify errors ?,6,0,17
dataset/preprocessed/test-data/face_detection/9,"Though tremendous strides have been made in object recognition , one of the remaining open challenges is detecting small objects .",7,1,21
dataset/preprocessed/test-data/face_detection/9,"We explore three aspects of the problem in the context of finding small faces : the role of scale invariance , image resolution , and contextual reasoning .",8,1,28
dataset/preprocessed/test-data/face_detection/9,"While most recognition approaches aim to be scale - invariant , the cues for recognizing a 3 px tall face are fundamentally different than those for recognizing a 300 px tall face .",9,0,33
dataset/preprocessed/test-data/face_detection/9,We take a different approach and train separate detectors for different scales .,10,0,13
dataset/preprocessed/test-data/face_detection/9,"To maintain efficiency , detectors are trained in a multi-task fashion : they make use of features extracted from multiple layers of single ( deep ) feature hierarchy .",11,0,29
dataset/preprocessed/test-data/face_detection/9,"While training detectors for large objects is straightforward , the crucial challenge remains training detectors for small objects .",12,0,19
dataset/preprocessed/test-data/face_detection/9,"We show that context is crucial , and define templates that make use of massively - large receptive fields ( where 99 % of the template extends beyond the object of interest ) .",13,0,34
dataset/preprocessed/test-data/face_detection/9,"Finally , we explore the role of scale in pre-trained deep networks , providing ways to extrapolate networks tuned for limited scales to rather extreme ranges .",14,0,27
dataset/preprocessed/test-data/face_detection/9,We demonstrate state - of - the - art results on massively - benchmarked face datasets ( FDDB and WIDER FACE ) .,15,0,23
dataset/preprocessed/test-data/face_detection/9,"In particular , when compared to prior art on WIDER FACE , our results reduce error by a factor of 2 ( our models produce an AP of 82 % while prior art ranges from 29 - 64 % ) .",16,0,41
dataset/preprocessed/test-data/face_detection/9,"Though tremendous strides have been made in object recognition , one of the remaining open challenges is detecting small objects .",18,0,21
dataset/preprocessed/test-data/face_detection/9,We explore three aspects of the prob - : Different approaches for capturing scale - invariance .,19,0,17
dataset/preprocessed/test-data/face_detection/9,Traditional approaches build a single - scale template that is applied on a finely - discretized image pyramid ( a ) .,20,0,22
dataset/preprocessed/test-data/face_detection/9,"To exploit different cues available at different resolutions , one could build different detectors for different object scales ( b ) .",21,0,22
dataset/preprocessed/test-data/face_detection/9,Such an approach may fail on extreme object scales that are rarely observed in training ( or pre-training ) data .,22,0,21
dataset/preprocessed/test-data/face_detection/9,We make use of a coarse image pyramid to capture extreme scale challenges in ( c ) .,23,0,18
dataset/preprocessed/test-data/face_detection/9,"Finally , to improve performance on small faces , we model additional context , which is efficiently implemented as a fixed - size receptive field across all scale - specific templates ( d ) .",24,0,35
dataset/preprocessed/test-data/face_detection/9,"We define templates over features extracted from multiple layers of a deep model , which is analogous to foveal descriptors ( e ) .",25,0,24
dataset/preprocessed/test-data/face_detection/9,"lem in the context of face detection : the role of scale invariance , image resolution and contextual reasoning .",26,1,20
dataset/preprocessed/test-data/face_detection/9,Scaleinvariance is a fundamental property of almost all current recognition and object detection systems .,27,0,15
dataset/preprocessed/test-data/face_detection/9,"But from a practical perspective , scale - invariance can not hold for sensors with finite resolution : the cues for recognizing a 300 px tall face are undeniably different that those for recognizing a 3 px tall face .",28,0,40
dataset/preprocessed/test-data/face_detection/9,Multi - task modeling of scales :,29,0,7
dataset/preprocessed/test-data/face_detection/9,"Much recent work in object detection makes use of scale - normalized classifiers ( e.g. , scanning - window detectors run on an image pyramid or region - classifiers run on "" ROI "" - pooled image features ) .",30,0,40
dataset/preprocessed/test-data/face_detection/9,"When resizing regions to a canonical template size , we ask a simple question - what should the size of the template be ?",31,0,24
dataset/preprocessed/test-data/face_detection/9,"On one hand , we want a small template that can detect small faces ; on the other hand , we want a large template that can exploit detailed features ( of say , facial parts ) to increase accuracy .",32,0,41
dataset/preprocessed/test-data/face_detection/9,"Instead of a "" one-size - fitsall "" approach , we train separate detectors tuned for different scales ( and aspect ratios ) .",33,0,24
dataset/preprocessed/test-data/face_detection/9,Training a large collection of scale - specific detectors may suffer from lack of training data for individual scales and inefficiency from running a large number of detectors attest time .,34,0,31
dataset/preprocessed/test-data/face_detection/9,"To address both concerns , we train and run scale - specific detectors in a multitask fashion : they make use of features defined over multiple layers of single ( deep ) feature hierarchy .",35,0,35
dataset/preprocessed/test-data/face_detection/9,"While such a strategy results in detectors of high accuracy for large objects , finding small things is still challenging .",36,0,21
dataset/preprocessed/test-data/face_detection/9,How to generalize pre-trained networks ?,37,0,6
dataset/preprocessed/test-data/face_detection/9,We provide two remaining key insights to the problem of finding small objects .,38,0,14
dataset/preprocessed/test-data/face_detection/9,The first is an analysis of how best to extract scale - invariant features from pre-trained deep networks .,39,0,19
dataset/preprocessed/test-data/face_detection/9,We demonstrate that existing networks are tuned for objects of a characteristic size ( encountered in pre-training datasets such as ImageNet ) .,40,0,23
dataset/preprocessed/test-data/face_detection/9,"To extend features fine - tuned from these networks to objects of novel sizes , we employ a simply strategy : resize images at test - time by interpolation and decimation .",41,0,32
dataset/preprocessed/test-data/face_detection/9,"While many recognition systems are applied in a "" multi-resolution "" fashion by processing an image pyramid , we find that interpolating the lowest layer of the pyramid is particularly crucial for finding small objects .",42,0,36
dataset/preprocessed/test-data/face_detection/9,Hence our final approach is a delicate mixture of scale - specific detectors that are used in a scale - invariant fashion ( by processing an image pyramid to capture large scale variations ) .,43,0,35
dataset/preprocessed/test-data/face_detection/9,How best to encode context ?,44,0,6
dataset/preprocessed/test-data/face_detection/9,Finding small objects is fundamentally challenging because there is little signal on the object to exploit .,45,0,17
dataset/preprocessed/test-data/face_detection/9,Hence we argue that one must use image evidence beyond the object extent .,46,0,14
dataset/preprocessed/test-data/face_detection/9,"This is often formulated as "" context "" .",47,0,9
dataset/preprocessed/test-data/face_detection/9,"In , we present a simple human experiment where users attempt to classify true and false positive faces ( as given by our detector ) .",48,0,26
dataset/preprocessed/test-data/face_detection/9,It is dramatically clear that humans need context to accurately classify small faces .,49,0,14
dataset/preprocessed/test-data/face_detection/19,Face Detection Using Improved Faster RCNN,2,1,6
dataset/preprocessed/test-data/face_detection/19,Faster RCNN has achieved great success for generic object detection including PASCAL object detection and MS COCO object detection .,4,0,20
dataset/preprocessed/test-data/face_detection/19,"In this report , we propose a detailed designed Faster RCNN method named FDNet1.0 for face detection .",5,0,18
dataset/preprocessed/test-data/face_detection/19,"Several techniques were employed including multi-scale training , multi-scale testing , light - designed RCNN , some tricks for inference and a vote - based ensemble method .",6,0,28
dataset/preprocessed/test-data/face_detection/19,"Our method achieves two 1th places and one 2nd place in three tasks over WIDER FACE validation dataset ( easy set , medium set , hard set ) .",7,0,29
dataset/preprocessed/test-data/face_detection/19,"object detectors including one stage methods ( e.g. , YOLO , SSD ) and two stage methods ( e.g. , Faster RCNN , RFCN ) .",9,0,26
dataset/preprocessed/test-data/face_detection/19,One stage methods refer broadly to architectures that use a single feed - forward full convolutional neural network to directly predict each proposal 's class and corresponding bounding box without requiring a second stage per-proposal classification operation and box refinement .,10,0,41
dataset/preprocessed/test-data/face_detection/19,"Two stage methods , especially Faster RCNN achieves better performance than one stage methods over many object detection benchmarks .",11,0,20
dataset/preprocessed/test-data/face_detection/19,"In the Faster R - CNN setting , object detection happens over two pipes .",12,0,15
dataset/preprocessed/test-data/face_detection/19,"In the first pipe , input image is directly processed by a feature extractor ( e.g. , Vgg16 , Inception , ResNet101 ) without any hand engineering , and features at the selected intermediate layer ( e.g. , "" conv 5_3 "" , "" re s 4 f "" ) will be fed to a convolutional layer , which simultaneously predict objectiveness scores and region bounds at each location on a regular grid according to predefined stride .",13,0,78
dataset/preprocessed/test-data/face_detection/19,The first pipe is also called region proposal network ( RPN ) .,14,0,13
dataset/preprocessed/test-data/face_detection/19,"In the second pipe , these proposals with higher scores in the RPN are used to crop features from the same intermediate feature map which are subsequently fed to the remainder of the feature extractor ( e.g. , two full connected layer , 5th block ) in order to predict a class and class - specific box refinement for each proposal .",15,0,62
dataset/preprocessed/test-data/face_detection/19,Face detection has achieved great success thanks to the appearance of one stage method and two stage methods .,16,0,19
dataset/preprocessed/test-data/face_detection/19,"However , there are still some issues with these methods that can be improved with elaborate design of the details .",17,0,21
dataset/preprocessed/test-data/face_detection/19,"In this report , we propose a detailed design Faster RCNN method named FDNet1.0 for face detection , which achieves more decent performance than previous methods .",18,0,27
dataset/preprocessed/test-data/face_detection/19,"A deformable layer with fewer channels is attached to the backbone network to produce a "" thin "" feature map , which is subsequently fed to a full connected layer , building an efficient yet accurate two - stage detector .",19,0,41
dataset/preprocessed/test-data/face_detection/19,"At testing time , we also find a comparable mean average precision ( m AP ) be achieved when the top - ranked proposals ( e.g. , 6000 ) are directly selected without NMS in the RPN stage over WIDER FACE dataset .",20,0,43
dataset/preprocessed/test-data/face_detection/19,It is also beneficial for hard set to keep the small proposals ( < 16 pixels width / height ) at training and testing stage as there are many tinny faces of WIDER FACE dataset .,21,0,36
dataset/preprocessed/test-data/face_detection/19,"Furthermore , the multi-scale training and testing strategy are also applied in our work .",22,0,15
dataset/preprocessed/test-data/face_detection/19,Our key contributions are summarized as follows : ( 1 ) A light head based two - stage framework named FDNet1.0 is developed for face detection .,23,0,27
dataset/preprocessed/test-data/face_detection/19,"( 2 ) Some useful tricks are found to improve final face detection performance including multi-scale training , multi-scale testing , keep the small proposals at training and testing stage , directly select top - ranked proposals ( e.g. , 6000 ) without NMS in the RPN stage for R - CNN , a vote - based NMS ensemble strategy .",24,0,61
dataset/preprocessed/test-data/face_detection/19,"( 3 ) Our framework achieves two 1st places and one 2nd place in three tasks over WIDER FACE validation dataset ( easy , medium , hard ) , one illustrative example of our results in the crowd case can be found in .",25,0,44
dataset/preprocessed/test-data/face_detection/19,"Face detection is one of the most fundamental and challenging problems in computer vision , and has been extensively studied for decades .",26,0,23
dataset/preprocessed/test-data/face_detection/19,"Compared against these hand - engineered features , a lot of progress for face detection has been made in recent years due to utilizing of modern object detectors , including Faster R - CNN , R - FCN , SSD , YOLO and their extensions .",27,0,46
dataset/preprocessed/test-data/face_detection/19,Hand - engineered approaches :,28,0,5
dataset/preprocessed/test-data/face_detection/19,A cascaded AdaBoost face detector is proposed to detect face by using Haar - like features .,29,0,17
dataset/preprocessed/test-data/face_detection/19,"Based on this groundbreaking work , more advanced hand - engineered features and more powerful machine learning algorithms are developed to improve face detection performance .",30,0,26
dataset/preprocessed/test-data/face_detection/19,"Additionally , deformable part models ( DPM ) is also employed for face detection by several research groups , which achieve remarkable performance .",31,0,24
dataset/preprocessed/test-data/face_detection/19,Single - stage approaches :,32,0,5
dataset/preprocessed/test-data/face_detection/19,CascadeCNN proposes a strategy to detect face coarse to fine .,33,0,11
dataset/preprocessed/test-data/face_detection/19,A mutli-task learning method named MTCNN is present to predict face and landmark location simultaneously .,34,0,16
dataset/preprocessed/test-data/face_detection/19,Dense - Box employs a fully deep convolutional neural network to directly predict face confidence and corresponding bounding box .,35,0,20
dataset/preprocessed/test-data/face_detection/19,"UnitBox introduces a novel intersection - over - union ( IoU ) loss to predict bounding box , which regresses the four bounds of a predicted box as a whole unit .",36,0,32
dataset/preprocessed/test-data/face_detection/19,SAFD and RSA unit devote to handle scale explicitly using CNN or RNN .,37,0,14
dataset/preprocessed/test-data/face_detection/19,S 3 FD presents a single shot scale - invariant face detector which achieves good result on WIDER FACE datasets .,38,0,21
dataset/preprocessed/test-data/face_detection/19,"Very recently , FAN presents an effective face detector based on feature pyramid network , which obtains state - of - the - art results .",39,0,26
dataset/preprocessed/test-data/face_detection/19,Two - stage approaches :,40,0,5
dataset/preprocessed/test-data/face_detection/19,Face R - CNN employs anew multi-task loss function based on Faster R - CNN framework to enhance performance .,41,0,20
dataset/preprocessed/test-data/face_detection/19,CMS - RCNN is proposed to enhance face detect performance by exploiting contextual information .,42,0,15
dataset/preprocessed/test-data/face_detection/19,Convnet introduces an end - to - end multi-task discriminative learning framework to increase occlusion robustness .,43,0,17
dataset/preprocessed/test-data/face_detection/19,"Based on R - FCN , Face R - FCN re-weights embedding responses on score maps and eliminates the effect of non-uniformed contribution in each facial part using a position - sensitive average pooling .",44,0,35
dataset/preprocessed/test-data/face_detection/19,"Faster RCNN , with two fully connected layers or all the convolution layers in ResNet 5 - th stage to predict RoI classification and regression , consumes a large memory and computing resource .",46,0,34
dataset/preprocessed/test-data/face_detection/19,"RFCN is fully convolutional with almost all computation shared on the entire image , but it has poor performance compared to Faster RCNN .",47,0,24
dataset/preprocessed/test-data/face_detection/19,"Inspired by , we develop a light - head Faster RCNN for face detection with good performance and inference speed .",48,0,21
dataset/preprocessed/test-data/face_detection/19,"In this section , we will present our method in detail .",49,0,12
dataset/preprocessed/test-data/face_detection/1,EXTD : Extremely Tiny Face Detector via Iterative Filter Reuse,2,0,10
dataset/preprocessed/test-data/face_detection/1,"In this paper , we propose a new multi-scale face detector having an extremely tiny number of parameters ( EXTD ) , less than 0.1 million , as well as achieving comparable performance to deep heavy detectors .",4,0,38
dataset/preprocessed/test-data/face_detection/1,"While existing multiscale face detectors extract feature maps with different scales from a single backbone network , our method generates the feature maps by iteratively reusing a shared lightweight and shallow backbone network .",5,0,34
dataset/preprocessed/test-data/face_detection/1,"This iterative sharing of the backbone network significantly reduces the number of parameters , and also provides the abstract image semantics captured from the higher stage of the network layers to the lower - level feature map .",6,0,38
dataset/preprocessed/test-data/face_detection/1,The proposed idea is employed by various model architectures and evaluated by extensive experiments .,7,0,15
dataset/preprocessed/test-data/face_detection/1,"From the experiments from WIDER FACE dataset , we show that the proposed face detector can handle faces with various scale and conditions , and achieved comparable performance to the more massive face detectors that few hundreds and tens times heavier in model size and floating point operations .",8,0,49
dataset/preprocessed/test-data/face_detection/1,"Detecting faces in an image is considered to be one of the most practical tasks in computer vision applications , and many studies are proposed from the beginning of the computer vision research .",10,1,34
dataset/preprocessed/test-data/face_detection/1,"After the advent of deep neural networks , many face detection algorithms applying the deep network have reported significant performance improvement to the conventional face detectors .",11,1,27
dataset/preprocessed/test-data/face_detection/1,The state - of - the - art ( SOTA ) face detectors for in - the - wild images employ the framework of the recent object detectors .,12,0,29
dataset/preprocessed/test-data/face_detection/1,"These methods can even handle a various scale of faces with difficult conditions such as distortion , rotation , and occlusion .",13,0,22
dataset/preprocessed/test-data/face_detection/1,"Among them , the face detectors using multiple feature - maps from different layer locations , which mainly stem from , are dominantly used since * Clova AI Research , NAVER Corp .",14,0,33
dataset/preprocessed/test-data/face_detection/1,We follow the alphabetical order except the first author .,15,0,10
dataset/preprocessed/test-data/face_detection/1,Our method ( star ) shows comparable mAP to S3FD with a significantly smaller model .,16,0,16
dataset/preprocessed/test-data/face_detection/1,Red stars denote the proposed models with various sizes .,17,0,10
dataset/preprocessed/test-data/face_detection/1,' S3FD + M ' denotes the S3FD variation using MobileFaceNet as a backbone network instead of VGG - 16 .,18,0,21
dataset/preprocessed/test-data/face_detection/1,Best viewed in wide and colored vision .,19,0,8
dataset/preprocessed/test-data/face_detection/1,these methods can handle the faces with various scale in a single forward path .,20,0,15
dataset/preprocessed/test-data/face_detection/1,"While these methods achieved impressive detection performance , they commonly share two problems .",21,0,14
dataset/preprocessed/test-data/face_detection/1,One is their large number of parameters .,22,0,8
dataset/preprocessed/test-data/face_detection/1,"Since they use a large classification network such as VGG - 16 , ResNet - 50 or 101 , and DenseNet - 169 , the number of total parameters exceed 20 million , over 80 Mb supposing 32 - bit floating point for each parameter .",23,0,46
dataset/preprocessed/test-data/face_detection/1,"Furthermroe , the amount of floating point operations ( FLOPs ) also exceeds 100G , and these make it nearly impossible to use the face detectors in CPU or mobile environment , where the most face applications run in .",24,0,40
dataset/preprocessed/test-data/face_detection/1,"The second problem , from the architecture perspective , is the limited capacity of the low - level feature map in capturing object semantics .",25,0,25
dataset/preprocessed/test-data/face_detection/1,The most single - shot detector ( SSD ) variant object and face detectors struggle the problem because the low - level feature map passes shallow convolutional layers .,26,0,29
dataset/preprocessed/test-data/face_detection/1,"To alleviate the problem , the variants of Feature pyramid network ( FPN ) architecture such as are used but requires additional parameters and memories for re-expanding the feature map .",27,0,31
dataset/preprocessed/test-data/face_detection/1,"In this paper , we propose a new multi-scale face detector with extremely tiny size ( EXTD ) resolving the two mentioned problems .",28,0,24
dataset/preprocessed/test-data/face_detection/1,"The main discovery is that we can share the network in generating each feature - map , as shown in .",29,0,21
dataset/preprocessed/test-data/face_detection/1,"As in the figure , we design a backbone network such that reduces the size of the feature map by half , and we can get the other feature maps with recurrently passing the network .",30,0,36
dataset/preprocessed/test-data/face_detection/1,"The sharing can significantly reduce the number of parameters , and this enables our model to use more layers to generate the low - level feature maps used for detecting small faces .",31,0,33
dataset/preprocessed/test-data/face_detection/1,"Also , the proposed iterative architecture makes the network to observe the features from various scale of faces and from various layer locations , and hence offer abundant semantic information to the network , without adding additional parameters .",32,0,39
dataset/preprocessed/test-data/face_detection/1,"Our baseline framework follows FPN - like structures , but can also be applied to SSD - like architecture .",33,0,20
dataset/preprocessed/test-data/face_detection/1,"For SSD based architecture , we adopt the setting from .",34,0,11
dataset/preprocessed/test-data/face_detection/1,"For the FPN architectures , we refer an up - sampling strategy from .",35,0,14
dataset/preprocessed/test-data/face_detection/1,The backbone network is designed to have less than 0.1 million parameters with employing inverted residual blocks proposed in MobileNet - V2 .,36,0,23
dataset/preprocessed/test-data/face_detection/1,"We note that our model does not require any extra layer commonly defined as in , and is trained from scratch .",37,0,22
dataset/preprocessed/test-data/face_detection/1,"We evaluated the proposed detector and its variants on WIDER FACE dataset , the most widely used and similar to the in - the - wild situation .",38,0,28
dataset/preprocessed/test-data/face_detection/1,The main contributions of this work can be summarized as follows :,39,0,12
dataset/preprocessed/test-data/face_detection/1,"We propose an iterative network sharing model for multi-stage face detection which can significantly reduce the parameter size , as well as provide abundant object semantic information to the lower stage feature maps .",40,0,34
dataset/preprocessed/test-data/face_detection/1,"( 2 ) We design a lightweight backbone network for the proposed iterative feature map generation with 0.1 M number of parameters , which less than 400 Kb , and achieved comparable mAP to the heavy face detection methods .",41,0,40
dataset/preprocessed/test-data/face_detection/1,"( 3 ) We employ the iterative network sharing idea to the widely used detection architectures , FPN and SSD , and show the effectiveness of the proposed scheme .",42,0,30
dataset/preprocessed/test-data/face_detection/1,Face detectors :,44,0,3
dataset/preprocessed/test-data/face_detection/1,Face detection has been an important research topic since an initial stage of computer vision researches .,45,0,17
dataset/preprocessed/test-data/face_detection/1,Viola et al .,46,0,4
dataset/preprocessed/test-data/face_detection/1,"proposed a face detection method using Haar features and Adaboost with decent performance , and several different approaches followed .",47,0,20
dataset/preprocessed/test-data/face_detection/1,"After deep learning has become dominant , many face detection methods applying the techniques have been published .",48,0,18
dataset/preprocessed/test-data/face_detection/1,"In the early stages , various attempts were tried to employ the deep architecture to face detection , such as cascade architecture , and occlusion handling .",49,0,27
dataset/preprocessed/test-data/face_detection/5,"Abstract - Face detection and alignment in unconstrained environment are challenging due to various poses , illuminations and occlusions .",5,1,20
dataset/preprocessed/test-data/face_detection/5,Recent studies show that deep learning approaches can achieve impressive performance on these two tasks .,6,0,16
dataset/preprocessed/test-data/face_detection/5,"In this paper , we propose a deep cascaded multi-task framework which exploits the inherent correlation between them to boost up their performance .",7,0,24
dataset/preprocessed/test-data/face_detection/5,"In particular , our framework adopts a cascaded structure with three stages of carefully designed deep convolutional networks that predict face and landmark location in a coarse - to - fine manner .",8,0,33
dataset/preprocessed/test-data/face_detection/5,"In addition , in the learning process , we propose a new online hard sample mining strategy that can improve the performance automatically without manual sample selection .",9,0,28
dataset/preprocessed/test-data/face_detection/5,"Our method achieves superior accuracy over the state - of - the - art techniques on the challenging FDDB and WIDER FACE benchmark for face detection , and AFLW benchmark for face alignment , while keeps real time performance .",10,0,40
dataset/preprocessed/test-data/face_detection/5,"Index Terms - Face detection , face alignment , cascaded convolutional neural network",11,0,13
dataset/preprocessed/test-data/face_detection/5,"ACE detection and alignment are essential to many face applications , such as face recognition and facial expression analysis .",13,0,20
dataset/preprocessed/test-data/face_detection/5,"However , the large visual variations of faces , such as occlusions , large pose variations and extreme lightings , impose great challenges for these tasks in real world applications .",14,0,31
dataset/preprocessed/test-data/face_detection/5,"The cascade face detector proposed by Viola and Jones utilizes Haar - Like features and AdaBoost to train cascaded classifiers , which achieve good performance with real - time efficiency .",15,0,31
dataset/preprocessed/test-data/face_detection/5,"However , quite a few works indicate that this detector may degrade significantly in real - world applications with larger visual variations of human faces even with more advanced features and classifiers .",16,0,33
dataset/preprocessed/test-data/face_detection/5,"Besides the cascade structure , introduce deformable part models ( DPM ) for face detection and achieve remarkable performance .",17,0,20
dataset/preprocessed/test-data/face_detection/5,"However , they need high computational expense and may usually require expensive annotation in the training stage .",18,0,18
dataset/preprocessed/test-data/face_detection/5,"Recently , convolutional neural networks ( CNNs ) achieve remarkable progresses in a variety of computer vision tasks , such as image classification and face recognition .",19,0,27
dataset/preprocessed/test-data/face_detection/5,"Inspired by the good per - K. - P. formance of CNNs in computer vision tasks , some of the CNNs based face detection approaches have been proposed in recent years .",20,0,32
dataset/preprocessed/test-data/face_detection/5,Yang et al.,21,0,3
dataset/preprocessed/test-data/face_detection/5,train deep convolution neural networks for facial attribute recognition to obtain high response in face regions which further yield candidate windows of faces .,22,0,24
dataset/preprocessed/test-data/face_detection/5,"However , due to its complex CNN structure , this approach is time costly in practice .",23,0,17
dataset/preprocessed/test-data/face_detection/5,Li et al.,24,0,3
dataset/preprocessed/test-data/face_detection/5,"use cascaded CNNs for face detection , but it requires bounding box calibration from face detection with extra computational expense and ignores the inherent correlation between facial landmarks localization and bounding box regression .",25,0,34
dataset/preprocessed/test-data/face_detection/5,Face alignment also attracts extensive interests .,26,0,7
dataset/preprocessed/test-data/face_detection/5,Regression - based methods and template fitting approaches are two popular categories .,27,0,13
dataset/preprocessed/test-data/face_detection/5,Zhang et al. proposed to use facial attribute recognition as an auxiliary task to enhance face alignment performance using deep convolutional neural network .,29,0,24
dataset/preprocessed/test-data/face_detection/5,"However , most of the available face detection and face alignment methods ignore the inherent correlation between these two tasks .",30,1,21
dataset/preprocessed/test-data/face_detection/5,"Though there exist several works attempt to jointly solve them , there are still limitations in these works .",31,0,19
dataset/preprocessed/test-data/face_detection/5,"For example ,",32,0,3
dataset/preprocessed/test-data/face_detection/5,Chen et al. jointly conduct alignment and detection with random forest using features of pixel value difference .,33,0,18
dataset/preprocessed/test-data/face_detection/5,"But , the handcraft features used limits its performance .",34,0,10
dataset/preprocessed/test-data/face_detection/5,Zhang et al.,35,0,3
dataset/preprocessed/test-data/face_detection/5,"use multi - task CNN to improve the accuracy of multi-view face detection , but the detection accuracy is limited by the initial detection windows produced by a weak face detector .",36,0,32
dataset/preprocessed/test-data/face_detection/5,"On the other hand , in the training process , mining hard samples in training is critical to strengthen the power of detector .",37,0,24
dataset/preprocessed/test-data/face_detection/5,"However , traditional hard sample mining usually performs an offline manner , which significantly increases the manual operations .",38,0,19
dataset/preprocessed/test-data/face_detection/5,"It is desirable to design an online hard sample mining method for face detection and alignment , which is adaptive to the current training process automatically .",39,0,27
dataset/preprocessed/test-data/face_detection/5,"In this paper , we propose a new framework to integrate these two tasks using unified cascaded CNNs by multi-task learning .",40,0,22
dataset/preprocessed/test-data/face_detection/5,The proposed CNNs consist of three stages .,41,0,8
dataset/preprocessed/test-data/face_detection/5,"In the first stage , it produces candidate windows quickly through a shallow CNN .",42,0,15
dataset/preprocessed/test-data/face_detection/5,"Then , it refines the windows to reject a large number of non-faces windows through a more complex CNN .",43,0,20
dataset/preprocessed/test-data/face_detection/5,"Finally , it uses a more powerful CNN to refine the result and output facial landmarks positions .",44,0,18
dataset/preprocessed/test-data/face_detection/5,"Thanks to this multi-task learning framework , the performance of the algorithm can be notably improved .",45,0,17
dataset/preprocessed/test-data/face_detection/5,The major contributions of this paper are summarized as follows :,46,0,11
dataset/preprocessed/test-data/face_detection/5,"In this section , we will describe our approach towards joint face detection and alignment .",49,0,16
dataset/preprocessed/test-data/face_detection/18,CMS- RCNN : Contextual Multi- Scale Region - based CNN for Unconstrained Face Detection,2,1,14
dataset/preprocessed/test-data/face_detection/18,"Robust face detection in the wild is one of the ultimate components to support various facial related problems , i.e. unconstrained face recognition , facial periocular recognition , facial landmarking and pose estimation , facial expression recognition , 3 D facial model construction , etc .",4,1,46
dataset/preprocessed/test-data/face_detection/18,"Although the face detection problem has been intensely studied for decades with various commercial applications , it still meets problems in some real - world scenarios due to numerous challenges , e.g. heavy facial occlusions , extremely low resolutions , strong illumination , exceptionally pose variations , image or video compression artifacts , etc .",5,1,55
dataset/preprocessed/test-data/face_detection/18,"In this paper , we present a face detection approach named Contextual Multi - Scale Region - based Convolution Neural Network ( CMS - RCNN ) to robustly solve the problems mentioned above .",6,0,34
dataset/preprocessed/test-data/face_detection/18,"Similar to the region - based CNNs , our proposed network consists of the region proposal component and the region - of - interest ( RoI ) detection component .",7,0,30
dataset/preprocessed/test-data/face_detection/18,"However , far apart of that network , there are two main contributions in our proposed network that play a significant role to achieve the state - of - theart performance in face detection .",8,0,35
dataset/preprocessed/test-data/face_detection/18,"Firstly , the multi-scale information is grouped both in region proposal and RoI detection to deal with tiny face regions .",9,0,21
dataset/preprocessed/test-data/face_detection/18,"Secondly , our proposed network allows explicit body contextual reasoning in the network inspired from the intuition of human vision system .",10,0,22
dataset/preprocessed/test-data/face_detection/18,"The proposed approach is benchmarked on two recent challenging face detection databases , i.e. the WIDER FACE Dataset which contains high degree of variability , as well as the Face Detection Dataset and Benchmark ( FDDB ) .",11,0,38
dataset/preprocessed/test-data/face_detection/18,"The experimental results show that our proposed approach trained on WIDER FACE Dataset outperforms strong baselines on WIDER FACE Dataset by a large margin , and consistently achieves competitive results on FDDB against the recent state - of - the - art face detection methods .",12,0,46
dataset/preprocessed/test-data/face_detection/18,"Detection and analysis on human subjects using facial feature based biometrics for access control , surveillance systems and other security applications have gained popularity over the past few years .",14,0,30
dataset/preprocessed/test-data/face_detection/18,Several such biometrics systems are deployed in security checkpoints across the globe with more being deployed everyday .,15,0,18
dataset/preprocessed/test-data/face_detection/18,"Particularly , face recognition has been one of the most popular biometrics modalities attractive to security departments .",16,0,18
dataset/preprocessed/test-data/face_detection/18,"Indeed , the uniqueness of facial features across individuals can be captured much more easily than other biometrics .",17,0,19
dataset/preprocessed/test-data/face_detection/18,"In order to take into account a face recognition algorithm , however , face detection usually needs to be done first .",18,0,22
dataset/preprocessed/test-data/face_detection/18,"The problem of face detection has been intensely studied for decades with the aim of ensuring the generalization of robust algorithms to unseen face images , , , , , , , , , , , .",19,0,37
dataset/preprocessed/test-data/face_detection/18,"Although the detection accuracy in recent face detection algorithms , , , , , has been highly improved due to the advancement of deep Convolutional Neural Networks ( CNN ) , they are still far from achieving the same detection capabilities as a human due to a number of challenges , are always the important factors that need to be considered .",20,0,62
dataset/preprocessed/test-data/face_detection/18,"This paper presents an advanced CNN based approach named Contextual Multi - Scale Region - based CNN ( CMS - RCNN ) to handle the problem of face detection in digital face images collected under numerous challenging conditions , e.g. heavy facial occlusion , illumination , extreme offangle , low - resolution , scale difference , etc .",21,0,58
dataset/preprocessed/test-data/face_detection/18,"Our designed region - based CNN architecture allows the network to simultaneously look at multi-scale features , as well as to explicitly look outside facial regions as the potential body regions .",22,0,32
dataset/preprocessed/test-data/face_detection/18,"In other words , this process tries to mimic the way of face detection by human in a sense that when humans are not sure about a face , seeing the body will increase our confidence .",23,0,37
dataset/preprocessed/test-data/face_detection/18,Additionally this architecture also helps to synchronize both the global semantic features in high level layers and the localization features in low level layers for facial representation .,24,0,28
dataset/preprocessed/test-data/face_detection/18,"Therefore , it is able to robustly deal with the challenges in the problem of unconstrained face detection .",25,0,19
dataset/preprocessed/test-data/face_detection/18,Our CMS - RCNN method introduces the Multi - Scale Region Proposal Network ( MS - RPN ) to generate a set of region candidates and the Contextual Multi - Scale Convolution Neural Network ( CMS - CNN ) to do inference on the region candidates of facial regions .,26,0,50
dataset/preprocessed/test-data/face_detection/18,A confidence score and bounding box regression are computed for every candidate .,27,0,13
dataset/preprocessed/test-data/face_detection/18,"In the end , the face detection system is able to decide the quality of the detection results by thresholding these generated confidence scores in given face images .",28,0,29
dataset/preprocessed/test-data/face_detection/18,The architecture of our proposed CMS - RCNN network for unconstrained face detection is illustrated in .,29,0,17
dataset/preprocessed/test-data/face_detection/18,Our approach is evaluated on two challenging face detection databases and compared against numerous recent face detection methods .,30,0,19
dataset/preprocessed/test-data/face_detection/18,"Firstly , the proposed CMS - RCNN method is compared against four strong baselines , , on the WIDER FACE Dataset , a large scale face detection benchmark database .",31,0,30
dataset/preprocessed/test-data/face_detection/18,"This experiment shows its capability to detect face images in the wild , e.g. under occlusions , illumination , facial poses , low - resolution conditions , etc .",32,0,29
dataset/preprocessed/test-data/face_detection/18,"Our method outperforms the baselines by a huge margin in all easy , medium , and hard partitions .",33,0,19
dataset/preprocessed/test-data/face_detection/18,"It is also benchmarked on the Face Detection Data Set and Benchmark ( FDDB ) , a dataset of face regions designed for studying the problem of unconstrained face detection .",34,0,31
dataset/preprocessed/test-data/face_detection/18,The experimental results show that the proposed CMS - RCNN approach consistently achieves highly competitive results against the other state - of - the - art face detection methods .,35,0,30
dataset/preprocessed/test-data/face_detection/18,The rest of this paper is organized as follows .,36,0,10
dataset/preprocessed/test-data/face_detection/18,"In section 2 , we summarize prior work in face detection .",37,0,12
dataset/preprocessed/test-data/face_detection/18,"Section 3 reviews a general deep learning framework , the background as well as the limitations of the Faster R - CNN in the problem of face detection .",38,0,29
dataset/preprocessed/test-data/face_detection/18,"In Section 4 , we introduce our proposed CMS - RCNN approach for the problem of unconstrained face detection .",39,0,20
dataset/preprocessed/test-data/face_detection/18,"Section 5 presents the experimental face detection results and comparisons obtained using our proposed approach on two challenging face detection databases , i.e. the WIDER FACE Dataset and the FDDB database .",40,0,32
dataset/preprocessed/test-data/face_detection/18,"Finally , our conclusions in this work are presented in Section 6 .",41,0,13
dataset/preprocessed/test-data/face_detection/18,Face detection has been a well studied area of computer vision .,43,0,12
dataset/preprocessed/test-data/face_detection/18,One of the first well performing approaches to the problem was the Viola - Jones face detector .,44,0,18
dataset/preprocessed/test-data/face_detection/18,It was capable of performing real time face detection using a cascade of boosted simple Haar classifiers .,45,0,18
dataset/preprocessed/test-data/face_detection/18,The concepts of boosting and using simple features has been the basis for many different approaches since the Viola - Jones face detector .,46,0,24
dataset/preprocessed/test-data/face_detection/18,These early detectors tended to work well on frontal face images but not very well on faces in different poses .,47,0,21
dataset/preprocessed/test-data/face_detection/18,"As time has passed , many of these methods have been able to deal with off - angle face detection by utilizing multiple models for the various poses of the face .",48,0,32
dataset/preprocessed/test-data/face_detection/18,This increases the model size but does afford more practical uses of the methods .,49,0,15
dataset/preprocessed/test-data/face_detection/15,FaceBoxes : A CPU Real - time Face Detector with High Accuracy,2,0,12
dataset/preprocessed/test-data/face_detection/15,"Although tremendous strides have been made in face detection , one of the remaining open challenges is to achieve real - time speed on the CPU as well as maintain high performance , since effective models for face detection tend to be computationally prohibitive .",4,1,45
dataset/preprocessed/test-data/face_detection/15,"To address this challenge , we propose a novel face detector , named FaceBoxes , with superior performance on both speed and accuracy .",5,0,24
dataset/preprocessed/test-data/face_detection/15,"Specifically , our method has a lightweight yet powerful network structure that consists of the Rapidly Digested Convolutional Layers ( RDCL ) and the Multiple Scale Convolutional Layers ( MSCL ) .",6,0,32
dataset/preprocessed/test-data/face_detection/15,The RDCL is designed to enable Face - Boxes to achieve real - time speed on the CPU .,7,0,19
dataset/preprocessed/test-data/face_detection/15,The MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle faces of various scales .,8,0,21
dataset/preprocessed/test-data/face_detection/15,"Besides , we propose a new anchor densification strategy to make different types of anchors have the same density on the image , which significantly improves the recall rate of small faces .",9,0,33
dataset/preprocessed/test-data/face_detection/15,"As a consequence , the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA - resolution images .",10,0,28
dataset/preprocessed/test-data/face_detection/15,"Moreover , the speed of FaceBoxes is invariant to the number of faces .",11,0,14
dataset/preprocessed/test-data/face_detection/15,"We comprehensively evaluate this method and present stateof - the - art detection performance on several face detection benchmark datasets , including the AFW , PASCAL face , and FDDB .",12,0,31
dataset/preprocessed/test-data/face_detection/15,Code is available at https://github.com/sfzhang15/FaceBoxes .,13,0,6
dataset/preprocessed/test-data/face_detection/15,Face detection is one of the fundamental problems in computer vision and pattern recognition .,15,0,15
dataset/preprocessed/test-data/face_detection/15,"It plays an important role in many subsequent face - related applications , such as face alignment , face recognition and face tracking .",16,0,24
dataset/preprocessed/test-data/face_detection/15,"With the great progress over the past few decades , especially the breakthrough of convolutional neural network , face detection has been successfully applied in our daily life under various scenarios .",17,0,32
dataset/preprocessed/test-data/face_detection/15,"However , there are still some tough challenges in uncontrolled face detection problem , especially for the CPU devices .",18,0,20
dataset/preprocessed/test-data/face_detection/15,The challenges mainly come from two requirements for face detectors :,19,0,11
dataset/preprocessed/test-data/face_detection/15,1 ) The large visual variation of faces in the cluttered backgrounds requires face detectors to accurately address a complicated face and non-face classification problem ; 2 ) The large search space of possible face positions and face sizes further imposes a time efficiency requirement .,20,0,46
dataset/preprocessed/test-data/face_detection/15,"These two requirements are conflicting , since high - accuracy face detectors tend to be computationally expensive .",21,0,18
dataset/preprocessed/test-data/face_detection/15,"Therefore , it is one of the remaining open issues for practical face detectors on the CPU devices to achieve real - time speed as well as maintain high performance .",22,0,31
dataset/preprocessed/test-data/face_detection/15,"In order to meet these two conflicting requirements , face detection has been intensely studied mainly in two ways .",23,0,20
dataset/preprocessed/test-data/face_detection/15,The early way is based on hand - craft features .,24,0,11
dataset/preprocessed/test-data/face_detection/15,"Following the pioneering work of Viola - Jones face detector , most of the early works focus on designing robust features and training effective classifiers .",25,0,26
dataset/preprocessed/test-data/face_detection/15,"Besides the cascade structure , the deformable part model ( DPM ) is introduced into face detection tasks and achieves remarkable performance .",26,0,23
dataset/preprocessed/test-data/face_detection/15,"However , these methods highly depend on nonrobust hand - craft features and optimize each component separately , making the face detection pipeline sub-optimal .",27,0,25
dataset/preprocessed/test-data/face_detection/15,"In brief , they are efficient on the CPU but not accurate enough against the large visual variation of faces .",28,0,21
dataset/preprocessed/test-data/face_detection/15,"The other way is based on the convolutional neural network ( CNN ) which has achieved remarkable successes in recent years , ranging from image classification to object detection .",29,0,30
dataset/preprocessed/test-data/face_detection/15,"Recently , CNN has been successfully introduced into the face detection task as feature extractor in the traditional face detection framewrok .",30,0,22
dataset/preprocessed/test-data/face_detection/15,"Moreover , some face detectors have inherited valid techniques from the generic object detection methods , such as Faster R - CNN .",31,0,23
dataset/preprocessed/test-data/face_detection/15,These CNN based face detection methods are robust to the large variation of facial appearances and demonstrate state - of - the - art performance .,32,0,26
dataset/preprocessed/test-data/face_detection/15,"But they are too time - consuming to achieve real - time speed , especially on the CPU devices .",33,0,20
dataset/preprocessed/test-data/face_detection/15,These two ways have their own advantages .,34,0,8
dataset/preprocessed/test-data/face_detection/15,The former has fast speed while the latter owns high accuracy .,35,0,12
dataset/preprocessed/test-data/face_detection/15,"To perform well on both speed and accuracy , one natural idea is to combine the advantages of these two types of methods .",36,0,24
dataset/preprocessed/test-data/face_detection/15,"Therefore , cascaded CNN based methods are proposed to put features learned by CNN into cascade framework in order to boost the performance and keep efficient .",37,0,27
dataset/preprocessed/test-data/face_detection/15,"However , there are three problems in cascaded CNN based methods :",38,0,12
dataset/preprocessed/test-data/face_detection/15,1 ) Their speed is negatively related to the number of faces on the image .,39,0,16
dataset/preprocessed/test-data/face_detection/15,"The speed would dramatically degrade as the number of faces increases ; 2 ) The cascade based detectors optimize each component separately , making the training process extremely complicated and the final model sub-optimal ; 3 ) For the VGA - resolution images , their runtime efficiency on the CPU is about 14 FPS , which is not fast enough to reach the real - time speed .",40,0,68
dataset/preprocessed/test-data/face_detection/15,"In this paper , inspired by the RPN in Faster R - CNN and the multi-scale mechanism in SSD , we develop a state - of - the - art face detector with real - time speed on the CPU .",41,0,41
dataset/preprocessed/test-data/face_detection/15,"Specifically , we propose a novel face detector named FaceBoxes , which only contains a single fully convolutional neural network and can be trained end - to - end .",42,0,30
dataset/preprocessed/test-data/face_detection/15,The proposed method has a lightweight yet powerful network structure ( as shown in ) that consists of the Rapidly Digested Convolutional Layers ( RDCL ) and the Multiple Scale Convolutional Layers ( MSCL ) .,43,0,36
dataset/preprocessed/test-data/face_detection/15,"The RDCL is designed to enable FaceBoxes to achieve real - time speed on the CPU , and the MSCL aims at enriching the receptive fields and discretizing anchors over different layers to handle various scales of faces .",44,0,39
dataset/preprocessed/test-data/face_detection/15,"Besides , we propose a new anchor densification strategy to make different types of anchors have the same density on the input image , which significantly improves the recall rate of small faces .",45,0,34
dataset/preprocessed/test-data/face_detection/15,"Consequently , for VGA - resolution images , our face detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU .",46,0,27
dataset/preprocessed/test-data/face_detection/15,"More importantly , the speed of FaceBoxes is invariant to the number of faces on the image .",47,0,18
dataset/preprocessed/test-data/face_detection/15,"We comprehensively evaluate this method and demonstrate state - of - the - art detection performance on several face detection benchmark datasets , including the AFW , PASCAL face , and FDDB .",48,0,33
dataset/preprocessed/test-data/face_detection/15,"For clarity , the main contributions of this work can be summarized as four - fold :",49,0,17
dataset/preprocessed/test-data/face_detection/8,Detecting Faces Using Region - based Fully Convolutional Networks,2,1,9
dataset/preprocessed/test-data/face_detection/8,Face detection has achieved great success using the region - based methods .,4,1,13
dataset/preprocessed/test-data/face_detection/8,"In this report , we propose a region - based face detector applying deep networks in a fully convolutional fashion , named Face R - FCN .",5,0,27
dataset/preprocessed/test-data/face_detection/8,"Based on Region - based Fully Convolutional Networks ( R - FCN ) , our face detector is more accurate and computationally efficient compared with the previous R - CNN based face detectors .",6,0,34
dataset/preprocessed/test-data/face_detection/8,"In our approach , we adopt the fully convolutional Residual Network ( ResNet ) as the backbone network .",7,0,19
dataset/preprocessed/test-data/face_detection/8,"Particularly , we exploit several new techniques including position - sensitive average pooling , multi-scale training and testing and on - line hard example mining strategy to improve the detection accuracy .",8,0,32
dataset/preprocessed/test-data/face_detection/8,"Over two most popular and challenging face detection benchmarks , FDDB and WIDER FACE , Face R - FCN achieves superior performance over state - of - the - arts .",9,0,31
dataset/preprocessed/test-data/face_detection/8,Face detection plays an important role in the modern face - relevant applications .,11,0,14
dataset/preprocessed/test-data/face_detection/8,"Despite the great progress made in recent years , the technical challenging of face detection still exists out of the complex variations of real - world face images .",12,0,29
dataset/preprocessed/test-data/face_detection/8,"As shown in , the visual faces vary a lot as the result of the affecting factors including occlusion on the facial part , different scales , illumination conditions , various poses of person , rich expressions , etc .",13,0,40
dataset/preprocessed/test-data/face_detection/8,"Recently , remarkable advances of objection detection have been driven by the success of region - based methods .",14,0,19
dataset/preprocessed/test-data/face_detection/8,"Among recent novel algorithms , Fast / Faster R - CNN are representative R - CNN based methods that perform region - wise detections on the regions of interest ( Ro Is ) .",15,0,34
dataset/preprocessed/test-data/face_detection/8,"However , directly applying the strategy of region - specific operation to fully convolutional networks , such as Residual Nets ( ResNets ) , results in inferior detection performance owing to the overwhelming classification accuracy .",16,0,36
dataset/preprocessed/test-data/face_detection/8,"In contrast , R - FCN is proposed to address the problem in the fully convolutional manner .",17,0,18
dataset/preprocessed/test-data/face_detection/8,"The ConvNet of R - FCN is built with the computations shared on the entire image , which leads to the improvement of training and testing efficiency .",18,0,28
dataset/preprocessed/test-data/face_detection/8,"Comparing with R - CNN based methods , R - FCN proposes much fewer region - wise layers to balance the learning of classification and detection for naturally combining fully convolutional network with region - based module .",19,0,38
dataset/preprocessed/test-data/face_detection/8,"As a specific area of generic object detection , face detection has achieved superior performance thanks to the appearance of region - based methods .",20,0,25
dataset/preprocessed/test-data/face_detection/8,Previous works primarily focus on the R - CNN based methods and achieve promising results .,21,0,16
dataset/preprocessed/test-data/face_detection/8,"In this report , we develop a face detector on the top of R - FCN with elaborate design of the details , which achieves more decent performance than the R - CNN face detectors .",22,0,36
dataset/preprocessed/test-data/face_detection/8,"According to the size of the general face , we carefully design size of anchors and RoIs .",23,0,18
dataset/preprocessed/test-data/face_detection/8,"Since the contribution of facial parts maybe different for detection , we introduce a position - sensitive average pooling to generate embedding features for enhancing discrimination , and eliminate the effect of non-uniformed contribution in each facial part .",24,0,39
dataset/preprocessed/test-data/face_detection/8,"Furthermore , we also apply the multi-scale training and testing strategy in this work .",25,0,15
dataset/preprocessed/test-data/face_detection/8,The on - line hard example mining ( OHEM ) technique is integrated into our network as well for boosting the learning on hard examples .,26,0,26
dataset/preprocessed/test-data/face_detection/8,Our key contributions are summarized below :,27,0,7
dataset/preprocessed/test-data/face_detection/8,( 1 ) We develop a face detection framework that takes the special properties of face into account by integrating several innovative and effective techniques .,28,0,26
dataset/preprocessed/test-data/face_detection/8,"The proposed approach is based on R - FCN and is well suited for face detection , thus we call it Face R - FCN .",29,0,26
dataset/preprocessed/test-data/face_detection/8,( 2 ) We introduce a novel position - sensitive average pooling to re-weight embedding responses on score maps and eliminate the effect of non-uniformed contribution in each facial part .,30,0,31
dataset/preprocessed/test-data/face_detection/8,"( 3 ) By far , the proposed algorithm is benchmarked on WIDER FACE dataset and FDDB dataset .",31,0,19
dataset/preprocessed/test-data/face_detection/8,Our Face R - FCN has reached the first - rate performance over the state - of - the - arts on both datasets .,32,0,25
dataset/preprocessed/test-data/face_detection/8,"In the past decades , face detection has been extensively studied .",34,0,12
dataset/preprocessed/test-data/face_detection/8,The pioneering work of Viola and Jones invents a cascaded AdaBoost face detector using Haar - like features .,35,0,19
dataset/preprocessed/test-data/face_detection/8,"After that , numerous of works have focused on developing more advanced features and more powerful classifiers .",36,0,18
dataset/preprocessed/test-data/face_detection/8,"Besides the boosted cascade methods , several studies apply deformable part models ( DPM ) for face detection .",37,0,19
dataset/preprocessed/test-data/face_detection/8,The DPM methods detect faces by modeling the relationship of deformable facial parts .,38,0,14
dataset/preprocessed/test-data/face_detection/8,Recent progress in face detection mainly benefits from the powerful deep learning approaches .,39,0,14
dataset/preprocessed/test-data/face_detection/8,The CNN - based detectors have achieved the highest performance .,40,0,11
dataset/preprocessed/test-data/face_detection/8,construct cascaded CNNs to learn face detectors with a coarse - to - fine strategy .,41,0,16
dataset/preprocessed/test-data/face_detection/8,MTCNN develops a multi- task training framework to jointly learn the face detection and alignment .,42,0,16
dataset/preprocessed/test-data/face_detection/8,"UnitBox propose the intersectionover - union ( IoU ) loss function , to directly minimize the IoUs of the predictions and the groundtruths .",43,0,24
dataset/preprocessed/test-data/face_detection/8,"Recently , several methods use the Faster R - CNN framework to improve the face detection performance .",44,0,18
dataset/preprocessed/test-data/face_detection/8,"explores the contextual information for face detection and proposes a framework achieving high performance , especially improving the accuracy of tiny faces .",45,0,23
dataset/preprocessed/test-data/face_detection/8,"Most recently , propose to use single stage framework for face detection , with carefully designed strategies and achieve the state - of - the - art performance .",46,0,29
dataset/preprocessed/test-data/face_detection/8,"Similar to face detection , general object detection is advancing rapidly thanks to the deep learning approaches .",47,0,18
dataset/preprocessed/test-data/face_detection/8,"Typical work including R - CNN , Fast R - CNN , Faster R - CNN and their extensions .",48,0,20
dataset/preprocessed/test-data/face_detection/8,"Among these studies , R - FCN makes the detection in a nearly fully convolutional manner , which greatly enhances the efficiency of training and testing .",49,0,27
dataset/preprocessed/test-data/face_detection/6,Robust Face Detection via Learning Small Faces on Hard Images,2,1,10
dataset/preprocessed/test-data/face_detection/6,"Recent anchor - based deep face detectors have achieved promising performance , but they are still struggling to detect hard faces , such as small , blurred and partially occluded faces .",4,0,32
dataset/preprocessed/test-data/face_detection/6,"A reason is that they treat all images and faces equally , without putting more effort on hard ones ; however , many training images only contain easy faces , which are less helpful to achieve better performance on hard images .",5,0,42
dataset/preprocessed/test-data/face_detection/6,"In this paper , we propose that the robustness of a face detector against hard faces can be improved by learning small faces on hard images .",6,0,27
dataset/preprocessed/test-data/face_detection/6,"Our intuitions are ( 1 ) hard images are the images which contain at least one hard face , thus they facilitate training robust face detectors ; ( 2 ) most hard faces are small faces and other types of hard faces can be easily converted to small faces by shrinking .",7,0,52
dataset/preprocessed/test-data/face_detection/6,"We build an anchor-based deep face detector , which only output a single feature map with small anchors , to specifically learn small faces and train it by a novel hard image mining strategy .",8,0,35
dataset/preprocessed/test-data/face_detection/6,"Extensive experiments have been conducted on WIDER FACE , FDDB , Pascal Faces , and AFW datasets to show the effectiveness of our method .",9,0,25
dataset/preprocessed/test-data/face_detection/6,"Our method achieves APs of 95.7 , 94.9 and 89.7 on easy , medium and hard WIDER FACE val dataset respectively , which surpass the previous state - of - the - arts , especially on the hard subset .",10,0,40
dataset/preprocessed/test-data/face_detection/6,Code and model are available at https,11,0,7
dataset/preprocessed/test-data/face_detection/6,"Face detection is a fundamental and important computer vision problem , which is critical for many face - related tasks , such as face alignment , tracking and recognition .",13,1,30
dataset/preprocessed/test-data/face_detection/6,"Stem from the recent successful development of deep neural networks , massive CNN - based face detection approaches have been proposed and achieved the state - of - the - art performance .",14,0,33
dataset/preprocessed/test-data/face_detection/6,"However , face detection remains a challenging task due to occlusion , illumination , makeup , as well as pose and scale variance , as shown in the benchmark dataset WIDER FACE .",15,0,33
dataset/preprocessed/test-data/face_detection/6,"Current state - of - the - art CNN - based face detectors at - tempt to address these challenges by employing more powerful backbone models , exploiting feature pyramid - style architectures to combine features from multiple detection feature maps , designing denser anchors and utilizing larger contextual information .",16,0,51
dataset/preprocessed/test-data/face_detection/6,"These methods and techniques have been shown to be successful to build a robust face detector , and improve the performance towards human - level for most images .",17,0,29
dataset/preprocessed/test-data/face_detection/6,"In spite of their success for most images , an evident performance gap still exists especially for those hard images which contain small , blurred and partially occluded faces .",18,0,30
dataset/preprocessed/test-data/face_detection/6,We realize that these hard images have become the main barriers for face detectors to achieve human - level detection performance .,19,0,22
dataset/preprocessed/test-data/face_detection/6,"In , we show that , even on the train set of WIDER FACE , the official pre-trained SSH 1 still fails on some of the images with extremely hard faces .",20,0,32
dataset/preprocessed/test-data/face_detection/6,We show two such hard training images in the upper right corner in .,21,0,14
dataset/preprocessed/test-data/face_detection/6,"On the other hand , most training images with easy faces can be almost perfectly detected ( see the illustration in the right lower corner of ) .",22,0,28
dataset/preprocessed/test-data/face_detection/6,"As shown in left part of , over two thirds of the training images already obtained perfect detection accuracy , which indicates that those easy images are less useful towards training a robust face detector .",23,0,36
dataset/preprocessed/test-data/face_detection/6,"To address this issue , in this paper , we propose a robust face detector by putting more training focus on those hard images .",24,0,25
dataset/preprocessed/test-data/face_detection/6,This issue is most related to anchor - level hard example mining discussed in OHEM .,25,0,16
dataset/preprocessed/test-data/face_detection/6,"However , due to the sparsity of ground - truth faces and positive anchors , traditional anchor - level hard example mining mainly focuses on mining hard negative anchors , and mining hard anchors on well - detected images exhibits less effectiveness since there is no useful information that can be further exploited in these easy images .",26,0,58
dataset/preprocessed/test-data/face_detection/6,"To address this issue , we propose to mine hard examples at image level in parallel with anchor level .",27,0,20
dataset/preprocessed/test-data/face_detection/6,"More specifically , we propose to dynamically assign difficulty scores to training images during the learning process , which can determine whether an image is already well - detected or still useful for further training .",28,0,36
dataset/preprocessed/test-data/face_detection/6,This allows us to fully utilize the images which were not perfectly detected to better facilitate the following learning process .,29,0,21
dataset/preprocessed/test-data/face_detection/6,"We show this strategy can make our detector more robust towards hard faces , without involving more complex network architecture and computation overhead .",30,0,24
dataset/preprocessed/test-data/face_detection/6,"Apart from mining the hard images , we also propose to improve the detection quality by exclusively exploiting small faces .",31,0,21
dataset/preprocessed/test-data/face_detection/6,Small faces are typically hard and have attracted extensive research attention .,32,0,12
dataset/preprocessed/test-data/face_detection/6,"Existing methods aim at building a scale - invariant face detector to learn and infer on both small and big faces , with multiple levels of detection features and anchors of different sizes .",33,0,34
dataset/preprocessed/test-data/face_detection/6,"Compared with these methods , our detector is more efficient since it is specially designed to aggressively leveraging the small faces during training .",34,0,24
dataset/preprocessed/test-data/face_detection/6,"More specifically , large faces are automatically ignored during training due to our anchor design , so that the model can fully focus on the small hard faces .",35,0,29
dataset/preprocessed/test-data/face_detection/6,"Additionally , experiments demonstrate that this design effectively achieves improvements on detecting all faces in spite of its simple and shallow architecture .",36,0,23
dataset/preprocessed/test-data/face_detection/6,"To conclude , in this paper , we propose a novel face detector with the following contributions :",37,0,18
dataset/preprocessed/test-data/face_detection/6,"We propose a hard image mining strategy , to improve the robustness of our detector to those extremely hard faces .",38,0,21
dataset/preprocessed/test-data/face_detection/6,"This is done without any extra modules , parameters or computation overhead added on the existing detector .",39,0,18
dataset/preprocessed/test-data/face_detection/6,"We design a single shot detector with only one detection feature map , which focuses on small faces with a specific range of sizes .",40,0,25
dataset/preprocessed/test-data/face_detection/6,This allows our model to be simple and focus on difficult small faces without struggling with scale variance .,41,0,19
dataset/preprocessed/test-data/face_detection/6,"Our face detector establishes state - of - the - art performance on all popular face detection datasets , including WIDER FACE , FDDB , Pascal Faces , and AFW .",42,0,31
dataset/preprocessed/test-data/face_detection/6,"We achieve 95.7 , 94.9 and 89.7 on easy , medium and hard WIDER FACE val dataset .",43,0,18
dataset/preprocessed/test-data/face_detection/6,"Our method also achieves APs of 99.00 and 99.60 on Pascal Faces and AFW respectively , as well as a TPR of 98.7 on FDDB .",44,0,26
dataset/preprocessed/test-data/face_detection/6,The remainder of this paper is organized as follows .,45,0,10
dataset/preprocessed/test-data/face_detection/6,"In Section 2 , we discuss some studies have been done which are related to our paper .",46,0,18
dataset/preprocessed/test-data/face_detection/6,"In Section 3 , we dive into details of our proposed method , and we discuss experiment results and ablation experiments in Section 4 .",47,0,25
dataset/preprocessed/test-data/face_detection/6,"Finally , conclusions are drawn in Section 5 .",48,0,9
dataset/preprocessed/test-data/face_detection/11,A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection,2,1,11
dataset/preprocessed/test-data/face_detection/11,"A unified deep neural network , denoted the multi -scale CNN ( MS - CNN ) , is proposed for fast multi-scale object detection .",4,1,25
dataset/preprocessed/test-data/face_detection/11,The MS - CNN consists of a proposal sub-network and a detection sub-network .,5,0,14
dataset/preprocessed/test-data/face_detection/11,"In the proposal sub-network , detection is performed at multiple output layers , so that receptive fields match objects of different scales .",6,0,23
dataset/preprocessed/test-data/face_detection/11,These complementary scale - specific detectors are combined to produce a strong multi-scale object detector .,7,0,16
dataset/preprocessed/test-data/face_detection/11,"The unified network is learned end - to - end , by optimizing a multi - task loss .",8,0,19
dataset/preprocessed/test-data/face_detection/11,"Feature upsampling by deconvolution is also explored , as an alternative to input upsampling , to reduce the memory and computation costs .",9,0,23
dataset/preprocessed/test-data/face_detection/11,"State - of - the - art object detection performance , at up to 15 fps , is reported on datasets , such as KITTI and Caltech , containing a substantial number of small objects .",10,1,36
dataset/preprocessed/test-data/face_detection/11,"Classical object detectors , based on the sliding window paradigm , search for objects at multiple scales and aspect ratios .",12,0,21
dataset/preprocessed/test-data/face_detection/11,"While real - time detectors are available for certain classes of objects , e.g. faces or pedestrians , it has proven difficult to build detectors of multiple object classes under this paradigm .",13,0,33
dataset/preprocessed/test-data/face_detection/11,"Recently , there has been interest in detectors derived from deep convolutional neural networks ( CNNs ) .",14,0,18
dataset/preprocessed/test-data/face_detection/11,"While these have shown much greater ability to address the multiclass problem , less progress has been made towards the detection of objects at multiple scales .",15,0,27
dataset/preprocessed/test-data/face_detection/11,"The R - CNN samples object proposals at multiple scales , using a preliminary attention stage , and then warps these proposals to the size ( e.g. 224224 ) supported by the CNN .",16,0,34
dataset/preprocessed/test-data/face_detection/11,"This is , however , very inefficient from a computational standpoint .",17,0,12
dataset/preprocessed/test-data/face_detection/11,The development of an effective and computationally efficient region proposal mechanism is still an open problem .,18,0,17
dataset/preprocessed/test-data/face_detection/11,"The more recent Faster - RCNN addresses the issue with a region proposal network ( RPN ) , which enables end - to - end training .",19,0,27
dataset/preprocessed/test-data/face_detection/11,"However , the RPN generates proposals of multiple scales by sliding a fixed set of filters over a fixed set of convolutional feature maps .",20,0,25
dataset/preprocessed/test-data/face_detection/11,"This creates an inconsistency between the sizes of objects , which are variable , and filter receptive fields , which are fixed .",21,0,23
dataset/preprocessed/test-data/face_detection/11,"As shown in , a fixed receptive field can not cover the multiple scales at which objects .",22,0,18
dataset/preprocessed/test-data/face_detection/11,"In natural images , objects can appear at very different scales , as illustrated by the yellow bounding boxes .",23,0,20
dataset/preprocessed/test-data/face_detection/11,"A single receptive field , such as that of the RPN ( shown in the shaded area ) , can not match this variability .",24,0,25
dataset/preprocessed/test-data/face_detection/11,appear in natural scenes .,25,0,5
dataset/preprocessed/test-data/face_detection/11,"This compromises detection performance , which tends to be particularly poor for small objects , like that in the center of .",26,0,22
dataset/preprocessed/test-data/face_detection/11,"In fact , handle such objects by upsampling the input image both at training and testing time .",27,0,18
dataset/preprocessed/test-data/face_detection/11,This increases the memory and computation costs of the detector .,28,0,11
dataset/preprocessed/test-data/face_detection/11,"This work proposes a unified multi-scale deep CNN , denoted the multi -scale CNN ( MS - CNN ) , for fast object detection .",29,0,25
dataset/preprocessed/test-data/face_detection/11,"Similar to , this network consists of two sub-networks : an object proposal network and an accurate detection network .",30,0,20
dataset/preprocessed/test-data/face_detection/11,Both of them are learned end - to - end and share computations .,31,0,14
dataset/preprocessed/test-data/face_detection/11,"However , to ease the inconsistency between the sizes of objects and receptive fields , object detection is performed with multiple output layers , each focusing on objects within certain scale ranges ( see ) .",32,0,36
dataset/preprocessed/test-data/face_detection/11,"The intuition is that lower network layers , such as "" conv - 3 , "" have smaller receptive fields , better matched to detect small objects .",33,0,28
dataset/preprocessed/test-data/face_detection/11,"Conversely , higher layers , such as "" conv - 5 , "" are best suited for the detection of large objects .",34,0,23
dataset/preprocessed/test-data/face_detection/11,The complimentary detectors at different output layers are combined to form a strong multi-scale detector .,35,0,16
dataset/preprocessed/test-data/face_detection/11,"This is shown to produce accurate object proposals on detection benchmarks with large variation of scale , such as KITTI , achieving a recall of over 95 % for only 100 proposals .",36,0,33
dataset/preprocessed/test-data/face_detection/11,A second contribution of this work is the use of feature upsampling as an alternative to input upsampling .,37,0,19
dataset/preprocessed/test-data/face_detection/11,"This is achieved by introducing a deconvolutional layer that increases the resolution of feature maps ( see ) , enabling small objects to produce larger regions of strong response .",38,0,30
dataset/preprocessed/test-data/face_detection/11,This is shown to reduce memory and computation costs .,39,0,10
dataset/preprocessed/test-data/face_detection/11,"While deconvolution has been explored for segmentation and edge detection , it is , as far as we know , for the first time used to speedup and improve detection .",40,0,31
dataset/preprocessed/test-data/face_detection/11,"When combined with efficient context encoding and hard negative mining , it results in a detector that advances the state - of - the - art detection on the KITTI and Caltech benchmarks .",41,0,34
dataset/preprocessed/test-data/face_detection/11,"Without image upsampling , the MS - CNN achieves speeds of 10 fps on KITTI ( 1250375 ) and 15 fps on Caltech ( 640480 ) images .",42,0,28
dataset/preprocessed/test-data/face_detection/11,One of the earliest methods to achieve real - time detection with high accuracy was the cascaded detector of .,44,0,20
dataset/preprocessed/test-data/face_detection/11,"This architecture has been widely used to implement sliding window detectors for faces , pedestrians and cars .",45,0,18
dataset/preprocessed/test-data/face_detection/11,Two main streams of research have been pursued to improve its speed : fast feature extraction and cascade learning .,46,0,20
dataset/preprocessed/test-data/face_detection/11,"In , a set of efficient Haar features was proposed with recourse to integral images .",47,0,16
dataset/preprocessed/test-data/face_detection/11,The aggregate feature channels ( ACF ) of made it possible to compute HOG features at about 100 fps .,48,0,20
dataset/preprocessed/test-data/face_detection/11,"On the learning front , proposed the soft - cascade , a method to transform a classifier learned with boosting into a cascade with certain guarantees in terms of false positive and detection rate .",49,0,35
dataset/preprocessed/test-data/face_detection/14,WIDER FACE : A Face Detection Benchmark,2,1,7
dataset/preprocessed/test-data/face_detection/14,Face detection is one of the most studied topics in the computer vision community .,4,0,15
dataset/preprocessed/test-data/face_detection/14,Much of the progresses have been made by the availability of face detection benchmark datasets .,5,0,16
dataset/preprocessed/test-data/face_detection/14,We show that there is a gap between current face detection performance and the real world requirements .,6,0,18
dataset/preprocessed/test-data/face_detection/14,"To facilitate future face detection research , we introduce the WIDER FACE dataset , which is 10 times larger than existing datasets .",7,0,23
dataset/preprocessed/test-data/face_detection/14,"The dataset contains rich annotations , including occlusions , poses , event categories , and face bounding boxes .",8,0,19
dataset/preprocessed/test-data/face_detection/14,"Faces in the proposed dataset are extremely challenging due to large variations in scale , pose and occlusion , as shown in Fig .",9,0,24
dataset/preprocessed/test-data/face_detection/14,"1 . Furthermore , we show that WIDER FACE dataset is an effective training source for face detection .",10,0,19
dataset/preprocessed/test-data/face_detection/14,"We benchmark several representative detection systems , providing an overview of state - of - the - art performance and propose a solution to deal with large scale variation .",11,0,30
dataset/preprocessed/test-data/face_detection/14,"Finally , we discuss common failure cases that worth to be further investigated .",12,0,14
dataset/preprocessed/test-data/face_detection/14,Dataset can be downloaded at : mmlab.ie.cuhk.edu.hk/projects/WIDERFace,13,0,7
dataset/preprocessed/test-data/face_detection/14,"Face detection is a critical step to all facial analysis algorithms , including face alignment , face recognition , face verification , and face parsing .",15,0,26
dataset/preprocessed/test-data/face_detection/14,"Given an arbitrary image , the goal of face detection is to determine whether or not there are any faces in the image and , if present , return the image location and extent of each face .",16,0,38
dataset/preprocessed/test-data/face_detection/14,"While this appears as an effortless task for human , it is a very difficult task for computers .",17,0,19
dataset/preprocessed/test-data/face_detection/14,"The challenges associated with face detection can be attributed to variations in pose , scale , facial expression , occlusion , and lighting condition , as shown in .",18,0,29
dataset/preprocessed/test-data/face_detection/14,Face detection has made significant progress after the seminal work by Viola and Jones .,19,0,15
dataset/preprocessed/test-data/face_detection/14,"Modern face detectors can easily detect near frontal faces and are widely used in real world applications , such as digital camera and electronic photo album .",20,0,27
dataset/preprocessed/test-data/face_detection/14,"Recent research in this area focuses on the unconstrained scenario , where a number of intricate factors .",21,0,18
dataset/preprocessed/test-data/face_detection/14,"We propose a WIDER FACE dataset for face detection , which has a high degree of variability in scale , pose , occlusion , expression , appearance and illumination .",22,0,30
dataset/preprocessed/test-data/face_detection/14,We show example images ( cropped ) and annotations .,23,0,10
dataset/preprocessed/test-data/face_detection/14,The annotated face bounding box is denoted in green color .,24,0,11
dataset/preprocessed/test-data/face_detection/14,"The WIDER FACE dataset consists of 393 , 703 labeled face bounding boxes in 32 , 203 images ( Best view in color ) .",25,0,25
dataset/preprocessed/test-data/face_detection/14,"such as extreme pose , exaggerated expressions , and large portion of occlusion can lead to large visual variations in face appearance .",26,0,23
dataset/preprocessed/test-data/face_detection/14,"Publicly available benchmarks such as FDDB , AFW , PASCAL FACE , have contributed to spurring interest and progress in face detection research .",27,0,24
dataset/preprocessed/test-data/face_detection/14,"However , as algorithm performance improves , more chal - lenging datasets are needed to trigger progress and to inspire novel ideas .",28,0,23
dataset/preprocessed/test-data/face_detection/14,"Current face detection datasets typically contain a few thousand faces , with limited variations in pose , scale , facial expression , occlusion , and background clutters , making it difficult to assess for real world performance .",29,0,38
dataset/preprocessed/test-data/face_detection/14,"As we will demonstrate , the limitations of datasets have partially contributed to the failure of some algorithms in coping with heavy occlusion , small scale , and atypical pose .",30,0,31
dataset/preprocessed/test-data/face_detection/14,"In this work , we make three contributions .",31,0,9
dataset/preprocessed/test-data/face_detection/14,We introduce a large - scale face detection dataset called WIDER FACE .,32,0,13
dataset/preprocessed/test-data/face_detection/14,"It consists of 32 , 203 images with 393 , 703 labeled faces , which is 10 times larger than the current largest face detection dataset .",33,0,27
dataset/preprocessed/test-data/face_detection/14,"The faces vary largely in appearance , pose , and scale , as shown in .",34,0,16
dataset/preprocessed/test-data/face_detection/14,"In order to quantify different types of errors , we annotate multiple attributes : occlusion , pose , and event categories , which allows in depth analysis of existing algorithms .",35,0,31
dataset/preprocessed/test-data/face_detection/14,"We show an example of using WIDER FACE through proposing a multi-scale two - stage cascade framework , which uses divide and conquer strategy to deal with large scale variations .",36,0,31
dataset/preprocessed/test-data/face_detection/14,"Within this framework , a set of convolutional networks with various size of input are trained to deal with faces with a specific range of scale .",37,0,27
dataset/preprocessed/test-data/face_detection/14,"We benchmark four representative algorithms , either obtained directly from the original authors or reimplemented using open - source codes .",38,0,21
dataset/preprocessed/test-data/face_detection/14,We evaluate these algorithms on different settings and analyze conditions in which existing methods fail .,39,0,16
dataset/preprocessed/test-data/face_detection/14,Brief review of recent face detection methods :,41,0,8
dataset/preprocessed/test-data/face_detection/14,Face detection has been studied for decades in the computer vision literature .,42,0,13
dataset/preprocessed/test-data/face_detection/14,"Modern face detection algorithms can be categorized into four categories : cascade based methods , part based methods , channel feature based methods , and neural network based methods .",43,0,30
dataset/preprocessed/test-data/face_detection/14,Here we highlight a few notable studies .,44,0,8
dataset/preprocessed/test-data/face_detection/14,A detailed survey can be found in .,45,0,8
dataset/preprocessed/test-data/face_detection/14,The seminal work by Viola and Jones introduces integral image to compute Haar - like features inconstant time .,46,0,19
dataset/preprocessed/test-data/face_detection/14,These features are then used to learn AdaBoost classifier with cascade structure for face detection .,47,0,16
dataset/preprocessed/test-data/face_detection/14,Various later studies follow a similar pipeline .,48,0,8
dataset/preprocessed/test-data/face_detection/14,"Among those variants , SURF cascade achieves competitive performance .",49,0,10
dataset/preprocessed/test-data/face_detection/17,Pyramid Box : A Context - assisted Single Shot Face Detector,2,0,11
dataset/preprocessed/test-data/face_detection/17,"Face detection has been well studied for many years and one of remaining challenges is to detect small , blurred and partially occluded faces in uncontrolled environment .",4,1,28
dataset/preprocessed/test-data/face_detection/17,"This paper proposes a novel contextassisted single shot face detector , named PyramidBox to handle the hard face detection problem .",5,0,21
dataset/preprocessed/test-data/face_detection/17,"Observing the importance of the context , we improve the utilization of contextual information in the following three aspects .",6,0,20
dataset/preprocessed/test-data/face_detection/17,"First , we design a novel context anchor to supervise high - level contextual feature learning by a semi-supervised method , which we call it PyramidAnchors .",7,0,27
dataset/preprocessed/test-data/face_detection/17,"Second , we propose the Low - level Feature Pyramid Network to combine adequate high - level context semantic feature and Low - level facial feature together , which also allows the PyramidBox to predict faces of all scales in a single shot .",8,0,44
dataset/preprocessed/test-data/face_detection/17,"Third , we introduce a contextsensitive structure to increase the capacity of prediction network to improve the final accuracy of output .",9,0,22
dataset/preprocessed/test-data/face_detection/17,"In addition , we use the method of Data - anchor - sampling to augment the training samples across different scales , which increases the diversity of training data for smaller faces .",10,0,33
dataset/preprocessed/test-data/face_detection/17,"By exploiting the value of context , PyramidBox achieves superior performance among the state - of - the - art over the two common face detection benchmarks , FDDB and WIDER FACE .",11,0,33
dataset/preprocessed/test-data/face_detection/17,Our code is available in Pad - dlePaddle : https://github.com/PaddlePaddle/models/tree/develop/,12,0,10
dataset/preprocessed/test-data/face_detection/17,Face detection is a fundamental and essential task in various face applications .,15,0,13
dataset/preprocessed/test-data/face_detection/17,The breakthrough work by Viola - Jones utilizes AdaBoost algorithm with Haar - Like features to train a cascade of face vs. non-face classifiers .,16,0,25
dataset/preprocessed/test-data/face_detection/17,"Since that , numerous of subsequent works are proposed for improving the cascade detectors .",17,0,15
dataset/preprocessed/test-data/face_detection/17,"Then , introduce deformable part models ( DPM ) into face detection tasks by modeling the relationship of deformable facial parts .",18,0,22
dataset/preprocessed/test-data/face_detection/17,These methods are mainly based on designed features which are less representable and trained by separated steps .,19,0,18
dataset/preprocessed/test-data/face_detection/17,"With the great breakthrough of convolutional neural networks ( CNN ) , a lot of progress for face detection has been made in recent years due to utilizing modern CNN - based object detectors , including R - CNN , SSD , YOLO , FocalLoss and their extensions .",20,0,49
dataset/preprocessed/test-data/face_detection/17,"Benefiting from the powerful deep learning approach and end - to - end optimization , the CNN - based face detectors have achieved much better performance and provided anew baseline for later methods .",21,0,34
dataset/preprocessed/test-data/face_detection/17,Recent anchor - based detection frameworks aim at detecting hard faces in uncontrolled environment such as WIDER FACE .,22,0,19
dataset/preprocessed/test-data/face_detection/17,SSH and S 3 FD develop scale - invariant networks to detect faces with different scales from different layers in a single network .,23,0,24
dataset/preprocessed/test-data/face_detection/17,Face R - FCN re-weights embedding responses on score maps and eliminates the effect of non-uniformed contribution in each facial part using a position - sensitive average pooling .,24,0,29
dataset/preprocessed/test-data/face_detection/17,FAN proposes an anchor - level attention by highlighting the features from the face region to detect the occluded faces .,25,0,21
dataset/preprocessed/test-data/face_detection/17,"Though these works give an effective way to design anchors and related networks to detect faces with different scales , how to use the contextual information in face detection has not been paid enough attention , which should play a significant role in detection of hard faces .",26,0,48
dataset/preprocessed/test-data/face_detection/17,"Actually , as shown in , it is clear that faces never occur isolated in the real world , usually with shoulders or bodies , providing a rich source of contextual associations to be exploited especially when the facial texture is not distinguishable for the sake of low - resolution , blur and occlusion .",27,0,55
dataset/preprocessed/test-data/face_detection/17,We address this issue by introducing a novel framework of context assisted network to make full use of contextual signals as the following steps .,28,0,25
dataset/preprocessed/test-data/face_detection/17,"Firstly , the network should be able to learn features for not only faces , but also contextual parts such as heads and bodies .",29,0,25
dataset/preprocessed/test-data/face_detection/17,"To achieve this goal , extra labels are needed and the anchors matched to these parts should be designed .",30,0,20
dataset/preprocessed/test-data/face_detection/17,"In this work , we use a semi-supervised solution to generate approximate labels for contextual parts related to faces and a series of anchors called PyramidAnchors are invented to be easily added to general anchor - based architectures .",31,0,39
dataset/preprocessed/test-data/face_detection/17,"Secondly , high - level contextual features should be adequately combined with the low - level ones .",32,0,18
dataset/preprocessed/test-data/face_detection/17,"The appearances of hard and easy faces can be quite differ -ent , which implies that not all high - level semantic features are really helpful to smaller targets .",33,0,30
dataset/preprocessed/test-data/face_detection/17,We investigate the performance of Feature Pyramid Networks ( FPN ) and modify it into a Low - level Feature Pyramid Network ( LFPN ) to join mutually helpful features together .,34,0,32
dataset/preprocessed/test-data/face_detection/17,"Thirdly , the predict branch network should make full use of the joint feature .",35,0,15
dataset/preprocessed/test-data/face_detection/17,We introduce the Context - sensitive prediction module ( CPM ) to incorporate context information around the target face with a wider and deeper network .,36,0,26
dataset/preprocessed/test-data/face_detection/17,"Meanwhile , we propose a max - in - out layer for the prediction module to further improve the capability of classification network .",37,0,24
dataset/preprocessed/test-data/face_detection/17,"In addition , we propose a training strategy named as Data - anchor - sampling to make an adjustment on the distribution of the training dataset .",38,0,27
dataset/preprocessed/test-data/face_detection/17,"In order to learn more representable features , the diversity of hard - set samples is important and can be gained by data augmentation across samples .",39,0,27
dataset/preprocessed/test-data/face_detection/17,"For clarity , the main contributions of this work can be summarized as fivefold :",40,0,15
dataset/preprocessed/test-data/face_detection/17,"We propose an anchor-based context assisted method , called PyramidAnchors , to introduce supervised information on learning contextual features for small , blurred and partially occluded faces .",42,0,28
dataset/preprocessed/test-data/face_detection/17,2 . We design the Low - level Feature Pyramid Networks ( LFPN ) to merge contextual features and facial features better .,43,0,23
dataset/preprocessed/test-data/face_detection/17,"Meanwhile , the proposed method can handle faces with different scales well in a single shot .",44,0,17
dataset/preprocessed/test-data/face_detection/17,"3 . We introduce a context - sensitive prediction module , consisting of a mixed network structure and max - in - out layer to learn accurate location and classification from the merged features .",45,0,35
dataset/preprocessed/test-data/face_detection/17,4 . We propose the scale aware Data - anchor - sampling strategy to change the distribution of training samples to put emphasis on smaller faces .,46,0,27
dataset/preprocessed/test-data/face_detection/17,5 . We achieve superior performance over state - of - the - art on the common face detection benchmarks FDDB and WIDER FACE .,47,0,25
dataset/preprocessed/test-data/face_detection/17,The rest of the paper is organized as follows .,48,0,10
dataset/preprocessed/test-data/face_detection/17,Section 2 provides an overview of the related works .,49,0,10
dataset/preprocessed/test-data/face_detection/0,Accurate Face Detection for High Performance,2,1,6
dataset/preprocessed/test-data/face_detection/0,Face detection has witnessed significant progress due to the advances of deep convolutional neural networks ( CNNs ) .,4,0,19
dataset/preprocessed/test-data/face_detection/0,Its central issue in recent years is how to improve the detection performance of tiny faces .,5,0,17
dataset/preprocessed/test-data/face_detection/0,"To this end , many recent works propose some specific strategies , redesign the architecture and introduce new loss functions for tiny object detection .",6,0,25
dataset/preprocessed/test-data/face_detection/0,"In this report , we start from the popular one - stage RetinaNet [ 20 ] approach and apply some recent tricks to obtain a high performance face detector namely AInnoFace .",7,0,32
dataset/preprocessed/test-data/face_detection/0,"Specifically , we apply the Intersection over Union ( IoU ) loss function [ 45 ] for regression , employ the two - step classification and regression [ 4 ] for detection , revisit the data augmentation based on data - anchor - sampling [ 32 ] for training , utilize the max - out operation [ 54 ] for classification and use the multi -scale testing strategy [ 54 ] for inference .",8,0,74
dataset/preprocessed/test-data/face_detection/0,"As a consequence , the proposed face detection method achieves state - of - the - art performance on the most popular and challenging face detection benchmark WIDER FACE [ 43 ] dataset .",9,0,34
dataset/preprocessed/test-data/face_detection/0,"Face detection is a tremendously important field in computer vision needed for face recognition , sentiment analysis , video surveillance , and many other fields .",11,0,26
dataset/preprocessed/test-data/face_detection/0,"Given an arbitrary image , the goal of face detection is to determine whether there are any faces in the image , and if present , return the image location and extent of each face .",12,0,36
dataset/preprocessed/test-data/face_detection/0,The recent issue of face detection is how to improve the detection performance in unrestricted scenarios .,13,0,17
dataset/preprocessed/test-data/face_detection/0,"Because detecting faces in real - world images has many difficulties including occlusion , significant scale variation , different illumination conditions , various facial poses , rich facial expressions , etc .",14,0,32
dataset/preprocessed/test-data/face_detection/0,Many works are devoted to solving this issue and great progress has been achieved with the development of deep convolutional neural networks ( CNNs ) .,15,0,26
dataset/preprocessed/test-data/face_detection/0,"For example , the average precision ( AP ) performance on the challenging WIDER FACE dataset has been improved from 40 % to 90 % over recent years .",16,0,29
dataset/preprocessed/test-data/face_detection/0,"To improve the performance of face detection in unrestricted scenarios where exists plenty of tiny faces , some works combine traditional methods ( e.g. , cascade - based mechanism and part - based in DPM ) with deep learning methods ( e.g. , CNN ) to perform face detection .",17,0,50
dataset/preprocessed/test-data/face_detection/0,A number of works resort to the context information around the face region to find tiny faces based on the Faster R - CNN and SSD detectors .,18,0,28
dataset/preprocessed/test-data/face_detection/0,Several works redesign the architecture of modern object detection to better detect tiny faces .,19,0,15
dataset/preprocessed/test-data/face_detection/0,A series of works propose some special strategies for tiny faces into the generic object detection methods to improve face detection performance .,20,0,23
dataset/preprocessed/test-data/face_detection/0,There are many works present some new data augmentations for tiny faces to improve the performance .,21,0,17
dataset/preprocessed/test-data/face_detection/0,Some works introduce the attention mechanism on the feature maps to focus on face regions for better detection performance .,22,0,20
dataset/preprocessed/test-data/face_detection/0,"In this work , we first modify the popular one - stage RetinaNet method to perform face detection as our baseline model .",23,0,23
dataset/preprocessed/test-data/face_detection/0,Then some recent tricks are applied on this baseline to develop a high performance face detector namely AInnoFace :,24,0,19
dataset/preprocessed/test-data/face_detection/0,( 1 ) Employing the two - step classification and regression for detection ; ( 2 ) Applying the Intersection over Union ( IoU ) loss function for regression ; ( 3 ) Revisiting the data augmentation based on data - anchor - sampling for training ; ( 4 ) Utilizing the max - out operation for robuster classification ; ( 5 ) Using the multi-scale testing strategy for inference .,25,0,71
dataset/preprocessed/test-data/face_detection/0,"Consequently , we achieve some new state - of - the - art AP results on the challenging face detection benchmark WIDER FACE dataset .",26,0,25
dataset/preprocessed/test-data/face_detection/0,Face detection has been extensively studied from its emergence in the 1990s to the present because of its wide practical applications .,29,0,22
dataset/preprocessed/test-data/face_detection/0,"The pioneering work of Viola and Jones uses the Haarlike feature and the AdaBoost strategy to train several cascaded face detectors , achieving a very good tradeoff between accuracy and efficiency in some simple and fixed scenarios .",30,0,38
dataset/preprocessed/test-data/face_detection/0,"Afterwards , subsequent works have made great progress by developing more advanced features and more powerful classifiers .",31,0,18
dataset/preprocessed/test-data/face_detection/0,"Apart from the boosted cascade methods , several studies introduce another famous framework of Deformable Part Model ( DPM ) to the filed of face detection task , which detect faces by modelling the relationship of deformable facial parts and achieve promising performance in some simple application scenarios .",32,0,49
dataset/preprocessed/test-data/face_detection/0,"However , these traditional face detectors are unreliable in complex scenarios because they depend on non-robust hand - crafted features and classifiers .",33,0,23
dataset/preprocessed/test-data/face_detection/0,Deep Learning Method,34,0,3
dataset/preprocessed/test-data/face_detection/0,Deep learning approaches significantly boost the recent progress in the face detection filed and the CNN - based face detectors have achieved the highest performance in the last few years .,35,0,31
dataset/preprocessed/test-data/face_detection/0,"The cascade CNN - based methods train a series of CNN models separately or jointly to perform face detection , and achieve promising accuracy and efficiency simultaneously .",36,0,28
dataset/preprocessed/test-data/face_detection/0,"After that , MTCNN and PCN add another extra branch to detect five facial landmarks and predict face angles via the multi-task learning in a coarse - to - fine manner under a cascade - style structure .",37,0,38
dataset/preprocessed/test-data/face_detection/0,Faceness obtains different scores according to the spatial structure and arrangement of facial parts to detect faces under severe occlusion and unconstrained pose variations .,38,0,25
dataset/preprocessed/test-data/face_detection/0,LDCF + utilizes the boosted decision tree classifier to detect faces .,39,0,12
dataset/preprocessed/test-data/face_detection/0,UnitBox introduces an Intersection - over - Union ( IoU ) loss to directly minimize the IoUs of the predictions and the ground - truths for more accurate location .,40,0,30
dataset/preprocessed/test-data/face_detection/0,Face detects different scales of faces via applying a specialized set of CNNs with different structures .,42,0,17
dataset/preprocessed/test-data/face_detection/0,SAFD develops a scale proposal stage to automatically normalize face sizes prior to detection .,43,0,15
dataset/preprocessed/test-data/face_detection/0,Hu et al .,44,0,4
dataset/preprocessed/test-data/face_detection/0,explore the contextual information with some separate detectors for different scales to find tiny faces .,45,0,16
dataset/preprocessed/test-data/face_detection/0,S 2 AP finds face via paying attention to specific scales in image pyramid and valid locations in each scales layer .,46,0,22
dataset/preprocessed/test-data/face_detection/0,Zhu et al. use the Expected Max Overlapping ( EMO ) score to evaluate the quality of anchor setting .,47,0,20
dataset/preprocessed/test-data/face_detection/0,Bai et al.,48,0,3
dataset/preprocessed/test-data/face_detection/0,generate a clear super- resolution face from a blurry small one via to GAN to detect blurry small faces .,49,0,20
dataset/preprocessed/test-data/face_detection/12,Segmentation Is All You Need,2,0,5
dataset/preprocessed/test-data/face_detection/12,Region proposal mechanisms are essential for existing deep learning approaches to object detection in images .,4,1,16
dataset/preprocessed/test-data/face_detection/12,"Although they can generally achieve a good detection performance under normal circumstances , their recall in a scene with extreme cases is unacceptably low .",5,0,25
dataset/preprocessed/test-data/face_detection/12,"This is mainly because bounding box annotations contain much environment noise information , and non-maximum suppression ( NMS ) is required to select target boxes .",6,0,26
dataset/preprocessed/test-data/face_detection/12,"Therefore , in this paper , we propose the first anchorfree and NMS - free object detection model , called weakly supervised multimodal annotation segmentation ( WSMA - Seg ) , which utilizes segmentation models to achieve an accurate and robust object detection without NMS .",7,0,46
dataset/preprocessed/test-data/face_detection/12,"In WSMA - Seg , multimodal annotations are proposed to achieve an instance - aware segmentation using weakly supervised bounding boxes ; we also develop a run-data - based following algorithm to trace contours of objects .",8,0,37
dataset/preprocessed/test-data/face_detection/12,"In addition , we propose a multi-scale pooling segmentation ( MSP - Seg ) as the underlying segmentation model of WSMA - Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA - Seg. Experimental results on multiple datasets show that the proposed WSMA - Seg approach outperforms the state - of - the - art detectors .",9,0,63
dataset/preprocessed/test-data/face_detection/12,Object detection in images is one of the most widely explored tasks in computer vision .,11,0,16
dataset/preprocessed/test-data/face_detection/12,"Existing deep learning approaches to solve this task ( e.g. , R - CNN and its variants ) mainly rely on region proposal mechanisms ( e.g. , region proposal networks ( RPN s ) ) to generate potential bounding boxes in an image and then classify these bounding boxes to achieve object detection .",12,1,54
dataset/preprocessed/test-data/face_detection/12,"Although such mechanisms can generally achieve a good detection performance under normal circumstances , their recall in a scene with extreme cases ( e.g. , complex occlusion ( ) , poor illumination ( ) , and large - scale small objects ( ) ) is unacceptably low .",13,0,48
dataset/preprocessed/test-data/face_detection/12,"Specifically , detecting objects under extreme cases via region proposal mechanisms encounters two challenges :",14,0,15
dataset/preprocessed/test-data/face_detection/12,"First , the performance of region proposal mechanisms highly depends on the purity of bounding boxes ; however , the annotated bounding boxes in extreme cases usually contain much more environment noise than those in normal cases .",15,0,38
dataset/preprocessed/test-data/face_detection/12,"This inevitably increases the difficulty of model learning and decreases the resulting confidence scores of bounding boxes , which consequently weakens the detection performance .",16,0,25
dataset/preprocessed/test-data/face_detection/12,"Second , non-maximum suppression ( NMS ) operations are used in region proposal mechanisms to select target boxes by setting an intersection over union ( IoU ) threshold to filter other bounding boxes .",17,0,34
dataset/preprocessed/test-data/face_detection/12,"However , it is very hard ( and sometimes even impossible ) to find an appropriate threshold to adapt to the very complex situations in extreme cases .",18,0,28
dataset/preprocessed/test-data/face_detection/12,"Motivated by this , in this work , we propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach , which uses segmentation models to achieve an accurate and robust object detection without NMS .",19,0,38
dataset/preprocessed/test-data/face_detection/12,"It consists of two phases , namely , a training and a testing phase .",20,0,15
dataset/preprocessed/test-data/face_detection/12,"In the training phase , WSMA - Seg first converts weakly supervised bounding box annotations in detection tasks to multi-channel segmentation - like masks , called multimodal annotations ; then , a segmentation model is trained using multimodal annotations as labels to learn multimodal heatmaps for the training images .",21,0,50
dataset/preprocessed/test-data/face_detection/12,"In the testing phase , the resulting heatmaps of a given test image are converted into an instance - aware segmentation map based on a pixel - level logic operation ; then , a contour tracing operation is conducted to generate contours for objects using the segmentation map ; finally , bounding boxes of objects are created as circumscribed quadrilaterals of their corresponding contours .",22,0,65
dataset/preprocessed/test-data/face_detection/12,"WSMA - Seg has the following advantages : ( i ) as an NMS - free solution , WSMA - Seg avoids all hyperparameters related to anchor boxes and NMS ; so , the above - mentioned threshold selection problem is also avoided ; ( ii ) the complex occlusion problem can be alleviated by utilizing the topological structure of segmentation - like multimodal annotations ; and ( iii ) multimodal annotations are pixel - level annotations ; so , they can describe the objects more accurately and overcome the above - mentioned environment noise problem .",23,0,97
dataset/preprocessed/test-data/face_detection/12,"Furthermore , it is obvious that the performance of the proposed WSMA - Seg approach greatly depends on the segmentation performance of the underlying segmentation model .",24,0,27
dataset/preprocessed/test-data/face_detection/12,"Therefore , in this work , we further propose a multi-scale pooling segmentation ( MSP - Seg ) model , which is used as the underlying segmentation model of WSMA - Seg to achieve a more accurate segmentation ( especially for extreme cases , e.g. , very small objects ) , and consequently enhances the detection accuracy of WSMA - Seg .",25,0,62
dataset/preprocessed/test-data/face_detection/12,The contributions of this paper are briefly as follows :,26,0,10
dataset/preprocessed/test-data/face_detection/12,"We propose a weakly supervised multimodal annotation segmentation ( WSMA - Seg ) approach to achieve an accurate and robust object detection without NMS , which is the first anchor-free and NMS - free object detection approach .",27,0,38
dataset/preprocessed/test-data/face_detection/12,We propose multimodal annotations to achieve an instance - aware segmentation using weakly supervised bounding boxes ; we also develop a run-data - based following algorithm to trace contours of objects .,28,0,32
dataset/preprocessed/test-data/face_detection/12,We propose a multi-scale pooling segmentation ( MSP - Seg ) model to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA - Seg .,29,0,29
dataset/preprocessed/test-data/face_detection/12,"We have conducted extensive experimental studies on the Rebar Head , WIDER Face , and MS COCO datasets ; the results show that the proposed WSMA - Seg approach outperforms the state - of - the - art detectors on all testing datasets .",30,0,44
dataset/preprocessed/test-data/face_detection/12,Weakly Supervised Multimodal Annotation Segmentation,31,0,5
dataset/preprocessed/test-data/face_detection/12,"In this section , we introduce our approach to object detection using weakly supervised multimodal annotation segmentation ( WSMA - Seg ) .",32,0,23
dataset/preprocessed/test-data/face_detection/12,WSMA - Seg generally consists of two phases : a training phase and a testing phase .,33,0,17
dataset/preprocessed/test-data/face_detection/12,"In the training phase , as shown in , WSMA - Seg first converts the weakly supervised bounding box annotations to pixel - level segmentation - like masks with three channels , representing interior , boundary , and boundary on interior masking information , respectively ; the resulting annotations are called multimodal annotations ; then , multimodal annotations are used as labels to train an underlying segmentation model to learn corresponding multimodal heatmaps for the training images .",34,0,78
dataset/preprocessed/test-data/face_detection/12,"In the testing phase , as shown in , we first send the given testing image into the well - trained segmentation model to obtain multimodal heatmaps ; then , the resulting three heatmaps are converted into an instance - aware segmentation map based on a pixel - level logic operation ; finally , a contour tracing operation is conducted to generate contours for objects using the segmentation map , and the bounding boxes of objects are created as circumscribed quadrilaterals of their contours .",35,0,85
dataset/preprocessed/test-data/face_detection/12,The rest of this section will introduce the main ingredients of WSMA - Seg .,36,0,15
dataset/preprocessed/test-data/face_detection/12,Generating Multimodal Annotations,37,0,3
dataset/preprocessed/test-data/face_detection/12,"Pixel - level segmentation annotations are much more representative than bounding box annotations , so they can resolve some extreme cases that are challenging for bounding box annotations .",38,0,29
dataset/preprocessed/test-data/face_detection/12,"However , creating well - designed pixel - level segmentation masks is very time - consuming , which is about 15 times of creating bounding box annotations .",39,0,28
dataset/preprocessed/test-data/face_detection/12,"Therefore , in this work , we propose a methodology to automatically convert bounding box annotations to segmentation - like multimodal annotations , which are pixel - level geometric segmentation - like multichannel annotations .",40,0,35
dataset/preprocessed/test-data/face_detection/12,"Here , "" geometric segmentationlike "" means that the multimodal annotations are not strict segmentation annotations ; rather , they are annotations generated from simple geometries , e.g. , inscribed ellipses of bounding boxes .",41,0,35
dataset/preprocessed/test-data/face_detection/12,"This is motivated by the finding in that pixel - level segmentation information is not fully utilized by segmentation models ; we thus believe that well - designed pixel - level segmentation annotations may not be essential to achieve a reasonable performance ; rather , pixel - level geometric annotations should be enough .",42,0,54
dataset/preprocessed/test-data/face_detection/12,"Furthermore , to generate a bounding box for each object in the image , an instance - aware segmentation is required ; to achieve this , multimodal annotations are designed to have multiple channels to introduce additional information .",43,0,39
dataset/preprocessed/test-data/face_detection/12,"Specifically , as shown in , multimodal annotations use three channels to represent pixellevel masking information regarding the interior , the boundary , and the boundary on the interior of geometries .",44,0,32
dataset/preprocessed/test-data/face_detection/12,These three different pixel - level masks are generated as follows :,45,0,12
dataset/preprocessed/test-data/face_detection/12,"Given an image with bounding box annotations , we first obtain an inscribed ellipse for each bounding box , then the interior mask ( channel 0 ) is obtained by setting the values of pixels on the edge of or inside the ellipses to 1 , and setting the values of other pixels to 0 .",46,0,56
dataset/preprocessed/test-data/face_detection/12,"Then , the boundary mask ( channel 1 ) is obtained by setting the values of pixels on the edge of or within the inner width w of the ellipses to 1 , and setting the rest to 0 .",47,0,40
dataset/preprocessed/test-data/face_detection/12,"Similarly , the boundary on the interior mask ( channel 2 ) is generated by setting the values of pixels on the edge of or within the inner width w of the area of the elliptical overlap to 1 .",48,0,40
dataset/preprocessed/test-data/face_detection/12,Multi - Scale Pooling Segmentation,49,0,5
dataset/preprocessed/test-data/face_detection/2,Selective Refinement Network for High Performance Face Detection,2,1,8
dataset/preprocessed/test-data/face_detection/2,"High performance face detection remains a very challenging problem , especially when there exists many tiny faces .",4,1,18
dataset/preprocessed/test-data/face_detection/2,"This paper presents a novel single - shot face detector , named Selective Refinement Network ( SRN ) , which introduces novel twostep classification and regression operations selectively into an anchor- based face detector to reduce false positives and improve location accuracy simultaneously .",5,0,44
dataset/preprocessed/test-data/face_detection/2,"In particular , the SRN consists of two modules : the Selective Two - step Classification ( STC ) module and the Selective Two - step Regression ( STR ) module .",6,0,32
dataset/preprocessed/test-data/face_detection/2,"The STC aims to filter out most simple negative anchors from low level detection layers to reduce the search space for the subsequent classifier , while the STR is designed to coarsely adjust the locations and sizes of anchors from high level detection layers to provide better initialization for the subsequent regressor .",7,0,53
dataset/preprocessed/test-data/face_detection/2,"Moreover , we design a Receptive Field Enhancement ( RFE ) block to provide more diverse receptive field , which helps to better capture faces in some extreme poses .",8,0,30
dataset/preprocessed/test-data/face_detection/2,"As a consequence , the proposed SRN detector achieves state - of - the - art performance on all the widely used face detection benchmarks , including AFW , PASCAL face , FDDB , and WIDER FACE datasets .",9,0,39
dataset/preprocessed/test-data/face_detection/2,Codes will be released to facilitate further studies on the face detection problem .,10,0,14
dataset/preprocessed/test-data/face_detection/2,"Face detection is a long - standing problem in computer vision with extensive applications including face alignment , face analysis , face recognition , etc .",12,0,26
dataset/preprocessed/test-data/face_detection/2,"Starting from the pioneering work of Viola - Jones , face detection has made great progress .",13,0,17
dataset/preprocessed/test-data/face_detection/2,"The performances on several well - known datasets have been improved consistently , even tend to be saturated .",14,0,19
dataset/preprocessed/test-data/face_detection/2,To further improve the performance of face detection has become a challenging issue .,15,0,14
dataset/preprocessed/test-data/face_detection/2,"In our opinion , there remains room for improvement in two aspects : ( a ) recall efficiency : number of false positives needs to be reduced at the high recall rates ; ( b ) location accuracy : accuracy of the bounding box location needs to be improved .",16,0,50
dataset/preprocessed/test-data/face_detection/2,These two problems are elaborated as follows .,17,0,8
dataset/preprocessed/test-data/face_detection/2,"On the one hand , the average precision ( AP ) of current face detection algorithms is already very high , but the precision is not high enough at high recall rates , e.g. , as shown *",18,0,38
dataset/preprocessed/test-data/face_detection/2,"These authors contributed equally to this work . The STR provides better initialization for the subsequent regressor , ( d ) which produces more accurate locations , i.e. , as the IoU threshold increases , the AP gap gradually increases .",19,0,41
dataset/preprocessed/test-data/face_detection/2,"in ( b ) of RetinaNet , the precision is only about 50 % ( half of detections are false positives ) when the recall rate is equal to 90 % , which we define as the low recall efficiency .",20,0,41
dataset/preprocessed/test-data/face_detection/2,"Reflected on the shape of the Precision - Recall curve , it has extended far enough to the right , but not steep enough .",21,0,25
dataset/preprocessed/test-data/face_detection/2,The reason is that existing algorithms pay more attention to pursuing high recall rate but ignore the problem of excessive false positives .,22,0,23
dataset/preprocessed/test-data/face_detection/2,"Analyzing with anchor-based face detectors , they detect faces by classifying and regressing a series of preset anchors , which are generated by regularly tiling a collection of boxes with different scales and aspect ratios .",23,0,36
dataset/preprocessed/test-data/face_detection/2,"To detect the tiny faces , e.g. , less than 16 16 pixels , it is necessary to tile plenty of small anchors over the image .",24,0,27
dataset/preprocessed/test-data/face_detection/2,"This can improve the recall rate yet cause the the extreme class imbalance problem , which is the culprit leading to excessive false positives .",25,0,25
dataset/preprocessed/test-data/face_detection/2,"To address this issue , researchers propose several solutions .",26,0,10
dataset/preprocessed/test-data/face_detection/2,R - CNN - like detectors ) address the class imbalance by a two - stage cascade and sampling heuristics .,27,0,21
dataset/preprocessed/test-data/face_detection/2,"As for single - shot detectors , RetinaNet proposes the focal loss to focus training on a sparse set of hard examples and down - weight the loss assigned to well - classified examples .",28,0,35
dataset/preprocessed/test-data/face_detection/2,Det addresses this issue using a preset threshold to filter out negative anchors .,30,0,14
dataset/preprocessed/test-data/face_detection/2,"However , Retina",31,0,3
dataset/preprocessed/test-data/face_detection/2,"Net takes all the samples into account , which also leads to quite a few false positives .",32,0,18
dataset/preprocessed/test-data/face_detection/2,"Although Refine Det filters out a large number of simple negative samples , it uses hard negative mining in both two steps , and does not make full use of negative samples .",33,0,33
dataset/preprocessed/test-data/face_detection/2,"Thus , the recall efficiency of them both can be improved .",34,0,12
dataset/preprocessed/test-data/face_detection/2,"On the other hand , the location accuracy in the face detection task is gradually attracting the attention of researchers .",35,0,21
dataset/preprocessed/test-data/face_detection/2,"Although current evaluation criteria of most face detection datasets ( Jain and Learned - Miller 2010 ; do not focus on the location accuracy , the WIDER Face Challenge 1 adopts MS COCO ) evaluation criterion , which puts more emphasis on bounding box location accuracy .",36,0,47
dataset/preprocessed/test-data/face_detection/2,"To visualize this issue , we use different IoU thresholds to evaluate our trained face detector based on RetinaNet on the WIDER FACE dataset .",37,0,25
dataset/preprocessed/test-data/face_detection/2,"As shown in ( d ) , as the IoU threshold increases , the AP drops dramatically , indicating that the accuracy of the bounding box location needs to be improved .",38,0,32
dataset/preprocessed/test-data/face_detection/2,"To this end , propose iterative regression during inference to improve the accuracy .",39,0,14
dataset/preprocessed/test-data/face_detection/2,Cascade R - CNN addresses this issue by cascading R - CNN with different IoU thresholds .,40,0,17
dataset/preprocessed/test-data/face_detection/2,RefineDet ) applies two - step regression to single - shot detector .,41,0,13
dataset/preprocessed/test-data/face_detection/2,"However , blindly adding multi-step regression to the specific task ( i.e. , face detection ) is often counterproductive .",42,0,20
dataset/preprocessed/test-data/face_detection/2,"In this paper , we investigate the effects of two - step classification and regression on different levels of detection layers and propose a novel face detection framework , named Selective Refinement Network ( SRN ) , which selectively applies two - step classification and regression to specific levels of detection layers .",43,0,53
dataset/preprocessed/test-data/face_detection/2,"The network structure of SRN is shown in , which consists of two key modules , named as the Selective Two - step Classification ( STC ) module and the Selective Two - step Regression ( STR ) module .",44,0,40
dataset/preprocessed/test-data/face_detection/2,"Specifically , the STC is applied to filter out most simple negative samples ( illustrated in ( a ) ) from the low levels of detection layers , which contains 88.9 % samples .",45,0,34
dataset/preprocessed/test-data/face_detection/2,"As shown in , RetinaNet with STC improves the recall efficiency to a certain extent .",46,0,16
dataset/preprocessed/test-data/face_detection/2,"On the other hand , the design of STR draws on the cascade idea to coarsely adjust the locations and sizes of anchors ( illustrated in ( c ) ) from high levels of detection layers to provide better initialization for the subsequent regressor .",47,0,45
dataset/preprocessed/test-data/face_detection/2,"In addition , we design a Receptive Field Enhancement ( RFE ) to provide more diverse receptive fields to better capture the extreme - pose faces .",48,0,27
dataset/preprocessed/test-data/face_detection/2,"Extensive experiments have been conducted on AFW , PASCAL face , FDDB , and WIDER FACE benchmarks and we set a new state - of - the - art performance .",49,0,31
dataset/preprocessed/test-data/face_detection/4,LFFD : A Light and Fast Face Detector for Edge Devices,2,0,11
dataset/preprocessed/test-data/face_detection/4,"Face detection , as a fundamental technology for various applications , is always deployed on edge devices which have limited memory storage and low computing power .",4,1,27
dataset/preprocessed/test-data/face_detection/4,This paper introduces a Light and Fast Face Detector ( LFFD ) for edge devices .,5,0,16
dataset/preprocessed/test-data/face_detection/4,The proposed method is anchorfree and belongs to the one - stage category .,6,0,14
dataset/preprocessed/test-data/face_detection/4,"Specifically , we rethink the importance of receptive field ( RF ) and effective receptive field ( ERF ) in the background of face detection .",7,0,26
dataset/preprocessed/test-data/face_detection/4,"Essentially , the RFs of neurons in a certain layer are distributed regularly in the input image and theses RFs are natural "" anchors "" .",8,0,26
dataset/preprocessed/test-data/face_detection/4,"Combining RF "" anchors "" and appropriate RF strides , the proposed method can detect a large range of continuous face scales with 100 % coverage in theory .",9,0,29
dataset/preprocessed/test-data/face_detection/4,The insightful understanding of relations between ERF and face scales motivates an efficient backbone for onestage detection .,10,0,18
dataset/preprocessed/test-data/face_detection/4,"The backbone is characterized by eight detection branches and common layers , resulting in efficient computation .",11,0,17
dataset/preprocessed/test-data/face_detection/4,Comprehensive and extensive experiments on popular benchmarks : WIDER FACE and FDDB are conducted .,12,0,15
dataset/preprocessed/test-data/face_detection/4,A new evaluation schema is proposed for application - oriented scenarios .,13,0,12
dataset/preprocessed/test-data/face_detection/4,"Under the new schema , the proposed method can achieve superior accuracy ( WIDER FACE Val / Test - Easy : 0.910/0.896 , Medium : 0.881/0.865 , Hard : 0.780/0.770 ; FDDB - discontinuous : 0.973 , continuous : 0.724 ) .",14,0,42
dataset/preprocessed/test-data/face_detection/4,Multiple hardware platforms are introduced to evaluate the running efficiency .,15,0,11
dataset/preprocessed/test-data/face_detection/4,The proposed method can obtain fast inference speed ( NVIDIA TITAN Xp : 131.45 FPS at 640480 ; NVIDIA TX2 : 136.99 PFS at 160120 ; Raspberry Pi 3 Model B + : 8.44 FPS at 160120 ) with model size of 9 MB .,16,0,45
dataset/preprocessed/test-data/face_detection/4,Method m AP ( % ),17,0,6
dataset/preprocessed/test-data/face_detection/4,Subset Easy Medium Hard ISRN 0.967 0.958 0.909 VIM-FD 0.967 0.957 0.907 DSFD 0.966 0.957 0.904 SRN 0.964 0.952 0.901 Pyramid Box 0.961 0.950 0.889 .,18,0,26
dataset/preprocessed/test-data/face_detection/4,Accuracy of the top - 5 methods on validation set of WIDER FACE .,19,0,14
dataset/preprocessed/test-data/face_detection/4,Face detection is a long - standing problem in computer vision .,21,0,12
dataset/preprocessed/test-data/face_detection/4,"In practice , it is the prerequisite to some face - related applications , such as face alignment and face recognition .",22,0,22
dataset/preprocessed/test-data/face_detection/4,"Besides , face detectors are always deployed on edge devices , such as mobile phones , IP cameras and IoT ( Internet of Things ) sensors .",23,0,27
dataset/preprocessed/test-data/face_detection/4,These devices have limited memory storage and low computing power .,24,0,11
dataset/preprocessed/test-data/face_detection/4,"Under such condition , face detectors that have high accuracy and fast running speed are in demand .",25,0,18
dataset/preprocessed/test-data/face_detection/4,"Current state of the art face detectors have achieved fairly high accuracy on convictive benchmark WIDER FACE by leveraging pre-trained heavy backbones like VGG16 , Resnet50/152 and Densenet 121 .",26,0,30
dataset/preprocessed/test-data/face_detection/4,We investigate the top - 5 methods on WIDER FACE and present their accuracy in .,27,0,16
dataset/preprocessed/test-data/face_detection/4,It can be observed that these methods have similar accuracy with marginal gaps which are hardly perceived in practical applications .,28,0,21
dataset/preprocessed/test-data/face_detection/4,It is difficult and unpractical to further boost the accuracy by using more complex and heavier backbones .,29,0,18
dataset/preprocessed/test-data/face_detection/4,"In our view , to better balance accuracy and latency is crucial for applying face detection to more applicable areas .",30,0,21
dataset/preprocessed/test-data/face_detection/4,Face detection is a fast - growing branch of general object detection in the past decade .,31,0,17
dataset/preprocessed/test-data/face_detection/4,The early work of Viola - Jones face detector proposes a classic detection framework - cascade classifiers with hand - crafted features .,32,0,23
dataset/preprocessed/test-data/face_detection/4,One of its well - known followers is aggregate channel features ( ACF ) which can take advantages of channel features effectively .,33,0,23
dataset/preprocessed/test-data/face_detection/4,"Although the methods mentioned above can achieve fast running speed , they rely on hand - crafted features and are not trained end - to - end , resulting in not robust detection accuracy .",34,0,35
dataset/preprocessed/test-data/face_detection/4,"Recently , convolutional neural network ( CNN ) based face detectors show great progress partially owing to the success of WIDER FACE benchmark .",35,0,24
dataset/preprocessed/test-data/face_detection/4,These methods can be roughly divided into two categories : two - stage methods and one - stage methods .,36,0,20
dataset/preprocessed/test-data/face_detection/4,"Two - stage methods consist of proposal selection and localization regression , which are mainly originated from R - CNN series .",37,0,22
dataset/preprocessed/test-data/face_detection/4,"Whereas , one - stage methods coherently combine classification and bounding box ( bbox ) regression , always achieving anchor- based and multi-scale detection simultaneously .",38,0,26
dataset/preprocessed/test-data/face_detection/4,"For most one - stage methods , anchor design and matching strategy is one of the essential components .",39,0,19
dataset/preprocessed/test-data/face_detection/4,"In order to improve the accuracy , these methods propose more complex modules based on heavy backbones .",40,0,18
dataset/preprocessed/test-data/face_detection/4,"Although the above methods can achieve state of the art results , they may not properly balance accuracy and latency .",41,0,21
dataset/preprocessed/test-data/face_detection/4,"In this paper , we propose a Light and Fast Face Detector ( LFFD ) for edge devices , considerably balancing both accuracy and running efficiency .",42,0,27
dataset/preprocessed/test-data/face_detection/4,The proposed method is inspired by the one - stage and multi-scale object detection method SSD which also enlightens some other face detectors .,43,0,24
dataset/preprocessed/test-data/face_detection/4,One of the characteristics of SSD is that pre-defined anchor boxes are manually designed for each detection branch .,44,0,19
dataset/preprocessed/test-data/face_detection/4,These boxes always have different sizes and aspect ratios to cover objects with different scales and shapes .,45,0,18
dataset/preprocessed/test-data/face_detection/4,"Therefore , anchors play an important role inmost one - stage detection methods .",46,0,14
dataset/preprocessed/test-data/face_detection/4,"For some face detectors , sophisticated anchor strategies are crucial parts of the contributions .",47,0,15
dataset/preprocessed/test-data/face_detection/4,"However , anchor based methods may face three challenges :",48,0,10
dataset/preprocessed/test-data/face_detection/4,1 ) anchor matching is unable to sufficiently coverall face scales .,49,0,12
dataset/preprocessed/test-data/face_detection/13,RetinaFace : Single - stage Dense Face Localisation in the Wild,2,1,11
dataset/preprocessed/test-data/face_detection/13,"Though tremendous strides have been made in uncontrolled face detection , accurate and efficient face localisation in the wild remains an open challenge .",4,1,24
dataset/preprocessed/test-data/face_detection/13,"This paper presents a robust single - stage face detector , named RetinaFace , which performs pixel - wise face localisation on various scales of faces by taking advantages of joint extra-supervised and self - supervised multi-task learning .",5,0,39
dataset/preprocessed/test-data/face_detection/13,We make contributions in the following five aspects :,7,0,9
dataset/preprocessed/test-data/face_detection/13,( 1 ) We manually annotate five facial landmarks on the WIDER FACE dataset and observe significant improvement in hard face detection with the assistance of this extra supervision signal .,8,0,31
dataset/preprocessed/test-data/face_detection/13,( 2 ) We further add a selfsupervised mesh decoder branch for predicting a pixel - wise 3D shape face information in parallel with the existing supervised branches .,9,0,29
dataset/preprocessed/test-data/face_detection/13,"( 3 ) On the WIDER FACE hard test set , RetinaFace outperforms the state of the art average precision ( AP ) by 1.1 % ( achieving AP equal to 91.4 % ) .",10,0,35
dataset/preprocessed/test-data/face_detection/13,"( 4 ) On the IJB - C test set , RetinaFace enables state of the art methods ( ArcFace ) to improve their results in face verification ( TAR = 89.59 % for FAR = 1 e - 6 ) .",11,0,42
dataset/preprocessed/test-data/face_detection/13,"( 5 ) By employing light - weight backbone networks , Retina Face can run real - time on a single CPU core fora VGA - resolution image .",12,0,29
dataset/preprocessed/test-data/face_detection/13,Extra annotations and code have been made available at : https://github.com/deepinsight/insightface/tree/master/RetinaFace.,13,0,11
dataset/preprocessed/test-data/face_detection/13,Automatic face localisation is the prerequisite step of facial image analysis for many applications such as facial attribute ( e.g. expression and age ) and facial identity recognition .,15,0,29
dataset/preprocessed/test-data/face_detection/13,"A narrow definition of face localisation may refer to traditional face detection , which aims at estimating the face bounding boxes without any scale and position prior .",16,0,28
dataset/preprocessed/test-data/face_detection/13,"Nevertheless , in this paper .",17,0,6
dataset/preprocessed/test-data/face_detection/13,The proposed single - stage pixel - wise face localisation method employs extra-supervised and self - supervised multi-task learning in parallel with the existing box classification and regression branches .,18,0,30
dataset/preprocessed/test-data/face_detection/13,"Each positive anchor outputs ( 1 ) a face score , ( 2 ) a face box , ( 3 ) five facial landmarks , and ( 4 ) dense 3 D face vertices projected on the image plane .",19,0,40
dataset/preprocessed/test-data/face_detection/13,"we refer to a broader definition of face localisation which includes face detection , face alignment , pixelwise face parsing and 3D dense correspondence regression .",20,0,26
dataset/preprocessed/test-data/face_detection/13,That kind of dense face localisation provides accurate facial position information for all different scales .,21,0,16
dataset/preprocessed/test-data/face_detection/13,"Inspired by generic object detection methods , which embraced all the recent advances in deep learning , face detection has recently achieved remarkable progress .",22,0,25
dataset/preprocessed/test-data/face_detection/13,"Different from generic object detection , face detection features smaller ratio variations ( from 1:1 to 1:1.5 ) but much larger scale variations ( from several pixels to thousand pixels ) .",23,0,32
dataset/preprocessed/test-data/face_detection/13,"The most recent state - of - the - art methods focus on singlestage design which densely samples face locations and scales on feature pyramids , demonstrating promising performance and yielding faster speed compared to twostage methods .",24,0,38
dataset/preprocessed/test-data/face_detection/13,"Following this route , we improve the single - stage face detection framework and propose a state - of - the - art dense face localisation method by exploiting multi-task losses coming from strongly supervised and self - supervised signals .",25,0,41
dataset/preprocessed/test-data/face_detection/13,Our idea is examplified in .,26,0,6
dataset/preprocessed/test-data/face_detection/13,"Typically , face detection training process contains both classification and box regression losses .",27,0,14
dataset/preprocessed/test-data/face_detection/13,Chen et al. proposed to combine face detection and alignment in a joint cascade framework based on the observation that aligned face shapes provide better features for face classification .,28,0,30
dataset/preprocessed/test-data/face_detection/13,"Inspired by , MTCNN and STN simultaneously detected faces and five facial landmarks .",29,0,14
dataset/preprocessed/test-data/face_detection/13,"Due to training data limitation , JDA , MTCNN and STN have not verified whether tiny face detection can benefit from the extra supervision of five facial landmarks .",30,0,29
dataset/preprocessed/test-data/face_detection/13,One of the questions we aim at answering in this paper is whether we can push forward the current best performance ( 90.3 % ) on the WIDER FACE hard test set by using extra supervision signal built of five facial landmarks .,31,0,43
dataset/preprocessed/test-data/face_detection/13,"In Mask R - CNN , the detection performance is significantly improved by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition and regression .",32,0,34
dataset/preprocessed/test-data/face_detection/13,That confirms that dense pixel - wise annotations are also beneficial to improve detection .,33,0,15
dataset/preprocessed/test-data/face_detection/13,"Unfortunately , for the challenging faces of WIDER FACE it is not possible to conduct dense face annotation ( either in the form of more landmarks or semantic segments ) .",34,0,31
dataset/preprocessed/test-data/face_detection/13,"Since supervised signals can not be easily obtained , the question is whether we can apply unsupervised methods to further improve face detection .",35,0,24
dataset/preprocessed/test-data/face_detection/13,"In FAN , an anchor - level attention map is proposed to improve the occluded face detection .",36,0,18
dataset/preprocessed/test-data/face_detection/13,"Nevertheless , the proposed attention map is quite coarse and does not contain semantic information .",37,0,16
dataset/preprocessed/test-data/face_detection/13,"Recently , self - supervised 3D morphable models have achieved promising 3 D face modelling in - the - wild .",38,0,21
dataset/preprocessed/test-data/face_detection/13,"Especially , Mesh Decoder achieves over real - time speed by exploiting graph convolutions on joint shape and texture .",39,0,20
dataset/preprocessed/test-data/face_detection/13,"However , the main challenges of applying mesh decoder into the single - stage detector are : ( 1 ) camera parameters are hard to estimate accurately , and ( 2 ) the joint latent shape and texture representation is predicted from a single feature vector ( 1 1 Conv on feature pyramid ) instead of the RoI pooled feature , which indicates the risk of feature shift .",40,0,69
dataset/preprocessed/test-data/face_detection/13,"In this paper , we employ a mesh decoder branch through self - supervision learning for predicting a pixel - wise 3 D face shape in parallel with the existing supervised branches .",41,0,33
dataset/preprocessed/test-data/face_detection/13,"To summarise , our key contributions are :",42,0,8
dataset/preprocessed/test-data/face_detection/13,"Based on a single - stage design , we propose a novel pixel - wise face localisation method named Reti- naFace , which employs a multi-task learning strategy to simultaneously predict face score , face box , five facial landmarks , and 3D position and correspondence of each facial pixel .",43,0,51
dataset/preprocessed/test-data/face_detection/13,"On the WIDER FACE hard subset , RetinaFace outperforms the AP of the state of the art two - stage method ( ISRN ) by 1.1 % ( AP equal to 91.4 % ) .",44,0,35
dataset/preprocessed/test-data/face_detection/13,"On the IJB - C dataset , RetinaFace helps to improve Ar - c Face 's verification accuracy ( with TAR equal to 89.59 % when FAR = 1 e - 6 ) .",45,0,34
dataset/preprocessed/test-data/face_detection/13,This indicates that better face localisation can significantly improve face recognition .,46,0,12
dataset/preprocessed/test-data/face_detection/13,"By employing light - weight backbone networks , Reti - na",47,0,11
dataset/preprocessed/test-data/face_detection/13,Face can run real - time on a single CPU core fora VGA - resolution image . Extra annotations and code have been released to facilitate future research .,48,0,29
dataset/preprocessed/test-data/face_detection/20,Aggregate Channel Features for Multi-view Face Detection,2,1,7
dataset/preprocessed/test-data/face_detection/20,Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones .,4,1,18
dataset/preprocessed/test-data/face_detection/20,"While many subsequences have improved the work with more powerful learning algorithms , the feature representation used for face detection still ca n't meet the demand for effectively and efficiently handling faces with large appearance variance in the wild .",5,0,40
dataset/preprocessed/test-data/face_detection/20,"To solve this bottleneck , we borrow the concept of channel features to the face detection domain , which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form .",6,0,43
dataset/preprocessed/test-data/face_detection/20,"We adopt a novel variant called aggregate channel features , make a full exploration of feature design , and discover a multiscale version of features with better performance .",7,0,29
dataset/preprocessed/test-data/face_detection/20,"To deal with poses of faces in the wild , we propose a multi-view detection approach featuring score re-ranking and detection adjustment .",8,0,23
dataset/preprocessed/test-data/face_detection/20,"Following the learning pipelines in Viola - Jones framework , the multi-view face detector using aggregate channel features shows competitive performance against state - of - the - art algorithms on AFW and FDDB testsets , while runs at 42 FPS on VGA images .",9,0,45
dataset/preprocessed/test-data/face_detection/20,Human face detection have long been one of the most fundamental problems in computer vision and humancomputer interaction .,11,1,19
dataset/preprocessed/test-data/face_detection/20,"In the past decade , the most influential work should be the face detection framework proposed by Viola and Jones .",12,0,21
dataset/preprocessed/test-data/face_detection/20,The Viola - Jones ( abbreviated as VJ below ) framework uses rectangular Haar - like features and learns the hypothesis using Adaboost algorithm .,13,0,25
dataset/preprocessed/test-data/face_detection/20,"Combined with the attentional cascade structure , the VJ detector achieved real - time face detection at that time .",14,0,20
dataset/preprocessed/test-data/face_detection/20,"Despite the great success of the VJ detector , the performance is still far from satisfactory due to the large appearance variance of faces in unconstrained settings .",15,0,28
dataset/preprocessed/test-data/face_detection/20,* Corresponding author ..,16,0,4
dataset/preprocessed/test-data/face_detection/20,An intuitive visualization of our multi-view face detector using aggregate channel features .,17,0,13
dataset/preprocessed/test-data/face_detection/20,The area with warmer color indicates more attention paid to by the detector .,18,0,14
dataset/preprocessed/test-data/face_detection/20,"To handle faces in the wild , many subsequences of VJ framework merged .",19,0,14
dataset/preprocessed/test-data/face_detection/20,"These methods mainly get the performance gains in two aspects , more complicated features and ( or ) more powerful learning algorithms .",20,0,23
dataset/preprocessed/test-data/face_detection/20,"As the combination of boosting and cascade has been proven to be quite effective in face detection , the bottleneck lies in the feature representation since complicated features adopted in the above literatures bring about limited performance gains at the cost of large computation cost .",21,0,46
dataset/preprocessed/test-data/face_detection/20,"Lately in another domain of pedestrian detection , a family of channel features has achieved record performances .",22,0,18
dataset/preprocessed/test-data/face_detection/20,Channel features compute registered maps of the original images like gradients and histograms of oriented gradients and then extract features on these extended channels .,23,0,25
dataset/preprocessed/test-data/face_detection/20,The classifier learning process follows the VJ framework pipeline .,24,0,10
dataset/preprocessed/test-data/face_detection/20,"In this paper , we adopt a variant of channel features called aggregate channel features , which are extracted directly as pixel values on subsampled channels .",25,0,27
dataset/preprocessed/test-data/face_detection/20,"Channel extension offers rich representation capacity , while simple feature form guarantees fast computation .",26,0,15
dataset/preprocessed/test-data/face_detection/20,"With these two superiorities , the aggregate channel features breakthrough the bottleneck in VJ framework and have the potential to make great advance in face detection .",27,0,27
dataset/preprocessed/test-data/face_detection/20,"As we mainly concentrate our efforts to the feature representation rather than learning algorithms in this paper , we not only just adopt the aggregate channel features in face detection , but also try to explore the full potential of this novel representation .",28,0,44
dataset/preprocessed/test-data/face_detection/20,"To do so , we make a deep and all - round investigation into the specific feature parameters concerning channel types , feature pool size , subsampling method , feature scale and soon , which gives insights into the feature design and hopefully provides helpful guidelines for practitioners .",29,0,49
dataset/preprocessed/test-data/face_detection/20,"Through the deep exploration , we find that : 1 ) multi-scaling the feature representation further enriches the representation capacity since original aggregate channel features have uniform feature scale ; 2 ) different combinations of channel types impact the performance greatly , while for face detection the color channel in LUV space , plus gradient magnitude channel and gradient histograms channels in RGB space show best result ; 3 ) multi-view detection is proven to be a good match with aggregate channel features as the representation naturally encodes the facial structure ( ) .",30,0,94
dataset/preprocessed/test-data/face_detection/20,"Although multi-view detection could effectively deal with diverse poses , additional issues come up as how to merge detections output by separately trained subview detectors , and how to deal with the offsets of location and scale between output detections and ground - truth .",31,0,45
dataset/preprocessed/test-data/face_detection/20,"We solve these problems by carefully designed post -processing including score re-ranking , detection merging and bounding box adjustment .",32,0,20
dataset/preprocessed/test-data/face_detection/20,"The detailed experimental exploration of aggregate channel features , along with our improvements on multiview detection , leads to large performance gain in face detection in the wild .",33,0,29
dataset/preprocessed/test-data/face_detection/20,"On two challenging face databases , AFW and FDDB , the proposed multi-view face detector shows competitive performance against state - of - the - art detectors in both detection accuracy and speed .",34,0,34
dataset/preprocessed/test-data/face_detection/20,The remaining parts of this paper are organized as follows .,35,0,11
dataset/preprocessed/test-data/face_detection/20,Section 2 revisits related work in face detection .,36,0,9
dataset/preprocessed/test-data/face_detection/20,Section 3 describes how we build the face detector using aggregate channel features .,37,0,14
dataset/preprocessed/test-data/face_detection/20,Section 4 addresses problems concerning multi-view face detection .,38,0,9
dataset/preprocessed/test-data/face_detection/20,Experimental results on AFW and FDDB are shown in section 5 and we conclude the paper in section 6 .,39,0,20
dataset/preprocessed/test-data/face_detection/20,Face detection has drawn much attention since the early time of computer vision .,41,0,14
dataset/preprocessed/test-data/face_detection/20,"Although many solutions had been put forward , it was not until Viola and Jones proposed their milestone work that face detection saw surprising progress in the past decades .",42,0,30
dataset/preprocessed/test-data/face_detection/20,"The VJ face detector features in three aspects : fast feature computation via integral image representation , classifier learning using Adaboost , and the attentional cascade structure .",43,0,28
dataset/preprocessed/test-data/face_detection/20,"One main drawback of the VJ framework is that the features have limited repre-sentation capacity , while the feature pool size is quite large to compensate for that .",44,0,29
dataset/preprocessed/test-data/face_detection/20,"Typically , in a 24 24 detection window , the number of Haar - like features is 160,000 .",45,0,19
dataset/preprocessed/test-data/face_detection/20,"To address the problem , efforts are made in two directions .",46,0,12
dataset/preprocessed/test-data/face_detection/20,"Some focus on more complicated features like HoG , SURF .",47,0,11
dataset/preprocessed/test-data/face_detection/20,Some aim to speedup the feature selection in a heuristic way .,48,0,12
dataset/preprocessed/test-data/face_detection/20,"However , the problem has n't been solved perfectly .",49,0,10
dataset/preprocessed/test-data/natural_language_inference/28,TRACKING THE WORLD STATE WITH RECURRENT ENTITY NETWORKS,2,1,8
dataset/preprocessed/test-data/natural_language_inference/28,"We introduce a new model , the Recurrent Entity Network ( EntNet ) .",4,0,14
dataset/preprocessed/test-data/natural_language_inference/28,It is equipped with a dynamic long - term memory which allows it to maintain and update a representation of the state of the world as it receives new data .,5,0,31
dataset/preprocessed/test-data/natural_language_inference/28,"For language understanding tasks , it can reason on - the - fly as it reads text , not just when it is required to answer a question or respond as is the case for a Memory Network ( Sukhbaatar et al. , 2015 ) .",6,0,46
dataset/preprocessed/test-data/natural_language_inference/28,"Like a Neural Turing Machine or Differentiable Neural Computer ( Graves et al. , 2014;2016 ) it maintains a fixed size memory and can learn to perform location and content - based read and write operations .",7,0,37
dataset/preprocessed/test-data/natural_language_inference/28,"However , unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously .",8,0,21
dataset/preprocessed/test-data/natural_language_inference/28,"The EntNet sets a new state - of - the - art on the bAbI tasks , and is the first method to solve all the tasks in the 10k training examples setting .",9,0,34
dataset/preprocessed/test-data/natural_language_inference/28,"We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts , which other methods are notable to solve , and can generalize past its training horizon .",10,0,35
dataset/preprocessed/test-data/natural_language_inference/28,"It can also be practically used on large scale datasets such as Children 's Book Test , where it obtains competitive performance , reading the story in a single pass .",11,0,31
dataset/preprocessed/test-data/natural_language_inference/28,The essence of intelligence is the ability to predict .,13,0,10
dataset/preprocessed/test-data/natural_language_inference/28,"An intelligent agent must be able to predict unobserved facts about their environment from limited percepts , combined with their knowledge of the past .",14,0,25
dataset/preprocessed/test-data/natural_language_inference/28,"In order to reason and plan , they must be able to predict how an observed event or action will affect the state of the world .",15,0,27
dataset/preprocessed/test-data/natural_language_inference/28,"Arguably , the ability to maintain an estimate of the current state of the world , combined with a forward model of how the world evolves , is a key feature of intelligent agents .",16,0,35
dataset/preprocessed/test-data/natural_language_inference/28,"A natural way for an agent to represent the world is to maintain a set of high - level concepts or entities together with their properties , which are updated as new information is received .",17,0,36
dataset/preprocessed/test-data/natural_language_inference/28,"For example , if a percept is the textual description of an event , such as "" John walks out of the kitchen "" , the agent should learn to update its estimate of John 's location , as well as the list ( and number ) of people present in each room .",18,0,54
dataset/preprocessed/test-data/natural_language_inference/28,"If John was carrying a bag , the location of the bag and the list of objects in the kitchen must also be updated .",19,0,25
dataset/preprocessed/test-data/natural_language_inference/28,"When we read a story , each sentence we read or hear causes us to update our internal representation of the current state of the world within the story .",20,0,30
dataset/preprocessed/test-data/natural_language_inference/28,The flow of the story is captured by the evolution of this state of the world .,21,0,17
dataset/preprocessed/test-data/natural_language_inference/28,"At any given time , an agent typically receives limited information about the state of the world , and should therefore be able to infer new information through partial observation .",22,0,31
dataset/preprocessed/test-data/natural_language_inference/28,"In this paper , we investigate this problem through a simple story understanding scenario , in which the agent is given a sequence of textual statements and events , and then given another series of statements about the final state of the world .",23,1,44
dataset/preprocessed/test-data/natural_language_inference/28,"If the second series of statements is given in the form of questions about the final state of the world together with their correct answers , the agent should be able to learn from them and its performance can be measured by the accuracy of its answers .",24,0,48
dataset/preprocessed/test-data/natural_language_inference/28,"Even with this weak form of supervision , the system may learn basic dynamical constraints about the world .",25,0,19
dataset/preprocessed/test-data/natural_language_inference/28,"For example , it may learn that a person or object can not be in two locations at the same time , or may learn simple update rules such as incrementing and decrementing the number of persons or objects in a room .",26,0,43
dataset/preprocessed/test-data/natural_language_inference/28,"It may also learn basic rules of approximate ( logical ) inference , such as the fact that objects belonging to the same category tend to have similar properties ( light objects can be carried over from rooms to rooms for instance ) .",27,0,44
dataset/preprocessed/test-data/natural_language_inference/28,We propose to handle this scenario with a new kind of memory - augmented neural network that uses a distributed memory and processor architecture : the Recurrent Entity Network ( EntNet ) .,28,0,33
dataset/preprocessed/test-data/natural_language_inference/28,"The model consists of a fixed number of dynamic memory cells , each containing a vector key w j and a vector value ( or content ) h j .",29,0,30
dataset/preprocessed/test-data/natural_language_inference/28,"Each cell is associated with its own "" processor "" , a simple gated recurrent network that may update the cell value given an input .",30,0,26
dataset/preprocessed/test-data/natural_language_inference/28,"If each cell learns to represent a concept or entity in the world , one can imagine a gating mechanism that , based on the key and content of the memory cells , will only modify the cells that concern the entities mentioned in the input .",31,0,47
dataset/preprocessed/test-data/natural_language_inference/28,"In the current version of the model , there is no direct interaction between the memory cells , hence the system can be seen as multiple identical processors functioning in parallel , with distributed local memory .",32,0,37
dataset/preprocessed/test-data/natural_language_inference/28,"Alternatively , the EntNet can be seen as a bank of gated RNNs ( all sharing the same parameters ) , whose hidden states correspond to latent concepts and attributes , and whose parameters describe the laws of the world according to which the attributes of objects are updated .",33,0,50
dataset/preprocessed/test-data/natural_language_inference/28,"The sharing of these parameters reflects an invariance of these laws across object instances , similarly to how the weight tying scheme in a CNN reflects an invariance of image statistics across locations .",34,0,34
dataset/preprocessed/test-data/natural_language_inference/28,"Their hidden state is updated only when new information relevant to their concept is received , and remains otherwise unchanged .",35,0,21
dataset/preprocessed/test-data/natural_language_inference/28,"The keys used in the addressing / gating mechanism also correspond to concepts or entities , but are modified only during learning , not during inference .",36,0,27
dataset/preprocessed/test-data/natural_language_inference/28,"The EntNet is able to solve all 20 bAb I question - answering tasks , a popular benchmark of story understanding , which to our knowledge sets a new state - of - the - art .",37,0,37
dataset/preprocessed/test-data/natural_language_inference/28,"Our experiments also indicate that the model indeed maintains an internal representation of the simplified world in which the stories take place , and that the model does not limit itself to storing the aspects of the world required to answer a specific question .",38,0,45
dataset/preprocessed/test-data/natural_language_inference/28,"We also introduce a new reasoning task which , unlike the bAbI tasks , requires a model to use a large number of supporting facts to answer the question , and show that the EntNet outperforms both LSTMs and Memory Networks by a significant margin .",39,0,46
dataset/preprocessed/test-data/natural_language_inference/28,It is also able to generalize to sequences longer than those seen during training .,40,0,15
dataset/preprocessed/test-data/natural_language_inference/28,"Finally , our model also obtains competitive results on the Childrens Book Test , and performs best among models that read the text in a single pass before receiving knowledge of the question .",41,0,34
dataset/preprocessed/test-data/natural_language_inference/28,"Our model is designed to process data in sequential form , and consists of three main parts : an input encoder , a dynamic memory and an output layer , which we now describe in detail .",43,0,37
dataset/preprocessed/test-data/natural_language_inference/28,"We developed it in the context of question answering on short stories where the inputs are word sequences , but the model could be adapted to many other contexts .",44,0,30
dataset/preprocessed/test-data/natural_language_inference/28,The encoding layer summarizes an element of the input sequence with a vector of fixed length .,46,0,17
dataset/preprocessed/test-data/natural_language_inference/28,"Typically the input element at time t is a sequence of words , e.g. a sentence or window of words .",47,0,21
dataset/preprocessed/test-data/natural_language_inference/28,"One is free to choose the encoding module to be any standard sequence encoder , which is an active area of research .",48,0,23
dataset/preprocessed/test-data/natural_language_inference/28,Typical choices include a bag - of - words ( BoW ) representation or the final state of a recurrent neural net ( RNN ) run over the sequence .,49,0,30
dataset/preprocessed/test-data/natural_language_inference/7,Teaching Machines to Read and Comprehend,2,0,6
dataset/preprocessed/test-data/natural_language_inference/7,Teaching machines to read natural language documents remains an elusive challenge .,4,0,12
dataset/preprocessed/test-data/natural_language_inference/7,"Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen , but until now large scale training and test datasets have been missing for this type of evaluation .",5,1,41
dataset/preprocessed/test-data/natural_language_inference/7,In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data .,6,0,21
dataset/preprocessed/test-data/natural_language_inference/7,This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure .,7,0,31
dataset/preprocessed/test-data/natural_language_inference/7,Progress on the path from shallow bag - of - words information retrieval algorithms to machines capable of reading and understanding documents has been slow .,9,0,26
dataset/preprocessed/test-data/natural_language_inference/7,"Traditional approaches to machine reading and comprehension have been based on either hand engineered grammars , or information extraction methods of detecting predicate argument triples that can later be queried as a relational database .",10,1,35
dataset/preprocessed/test-data/natural_language_inference/7,"Supervised machine learning approaches have largely been absent from this space due to both the lack of large scale training datasets , and the difficulty in structuring statistical models flexible enough to learn to exploit document structure .",11,0,38
dataset/preprocessed/test-data/natural_language_inference/7,"While obtaining supervised natural language reading comprehension data has proved difficult , some researchers have explored generating synthetic narratives and queries .",12,0,22
dataset/preprocessed/test-data/natural_language_inference/7,Such approaches allow the generation of almost unlimited amounts of supervised data and enable researchers to isolate the performance of their algorithms on individual simulated phenomena .,13,0,27
dataset/preprocessed/test-data/natural_language_inference/7,"Work on such data has shown that neural network based models hold promise for modelling reading comprehension , something that we will build upon here .",14,0,26
dataset/preprocessed/test-data/natural_language_inference/7,"Historically , however , many similar approaches in Computational Linguistics have failed to manage the transition from synthetic data to real environments , as such closed worlds inevitably fail to capture the complexity , richness , and noise of natural language .",15,0,42
dataset/preprocessed/test-data/natural_language_inference/7,In this work we seek to directly address the lack of real natural language training data by introducing a novel approach to building a supervised reading comprehension data set .,16,0,30
dataset/preprocessed/test-data/natural_language_inference/7,"We observe that summary and paraphrase sentences , with their associated documents , can be readily converted to context - query - answer triples using simple entity detection and anonymisation algorithms .",17,0,32
dataset/preprocessed/test-data/natural_language_inference/7,Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites .,18,0,26
dataset/preprocessed/test-data/natural_language_inference/7,We demonstrate the efficacy of our new corpora by building novel deep learning models for reading comprehension .,19,0,18
dataset/preprocessed/test-data/natural_language_inference/7,These models draw on recent developments for incorporating attention mechanisms into recurrent neural network architectures .,20,0,16
dataset/preprocessed/test-data/natural_language_inference/7,"This allows a model to focus on the aspects of a document that it believes will help it answer a question , and also allows us to visualises its inference process .",21,0,32
dataset/preprocessed/test-data/natural_language_inference/7,We compare these neural models to a range of baselines and heuristic benchmarks based upon a traditional frame semantic analysis provided by a state - of - the - art natural language processing :,22,0,34
dataset/preprocessed/test-data/natural_language_inference/7,Percentage of time that the correct answer is contained in the top N most frequent entities in a given document .,23,0,21
dataset/preprocessed/test-data/natural_language_inference/7,( NLP ) pipeline .,24,0,5
dataset/preprocessed/test-data/natural_language_inference/7,"Our results indicate that the neural models achieve a higher accuracy , and do so without any specific encoding of the document or query structure .",25,0,26
dataset/preprocessed/test-data/natural_language_inference/7,Supervised training data for reading comprehension,26,0,6
dataset/preprocessed/test-data/natural_language_inference/7,The reading comprehension task naturally lends itself to a formulation as a supervised learning problem .,27,0,16
dataset/preprocessed/test-data/natural_language_inference/7,"Specifically we seek to estimate the conditional probability p ( a | c , q ) , where c is a context document , q a query relating to that document , and a the answer to that query .",28,0,40
dataset/preprocessed/test-data/natural_language_inference/7,"For a focused evaluation we wish to be able to exclude additional information , such as world knowledge gained from co-occurrence statistics , in order to test a model 's core capability to detect and understand the linguistic relationships between entities in the context document .",29,0,46
dataset/preprocessed/test-data/natural_language_inference/7,Such an approach requires a large training corpus of document - query - answer triples and until now such corpora have been limited to hundreds of examples and thus mostly of use only for testing .,30,0,36
dataset/preprocessed/test-data/natural_language_inference/7,This limitation has meant that most work in this area has taken the form of unsupervised approaches which use templates or syntactic / semantic analysers to extract relation tuples from the document to form a knowledge graph that can be queried .,31,0,42
dataset/preprocessed/test-data/natural_language_inference/7,"Here we propose a methodology for creating real - world , large scale supervised training data for learning reading comprehension models .",32,0,22
dataset/preprocessed/test-data/natural_language_inference/7,"Inspired by work in summarisation , we create two machine reading corpora by exploiting online newspaper articles and their matching summaries .",33,0,22
dataset/preprocessed/test-data/natural_language_inference/7,We have collected 93k articles from the CNN 1 and 220 k articles from the Daily Mail 2 websites .,34,0,20
dataset/preprocessed/test-data/natural_language_inference/7,"Both news providers supplement their articles with a number of bullet points , summarising aspects of the information contained in the article .",35,0,23
dataset/preprocessed/test-data/natural_language_inference/7,Of key importance is that these summary points are abstractive and do not simply copy sentences from the documents .,36,0,20
dataset/preprocessed/test-data/natural_language_inference/7,We construct a corpus of document - queryanswer triples by turning these bullet points into Cloze style questions by replacing one entity at a time with a placeholder .,37,0,29
dataset/preprocessed/test-data/natural_language_inference/7,This results in a combined corpus of roughly 1 M data points .,38,0,13
dataset/preprocessed/test-data/natural_language_inference/7,Code to replicate our datasets - and to apply this method to other sources - is available online 3 .,39,0,20
dataset/preprocessed/test-data/natural_language_inference/7,Entity replacement and permutation,40,0,4
dataset/preprocessed/test-data/natural_language_inference/7,"Note that the focus of this paper is to provide a corpus for evaluating a model 's ability to read and comprehend a single document , not world knowledge or co-occurrence .",41,0,32
dataset/preprocessed/test-data/natural_language_inference/7,To understand that distinction consider for instance the following Cloze form queries ( created from headlines in the Daily Mail validation set ) : a),42,0,25
dataset/preprocessed/test-data/natural_language_inference/7,The hi-tech bra that helps you beat breast X ; b ) Could Saccharin help beat X ? ; c),43,0,20
dataset/preprocessed/test-data/natural_language_inference/7,Can fish oils help fight prostate X ?,44,0,8
dataset/preprocessed/test-data/natural_language_inference/7,"An ngram language model trained on the Daily Mail would easily correctly predict that ( X = cancer ) , regardless of the contents of the context document , simply because this is a very frequently cured entity in the Daily Mail corpus .",45,0,44
dataset/preprocessed/test-data/natural_language_inference/7,"The BBC producer allegedly struck by Jeremy Clarkson will not press charges against the "" Top Gear "" host , his lawyer said Friday .",49,0,25
dataset/preprocessed/test-data/natural_language_inference/3,Exploring Question Understanding and Adaptation in Neural - Network - Based Question Answering,2,1,13
dataset/preprocessed/test-data/natural_language_inference/3,The last several years have seen intensive interest in exploring neural - networkbased models for machine comprehension ( MC ) and question answering ( QA ) .,4,1,27
dataset/preprocessed/test-data/natural_language_inference/3,"In this paper , we approach the problems by closely modelling questions in a neural network framework .",5,0,18
dataset/preprocessed/test-data/natural_language_inference/3,We first introduce syntactic information to help encode questions .,6,0,10
dataset/preprocessed/test-data/natural_language_inference/3,We then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them .,7,0,26
dataset/preprocessed/test-data/natural_language_inference/3,"On the Stanford Question Answering Dataset ( SQuAD ) , we show that these approaches can help attain better results over a competitive baseline .",8,0,25
dataset/preprocessed/test-data/natural_language_inference/3,"Enabling computers to understand given documents and answer questions about their content has recently attracted intensive interest , including but not limited to the efforts as in .",10,0,28
dataset/preprocessed/test-data/natural_language_inference/3,Many specific problems such as machine comprehension and question answering often involve modeling such question - document pairs .,11,0,19
dataset/preprocessed/test-data/natural_language_inference/3,"The recent availability of relatively large training datasets ( see Section 2 for more details ) has made it more feasible to train and estimate rather complex models in an end - to - end fashion for these problems , in which a whole model is fit directly with given question - answer tuples and the resulting model has shown to be rather effective .",12,0,65
dataset/preprocessed/test-data/natural_language_inference/3,"In this paper , we take a closer look at modeling questions in such an end - to - end neural network framework , since we regard question understanding is of importance for such problems .",13,0,36
dataset/preprocessed/test-data/natural_language_inference/3,We first introduced syntactic information to help encode questions .,14,0,10
dataset/preprocessed/test-data/natural_language_inference/3,We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them .,15,0,26
dataset/preprocessed/test-data/natural_language_inference/3,"On the Stanford Question Answering Dataset ( SQuAD ) , we show that these approaches can help attain better results on our competitive baselines .",16,0,25
dataset/preprocessed/test-data/natural_language_inference/3,"named entities , common nouns , verbs , and prepositions to test reading comprehension .",17,0,15
dataset/preprocessed/test-data/natural_language_inference/3,"The Stanford Question Answering Dataset ( SQuAD ) is more recently released dataset , which consists of more than 100,000 questions for documents taken from Wikipedia across a wide range of topics .",18,0,33
dataset/preprocessed/test-data/natural_language_inference/3,The question - answer pairs are annotated through crowdsourcing .,19,0,10
dataset/preprocessed/test-data/natural_language_inference/3,Answers are spans of text marked in the original documents .,20,0,11
dataset/preprocessed/test-data/natural_language_inference/3,"In this paper , we use SQuAD to evaluate our models .",21,0,12
dataset/preprocessed/test-data/natural_language_inference/3,Many neural network models have been studied on the SQuAD task .,22,0,12
dataset/preprocessed/test-data/natural_language_inference/3,proposed match LSTM to associate documents and questions and adapted the so - called pointer Network to determine the positions of the answer text spans .,23,0,26
dataset/preprocessed/test-data/natural_language_inference/3,proposed a dynamic chunk reader to extract and rank a set of answer candidates .,24,0,15
dataset/preprocessed/test-data/natural_language_inference/3,focused on word representation and presented a fine - grained gating mechanism to dynamically combine word - level and character - level representations based on the properties of words .,25,0,30
dataset/preprocessed/test-data/natural_language_inference/3,"proposed a multi-perspective context matching ( MPCM ) model , which matched an encoded document and question from multiple perspectives .",26,0,21
dataset/preprocessed/test-data/natural_language_inference/3,proposed a dynamic decoder and so - called highway maxout network to improve the effectiveness of the decoder .,27,0,19
dataset/preprocessed/test-data/natural_language_inference/3,The bi-directional attention flow ( BIDAF ) used the bi-directional attention to obtain a question - aware context representation .,28,0,20
dataset/preprocessed/test-data/natural_language_inference/3,"In this paper , we introduce syntactic information to encode questions with a specific form of recursive neural networks .",29,0,20
dataset/preprocessed/test-data/natural_language_inference/3,"More specifically , we explore a tree - structured LSTM which extends the linear - chain long short - term memory ( LSTM ) ] to a recursive structure , which has the potential to capture long - distance interactions over the structures .",30,0,44
dataset/preprocessed/test-data/natural_language_inference/3,Different types of questions are often used to seek for different types of information .,31,0,15
dataset/preprocessed/test-data/natural_language_inference/3,"For example , a "" what "" question could have very different property from that of a "" why "" question , while they may share information and need to be trained together instead of separately .",32,0,37
dataset/preprocessed/test-data/natural_language_inference/3,"We view this as a "" adaptation "" problem to let different types of questions share a basic model but still discriminate them when needed .",33,0,26
dataset/preprocessed/test-data/natural_language_inference/3,"Specifically , we are motivated by the ideas "" i-vector "" in speech recognition , where neural network based adaptation is performed among different ( groups ) of speakers and we focused instead on different types of questions here .",34,0,40
dataset/preprocessed/test-data/natural_language_inference/3,We concatenate embedding at two levels to represent a word : the character composition and word - level embedding .,36,0,20
dataset/preprocessed/test-data/natural_language_inference/3,The character composition feeds all characters of a word into a convolutional neural network ( CNN ) to obtain a representation for the word .,37,0,25
dataset/preprocessed/test-data/natural_language_inference/3,And we use the pre-trained 300 - D Glo Ve vectors ( see the experiment section for details ) to initialize our word - level embedding .,38,0,27
dataset/preprocessed/test-data/natural_language_inference/3,Each word is therefore represented as the concatenation of the character - composition vector and word - level embedding .,39,0,20
dataset/preprocessed/test-data/natural_language_inference/3,"This is performed on both questions and documents , resulting in two matrices : the Q e ?",40,0,18
dataset/preprocessed/test-data/natural_language_inference/3,RN dw fora question and the D e ?,41,0,9
dataset/preprocessed/test-data/natural_language_inference/3,"R M dw fora document , where N is the question length ( number of word tokens ) , M is the document length , and d w is the embedding dimensionality .",42,0,33
dataset/preprocessed/test-data/natural_language_inference/3,"The above word representation focuses on representing individual words , and an input encoder here employs recurrent neural networks to obtain the representation of a word under its context .",44,0,30
dataset/preprocessed/test-data/natural_language_inference/3,We use bi-directional GRU ( BiGRU ) for both documents and questions .,45,0,13
dataset/preprocessed/test-data/natural_language_inference/3,"A BiGRU runs a forward and backward GRU on a sequence starting from the left and the right end , respectively .",46,0,22
dataset/preprocessed/test-data/natural_language_inference/3,"By concatenating the hidden states of these two GRUs for each word , we obtain the a representation fora question or document : Q c ?",47,0,26
dataset/preprocessed/test-data/natural_language_inference/3,RN dc fora question and D c ?,48,0,8
dataset/preprocessed/test-data/natural_language_inference/3,R M dc fora document .,49,0,6
dataset/preprocessed/test-data/natural_language_inference/25,A Deep Cascade Model for Multi - Document Reading Comprehension,2,1,10
dataset/preprocessed/test-data/natural_language_inference/25,A fundamental trade - off between effectiveness and efficiency needs to be balanced when designing an online question answering system .,4,0,21
dataset/preprocessed/test-data/natural_language_inference/25,"Effectiveness comes from sophisticated functions such as extractive machine reading comprehension ( MRC ) , while efficiency is obtained from improvements in preliminary retrieval components such as candidate document selection and paragraph ranking .",5,0,34
dataset/preprocessed/test-data/natural_language_inference/25,"Given the complexity of the real - world multi-document MRC scenario , it is difficult to jointly optimize both in an end - to - end system .",6,0,28
dataset/preprocessed/test-data/natural_language_inference/25,"To address this problem , we develop a novel deep cascade learning model , which progressively evolves from the documentlevel and paragraph - level ranking of candidate texts to more precise answer extraction with machine reading comprehension .",7,0,38
dataset/preprocessed/test-data/natural_language_inference/25,"Specifically , irrelevant documents and paragraphs are first filtered outwith simple functions for efficiency consideration .",8,0,16
dataset/preprocessed/test-data/natural_language_inference/25,"Then we jointly train three modules on the remaining texts for better tracking the answer : the document extraction , the paragraph extraction and the answer extraction .",9,0,28
dataset/preprocessed/test-data/natural_language_inference/25,"Experiment results show that the proposed method outperforms the previous state - of - the - art methods on two large - scale multidocument benchmark datasets , i.e. , TriviaQA and DuReader .",10,0,33
dataset/preprocessed/test-data/natural_language_inference/25,"In addition , our online system can stably serve typical scenarios with millions of daily requests in less than 50 ms .",11,0,22
dataset/preprocessed/test-data/natural_language_inference/25,"Machine reading comprehension ( MRC ) , which empowers computers with the ability to read and comprehend knowledge and then answer questions from textual data , has made rapid progress in recent years .",13,1,34
dataset/preprocessed/test-data/natural_language_inference/25,"From the early cloze - style test to answer extraction from a single paragraph , and to the more complex open - domain question answering from web data , great efforts have been made to push the MRC technique to more practical applications .",14,1,44
dataset/preprocessed/test-data/natural_language_inference/25,"The rapid progress of MRC in recent years mostly owes to the release of the single - paragraph benchmark dataset SQuAD ) , on which various deep attention - based methods have been proposed to constantly push the state - of - the - art performance .",15,0,47
dataset/preprocessed/test-data/natural_language_inference/25,"It is a significant mile - Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",16,0,21
dataset/preprocessed/test-data/natural_language_inference/25,All rights reserved .,17,0,4
dataset/preprocessed/test-data/natural_language_inference/25,stone that several MRC models have exceeded the performance of human annotators on the SQuAD dataset,18,0,16
dataset/preprocessed/test-data/natural_language_inference/25,"However , the SQuAD dataset makes a strong assumption that the answers are contained in the given paragraphs .",20,0,19
dataset/preprocessed/test-data/natural_language_inference/25,"Besides , the parapraphs are rather short , approximately 200 words on average , while a real - world scenario usually involves multiple documents of much longer length .",21,0,29
dataset/preprocessed/test-data/natural_language_inference/25,"Therefore , several latest studies begin to re-design the task into more realistic settings : the MRC models are required to read and comprehend multiple documents to reach the final answer .",22,0,32
dataset/preprocessed/test-data/natural_language_inference/25,"In multi-document MRC , depending on the way of combining the two components , document selection and extractive reading comprehension , there are two categories of approaches :",23,0,28
dataset/preprocessed/test-data/natural_language_inference/25,"1 ) The pipeline approach treats the document selection and extractive reading comprehension as two separate parts , where a document is firstly selected through document ranking and then passed to the MRC model for extracting the final answer ; 2 ) Several recent studies ) adopt a joint learning method to optimize both sub - tasks in a unified framework simultaneously .",24,0,63
dataset/preprocessed/test-data/natural_language_inference/25,The pipeline method relies heavily on the quality of the document ranking module .,25,0,14
dataset/preprocessed/test-data/natural_language_inference/25,"When it fails to give the relevant documents higher ranks or filters out the ones that contain the correct answers , the downstream MRC module has noway to recover and extract the answers of interest .",26,0,36
dataset/preprocessed/test-data/natural_language_inference/25,"For the joint learning method , it is computationally expensive to jointly optimize both tasks with all the documents .",27,0,20
dataset/preprocessed/test-data/natural_language_inference/25,"This computation cost limits its application to the operational online environment , such as Amazon 2 and Taobao 3 , where efficiency is a critical factor to be considered .",28,0,30
dataset/preprocessed/test-data/natural_language_inference/25,"To address the above problems , we propose a deep cascade model which combines the advantages of both methods in a coarse - to - fine manner .",29,0,28
dataset/preprocessed/test-data/natural_language_inference/25,The deep cascade model is designed to properly keep the balance between the effectiveness and efficiency .,30,0,17
dataset/preprocessed/test-data/natural_language_inference/25,"At early stages of the model , simple features and ranking functions are used to select a candidate set of most relevant contents , filtering out the irrelevant documents and paragraphs as much as possible .",31,0,36
dataset/preprocessed/test-data/natural_language_inference/25,Then the selected paragraphs are passed to the attention - based deep MRC model for extracting the actual answer span at word level .,32,0,24
dataset/preprocessed/test-data/natural_language_inference/25,"To better support the answer extraction , we also introduce the document extraction and paragraph extraction as two auxiliary tasks , which helps to quickly narrow down the entire search space .",33,0,32
dataset/preprocessed/test-data/natural_language_inference/25,"We jointly optimize all the three tasks in a unified deep MRC model , which shares some common bottom layers .",34,0,21
dataset/preprocessed/test-data/natural_language_inference/25,"This cascaded structure enables the models to perform a coarse - to - fine pruning at different stages , better models can be learnt effectively and efficiently .",35,0,28
dataset/preprocessed/test-data/natural_language_inference/25,"The overall framework of our model is demonstrated in , which consists of three modules : document retrieval , paragraph retrieval and answer extraction .",36,0,25
dataset/preprocessed/test-data/natural_language_inference/25,The first module takes the question and a collection of raw documents as input .,37,0,15
dataset/preprocessed/test-data/natural_language_inference/25,"The module at each subsequent stage consumes the output from the previous stage , and further prunes the documents , paragraphs and answer spans given the question .",38,0,28
dataset/preprocessed/test-data/natural_language_inference/25,"For each of the first two modules , we define a ranking function and an extraction function .",39,0,18
dataset/preprocessed/test-data/natural_language_inference/25,"The ranking function is first used as a preliminary filter to discard most of the irrelevant documents or paragraphs , so as to keep our framework efficient .",40,0,28
dataset/preprocessed/test-data/natural_language_inference/25,"The extraction function is then designed to deal with the auxiliary document and paragraph extraction tasks , which is jointly optimized with the final answer extraction module for better extraction performance .",41,0,32
dataset/preprocessed/test-data/natural_language_inference/25,"The local ranking functions in different modules gradually increase in cost and complexity , to properly keep the balance between the effectiveness and efficiency .",42,0,25
dataset/preprocessed/test-data/natural_language_inference/25,"The main contributions can be summarized as follow : We propose a deep cascade learning framework to address the practical multi-document machine reading comprehension task , which considers both the effectiveness and efficiency in a coarse - to - fine manner .",43,0,42
dataset/preprocessed/test-data/natural_language_inference/25,"We incorporate the auxiliary document extraction and paragraph extraction tasks to the pure answer span prediction , which helps to narrow down the search space and improves the final extraction result in multi-document MRC scenario .",44,0,36
dataset/preprocessed/test-data/natural_language_inference/25,We conduct extensive experiments on two largescale multi-document MRC benchmark datasets : Trivi - a QA and DuReader .,45,0,19
dataset/preprocessed/test-data/natural_language_inference/25,The results show that our deep cascade model can outperform the previous state - of - the - art performance on both datasets .,46,0,24
dataset/preprocessed/test-data/natural_language_inference/25,"Besides , the proposed model has also been successfully applied in our online system and stably serve various scenarios in a quick response time of less than 50 ms .",47,0,30
dataset/preprocessed/test-data/natural_language_inference/25,Related Work Machine Reading Comprehension,48,0,5
dataset/preprocessed/test-data/natural_language_inference/25,"Recently , we can see emerging interests in multi-document MRC research , where multiple documents are given as input .",49,0,20
dataset/preprocessed/test-data/natural_language_inference/16,Text Understanding with the Attention Sum Reader Network,2,1,8
dataset/preprocessed/test-data/natural_language_inference/16,Several large cloze - style context - questionanswer datasets have been introduced recently : the CNN and Daily Mail news data and the Children 's Book Test .,4,0,28
dataset/preprocessed/test-data/natural_language_inference/16,"Thanks to the size of these datasets , the associated text comprehension task is well suited for deep - learning techniques that currently seem to outperform all alternative approaches .",5,1,30
dataset/preprocessed/test-data/natural_language_inference/16,"We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .",6,0,40
dataset/preprocessed/test-data/natural_language_inference/16,This makes the model particularly suitable for questionanswering problems where the answer is a single word from the document .,7,0,20
dataset/preprocessed/test-data/natural_language_inference/16,Ensemble of our models sets new state of the art on all evaluated datasets .,8,0,15
dataset/preprocessed/test-data/natural_language_inference/16,Most of the information humanity has gathered up to this point is stored in the form of plain text .,10,0,20
dataset/preprocessed/test-data/natural_language_inference/16,Hence the task of teaching machines how to understand this data is of utmost importance in the field of Artificial Intelligence .,11,0,22
dataset/preprocessed/test-data/natural_language_inference/16,One way of testing the level of text understanding is simply to ask the system questions for which the answer can be inferred from the text .,12,0,27
dataset/preprocessed/test-data/natural_language_inference/16,A well - known example of a system that could make use of a huge collection of unstructured documents to answer questions is for instance IBM 's Watson system used for the Jeopardy challenge .,13,0,35
dataset/preprocessed/test-data/natural_language_inference/16,"Cloze - style questions , i.e. questions formed by removing a phrase from a sentence , are an appealing form of such questions ( for example see ) .",14,0,29
dataset/preprocessed/test-data/natural_language_inference/16,"While the task is easy to evaluate , one can vary the context , the question Document :",15,0,18
dataset/preprocessed/test-data/natural_language_inference/16,What was supposed to be a fantasy sports car ride at Walt Disney World Speedway turned deadly when a Lamborghini crashed into a guardrail .,16,0,25
dataset/preprocessed/test-data/natural_language_inference/16,"The crash took place Sunday at the Exotic Driving Experience , which bills itself as a chance to drive your dream car on a racetrack .",17,0,26
dataset/preprocessed/test-data/natural_language_inference/16,"The Lamborghini 's passenger , 36 year - old Gary Terry of Davenport , Florida , died at the scene , Florida Highway Patrol said .",18,0,26
dataset/preprocessed/test-data/natural_language_inference/16,"The driver of the Lamborghini , 24 - year - old .",19,0,12
dataset/preprocessed/test-data/natural_language_inference/16,Anonymization of this example that makes the task harder is shown in .,20,0,13
dataset/preprocessed/test-data/natural_language_inference/16,sentence or the specific phrase missing in the question to dramatically change the task structure and difficulty .,21,0,18
dataset/preprocessed/test-data/natural_language_inference/16,"One way of altering the task difficulty is to vary the word type being replaced , as in .",22,0,19
dataset/preprocessed/test-data/natural_language_inference/16,The complexity of such variation comes from the fact that the level of context understanding needed in order to correctly predict different types of words varies greatly .,23,0,28
dataset/preprocessed/test-data/natural_language_inference/16,"While predicting prepositions can easily be done using relatively simple models with very little context knowledge , predicting named entities requires a deeper understanding of the context .",24,0,28
dataset/preprocessed/test-data/natural_language_inference/16,"Also , as opposed to selecting a random sentence from a text as in ) , the question can be formed from a specific part of the document , such as a short summary or a list of tags .",25,0,40
dataset/preprocessed/test-data/natural_language_inference/16,and the statistics provided with the CBT data set .,26,0,10
dataset/preprocessed/test-data/natural_language_inference/16,"Since such sentences often paraphrase in a condensed form what was said in the text , they are particularly suitable for testing text comprehension .",29,0,25
dataset/preprocessed/test-data/natural_language_inference/16,An important property of cloze - style questions is that a large amount of such questions can be automatically generated from real world documents .,30,0,25
dataset/preprocessed/test-data/natural_language_inference/16,This opens the task to data - hungry techniques such as deep learning .,31,0,14
dataset/preprocessed/test-data/natural_language_inference/16,This is an advantage compared to smaller machine understanding datasets like MCTest that have only hundreds of training examples and therefore the best performing systems usually rely on handcrafted features .,32,0,31
dataset/preprocessed/test-data/natural_language_inference/16,In the first part of this article we introduce the task at hand and the main aspects of the relevant datasets .,33,0,22
dataset/preprocessed/test-data/natural_language_inference/16,Then we present our own model to tackle the problem .,34,0,11
dataset/preprocessed/test-data/natural_language_inference/16,Subsequently we compare the model to previously proposed architectures and finally describe the experimental results on the performance of our model .,35,0,22
dataset/preprocessed/test-data/natural_language_inference/16,Task and datasets,36,0,3
dataset/preprocessed/test-data/natural_language_inference/16,In this section we introduce the task that we are seeking to solve and relevant large - scale datasets that have recently been introduced for this task .,37,0,28
dataset/preprocessed/test-data/natural_language_inference/16,Formal Task Description,38,0,3
dataset/preprocessed/test-data/natural_language_inference/16,"The task consists of answering a cloze - style question , the answer to which depends on the understanding of a context document provided with the question .",39,0,28
dataset/preprocessed/test-data/natural_language_inference/16,The model is also provided with a set of possible answers from which the correct one is to be selected .,40,0,21
dataset/preprocessed/test-data/natural_language_inference/16,This can be formalized as follows :,41,0,7
dataset/preprocessed/test-data/natural_language_inference/16,"The training data consist of tuples ( q , d , a , A ) , where q is a question , dis a document that con-tains the answer to question q , A is a set of possible answers and a ?",42,0,43
dataset/preprocessed/test-data/natural_language_inference/16,A is the ground truth answer .,43,0,7
dataset/preprocessed/test-data/natural_language_inference/16,Both q and dare sequences of words from vocabulary V .,44,0,11
dataset/preprocessed/test-data/natural_language_inference/16,"We also assume that all possible answers are words from the vocabulary , that is A ?",45,0,17
dataset/preprocessed/test-data/natural_language_inference/16,"V , and that the ground truth answer a appears in the document , that is a ?",46,0,18
dataset/preprocessed/test-data/natural_language_inference/16,We will now briefly summarize important features of the datasets .,49,0,11
dataset/preprocessed/test-data/natural_language_inference/10,A Simple and Effective Approach to the Story Cloze Test,2,1,10
dataset/preprocessed/test-data/natural_language_inference/10,"In the Story Cloze Test , a system is presented with a 4 - sentence prompt to a story , and must determine which one of two potential endings is the ' right ' ending to the story .",4,0,39
dataset/preprocessed/test-data/natural_language_inference/10,Previous work has shown that ignoring the training set and training a model on the validation set can achieve high accuracy on this task due to stylistic differences between the story endings in the training set and validation and test sets .,5,0,42
dataset/preprocessed/test-data/natural_language_inference/10,"Following this approach , we present a simpler fully - neural approach to the Story Cloze Test using skip - thought embeddings of the stories in a feed - forward network that achieves close to state - of - the - art performance on this task without any feature engineering .",6,0,51
dataset/preprocessed/test-data/natural_language_inference/10,We also find that considering just the last sentence of the prompt instead of the whole prompt yields higher accuracy with our approach .,7,0,24
dataset/preprocessed/test-data/natural_language_inference/10,"1 Introduction introduced the Story Cloze Test : given a four - sentence story prompt ( or ' context ' ) , the task is to pick the ' right ' commonsense ending from two options .",9,0,37
dataset/preprocessed/test-data/natural_language_inference/10,"Test is intended to be a general framework for evaluating story understanding , since it ostensibly requires combining semantic understanding and commonsense knowledge of our world .",11,0,27
dataset/preprocessed/test-data/natural_language_inference/10,The task is accompanied by the Rochester story ( ROCstory ) corpus .,12,0,13
dataset/preprocessed/test-data/natural_language_inference/10,The training set consists of crowdsourced five - sentence stories designed to capture common events in daily life .,13,0,19
dataset/preprocessed/test-data/natural_language_inference/10,The validation and testing sets consist of four - sentence prompts and labeled ' right ' and ' wrong ' story endings .,14,0,23
dataset/preprocessed/test-data/natural_language_inference/10,shows such a sample story from the Rochester corpus validation set with a labeled right and wrong ending .,15,0,19
dataset/preprocessed/test-data/natural_language_inference/10,"Many previous approaches to the Cloze Test have ignored the training set entirely and trained on the validation set since the former lacks ' negative ' examples ; although this greatly reduces the available training data , it circumvents the issue of obtaining negative examples during training .",16,0,48
dataset/preprocessed/test-data/natural_language_inference/10,Bob loved to watch movies .,18,0,6
dataset/preprocessed/test-data/natural_language_inference/10,He was looking forward to a three day weekend coming up .,19,0,12
dataset/preprocessed/test-data/natural_language_inference/10,He made a list of his favorite movies and invited some friends over .,20,0,14
dataset/preprocessed/test-data/natural_language_inference/10,He spent the weekend with his friends watching all his favorite movies .,21,0,13
dataset/preprocessed/test-data/natural_language_inference/10,Right Ending : Bob had a great time .,22,0,9
dataset/preprocessed/test-data/natural_language_inference/10,Wrong Ending : Bob stopped talking to those friends .,23,0,10
dataset/preprocessed/test-data/natural_language_inference/10,Our contribution to this task is two - fold .,24,0,10
dataset/preprocessed/test-data/natural_language_inference/10,"First , we achieve near state - of - the - art performance ( within 1.1 % ) but with a much simpler , fullyneural approach .",25,0,27
dataset/preprocessed/test-data/natural_language_inference/10,"Where previous approaches rely on feature engineering or involved neural network architectures , we achieve high accuracy with a fully neural approach involving only a single feedforward network and pre-trained skip - thought embeddings .",26,0,35
dataset/preprocessed/test-data/natural_language_inference/10,"Second , we find that considering only the last sentence of the context outperforms models that consider the full context .",27,0,21
dataset/preprocessed/test-data/natural_language_inference/10,Previous approaches focused on the accuracy achieved by either considering the whole context or ignoring the whole context of the story .,28,0,22
dataset/preprocessed/test-data/natural_language_inference/10,"In sum , our approach differs from previous efforts in the joint use of three strategies : ( 1 ) using skip - thought embeddings for sentences in the story in a feed - forward neural network , ( 2 ) training the model on the provided validation set , and ( 3 ) considering the two endings with only the last sentence in the prompt .",29,0,67
dataset/preprocessed/test-data/natural_language_inference/10,"This paper is structured as follows : we will discuss previous approaches to the problem and how they compare to our approach , describe our model and the experiments we ran in detail , and finally discuss reasons for our model 's superior performance and why ignoring the first three sentences of the story produces better accuracy .",30,0,58
dataset/preprocessed/test-data/natural_language_inference/10,"92 presented the original Story Cloze Test , and showed that while humans could achieve 100 % accuracy on the task , a deep structured semantic model was the best performing artificial baseline , with a test - set accuracy of 58.5 % .",31,0,44
dataset/preprocessed/test-data/natural_language_inference/10,"While they do consider using skip - thought embeddings for this task , they do so by choosing the ending whose embedding was closer to the average skip - thought embedding of the context .",32,0,35
dataset/preprocessed/test-data/natural_language_inference/10,This only achieves a test - set accuracy of 55.2 % .,33,0,12
dataset/preprocessed/test-data/natural_language_inference/10,"On the other hand , we train a feed - forward network using skip - thought embeddings .",34,0,18
dataset/preprocessed/test-data/natural_language_inference/10,The Story Cloze,35,0,3
dataset/preprocessed/test-data/natural_language_inference/10,"Test was the shared task at LS - DSem 2017 , and summarize the approaches by various teams on this task .",36,0,22
dataset/preprocessed/test-data/natural_language_inference/10,The best - performing system by achieved a test - set accuracy of 75.2 % .,37,0,16
dataset/preprocessed/test-data/natural_language_inference/10,"Like us , they train their model on the validation set , but their approach relies more heavily on feature engineering .",38,0,22
dataset/preprocessed/test-data/natural_language_inference/10,"They find that they could achieve 72.4 % accuracy using just the stylistic features of the endings , suggesting that many of the ' right ' endings on this task could be identified independent of the story context .",39,0,39
dataset/preprocessed/test-data/natural_language_inference/10,"Upon further investigation , find differences not only between the ' right ' and ' wrong ' endings in the validation set , but also between these and the ' right ' endings from the training set , providing some explanation for why models trained on the validation set outperform models trained on the training set - their data distributions are somewhat different .",40,0,64
dataset/preprocessed/test-data/natural_language_inference/10,"Further work by established a neural baseline for models trained on the validation set , with a test - set accuracy of 74.7 % .",41,0,25
dataset/preprocessed/test-data/natural_language_inference/10,They were also able to achieve a marginally better accuracy of 72.5 % ( compared to ) when using just the sentence endings and ignoring the context ; and this approach did not require any feature engineering .,42,0,38
dataset/preprocessed/test-data/natural_language_inference/10,"They showed that a human can distinguish ' right ' from ' wrong ' endings without the context with 78 % accuracy , further backing the claim that the importance of context in determining the right ending is more limited than desirable on this task .",43,0,46
dataset/preprocessed/test-data/natural_language_inference/10,"Their approach involves training a hierarchical bidirectional LSTM with attention to first encode sentences and then stories , with a hinge - loss objective function .",44,0,26
dataset/preprocessed/test-data/natural_language_inference/10,"use skip - thought em-beddings for this task , but they encode the entire context using a GRU , with a binary classifier to determine if an ending was right or wrong .",45,0,33
dataset/preprocessed/test-data/natural_language_inference/10,"They train their model on the provided training set , sampling negative examples from the training set itself .",46,0,19
dataset/preprocessed/test-data/natural_language_inference/10,Their best model achieves 67.2 % accuracy on this task .,47,0,11
dataset/preprocessed/test-data/natural_language_inference/10,"Currently , the comprehensive approach taken by , where they model event sequence , sentiment trajectory , and topical consistency fora hidden coherence model , achieves the state - of - the - art performance on this task , with a test - set accuracy of 77.6 % .",48,0,49
dataset/preprocessed/test-data/natural_language_inference/26,U - Net : Machine Reading Comprehension with Unanswerable Questions,2,1,10
dataset/preprocessed/test-data/natural_language_inference/26,Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing .,4,0,16
dataset/preprocessed/test-data/natural_language_inference/26,A key subtask is to reliably predict whether the question is unanswerable .,5,0,13
dataset/preprocessed/test-data/natural_language_inference/26,"In this paper , we propose a unified model , called U - Net , with three important components : answer pointer , no - answer pointer , and answer verifier .",6,0,32
dataset/preprocessed/test-data/natural_language_inference/26,We introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens .,7,0,22
dataset/preprocessed/test-data/natural_language_inference/26,"The universal node encodes the fused information from both the question and passage , and plays an important role to predict whether the question is answerable and also greatly improves the conciseness of the U - Net.",8,0,37
dataset/preprocessed/test-data/natural_language_inference/26,"Different from the stateof - art pipeline models , U - Net can be learned in an end - to - end fashion .",9,0,24
dataset/preprocessed/test-data/natural_language_inference/26,The experimental results on the SQuAD 2.0 dataset show that U - Net can effectively predict the unanswerability of questions and achieves an F1 score of 71.7 on SQuAD 2.0 .,10,0,31
dataset/preprocessed/test-data/natural_language_inference/26,"Machine reading comprehension ( MRC ) is a challenging task in natural language processing , which requires that machine can read , understand , and answer questions about a text .",12,1,31
dataset/preprocessed/test-data/natural_language_inference/26,"Benefiting from the rapid development of deep learning techniques and large - scale benchmarks , the end - to - end neural methods have achieved promising results on MRC task .",13,1,31
dataset/preprocessed/test-data/natural_language_inference/26,"The best systems have even surpassed human performance on the Stanford Question Answering Dataset ( SQuAD ) , one of the most widely used MRC benchmarks .",14,0,27
dataset/preprocessed/test-data/natural_language_inference/26,"However , one of the limitations of the SQuAD task is that each question has a correct answer in the context passage , therefore most models just need to select the most relevant text span as the answer , without necessarily checking whether it is indeed the answer to the question .",15,0,52
dataset/preprocessed/test-data/natural_language_inference/26,"To remedy the deficiency of SQuAD , developed SQuAD 2.0 that combines SQuAD with new unanswerable questions .",16,0,18
dataset/preprocessed/test-data/natural_language_inference/26,shows two examples of unanswerable questions .,17,0,7
dataset/preprocessed/test-data/natural_language_inference/26,The new dataset requires the MRC systems to know what they do n't know .,18,0,15
dataset/preprocessed/test-data/natural_language_inference/26,"To do well on MRC with unanswerable questions , the model needs to comprehend the question , reason among the * Corresponding Author .",19,0,24
dataset/preprocessed/test-data/natural_language_inference/26,"Article : Endangered Species Act Paragraph : "" ...",20,0,9
dataset/preprocessed/test-data/natural_language_inference/26,"Other legislation followed , including the Migratory Bird Conservation Act of 1929 , a 1937 treaty prohibiting the hunting of right and gray whales , and the Bald Eagle Protection Act of 1940 .",21,0,34
dataset/preprocessed/test-data/natural_language_inference/26,These later laws had a low cost to society the species were relatively rareand little opposition was raised .,22,0,19
dataset/preprocessed/test-data/natural_language_inference/26,1 : Which laws faced significant opposition ?,24,0,8
dataset/preprocessed/test-data/natural_language_inference/26,Plausible Answer : later laws Question,25,0,6
dataset/preprocessed/test-data/natural_language_inference/26,2 : What was the name of the 1937 treaty ?,26,0,11
dataset/preprocessed/test-data/natural_language_inference/26,Plausible Answer : Bald Eagle Protection,27,0,6
dataset/preprocessed/test-data/natural_language_inference/26,"Act passage , judge the unanswerability and then identify the answer span .",28,0,13
dataset/preprocessed/test-data/natural_language_inference/26,"Since extensive work has been done on how to correctly predict the answer span when the question is answerable ( e.g. , SQuAD 1.1 ) , the main challenge of this task lies in how to reliably determine whether a question is not answerable from the passage .",29,0,48
dataset/preprocessed/test-data/natural_language_inference/26,There are two kinds of approaches to model the answerability of a question .,30,0,14
dataset/preprocessed/test-data/natural_language_inference/26,One approach is to directly extend previous MRC models by introducing a no-answer score to the score vector of the answer span .,31,0,23
dataset/preprocessed/test-data/natural_language_inference/26,But this kind of approaches is relatively simple and can not effectively model the answerability of a question .,32,0,19
dataset/preprocessed/test-data/natural_language_inference/26,Another approach introduces an answer verifier to determine whether the question is unanswerable ) .,33,0,15
dataset/preprocessed/test-data/natural_language_inference/26,"However , this kind of approaches usually has a pipeline structure .",34,0,12
dataset/preprocessed/test-data/natural_language_inference/26,"The answer pointer and answer verifier have their respective models , which are trained separately .",35,0,16
dataset/preprocessed/test-data/natural_language_inference/26,"Intuitively , it is unnecessary since the underlying comprehension and reasoning of language for these components is the same .",36,0,20
dataset/preprocessed/test-data/natural_language_inference/26,"In this paper , we decompose the problem of MRC with unanswerable questions into three sub - tasks : answer pointer , no - answer pointer , and answer verifier .",37,0,31
dataset/preprocessed/test-data/natural_language_inference/26,"Since these three sub - tasks are highly related , we regard the MRC with unanswerable questions as a multi-task learning problem ( Caruana 1997 ) by sharing some meta-knowledge .",38,0,31
dataset/preprocessed/test-data/natural_language_inference/26,We propose the U - Net to incorporate these three sub - tasks into a unified model :,39,0,18
dataset/preprocessed/test-data/natural_language_inference/26,"1 ) an answer pointer to predict a can - didate answer span for a question ; 2 ) a no -answer pointer to avoid selecting any text span when a question has no answer ; and 3 ) an answer verifier to determine the probability of the "" unanswerability "" of a question with candidate answer information .",40,0,59
dataset/preprocessed/test-data/natural_language_inference/26,"Additionally , we also introduce a universal node and process the question and its context passage as a single contiguous sequence of tokens , which greatly improves the conciseness of U - Net .",41,0,34
dataset/preprocessed/test-data/natural_language_inference/26,The universal node acts on both question and passage to learn whether the question is answerable .,42,0,17
dataset/preprocessed/test-data/natural_language_inference/26,"Different from the previous pipeline models , U - Net can be learned in an end - to - end fashion .",43,0,22
dataset/preprocessed/test-data/natural_language_inference/26,Our experimental results on the SQuAD 2.0 dataset show that U - Net effectively predicts the unanswerability of questions and achieves an F1 score of 72.6 .,44,0,27
dataset/preprocessed/test-data/natural_language_inference/26,The contributions of this paper can be summarized as follows .,45,0,11
dataset/preprocessed/test-data/natural_language_inference/26,"We decompose the problem of MRC with unanswerable questions into three sub - tasks and combine them into a unified model , which uses the shared encoding and interaction layers .",46,0,31
dataset/preprocessed/test-data/natural_language_inference/26,"Thus , the three - tasks can be trained simultaneously in an end - to - end fashion .",47,0,19
dataset/preprocessed/test-data/natural_language_inference/26,We introduce a universal node to encode the common information of the question and passage .,48,0,16
dataset/preprocessed/test-data/natural_language_inference/26,"Thus , we can use a unified representation to model the question and passage , which makes our model more condensed . U - Net is very easy to implement yet effective .",49,0,33
dataset/preprocessed/test-data/natural_language_inference/21,Attention - over - Attention Neural Networks for Reading Comprehension,2,1,10
dataset/preprocessed/test-data/natural_language_inference/21,Cloze - style reading comprehension is a representative problem in mining relationship between document and query .,4,1,17
dataset/preprocessed/test-data/natural_language_inference/21,"In this paper , we present a simple but novel model called attention - over - attention reader for better solving cloze - style reading comprehension task .",5,0,28
dataset/preprocessed/test-data/natural_language_inference/21,"The proposed model aims to place another attention mechanism over the document - level attention and induces "" attended attention "" for final answer predictions .",6,0,26
dataset/preprocessed/test-data/natural_language_inference/21,One advantage of our model is that it is simpler than related works while giving excellent performance .,7,0,18
dataset/preprocessed/test-data/natural_language_inference/21,"In addition to the primary model , we also propose an N - best re-ranking strategy to double check the validity of the candidates and further improve the performance .",8,0,30
dataset/preprocessed/test-data/natural_language_inference/21,"Experimental results show that the proposed methods significantly outperform various state - of the - art systems by a large margin in public datasets , such as CNN and Children 's Book Test .",9,0,34
dataset/preprocessed/test-data/natural_language_inference/21,"To read and comprehend the human languages are challenging tasks for the machines , which requires that the understanding of natural languages and the ability to do reasoning over various clues .",11,1,32
dataset/preprocessed/test-data/natural_language_inference/21,"Reading comprehension is a general problem in the real world , which aims to read and comprehend a given article or context , and answer the questions based on it .",12,0,31
dataset/preprocessed/test-data/natural_language_inference/21,"Recently , the cloze - style reading comprehension problem has become a popular task in the community .",13,0,18
dataset/preprocessed/test-data/natural_language_inference/21,The cloze - style query is a problem that to fill in an appropriate word in the given sentences while taking the context information into account .,14,0,27
dataset/preprocessed/test-data/natural_language_inference/21,"To teach the machine to do cloze - style reading comprehensions , large - scale training data is necessary for learning relationships between the given document and query .",15,0,29
dataset/preprocessed/test-data/natural_language_inference/21,"To create large - scale training data for neural networks , released the CNN / Daily Mail news dataset , where the document is formed by the news articles and the queries are extracted from the summary of the news .",16,0,41
dataset/preprocessed/test-data/natural_language_inference/21,released the Children 's Book,17,0,5
dataset/preprocessed/test-data/natural_language_inference/21,"Test dataset afterwards , where the training samples are generated from consecutive 20 sentences from books , and the query is formed by 21st sentence .",18,0,26
dataset/preprocessed/test-data/natural_language_inference/21,"Following these datasets , avast variety of neural network approaches have been proposed , and most of them stem from the attention - based neural network , which has become a stereotype inmost of the NLP tasks and is well - known by its capability of learning the "" importance "" distribution over the inputs .",19,0,56
dataset/preprocessed/test-data/natural_language_inference/21,"In this paper , we present a novel neural network architecture , called attention - over - attention model .",20,0,20
dataset/preprocessed/test-data/natural_language_inference/21,"As we can understand the meaning literally , our model aims to place another attention mechanism over the existing document - level attention .",21,0,24
dataset/preprocessed/test-data/natural_language_inference/21,"Unlike the previous works , that are using heuristic merging functions , or setting various pre-defined non-trainable terms , our model could automatically generate an "" attended attention "" over various document - level attentions , and make a mutual look not only from query - to - document but also document - to - query , which will benefit from the interactive information .",22,0,65
dataset/preprocessed/test-data/natural_language_inference/21,"To sum up , the main contributions of our work are listed as follows .",23,0,15
dataset/preprocessed/test-data/natural_language_inference/21,"the mechanism of nesting another attention over the existing attentions is proposed , i.e. attention - over - attention mechanism .",24,0,21
dataset/preprocessed/test-data/natural_language_inference/21,"Unlike the previous works on introducing complex architectures or many non-trainable hyper - parameters to the model , our model is much more simple but outperforms various state - of - the - art systems by a large margin .",25,0,40
dataset/preprocessed/test-data/natural_language_inference/21,We also propose an N - best re-ranking strategy to re-score the candidates in various aspects and further improve the performance .,26,0,22
dataset/preprocessed/test-data/natural_language_inference/21,The following of the paper will be organized as follows .,27,0,11
dataset/preprocessed/test-data/natural_language_inference/21,"In Section 2 , we will give a brief introduction to the cloze - style reading comprehension task as well as related public datasets .",28,0,25
dataset/preprocessed/test-data/natural_language_inference/21,Then the proposed attention - over - attention reader will be presented in detail in Section 3 and N - best reranking strategy in Section 4 .,29,0,27
dataset/preprocessed/test-data/natural_language_inference/21,The experimental results and analysis will be given in Section 5 and Section 6 .,30,0,15
dataset/preprocessed/test-data/natural_language_inference/21,Related work will be discussed in Section 7 .,31,0,9
dataset/preprocessed/test-data/natural_language_inference/21,"Finally , we will give a conclusion of this paper and envisions on future work .",32,0,16
dataset/preprocessed/test-data/natural_language_inference/21,Cloze- style Reading Comprehension,33,0,4
dataset/preprocessed/test-data/natural_language_inference/21,"In this section , we will give a brief introduction to the cloze - style reading comprehension task at the beginning .",34,0,22
dataset/preprocessed/test-data/natural_language_inference/21,"And then , several existing public datasets will be described in detail .",35,0,13
dataset/preprocessed/test-data/natural_language_inference/21,"Formally , a general Cloze - style reading comprehension problem can be illustrated as a triple :",37,0,17
dataset/preprocessed/test-data/natural_language_inference/21,"D , Q , A",38,0,5
dataset/preprocessed/test-data/natural_language_inference/21,"The triple consists of a document D , a query Q and the answer to the query A. Note that the answer is usually a single word in the document , which requires the human to exploit context information in both document and query .",39,0,45
dataset/preprocessed/test-data/natural_language_inference/21,The type of the answer word varies from predicting a preposition given a fixed collocation to identifying a named entity from a factual illustration .,40,0,25
dataset/preprocessed/test-data/natural_language_inference/21,Existing Public Datasets,41,0,3
dataset/preprocessed/test-data/natural_language_inference/21,Large - scale training data is essential for training neural networks .,42,0,12
dataset/preprocessed/test-data/natural_language_inference/21,Several public datasets for the cloze - style reading comprehension has been released .,43,0,14
dataset/preprocessed/test-data/natural_language_inference/21,"Here , we introduce two representative and widely - used datasets .",44,0,12
dataset/preprocessed/test-data/natural_language_inference/21,have firstly published two datasets :,45,0,6
dataset/preprocessed/test-data/natural_language_inference/21,CNN and Daily Mail news data,46,0,6
dataset/preprocessed/test-data/natural_language_inference/21,They construct these datasets with web - crawled CNN and Daily Mail news data .,48,0,15
dataset/preprocessed/test-data/natural_language_inference/21,One of the characteristics of these datasets is that the news article is often associated with a summary .,49,0,19
dataset/preprocessed/test-data/natural_language_inference/30,Convolutional Neural Network Architectures for Matching Natural Language Sentences,2,1,9
dataset/preprocessed/test-data/natural_language_inference/30,"Semantic matching is of central importance to many natural language tasks [ 2,28 ] .",4,1,15
dataset/preprocessed/test-data/natural_language_inference/30,A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them .,5,0,20
dataset/preprocessed/test-data/natural_language_inference/30,"As a step toward this goal , we propose convolutional neural network models for matching two sentences , by adapting the convolutional strategy in vision and speech .",6,0,28
dataset/preprocessed/test-data/natural_language_inference/30,"The proposed models not only nicely represent the hierarchical structures of sentences with their layerby - layer composition and pooling , but also capture the rich matching patterns at different levels .",7,0,32
dataset/preprocessed/test-data/natural_language_inference/30,"Our models are rather generic , requiring no prior knowledge on language , and can hence be applied to matching tasks of different nature and in different languages .",8,0,29
dataset/preprocessed/test-data/natural_language_inference/30,The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models .,9,0,29
dataset/preprocessed/test-data/natural_language_inference/30,Matching two potentially heterogenous language objects is central to many natural language applications .,11,1,14
dataset/preprocessed/test-data/natural_language_inference/30,"It generalizes the conventional notion of similarity ( e.g. , in paraphrase identification ) or relevance ( e.g. , in information retrieval ) , since it aims to model the correspondence between "" linguistic objects "" of different nature at different levels of abstractions .",12,0,45
dataset/preprocessed/test-data/natural_language_inference/30,"Examples include top -k re-ranking in machine translation ( e.g. , comparing the meanings of a French sentence and an English sentence ) and dialogue ( e.g. , evaluating the appropriateness of a response to a given utterance ) .",13,0,40
dataset/preprocessed/test-data/natural_language_inference/30,"Natural language sentences have complicated structures , both sequential and hierarchical , that are essential for understanding them .",14,0,19
dataset/preprocessed/test-data/natural_language_inference/30,A successful sentence - matching algorithm therefore needs to capture not only the internal structures of sentences but also the rich patterns in their interactions .,15,0,26
dataset/preprocessed/test-data/natural_language_inference/30,"Towards this end , we propose deep neural network models , which adapt the convolutional strategy ( proven successful on image and speech ) to natural language .",16,0,28
dataset/preprocessed/test-data/natural_language_inference/30,"To further explore the relation between representing sentences and matching them , we devise a novel model that can naturally host both the hierarchical composition for sentences and the simple - to - comprehensive fusion of matching patterns with the same convolutional architecture .",17,0,44
dataset/preprocessed/test-data/natural_language_inference/30,"Our model is generic , requiring no prior knowledge of natural language ( e.g. , parse tree ) and putting essentially no constraints on the matching tasks .",18,0,28
dataset/preprocessed/test-data/natural_language_inference/30,This is part of our continuing effort 1 in understanding natural language objects and the matching between them .,19,0,19
dataset/preprocessed/test-data/natural_language_inference/30,Our main contributions can be summarized as follows .,20,0,9
dataset/preprocessed/test-data/natural_language_inference/30,"First , we devise novel deep convolutional network architectures that can naturally combine 1 ) the hierarchical sentence modeling through layer - by - layer composition and pooling , and 2 ) the capturing of the rich matching patterns at different levels of abstraction ; Second , we perform extensive empirical study on tasks with different scales and characteristics , and demonstrate the superior power of the proposed architectures over competitor methods .",21,0,73
dataset/preprocessed/test-data/natural_language_inference/30,"We start by introducing a convolution network in Section 2 as the basic architecture for sentence modeling , and how it is related to existing sentence models .",23,0,28
dataset/preprocessed/test-data/natural_language_inference/30,"Based on that , in Section 3 , we propose two architectures for sentence matching , with a detailed discussion of their relation .",24,0,24
dataset/preprocessed/test-data/natural_language_inference/30,"In Section 4 , we briefly discuss the learning of the proposed architectures .",25,0,14
dataset/preprocessed/test-data/natural_language_inference/30,"Then in Section 5 , we report our empirical study , followed by a brief discussion of related work in Section 6 .",26,0,23
dataset/preprocessed/test-data/natural_language_inference/30,Convolutional Sentence Model,27,0,3
dataset/preprocessed/test-data/natural_language_inference/30,We start with proposing anew convolutional architecture for modeling sentences .,28,0,11
dataset/preprocessed/test-data/natural_language_inference/30,"As illustrated in , it takes as input the embedding of words ( often trained beforehand with unsupervised methods ) in the sentence aligned sequentially , and summarize the meaning of a sentence through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer .",29,0,52
dataset/preprocessed/test-data/natural_language_inference/30,"As inmost convolutional models , we use convolution units with a local "" receptive field "" and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .",30,0,39
dataset/preprocessed/test-data/natural_language_inference/30,"As shown in , the convolution in Layer - 1 operates on sliding windows of words ( width k 1 ) , and the convolutions in deeper layers are defined in a similar way .",32,0,35
dataset/preprocessed/test-data/natural_language_inference/30,"Generally , with sentence input x , the convolution unit for feature map of type -f ( among F of them ) on Layer - is",33,0,26
dataset/preprocessed/test-data/natural_language_inference/30,and it s matrix form is z,34,0,7
dataset/preprocessed/test-data/natural_language_inference/30,gives the output of feature map of type - f for location i in Layer - ;,35,0,17
dataset/preprocessed/test-data/natural_language_inference/30,"w ( , f ) is the parameters for f on Layer - , with matrix form",36,0,17
dataset/preprocessed/test-data/natural_language_inference/30,"? ( ) is the activation function ( e.g. , Sigmoid or Relu )",37,0,14
dataset/preprocessed/test-data/natural_language_inference/30,( ?1 ) i denotes the segment of Layer - ?,39,0,11
dataset/preprocessed/test-data/natural_language_inference/30,"1 for the convolution at location i , whil",40,0,9
dataset/preprocessed/test-data/natural_language_inference/30,concatenates the vectors fork 1 ( width of sliding window ) words from sentence input x .,41,0,17
dataset/preprocessed/test-data/natural_language_inference/30,Max - Pooling,42,0,3
dataset/preprocessed/test-data/natural_language_inference/30,"We take a max - pooling in every two - unit window for every f , after each convolution",43,0,19
dataset/preprocessed/test-data/natural_language_inference/30,The effects of pooling are two - fold :,44,0,9
dataset/preprocessed/test-data/natural_language_inference/30,"1 ) it shrinks the size of the representation by half , thus quickly absorbs the differences in length for sentence representation , and 2 ) it filters out undesirable composition of words ( see Section 2.1 for some analysis ) .",45,0,42
dataset/preprocessed/test-data/natural_language_inference/30,The variable length of sentences in a fairly broad range can be readily handled with the convolution and pooling strategy .,47,0,21
dataset/preprocessed/test-data/natural_language_inference/30,"More specifically , we put all - zero padding vectors after the last word of the sentence until the maximum length .",48,0,22
dataset/preprocessed/test-data/natural_language_inference/30,"To eliminate the boundary effect caused by the great variability of sentence lengths , we add to the convolutional unit agate which sets the output vectors to all - zeros if the input is all zeros .",49,0,37
dataset/preprocessed/test-data/natural_language_inference/23,Deep Fusion LSTMs for Text Semantic Matching,2,1,7
dataset/preprocessed/test-data/natural_language_inference/23,"Recently , there is rising interest in modelling the interactions of text pair with deep neural networks .",4,0,18
dataset/preprocessed/test-data/natural_language_inference/23,"In this paper , we propose a model of deep fusion LSTMs ( DF - LSTMs ) to model the strong interaction of text pair in a recursive matching way .",5,0,31
dataset/preprocessed/test-data/natural_language_inference/23,"Specifically , DF - LSTMs consist of two interdependent LSTMs , each of which models a sequence under the influence of another .",6,0,23
dataset/preprocessed/test-data/natural_language_inference/23,"We also use external memory to increase the capacity of LSTMs , thereby possibly capturing more complicated matching patterns .",7,0,20
dataset/preprocessed/test-data/natural_language_inference/23,Experiments on two very large datasets demonstrate the efficacy of our proposed architecture .,8,0,14
dataset/preprocessed/test-data/natural_language_inference/23,"Furthermore , we present an elaborate qualitative analysis of our models , giving an intuitive understanding how our model worked .",9,0,21
dataset/preprocessed/test-data/natural_language_inference/23,* Corresponding author and soon .,10,0,6
dataset/preprocessed/test-data/natural_language_inference/23,"These models first encode two sequences into continuous dense vectors by separated neural models , and then compute the matching score based on sentence encoding .",11,0,26
dataset/preprocessed/test-data/natural_language_inference/23,"In this paradigm , two sentences have no interaction until arriving final phase .",12,0,14
dataset/preprocessed/test-data/natural_language_inference/23,"Among many natural language processing ( NLP ) tasks , such as text classification , question answering and machine translation , a common problem is modelling the relevance / similarity of a pair of texts , which is also called text semantic matching .",14,1,44
dataset/preprocessed/test-data/natural_language_inference/23,"Due to the semantic gap problem , text semantic matching is still a challenging problem .",15,0,16
dataset/preprocessed/test-data/natural_language_inference/23,"Recently , deep learning is rising a substantial interest in text semantic matching and has achieved some great progresses .",16,0,20
dataset/preprocessed/test-data/natural_language_inference/23,"According to their interaction ways , previous models can be classified into three categories :",17,0,15
dataset/preprocessed/test-data/natural_language_inference/23,"Models Some early works focus on sentence level interactions , such as ARC - I , CNTN Semi-interaction Models",19,0,19
dataset/preprocessed/test-data/natural_language_inference/23,"Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence , such as ABCNN , Attention LSTM .",20,0,30
dataset/preprocessed/test-data/natural_language_inference/23,These models can alleviate the weak interaction problem to some extent .,21,0,12
dataset/preprocessed/test-data/natural_language_inference/23,Strong Interaction Models,22,0,3
dataset/preprocessed/test-data/natural_language_inference/23,"Some models build the interaction at different granularity ( word , phrase and sentence level ) , such as ARC - II , MultiGranCNN , Multi - Perspective CNN , , MatchPyramid .",23,0,33
dataset/preprocessed/test-data/natural_language_inference/23,The final matching score depends on these different levels of interactions .,24,0,12
dataset/preprocessed/test-data/natural_language_inference/23,"In this paper , we adopt a deep fusion strategy to model the strong interactions of two sentences .",25,0,19
dataset/preprocessed/test-data/natural_language_inference/23,"Given two texts x 1 :m and y 1 :n , we define a matching vector h i , j to represent the interaction of the subsequences x 1:i and y 1:j .",26,0,33
dataset/preprocessed/test-data/natural_language_inference/23,"h i , j depends on the matching vectors h s ,t on previous interactions 1 ? s < i and 1 ? t < j.",27,0,26
dataset/preprocessed/test-data/natural_language_inference/23,"Thus , text matching can be regarded as modelling the interaction of two texts in a recursive matching way .",28,0,20
dataset/preprocessed/test-data/natural_language_inference/23,"Following this idea , we propose deep fusion long short - term memory neural networks ( DF - LSTMs ) to model the interactions recursively .",29,0,26
dataset/preprocessed/test-data/natural_language_inference/23,"More concretely , DF - LSTMs consist of two interconnected conditional LSTMs , each of which models apiece of text under the influence of another .",30,0,26
dataset/preprocessed/test-data/natural_language_inference/23,The output vector of DF - LSTMs is fed into a task - specific output layer to compute the match - ing score .,31,0,24
dataset/preprocessed/test-data/natural_language_inference/23,The contributions of this paper can be summarized as follows .,32,0,11
dataset/preprocessed/test-data/natural_language_inference/23,"Different with previous models , DF - LSTMs model the strong interactions of two texts in a recursive matching way , which consist of two inter -and intra-dependent LSTMs .",34,0,30
dataset/preprocessed/test-data/natural_language_inference/23,"Compared to the previous works on text matching , we perform extensive empirical studies on two very large datasets .",36,0,20
dataset/preprocessed/test-data/natural_language_inference/23,Experiment results demonstrate that our proposed architecture is more effective .,37,0,11
dataset/preprocessed/test-data/natural_language_inference/23,"We present an elaborate qualitative analysis of our model , giving an intuitive understanding how our model worked .",39,0,19
dataset/preprocessed/test-data/natural_language_inference/23,Recursively Text Semantic Matching,40,0,4
dataset/preprocessed/test-data/natural_language_inference/23,"To facilitate our model , we firstly give some definitions .",41,0,11
dataset/preprocessed/test-data/natural_language_inference/23,"Given two sequences X = x 1 , x 2 , , x m and Y = y 1 , y 2 , , y n , most deep neural models try to represent their semantic relevance by a matching vector h( X , Y ) , which is followed by a score function to calculate the matching score .",42,0,60
dataset/preprocessed/test-data/natural_language_inference/23,"The weak interaction methods decompose matching vector by h( X , Y ) = f ( h ( X ) , h( Y ) ) , where function f ( ) maybe one of some basic operations or the combination of them : concatenation , affine transformation , bilinear , and soon .",43,0,53
dataset/preprocessed/test-data/natural_language_inference/23,"In this paper , we propose a strong interaction of two sequences to decompose matching vector h( X , Y ) in a recursive way .",44,0,26
dataset/preprocessed/test-data/natural_language_inference/23,"We refer to the interaction of the subsequences x 1:i and y 1:j ash i , j ( X , Y ) , which depends on previous interactions h s , t ( X , Y ) for 1 ? s < i and 1 ? t < j. gives an example to illustrate this .",45,0,56
dataset/preprocessed/test-data/natural_language_inference/23,"For sentence pair X = "" Female gymnast warm up before a competition "" , Y = "" Gymnast get ready fora competition "" , considering the interaction ) between x 1:4 = "" Female gymnast warm up "" and y 1:4 = "" Gymnast get ready for "" , which is composed by the interactions between their subsequences ( h 1 , 4 , , h 3 , 4 , h 4 ,1 , , h 4,3 ) .",46,0,80
dataset/preprocessed/test-data/natural_language_inference/23,We can see that a strong interaction between two sequences can be decomposed in recursive topology structure .,47,0,18
dataset/preprocessed/test-data/natural_language_inference/23,"The matching vector h i , j ( X , Y ) can be written as",48,0,16
dataset/preprocessed/test-data/natural_language_inference/23,"where h i , j ( X| Y ) refers to conditional encoding of subsequence x 1:i influenced by y 1 : j .",49,0,24
dataset/preprocessed/test-data/natural_language_inference/9,A BERT Baseline for the Natural Questions,2,1,7
dataset/preprocessed/test-data/natural_language_inference/9,"This technical note describes a new baseline for the Natural Questions ( Kwiatkowski et al. , 2019 ) .",4,0,19
dataset/preprocessed/test-data/natural_language_inference/9,"Our model is based on BERT ( Devlin et al. , 2018 ) and reduces the gap between the model F 1 scores reported in the original dataset paper and the human upper bound by 30 % and 50 % relative for the long and short answer tasks respectively .",5,0,50
dataset/preprocessed/test-data/natural_language_inference/9,"This baseline has been submitted to the official NQ leaderboard . Code , preprocessed data and pretrained model are available . https://ai.google.com/research/NaturalQuestions https://github.com/google-research/language/tree/",6,0,23
dataset/preprocessed/test-data/natural_language_inference/9,"master /language/question answering / bert joint * Also affiliated with Columbia University , work done at Google .",7,0,18
dataset/preprocessed/test-data/natural_language_inference/9,"The release of BERT has substantially advanced the state - of - the - art in a number of NLP tasks , in question answering in particular .",9,0,28
dataset/preprocessed/test-data/natural_language_inference/9,"For example , as of this writing , the top 17 systems on the SQuAD 2.0 leaderboard and the top 5 systems on the CoQA leaderboard are all based on BERT .",10,0,32
dataset/preprocessed/test-data/natural_language_inference/9,"The results obtained by BERT - based question answering models are also rapidly approaching the reported human performance for these datasets , with 2.5 F1 points of headroom left on SQuAD 2.0 and 6 F1 points on CoQA .",11,0,39
dataset/preprocessed/test-data/natural_language_inference/9,"We hypothesize that the Natural Questions ( NQ ) might represent a substantially harder research challenge than question answering tasks like SQuAD 2.0 and CoQA , and that consequently NQ might currently be a good benchmark for the NLP community to focus on .",12,0,44
dataset/preprocessed/test-data/natural_language_inference/9,"The qualities that we think make NQ more challenging than other question answering datasets are the following : ( 1 ) the questions in NQ were formulated by people out of genuine curiosity or out of need for an answer to complete another task , ( 2 ) the questions were formulated by people before they had seen the document that might contain the answer , ( 3 ) the documents in which the answer is to be found are much longer than the documents used in some of the existing question answering challenges .",13,0,95
dataset/preprocessed/test-data/natural_language_inference/9,In this technical note we describe a BERT - based model for the Natural Questions .,14,0,16
dataset/preprocessed/test-data/natural_language_inference/9,"BERT performs very well on this dataset , reducing the gap between the model F 1 scores reported in the original dataset paper and the human upper bound by 30 % and 50 % relative for the long and short answer tasks respectively .",15,0,44
dataset/preprocessed/test-data/natural_language_inference/9,"However , there is still ample room for improvement : 22.5 F1 points for the long answer task and 23 F1 points for the short answer task .",16,0,28
dataset/preprocessed/test-data/natural_language_inference/9,"The key insights in our approach are 1 . to jointly predict short and long answers in a single model rather than using a pipeline approach , 2 . to split each document into multiple training instances by using overlapping windows of tokens , like in the original BERT model for the SQuAD task , 3 . to aggressively downsample null instances ( i.e. instances without an answer ) at training time to create a balanced training set , 4 . to use the "" [ CLS ] "" token at training time to predict null instances and rank spans at inference time by the difference between the span score and the "" [ CLS ] "" score .",17,0,119
dataset/preprocessed/test-data/natural_language_inference/9,We refer to our model as BERT joint to emphasize the fact that we are modeling short and long answers in a single model rather than in a pipeline of two models .,18,0,33
dataset/preprocessed/test-data/natural_language_inference/9,"In the rest of this note we give further details on how the NQ dataset was preprocessed , we explain the modeling choices we made in our BERT - based model in order to adapt it to the NQ task , and we finally present our results .",19,0,48
dataset/preprocessed/test-data/natural_language_inference/9,"The Natural Questions ( NQ ) is a question answering dataset containing 307,373 training examples , 7,830 development examples , and 7,842 test examples .",21,0,25
dataset/preprocessed/test-data/natural_language_inference/9,Each example is comprised of a google.com query and a corresponding Wikipedia page .,22,0,14
dataset/preprocessed/test-data/natural_language_inference/9,Each Wikipedia page has a passage ( or long answer ) annotated on the page that answers the question and one or more short spans from the annotated passage containing the actual answer .,23,0,34
dataset/preprocessed/test-data/natural_language_inference/9,The long and the short answer annotations can however be empty .,24,0,12
dataset/preprocessed/test-data/natural_language_inference/9,"If they are both empty , then there is no answer on the page at all .",25,0,17
dataset/preprocessed/test-data/natural_language_inference/9,"If the long answer annotation is non-empty , but the short answer annotation is empty , then the annotated passage answers the question but no explicit short answer could be found .",26,0,32
dataset/preprocessed/test-data/natural_language_inference/9,"Finally 1 % of the documents have a passage annotated with a short answer that is "" yes "" or "" no "" , instead of a list of short spans .",27,0,32
dataset/preprocessed/test-data/natural_language_inference/9,"Following we tokenize every example in NQ using a 30,522 wordpiece vocabulary , then generate multiple instances per example by concatenating a "" [ CLS ] "" token , the tokenized question , a "" [ SEP ] "" token , tokens from the content of the document , and a final "" [ SEP ] "" token , limiting the total size of each instance to 512 tokens .",28,0,70
dataset/preprocessed/test-data/natural_language_inference/9,"For each document we generate all possible instances , by listing the document content starting at multiples of 128 tokens , effectively sliding a 512 token size window over the entire length of the document with astride of 128 tokens .",29,0,41
dataset/preprocessed/test-data/natural_language_inference/9,On average we generate 30 instances per NQ example .,30,0,10
dataset/preprocessed/test-data/natural_language_inference/9,Each instance will be processed independently by BERT .,31,0,9
dataset/preprocessed/test-data/natural_language_inference/9,For each training instance we compute start and end token indices to represent the target answer span .,32,0,18
dataset/preprocessed/test-data/natural_language_inference/9,"If all annotated short spans are contained in the instance , we set the start and end target indices to point to the smallest span containing all the annotated short answer spans .",33,0,33
dataset/preprocessed/test-data/natural_language_inference/9,"If there are no annotated short spans but there is an annotated long answer span completely contained in the instance , we set the start and end target indices to point to the entire long answer span .",34,0,38
dataset/preprocessed/test-data/natural_language_inference/9,"If no short or long span can be found in the current instance , we set the target start and end indices to point to the "" [ CLS ] "" token .",35,0,33
dataset/preprocessed/test-data/natural_language_inference/9,"We dub the instances in the last category "" null instances "" .",36,0,13
dataset/preprocessed/test-data/natural_language_inference/9,"Given the large size of documents in NQ and the fact that 51 % of the documents are annotated as not having an answer to the query at all , we find that about 98 % of generated instances are null , therefore for training we downsample null instances by 50 times in order to obtain a training set that has roughly as many null instances as non -null instances .",37,0,71
dataset/preprocessed/test-data/natural_language_inference/9,"This leads to a training set that has approximately 500,000 instances of 512 tokens each .",38,0,16
dataset/preprocessed/test-data/natural_language_inference/9,We introduce special markup tokens in the document to give the model a notion of which part of the document it is reading .,39,0,24
dataset/preprocessed/test-data/natural_language_inference/9,"The special tokens we introduced are of the form "" [ Paragraph = N ] "" , "" [ Table = N ] "" , and "" [ List = N ] "" at the beginning of the N - th paragraph , list and table respectively in the document .",40,0,51
dataset/preprocessed/test-data/natural_language_inference/9,This decision was based on the observation that the first few paragraphs and tables in the document are much more likely than the rest of the document to contain the annotated answer and so the model could benefit from knowing whether it is processing one of these passages .,41,0,49
dataset/preprocessed/test-data/natural_language_inference/9,"Special tokens are atomic , meaning that they are not split further by the wordpiece model .",42,0,17
dataset/preprocessed/test-data/natural_language_inference/9,"We finally compute for each instance a target answer type as one of five values : "" short "" for instances that contain all annotated short spans , "" yes "" and "" no "" for yes / no annotations where the instance contains the long answer span , "" long "" when the instance contains the long answer span but there is no short or yes / no answer , and "" noanswer "" otherwise .",43,0,77
dataset/preprocessed/test-data/natural_language_inference/9,"Null instances correspond to the set of instances with the "" no-answer "" target answer type .",44,0,17
dataset/preprocessed/test-data/natural_language_inference/9,"Formally , we define a training set instance as a four - tuple where c is a context of 512 wordpiece ids ( including question , document tokens and markup ) , s , e ? { 0 , 1 , . . . , 511 } are inclusive indices pointing to the start and end of the target answer span , and t ? { 0 , 1 , 2 , 3 , 4 } is the annotated answer type , corresponding to the labels "" short "" , "" long "" , "" yes "" , "" no "" , and "" no - answer "" .",46,0,109
dataset/preprocessed/test-data/natural_language_inference/9,We define the loss of our model fora training and return the highest scoring span in the document as the predicted short answer span .,47,0,25
dataset/preprocessed/test-data/natural_language_inference/9,"Note that g ( c , s , e ) is exactly the log - odds between the likelihood of an answer span ( defined by the product p start pend ) and the "" [ CLS ] "" span .",48,0,41
dataset/preprocessed/test-data/natural_language_inference/9,"We select the predicted long answer span as the DOM treetop level node containing the predicted short answer span , and assign to both long and short prediction the same score equal to the maximum value of g ( c , s , e ) for the document .",49,0,49
dataset/preprocessed/test-data/natural_language_inference/19,Natural Language Inference by Tree - Based Convolution and Heuristic Matching,2,1,11
dataset/preprocessed/test-data/natural_language_inference/19,"In this paper , we propose the TBCNNpair model to recognize entailment and contradiction between two sentences .",4,1,18
dataset/preprocessed/test-data/natural_language_inference/19,"In our model , a tree - based convolutional neural network ( TBCNN ) captures sentencelevel semantics ; then heuristic matching layers like concatenation , element - wise product / difference combine the information in individual sentences .",5,0,38
dataset/preprocessed/test-data/natural_language_inference/19,Experimental results show that our model outperforms existing sentence encoding - based approaches by a large margin .,6,0,18
dataset/preprocessed/test-data/natural_language_inference/19,Recognizing entailment and contradiction between two sentences ( called a premise and a hypothesis ) is known as natural language inference ( NLI ) in .,8,1,26
dataset/preprocessed/test-data/natural_language_inference/19,"Provided with a premise sentence , the task is to judge whether the hypothesis can be inferred ( entailment ) , or the hypothesis can not be true .",9,0,29
dataset/preprocessed/test-data/natural_language_inference/19,"Several examples are illustrated in NLI is in the core of natural language understanding and has wide applications in NLP , e.g. , question answering and automatic summarization .",10,1,29
dataset/preprocessed/test-data/natural_language_inference/19,"Moreover , NLI is also related to other tasks of sentence pair modeling , including paraphrase detection , relation recognition of discourse units , etc .",11,0,26
dataset/preprocessed/test-data/natural_language_inference/19,Traditional approaches to NLI mainly fall into two groups : feature - rich models and formal reasoning methods .,12,0,19
dataset/preprocessed/test-data/natural_language_inference/19,"Feature - based approaches typically leverage machine learning models , but require intensive human engineering to represent lexical and syntactic information in two sentences * Equal contribution .",13,0,28
dataset/preprocessed/test-data/natural_language_inference/19,Corresponding authors .,14,0,3
dataset/preprocessed/test-data/natural_language_inference/19,Two men on bicycles competing in a race .,16,0,9
dataset/preprocessed/test-data/natural_language_inference/19,People are riding bikes .,17,0,5
dataset/preprocessed/test-data/natural_language_inference/19,Men are riding bicycles on the streets .,19,0,8
dataset/preprocessed/test-data/natural_language_inference/19,CA few people are catching fish .,20,0,7
dataset/preprocessed/test-data/natural_language_inference/19,N : Examples of relations between a premise and a hypothesis :,21,0,12
dataset/preprocessed/test-data/natural_language_inference/19,"Entailment , Contradiction , and",22,0,5
dataset/preprocessed/test-data/natural_language_inference/19,Neutral ( irrelevant ) .,23,0,5
dataset/preprocessed/test-data/natural_language_inference/19,"Formal reasoning , on the other hand , converts a sentence into a formal logical representation and uses interpreters to search fora proof .",25,0,24
dataset/preprocessed/test-data/natural_language_inference/19,"However , such approaches are limited in terms of scope and accuracy .",26,0,13
dataset/preprocessed/test-data/natural_language_inference/19,"The renewed prosperity of neural networks has made significant achievements in various NLP applications , including individual sentence modeling as well as sentence matching .",27,0,25
dataset/preprocessed/test-data/natural_language_inference/19,"A typical neural architecture to model sentence pairs is the "" Siamese "" structure , which involves an underlying sentence model and a matching layer to determine the relationship between two sentences .",28,0,33
dataset/preprocessed/test-data/natural_language_inference/19,Prevailing sentence models include convolutional networks and recurrent / recursive networks .,29,0,12
dataset/preprocessed/test-data/natural_language_inference/19,"Although they have achieved high performance , they may either fail to fully make use of the syntactical information in sentences or be difficult to train due to the long propagation path .",30,0,33
dataset/preprocessed/test-data/natural_language_inference/19,"Recently , we propose a novel tree - based convolutional neural network ( TBCNN ) to alleviate the aforementioned problems and have achieved higher performance in two sentence classification tasks .",31,0,31
dataset/preprocessed/test-data/natural_language_inference/19,"However , it is less clear whether TBCNN can be harnessed to model sentence pairs for implicit logical inference , as is in the NLI task .",32,0,27
dataset/preprocessed/test-data/natural_language_inference/19,"In this paper , we propose the TBCNN - pair neural model to recognize entailment and contradiction between two sentences .",33,0,21
dataset/preprocessed/test-data/natural_language_inference/19,"We lever- age our newly proposed TBCNN model to capture structural information in sentences , which is important to NLI .",34,0,21
dataset/preprocessed/test-data/natural_language_inference/19,"For example , the phrase "" riding bicycles on the streets "" in can be well recognized by TBCNN via the dependency relations dobj ( riding , bicycles ) and prep on ( riding , street ) .",35,0,38
dataset/preprocessed/test-data/natural_language_inference/19,"As we can see , TBCNN is more robust than sequential convolution in terms of word order distortion , which maybe introduced by determinators , modifiers , etc .",36,0,29
dataset/preprocessed/test-data/natural_language_inference/19,"A pooling layer then aggregates information along the tree , serving as away of semantic compositonality .",37,0,17
dataset/preprocessed/test-data/natural_language_inference/19,"Finally , two sentences ' information is combined by several heuristic matching layers , including concatenation , element - wise product and difference ; they are effective in capturing relationships between two sentences , but remain low complexity .",38,0,39
dataset/preprocessed/test-data/natural_language_inference/19,"To sum up , the main contributions of this paper are two - fold :",39,0,15
dataset/preprocessed/test-data/natural_language_inference/19,"( 1 ) We are the first to introduce tree - based convolution to sentence pair modeling tasks like NLI ; ( 2 ) Leveraging additional heuristics further improves the accuracy while remaining low complexity , outperforming existing sentence encoding - based approaches to a large extent , including feature - rich methods and long short term memory ( LSTM ) - based recurrent networks .",40,0,66
dataset/preprocessed/test-data/natural_language_inference/19,Entailment recognition can be viewed as a task of sentence pair modeling .,43,0,13
dataset/preprocessed/test-data/natural_language_inference/19,"Most neural networks in this field involve a sentence - level model , followed by one or a few matching layers .",44,0,22
dataset/preprocessed/test-data/natural_language_inference/19,"They are sometimes called "" Siamese "" architectures .",45,0,9
dataset/preprocessed/test-data/natural_language_inference/19,"and Yin and Schtze ( 2015 ) apply convolutional neural networks ( CNNs ) as the individual sentence model , where a set of feature detectors over successive words are designed to extract local features .",46,0,36
dataset/preprocessed/test-data/natural_language_inference/19,build sentence pair models upon recurrent neural networks ( RNNs ) to iteratively integrate information along a sentence .,47,0,19
dataset/preprocessed/test-data/natural_language_inference/19,dynamically construct tree structures ( analogous to parse trees ) by recursive autoencoders to detect paraphrase between two sentences .,48,0,20
dataset/preprocessed/test-data/natural_language_inference/19,"As shown , inherent structural information in sentences is oftentimes important to natural language understanding .",49,0,16
dataset/preprocessed/test-data/natural_language_inference/22,Multi- task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems,2,1,12
dataset/preprocessed/test-data/natural_language_inference/22,Question Answering ( QA ) systems are used to provide proper responses to users ' questions automatically .,4,1,18
dataset/preprocessed/test-data/natural_language_inference/22,Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification ( PI ) problem .,5,1,23
dataset/preprocessed/test-data/natural_language_inference/22,"Given a question , the aim of the task is to find the most similar question from a QA knowledge base .",6,0,22
dataset/preprocessed/test-data/natural_language_inference/22,"In this paper , we propose a Multi - task Sentence Encoding Model ( MSEM ) for the PI problem , wherein a connected graph is employed to depict the relation between sentences , and a multi-task learning model is applied to address both the sentence matching and sentence intent classification problem .",7,0,53
dataset/preprocessed/test-data/natural_language_inference/22,"In addition , we implement a general semantic retrieval framework that combines our proposed model and the Approximate Nearest Neighbor ( ANN ) technology , which enables us to find the most similar question from all available candidates very quickly during online serving .",8,0,44
dataset/preprocessed/test-data/natural_language_inference/22,The experiments show the superiority of our proposed method as compared with the existing sentence matching models .,9,0,18
dataset/preprocessed/test-data/natural_language_inference/22,Question answering systems have been widely studied in both the academic and industrial community and are widely applied to various scenarios .,11,0,22
dataset/preprocessed/test-data/natural_language_inference/22,"There are full - blown applications like Amazon 's Alexa , Apple 's Siri , Baidu 's DuerOS , Google 's Assistant and Microsoft 's Cortana .",12,0,27
dataset/preprocessed/test-data/natural_language_inference/22,"Generally , there are two types of question answering systems : ( 1 ) information retrievalbased ( IR - based ) , and ( 2 ) generation - based .",13,0,30
dataset/preprocessed/test-data/natural_language_inference/22,"In this work , we focus on building an IR - based QA system to answer the Frequently Asked Questions ( FAQ ) .",14,0,24
dataset/preprocessed/test-data/natural_language_inference/22,"The critical part of IRbased QA system is to find the most similar question from a massive QA knowledge base , which could be further reformulated as a Paraphrase Identification ( PI ) problem , also known as sentence matching .",15,0,41
dataset/preprocessed/test-data/natural_language_inference/22,"In recent years , neural network models have achieved great success in sentence matching .",16,0,15
dataset/preprocessed/test-data/natural_language_inference/22,"Depends on whether to use crosssentence features or not , sentence - matching models can be classified roughly into two types : ( 1 ) encoding - based , and ( 2 ) interaction - based .",17,0,37
dataset/preprocessed/test-data/natural_language_inference/22,It is generally accepted that the interactionbased models could get better performance than the encodingbased models on certain datasets because they have abundant interaction features .,18,0,26
dataset/preprocessed/test-data/natural_language_inference/22,"However , the leaderboards of published large datasets such as SNLI and MultiNLI encourage conducting research on the encoding - based models around the semantic representation , because the encoding - based models can learn vector representations of individual sentences , which can be further applied to other natural language processing tasks .",19,0,53
dataset/preprocessed/test-data/natural_language_inference/22,Models in practical QA systems have two main disadvantages .,20,0,10
dataset/preprocessed/test-data/natural_language_inference/22,"Firstly , they consider the semantic sentence matching as a binary classification problem , assuming that samples are independent of one another as default .",21,0,25
dataset/preprocessed/test-data/natural_language_inference/22,"However , the paraphrase relation between sentences could be transmitted .",22,0,11
dataset/preprocessed/test-data/natural_language_inference/22,"For example , if question1 and question2 are paraphrases , and question2 and question3 are paraphrases , we can infer that question1 and question3 are also paraphrases .",23,0,28
dataset/preprocessed/test-data/natural_language_inference/22,"Secondly , because of the hard time delay constraint in the online prediction procedure of a traditional IR - based QA system , as shown in , existing models often play the role of a re-rank module that depends on the results from the question analysis and recall module .",24,0,50
dataset/preprocessed/test-data/natural_language_inference/22,"Thus they could only re-rank a few candidates from term - based index recall modules like Lucene , instead of retrieving the most similar question from all candidates .",25,0,29
dataset/preprocessed/test-data/natural_language_inference/22,"In this paper , we aim to address these two challenges .",26,0,12
dataset/preprocessed/test-data/natural_language_inference/22,The main contributions of this work are summarized as follows :,27,0,11
dataset/preprocessed/test-data/natural_language_inference/22,"We employ a connected graph to depict the paraphrase relation between sentences for the PI task , and propose a multi-task sentence - encoding model , which solves the paraphrase identification task and the sentence intent classification task simultaneously .",28,0,40
dataset/preprocessed/test-data/natural_language_inference/22,"We propose a semantic retrieval framework that integrates the encoding - based sentence matching model with the approximate nearest neighbor search technology , which allows us to find the most similar question very quickly from all available questions , instead of within only a few candidates , in the QA knowledge base .",29,0,53
dataset/preprocessed/test-data/natural_language_inference/22,We evaluated our proposed method on various QA datasets and the experimental results show the effectiveness and superiority of our method .,30,0,22
dataset/preprocessed/test-data/natural_language_inference/22,"First , it proves that we can achieve better performance with multi -task learning .",31,0,15
dataset/preprocessed/test-data/natural_language_inference/22,"Besides , our method can achieve state - of - the - art performance compared with existing encoding - based models and interaction - based models .",32,0,27
dataset/preprocessed/test-data/natural_language_inference/22,A. Natural Language Sentence Matching,35,0,5
dataset/preprocessed/test-data/natural_language_inference/22,Natural language sentence matching ( NLSM ) has gone through substantial developments in recent years .,36,0,16
dataset/preprocessed/test-data/natural_language_inference/22,It plays a central role in a large number of natural language processing tasks .,37,0,15
dataset/preprocessed/test-data/natural_language_inference/22,"For the paraphrase identification ( PI ) task , NLSM is utilized to determine whether two sentences are paraphrases or not .",38,0,22
dataset/preprocessed/test-data/natural_language_inference/22,The developments of deep neural networks and the emergence of large - scale annotated datasets have led great progress on NLSM tasks .,39,0,23
dataset/preprocessed/test-data/natural_language_inference/22,"Depending on whether the crosssentence features or attention from one sentence to another were used , two types of deep neural network models have been proposed for NLSM .",40,0,29
dataset/preprocessed/test-data/natural_language_inference/22,"The first type of models is encoding - based , where sentences are encoded into sentence vectors without any cross interaction , then the matching decision is made solely based on the two sentence vectors .",41,0,36
dataset/preprocessed/test-data/natural_language_inference/22,"Typical representatives of such methods include Stack - augmented Parser - Interpreter Neural Network ( SPINN ) , Shortcut - Stacked Sentence Encoders ( SSE ) , or Gumbel Tree - LSTM .",42,0,33
dataset/preprocessed/test-data/natural_language_inference/22,"The other type of methods , called interaction - based model , make use of cross interaction of small units ( such as words ) to express word - level or phrase - level alignments for performance improvements .",43,0,39
dataset/preprocessed/test-data/natural_language_inference/22,"The main representatives are Enhanced Sequential Inference Model ( ESIM ) , Bilateral Multi - Perspective Matching Model ( BiMPM ) , and Densely Interactive Inference Network ( DIIN ) model .",44,0,32
dataset/preprocessed/test-data/natural_language_inference/22,"Generally , the second type of methods captures more interactive features between the two input sentences , so it can achieve better performance .",45,0,24
dataset/preprocessed/test-data/natural_language_inference/22,"On the other hand , the encoding - based model is much smaller and easier to train , and the vector representations can be further used for sentence clustering , semantic search , visualization and many other tasks .",46,0,39
dataset/preprocessed/test-data/natural_language_inference/22,"The advantages of encoding - based models are much more significant to QA systems in the industry , so we focus on the research of encoding - based models .",47,0,30
dataset/preprocessed/test-data/natural_language_inference/22,B. Approximate Nearest Neighbor,48,0,4
dataset/preprocessed/test-data/natural_language_inference/22,"Approximate nearest neighbor ( ANN ) search has been a hot topic over decades and plays an important role in machine learning , computer vision and information retrieval etc .",49,0,30
dataset/preprocessed/test-data/natural_language_inference/1,Cut to the Chase : A Context Zoom - in Network for Reading Comprehension,2,1,14
dataset/preprocessed/test-data/natural_language_inference/1,In recent years many deep neural networks have been proposed to solve Reading Comprehension ( RC ) tasks .,4,1,19
dataset/preprocessed/test-data/natural_language_inference/1,Most of these models suffer from reasoning overlong documents and do not trivially generalize to cases where the answer is not present as a span in a given document .,5,0,30
dataset/preprocessed/test-data/natural_language_inference/1,We present a novel neural - based architecture that is capable of extracting relevant regions based on a given question - document pair and generating a well - formed answer .,6,0,31
dataset/preprocessed/test-data/natural_language_inference/1,"To show the effectiveness of our architecture , we conducted several experiments on the recently proposed and challenging RC dataset ' Nar - rative QA ' .",7,1,27
dataset/preprocessed/test-data/natural_language_inference/1,"The proposed architecture outperforms state - of - the - art results ( Tay et al. , 2018 ) by 12.62 % ( ROUGE - L ) relative improvement .",8,0,30
dataset/preprocessed/test-data/natural_language_inference/1,Building Artificial Intelligence ( AI ) algorithms to teach machines to read and to comprehend text is a long - standing challenge in Natural Language Processing ( NLP ) .,10,0,30
dataset/preprocessed/test-data/natural_language_inference/1,A common strategy for assessing these AI algorithms is by treating them as RC tasks .,11,0,16
dataset/preprocessed/test-data/natural_language_inference/1,This can be formulated as finding an answer to a question given the document ( s ) as evidence .,12,0,20
dataset/preprocessed/test-data/natural_language_inference/1,"Recently , many deep - learning based models have been proposed to solve RC tasks based on the SQuAD and Trivi - aQA datasets , reaching human level performance .",13,0,30
dataset/preprocessed/test-data/natural_language_inference/1,A common approach in these models is to score and / or extract candidate spans conditioned on a given question - document pair .,14,0,24
dataset/preprocessed/test-data/natural_language_inference/1,Most of these models have limited applicability to real problems for the following reasons .,15,0,15
dataset/preprocessed/test-data/natural_language_inference/1,"They do not generalize well to scenarios where the answer is not present as a span , or where several discontinuous parts of the document are required to * To whom correspondence should be addressed .",16,0,36
dataset/preprocessed/test-data/natural_language_inference/1,form the answer .,17,0,4
dataset/preprocessed/test-data/natural_language_inference/1,"In addition , unlike humans , they can not easily skip through irrelevant parts to comprehend long documents .",18,0,19
dataset/preprocessed/test-data/natural_language_inference/1,"To address the issues above we develop a novel context zoom - in network ( ConZNet ) for RC tasks , which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text .",19,0,42
dataset/preprocessed/test-data/natural_language_inference/1,The ConZNet architecture consists of two phases .,20,0,8
dataset/preprocessed/test-data/natural_language_inference/1,In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm .,21,0,18
dataset/preprocessed/test-data/natural_language_inference/1,"These relevant regions are not only useful to generate the answer , but can also be presented to the user as supporting information along with the answer .",22,0,28
dataset/preprocessed/test-data/natural_language_inference/1,"The second phase is based on an encoder - decoder architecture , which comprehends the identified regions of text and generates the answer by using a residual self - attention network as encoder and a RNNbased sequence generator along with a pointer network as the decoder .",23,0,47
dataset/preprocessed/test-data/natural_language_inference/1,It has the ability to generate better well - formed answers not verbatim present in the document than span prediction models .,24,0,22
dataset/preprocessed/test-data/natural_language_inference/1,"Recently , there have been several attempts to adopt condensing documents in RC tasks .",25,0,15
dataset/preprocessed/test-data/natural_language_inference/1,retrieve a relevant paragraph based on the question and predict the answer span . select sentence ( s ) to make a summary of the entire document with a feed - forward network and generate an answer based on the summary .,26,0,42
dataset/preprocessed/test-data/natural_language_inference/1,"Unlike existing approaches , our method has the ability to select relevant regions of text not just based on the question but also on how well regions are related to each other .",27,0,33
dataset/preprocessed/test-data/natural_language_inference/1,"Moreover , our decoder combines span prediction and sequence generation .",28,0,11
dataset/preprocessed/test-data/natural_language_inference/1,This allows the decoder to copy words from the relevant regions of text as well as to generate words from a fixed vocabulary .,29,0,24
dataset/preprocessed/test-data/natural_language_inference/1,"We evaluate our model using one of the challenging RC datasets , called ' NarrativeQA ' , which was released recently by .",30,0,23
dataset/preprocessed/test-data/natural_language_inference/1,Experimental results show the usefulness of our framework for RC tasks and we outperform stateof - the - art results on this dataset .,31,0,24
dataset/preprocessed/test-data/natural_language_inference/1,"An overview of our architecture is shown in , which consists of two phases .",33,0,15
dataset/preprocessed/test-data/natural_language_inference/1,"First , the identification of relevant regions of text is computed by the Co-attention and Context Zoom layers as explained in Sections 2.1 and 2.2 .",34,0,26
dataset/preprocessed/test-data/natural_language_inference/1,"Second , the comprehension of identified regions of text and output generation is computed by Answer Generation block as explained in Section 2.3 .",35,0,24
dataset/preprocessed/test-data/natural_language_inference/1,"The words in the document , question and answer are represented using pre-trained word embeddings .",37,0,16
dataset/preprocessed/test-data/natural_language_inference/1,These wordbased embeddings are concatenated with their corresponding char embeddings .,38,0,11
dataset/preprocessed/test-data/natural_language_inference/1,The char embeddings are learned by feeding all the characters of a word into a Convolutional Neural Network ( CNN ) .,39,0,22
dataset/preprocessed/test-data/natural_language_inference/1,We further encode the document and question embeddings using a shared bi-directional GRU to get context - aware representations .,40,0,20
dataset/preprocessed/test-data/natural_language_inference/1,We compute the co-attention between document and question to get question - aware representations for the document by using tri-linear attention as proposed by .,41,0,25
dataset/preprocessed/test-data/natural_language_inference/1,"Let d i be the vector representation for the document word i , q j be the vector for the question word j , and l d and l q be the lengths of the document and question respectively .",42,0,40
dataset/preprocessed/test-data/natural_language_inference/1,The tri-linear attention is calculated as,43,0,6
dataset/preprocessed/test-data/natural_language_inference/1,"where w d , w q , and w dq are learnable parameters and denotes the element - wise multiplication .",44,0,21
dataset/preprocessed/test-data/natural_language_inference/1,We compute the attended document wordd i by first computing ?,45,0,11
dataset/preprocessed/test-data/natural_language_inference/1,i = sof tmax ( a i : ) and followed byd i = lq j=1 ?,46,0,17
dataset/preprocessed/test-data/natural_language_inference/1,ij q j .,47,0,4
dataset/preprocessed/test-data/natural_language_inference/1,"Similarly , we compute a question to document attention vectorq by first computing b = sof tmax ( max ( a i : )) and followed byq",48,0,27
dataset/preprocessed/test-data/natural_language_inference/1,to yield a query - aware contextual representation for each word in the document .,49,0,15
dataset/preprocessed/test-data/natural_language_inference/5,Sentence Similarity Learning by Lexical Decomposition and Composition,2,1,8
dataset/preprocessed/test-data/natural_language_inference/5,"Most conventional sentence similarity methods only focus on similar parts of two input sentences , and simply ignore the dissimilar parts , which usually give us some clues and semantic meanings about the sentences .",4,1,35
dataset/preprocessed/test-data/natural_language_inference/5,"In this work , we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences .",5,0,26
dataset/preprocessed/test-data/natural_language_inference/5,"The model represents each word as a vector , and calculates a semantic matching vector for each word based on all words in the other sentence .",6,0,27
dataset/preprocessed/test-data/natural_language_inference/5,"Then , each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector .",7,0,22
dataset/preprocessed/test-data/natural_language_inference/5,"After this , a two - channel CNN model is employed to capture features by composing the similar and dissimilar components .",8,0,22
dataset/preprocessed/test-data/natural_language_inference/5,"Finally , a similarity score is estimated over the composed feature vectors .",9,0,13
dataset/preprocessed/test-data/natural_language_inference/5,"Experimental results show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",10,0,34
dataset/preprocessed/test-data/natural_language_inference/5,Sentence similarity is a fundamental metric to measure the degree of likelihood between a pair of sentences .,11,0,18
dataset/preprocessed/test-data/natural_language_inference/5,It plays an important role for a variety of tasks in both NLP and IR communities .,12,0,17
dataset/preprocessed/test-data/natural_language_inference/5,"For example , in paraphrase identification task , sentence similarity is used to determine whether two sentences are paraphrases or not ( Yin and Schtze , 2015 ; He et al. , 2015 ) .",13,0,35
dataset/preprocessed/test-data/natural_language_inference/5,"For question answering and information retrieval tasks , sentence similarities between query - answer pairs are used for assessing the relevance and ranking all the candidate answers ( Severyn and Moschitti , 2015 ; Wang and Ittycheriah , 2015 ) .",14,0,41
dataset/preprocessed/test-data/natural_language_inference/5,"However , sentence similarity learning has following challenges :",15,0,9
dataset/preprocessed/test-data/natural_language_inference/5,There is a lexical gap between semantically equivalent sentences .,17,0,10
dataset/preprocessed/test-data/natural_language_inference/5,"Take the E 1 and E 2 in Table 1 for example , they have the similar meaning but with different lexicons .",18,0,23
dataset/preprocessed/test-data/natural_language_inference/5,"2 . Semantic similarity should be measured at different levels of granularity ( word - level , phrase - level and syntax - level ) .",19,0,26
dataset/preprocessed/test-data/natural_language_inference/5,"E.g. , "" not related "" in E 2 is an indivisible phrase when matching with "" irrelevant "" in E 1 ( shown in square brackets ) .",20,0,29
dataset/preprocessed/test-data/natural_language_inference/5,"The dissimilarity ( shown in angle brackets ) between two sentences is also a significant clue ( Qiu et al. , 2006 ) .",22,0,24
dataset/preprocessed/test-data/natural_language_inference/5,"For example , by judging the dissimilar parts , we can easily identify that E 3 and E 5 share the similar meaning "" The study is about salmon "" , because "" sockeye "" belongs to the salmon family , and "" flounder "" does not .",23,0,48
dataset/preprocessed/test-data/natural_language_inference/5,"Whereas the meaning of E 4 is quite different from E 3 , which emphasizes "" The study is about red ( a special kind of ) salmon "" , because both "" sockeye "" and "" coho "" are in the salmon family .",24,0,45
dataset/preprocessed/test-data/natural_language_inference/5,How we can extract and utilize those information becomes another challenge .,25,0,12
dataset/preprocessed/test-data/natural_language_inference/5,"In order to handle the above challenges , researchers have been working on sentence similarity algorithms for a longtime .",26,0,20
dataset/preprocessed/test-data/natural_language_inference/5,"To bridge the lexical gap ( challenge 1 ) , some word similarity metrics were proposed to match different but semantically related words .",27,0,24
dataset/preprocessed/test-data/natural_language_inference/5,"Examples include knowledge - based metrics ( Resnik , 1995 ) and corpus - based metrics ( Jiang and Conrath , 1997 ; Yin and Schtze , 2015 ; He et al. , 2015 ) .",28,0,36
dataset/preprocessed/test-data/natural_language_inference/5,"To measure sentence similarity from various granularities ( challenge 2 ) , researchers have explored features extracted from n-grams , continuous phrases , discontinuous phrases , and parse trees ( Yin and Schtze , 2015 ; He et al. , 2015 ; Heilman and Smith , 2010 ) .",29,0,49
dataset/preprocessed/test-data/natural_language_inference/5,The third challenge did not get much,30,0,7
dataset/preprocessed/test-data/natural_language_inference/5,The research is to sockeye .,33,0,6
dataset/preprocessed/test-data/natural_language_inference/5,The study is [ not related ] to salmon .,35,0,10
dataset/preprocessed/test-data/natural_language_inference/5,The research is relevant to salmon .,37,0,7
dataset/preprocessed/test-data/natural_language_inference/5,"The study is relevant to sockeye , instead of coho .",39,0,11
dataset/preprocessed/test-data/natural_language_inference/5,"The study is relevant to sockeye , rather than flounder .:",41,0,11
dataset/preprocessed/test-data/natural_language_inference/5,"Examples for sentence similarity learning , where sockeye means "" red salmon "" , and coho means "" silver salmon "" .",42,0,22
dataset/preprocessed/test-data/natural_language_inference/5,""" coho "" and "" sockeye "" are in the salmon family , while "" flounder "" is not .",43,0,20
dataset/preprocessed/test-data/natural_language_inference/5,"attention in the past , the only related work of explored the dissimilarity between sentences in a pair for paraphrase identification task , but they require human annotations in order to train a classifier , and their performance is still below the state of the art .",44,0,47
dataset/preprocessed/test-data/natural_language_inference/5,"In this paper , we propose a novel model to tackle all these challenges jointly by decomposing and composing lexical semantics over sentences .",45,0,24
dataset/preprocessed/test-data/natural_language_inference/5,"Given a sentence pair , the model represents each word as a low -dimensional vector ( challenge 1 ) , and calculates a semantic matching vector for each word based on all words in the other sentence ( challenge 2 ) .",46,0,42
dataset/preprocessed/test-data/natural_language_inference/5,"Then based on the semantic matching vector , each word vector is decomposed into two components : a similar component and a dissimilar component ( challenge 3 ) .",47,0,29
dataset/preprocessed/test-data/natural_language_inference/5,"We use similar components of all the words to represent the similar parts of the sentence pair , and dissimilar components of every word to model the dissimilar parts explicitly .",48,0,31
dataset/preprocessed/test-data/natural_language_inference/5,"After this , a two - channel CNN operation is performed to compose the similar and dissimilar components into a feature vector ( challenge 2 and 3 ) .",49,0,29
dataset/preprocessed/test-data/natural_language_inference/27,SDNET : CONTEXTUALIZED ATTENTION - BASED DEEP NETWORK FOR CONVERSATIONAL QUESTION AN - SWERING,2,1,14
dataset/preprocessed/test-data/natural_language_inference/27,Conversational question answering ( CQA ) is a novel QA task that requires understanding of dialogue context .,4,1,18
dataset/preprocessed/test-data/natural_language_inference/27,"Different from traditional single - turn machine reading comprehension ( MRC ) tasks , CQA includes passage comprehension , coreference resolution , and contextual understanding .",5,1,26
dataset/preprocessed/test-data/natural_language_inference/27,"In this paper , we propose an innovated contextualized attention - based deep neural network , SDNet , to fuse context into traditional MRC models .",6,0,26
dataset/preprocessed/test-data/natural_language_inference/27,Our model leverages both inter-attention and self - attention to comprehend conversation context and extract relevant information from passage .,7,0,20
dataset/preprocessed/test-data/natural_language_inference/27,"Furthermore , we demonstrated a novel method to integrate the latest BERT contextual model .",8,0,15
dataset/preprocessed/test-data/natural_language_inference/27,"Empirical results show the effectiveness of our model , which sets the new state of the art result in CoQA leaderboard , outperforming the previous best model by 1.6 % F 1 .",9,0,33
dataset/preprocessed/test-data/natural_language_inference/27,Our ensemble model further improves the result by 2.7 % F 1 .,10,0,13
dataset/preprocessed/test-data/natural_language_inference/27,Traditional machine reading comprehension ( MRC ) tasks share the single - turn setting of answering a single question related to a passage .,12,0,24
dataset/preprocessed/test-data/natural_language_inference/27,There is usually no connection between different questions and answers to the same passage .,13,0,15
dataset/preprocessed/test-data/natural_language_inference/27,"However , the most natural way humans seek answers is via conversation , which carries over context through the dialogue flow .",14,0,22
dataset/preprocessed/test-data/natural_language_inference/27,"To incorporate conversation into reading comprehension , recently there are several public datasets that evaluate QA model 's efficacy in conversational setting , such as CoQA , QuAC and QBLink .",15,0,31
dataset/preprocessed/test-data/natural_language_inference/27,"In these datasets , to generate correct responses , models are required to fully understand the given passage as well as the context of previous questions and answers .",16,0,29
dataset/preprocessed/test-data/natural_language_inference/27,"Thus , traditional neural MRC models are not suitable to be directly applied to this scenario .",17,0,17
dataset/preprocessed/test-data/natural_language_inference/27,"Existing approaches to conversational QA tasks include BiDAF + + , , DrQA + PGNet , which all try to find the optimal answer span given the passage and dialogue history .",18,0,32
dataset/preprocessed/test-data/natural_language_inference/27,"In this paper , we propose SDNet , a contextual attention - based deep neural network for the task of conversational question answering .",19,0,24
dataset/preprocessed/test-data/natural_language_inference/27,"Our network stems from machine reading comprehension models , but has several unique characteristics to tackle contextual understanding during conversation .",20,0,21
dataset/preprocessed/test-data/natural_language_inference/27,"Firstly , we apply both inter-attention and self - attention on passage and question to obtain a more effective understanding of the passage and dialogue history .",21,0,27
dataset/preprocessed/test-data/natural_language_inference/27,"Secondly , SDNet leverages the latest breakthrough in NLP : BERT contextual embedding .",22,0,14
dataset/preprocessed/test-data/natural_language_inference/27,"Different from the canonical way of appending a thin layer after BERT structure according to , we innovatively employed a weighted sum of BERT layer outputs , with locked BERT parameters .",23,0,32
dataset/preprocessed/test-data/natural_language_inference/27,"Thirdly , we prepend previous rounds of questions and answers to the current question to incorporate contextual information .",24,0,19
dataset/preprocessed/test-data/natural_language_inference/27,Empirical results show that each of these components has substantial gains in prediction accuracy .,25,0,15
dataset/preprocessed/test-data/natural_language_inference/27,"We evaluated SDNet on CoQA dataset , which improves the previous state - of - the - art model 's result by 1.6 % ( from 75.0 % to 76.6 % ) overall F 1 score .",26,0,37
dataset/preprocessed/test-data/natural_language_inference/27,The ensemble model further increase the F 1 score to 79.3 % .,27,0,13
dataset/preprocessed/test-data/natural_language_inference/27,"Moreover , SDNet is the first model ever to pass 80 % on CoQA 's in - domain dataset .",28,0,20
dataset/preprocessed/test-data/natural_language_inference/27,"In this section , we propose the neural model , SDNet , for the conversational question answering task , which is formulated as follows .",30,0,25
dataset/preprocessed/test-data/natural_language_inference/27,"Given a passage C , and history question and answer utterances Q 1 , A 1 , Q 2 , A 2 , ... , Q k?1 , A k?1 , the task is to generate response A k given the latest question Q k  .",31,0,46
dataset/preprocessed/test-data/natural_language_inference/27,The response is dependent on both the passage and history utterances .,32,0,12
dataset/preprocessed/test-data/natural_language_inference/27,"To incorporate conversation history into response generation , we employ the idea from DrQA + PGNet to prepend the latest N rounds of utterances to the current question Q k  .",33,0,31
dataset/preprocessed/test-data/natural_language_inference/27,The problem is then converted into a machine reading comprehension task .,34,0,12
dataset/preprocessed/test-data/natural_language_inference/27,"In other words , the reformulate question is Q k = { Q k?N ; A k?N ; ... , Q k?1 ; A k?1 ; Q k }.",35,0,29
dataset/preprocessed/test-data/natural_language_inference/27,"To differentiate between question and answering , we add symbol Q before each question and A before each answer in the experiment .",36,0,23
dataset/preprocessed/test-data/natural_language_inference/27,"Encoding layer encodes each token in passage and question into a fixed - length vector , which includes both word embeddings and contextualized embeddings .",38,0,25
dataset/preprocessed/test-data/natural_language_inference/27,"For contextualized embedding , we utilize the latest result from BERT .",39,0,12
dataset/preprocessed/test-data/natural_language_inference/27,"Different from previous work , we fix the parameters in BERT model and use the linear combination of embeddings from different layers in BERT .",40,0,25
dataset/preprocessed/test-data/natural_language_inference/27,Integration layer uses multi - layer recurrent neural networks ( RNN ) to capture contextual information within passage and question .,41,0,21
dataset/preprocessed/test-data/natural_language_inference/27,"To characterize the relationship between passage and question , we conduct word - level attention from question to passage both before and after the RNNs .",42,0,26
dataset/preprocessed/test-data/natural_language_inference/27,We employ the idea of history - of - word from FusionNet to reduce the dimension of output hidden vectors .,43,0,21
dataset/preprocessed/test-data/natural_language_inference/27,"Furthermore , we conduct self - attention to extract relationship between words at different positions of context and question .",44,0,20
dataset/preprocessed/test-data/natural_language_inference/27,Output layer computes the final answer span .,45,0,8
dataset/preprocessed/test-data/natural_language_inference/27,"It uses attention to condense the question into a fixedlength vector , which is then used in a bilinear projection to obtain the probability that the answer should start and end at each position .",46,0,35
dataset/preprocessed/test-data/natural_language_inference/27,An illustration of our model SDNet is in .,47,0,9
dataset/preprocessed/test-data/natural_language_inference/27,We use 300 - dim Glo Ve embedding and contextualized embedding for each word in context and question .,49,0,19
dataset/preprocessed/test-data/natural_language_inference/18,Parameter Re-Initialization through Cyclical Batch Size Schedules,2,1,7
dataset/preprocessed/test-data/natural_language_inference/18,Optimal parameter initialization remains a crucial problem for neural network training .,4,1,12
dataset/preprocessed/test-data/natural_language_inference/18,A poor weight initialization may take longer to train and / or converge to sub-optimal solutions .,5,0,17
dataset/preprocessed/test-data/natural_language_inference/18,"Here , we propose a method of weight re-initialization by repeated annealing and injection of noise in the training process .",6,0,21
dataset/preprocessed/test-data/natural_language_inference/18,We implement this through a cyclical batch size schedule motivated by a Bayesian perspective of neural network training .,7,0,19
dataset/preprocessed/test-data/natural_language_inference/18,"We evaluate our methods through extensive experiments on tasks in language modeling , natural language inference , and image classification .",8,0,21
dataset/preprocessed/test-data/natural_language_inference/18,"We demonstrate the ability of our method to improve language modeling performance by up to 7.91 perplexity and reduce training iterations by up to 61 % , in addition to its flexibility in enabling snapshot ensembling and use with adversarial training .",9,0,42
dataset/preprocessed/test-data/natural_language_inference/18,"Despite many promising empirical results at using stochastic optimization methods to train highly non-convex modern deep neural networks , we still lack theoretically robust practical methods which are able to escape saddle points and / or sub-optimal local minima and converge to parameters that retain high testing performance .",11,0,49
dataset/preprocessed/test-data/natural_language_inference/18,This lack of understanding leads to practical training challenges .,12,0,10
dataset/preprocessed/test-data/natural_language_inference/18,Stochastic Gradient Descent ( SGD ) is currently the de-facto optimization method for training deep neural networks ( DNNs ) .,13,0,21
dataset/preprocessed/test-data/natural_language_inference/18,"Through extensive hyper - parameter tuning , SGD can avoid poor local optima and achieve good generalization ability .",14,0,19
dataset/preprocessed/test-data/natural_language_inference/18,One important hyper - parameter that can significantly affect SGD performance is the weight initialization .,15,0,16
dataset/preprocessed/test-data/natural_language_inference/18,"For instance , initializing the weights to all zeros or all ones leads to extremely poor performance .",16,0,18
dataset/preprocessed/test-data/natural_language_inference/18,"Different approaches have been proposed for weight initialization such as Xavier , MSRA , Ortho , LSUV .",17,0,18
dataset/preprocessed/test-data/natural_language_inference/18,These are mostly agnostic to the model architecture and the specific learning task .,18,0,14
dataset/preprocessed/test-data/natural_language_inference/18,Our work explores the idea of adapting the weight initialization to the optimization dynamics of the specific learning task at hand .,19,0,22
dataset/preprocessed/test-data/natural_language_inference/18,"From the Bayesian perspective , improved weight initialization can be viewed as starting with a better prior , which leads to a more accurate posterior and thus better generalization ability .",20,0,31
dataset/preprocessed/test-data/natural_language_inference/18,This problem has been explored extensively in Bayesian optimization .,21,0,10
dataset/preprocessed/test-data/natural_language_inference/18,"For example , in the seminal works , an adaptive prior is implemented via Markov Chain Monte Carlo ( MCMC ) methods .",22,0,23
dataset/preprocessed/test-data/natural_language_inference/18,"Motivated by these ideas , we incorporate an "" adaptive initialization "" for neural network training ( see section 2 for details ) , where we use cyclical batch size schedules to control the noise ( or temperature ) of SGD .",23,0,42
dataset/preprocessed/test-data/natural_language_inference/18,"As argued in , both learning rate and batch size can be used to control the noise of SGD but the latter has an advantage in that it allows more parallelization opportunity .",24,0,33
dataset/preprocessed/test-data/natural_language_inference/18,The idea of using batch size to control the noise in a simple cyclical schedule was recently proposed in .,25,0,20
dataset/preprocessed/test-data/natural_language_inference/18,"Here , we build upon this work by studying different cyclical annealing strategies for a wide range of problems .",26,0,20
dataset/preprocessed/test-data/natural_language_inference/18,"Additionally , we discuss how this can be combined with anew adversarial regularization scheme recently proposed in , as well as prior work in order to obtain ensembles of models at no additional cost .",27,0,35
dataset/preprocessed/test-data/natural_language_inference/18,"In summary , our contributions are as follows :",28,0,9
dataset/preprocessed/test-data/natural_language_inference/18,"We explore different cyclical batch size ( CBS ) schedules for training neural networks inspired by Bayesian statistics , particularly adaptive MCMC methods .",29,0,24
dataset/preprocessed/test-data/natural_language_inference/18,The CBS schedule leads to multiple perplexity improvement ( up to 7.91 ) in language modeling and minor improvements in natural language inference and image classification .,30,0,27
dataset/preprocessed/test-data/natural_language_inference/18,"Furthermore , we show that CBS schedule can alleviate problems with overfitting and sub-optimal parameter initialization .",31,0,17
dataset/preprocessed/test-data/natural_language_inference/18,"Additionally , CBS schedules require up to 3 fewer SGD iterations due to larger batch sizes , which allows for more parallelization opportunity .",32,0,24
dataset/preprocessed/test-data/natural_language_inference/18,This reflects the benefit of cycling the batch size instead of the learning rate as in prior work We showcase the flexibility of CBS schedules for use with additional techniques .,33,0,31
dataset/preprocessed/test-data/natural_language_inference/18,We propose a simple but effective ensembling method that combines models saved during different cycles at no additional training cost .,34,0,21
dataset/preprocessed/test-data/natural_language_inference/18,"In addition , we show that CBS schedule can be combined with other approaches such as the recently proposed adversarial regularization to yield further classification accuracy improvement of 0.26 % .",35,0,31
dataset/preprocessed/test-data/natural_language_inference/18,"[ 5 ] introduced Xavier initialization , which keeps the variance of input and output of all layers within a similar range in order to prevent vanishing or exploding values in both the forward and backward passes .",37,0,38
dataset/preprocessed/test-data/natural_language_inference/18,"Building off this idea , explored anew strategy known as MSRA to keep the variance constant for all convolutional layers .",38,0,21
dataset/preprocessed/test-data/natural_language_inference/18,"proposed an orthogonal initialization ( Ortho ) to achieve faster convergence , and more recently , combined ideas from previous work and showed that a unit variance orthogonal initialization is beneficial for deep models .",39,0,35
dataset/preprocessed/test-data/natural_language_inference/18,show that the noise of SGD is controlled by the ratio of learning rate to batch size .,40,0,18
dataset/preprocessed/test-data/natural_language_inference/18,The authors argued that the SGD algorithm can be derived through Euler - Maruyama discretization of a Stochastic Differential Equation ( SDE ) .,41,0,24
dataset/preprocessed/test-data/natural_language_inference/18,"The SDE dynamics are governed by a "" noise scale "" g ?",42,0,13
dataset/preprocessed/test-data/natural_language_inference/18,"N / B for the learning rate , N the training dataset size , and B the batch size .",43,0,20
dataset/preprocessed/test-data/natural_language_inference/18,They conclude that a higher noise scale prevents SGD from settling into sharper minima .,44,0,15
dataset/preprocessed/test-data/natural_language_inference/18,"This result supports a prior empirical observation that under certain mild assumptions such as NB , the effect of dividing the learning rate by a constant factor is equivalent to that of multiplying the batch size by the same constant factor .",45,0,42
dataset/preprocessed/test-data/natural_language_inference/18,"In related work , applied this understanding and used batch size as a knob to control the noise , and empirically showed that the baseline performance could be matched .",46,0,30
dataset/preprocessed/test-data/natural_language_inference/18,further explored how to use second - order information and adversarial training to control the noise for training large batch size .,47,0,22
dataset/preprocessed/test-data/natural_language_inference/18,"showed using a statistical mechanics argument that many other hyper - parameters in neural network training , e.g. data quality , can also act as temperature knobs .",48,0,28
dataset/preprocessed/test-data/natural_language_inference/15,Neural Natural Language Inference Models Enhanced with External Knowledge,2,1,9
dataset/preprocessed/test-data/natural_language_inference/15,Modeling natural language inference is a very challenging task .,4,1,10
dataset/preprocessed/test-data/natural_language_inference/15,"With the availability of large annotated data , it has recently become feasible to train complex models such as neural - network - based inference models , which have shown to achieve the state - of - the - art performance .",5,1,42
dataset/preprocessed/test-data/natural_language_inference/15,"Although there exist relatively large annotated data , can machines learn all knowledge needed to perform natural language inference ( NLI ) from these data ?",6,1,26
dataset/preprocessed/test-data/natural_language_inference/15,"If not , how can neural - network - based NLI models benefit from external knowledge and how to build NLI models to leverage it ?",7,1,26
dataset/preprocessed/test-data/natural_language_inference/15,"In this paper , we enrich the state - of - the - art neural natural language inference models with external knowledge .",8,0,23
dataset/preprocessed/test-data/natural_language_inference/15,We demonstrate that the proposed models improve neural NLI models to achieve the state - of - the - art performance on the SNLI and MultiNLI datasets .,9,0,28
dataset/preprocessed/test-data/natural_language_inference/15,Reasoning and inference are central to both human and artificial intelligence .,11,0,12
dataset/preprocessed/test-data/natural_language_inference/15,"Natural language inference ( NLI ) , also known as recognizing textual entailment ( RTE ) , is an important NLP problem concerned with determining inferential relationship ( e.g. , entailment , contradiction , or neutral ) between a premise p and a hypothesis h.",12,1,45
dataset/preprocessed/test-data/natural_language_inference/15,"In general , modeling informal inference in language is a very challenging and basic problem towards achieving true natural language understanding .",13,0,22
dataset/preprocessed/test-data/natural_language_inference/15,"In the last several years , larger annotated datasets were made available , e.g. , the and MultiNLI datasets , which made it feasible to train rather complicated neuralnetwork - based models that fit a large set of parameters to better model NLI .",14,0,44
dataset/preprocessed/test-data/natural_language_inference/15,Such models have shown to achieve the state - of - the - art performance .,15,0,16
dataset/preprocessed/test-data/natural_language_inference/15,"While neural networks have been shown to be very effective in modeling NLI with large training data , they have often focused on end - to - end training by assuming that all inference knowledge is learnable from the provided training data .",16,0,43
dataset/preprocessed/test-data/natural_language_inference/15,"In this paper , we relax this assumption and explore whether external knowledge can further help NLI .",17,0,18
dataset/preprocessed/test-data/natural_language_inference/15,Consider an example :,18,0,4
dataset/preprocessed/test-data/natural_language_inference/15,p : A lady standing in a wheat field .,19,0,10
dataset/preprocessed/test-data/natural_language_inference/15,h : A person standing in acorn field .,20,0,9
dataset/preprocessed/test-data/natural_language_inference/15,"In this simplified example , when computers are asked to predict the relation between these two sentences and if training data do not provide the knowledge of relationship between "" wheat "" and "" corn "" ( e.g. , if one of the two words does not appear in the training data or they are not paired in any premise - hypothesis pairs ) , it will be hard for computers to correctly recognize that the premise contradicts the hypothesis .",21,0,81
dataset/preprocessed/test-data/natural_language_inference/15,"In general , although in many tasks learning tabula rasa achieved state - of - the - art performance , we believe complicated NLP problems such as NLI could benefit from leveraging knowledge accumulated by humans , particularly in a foreseeable future when machines are unable to learn it by themselves .",22,0,52
dataset/preprocessed/test-data/natural_language_inference/15,"In this paper we enrich neural - network - based NLI models with external knowledge in coattention , local inference collection , and inference composition components .",23,0,27
dataset/preprocessed/test-data/natural_language_inference/15,We show the proposed model improves the state - of - the - art NLI models to achieve better performances on the SNLI and MultiNLI datasets .,24,0,27
dataset/preprocessed/test-data/natural_language_inference/15,"The advantage of using external knowledge is more significant when the size of training data is restricted , suggesting that if more knowledge can be obtained , it may bring more benefit .",25,0,33
dataset/preprocessed/test-data/natural_language_inference/15,"In addition to attaining the state - of - theart performance , we are also interested in understanding how external knowledge contributes to the major components of typical neural - networkbased NLI models .",26,0,34
dataset/preprocessed/test-data/natural_language_inference/15,"Early research on natural language inference and recognizing textual entailment has been performed on relatively small datasets ( refer to MacCartney ( 2009 ) fora good literature survey ) , which includes a large bulk of contributions made under the name of RTE , such as , among many others .",28,0,51
dataset/preprocessed/test-data/natural_language_inference/15,"More recently the availability of much larger annotated data , e.g. , and MultiNLI , has made it possible to train more complex models .",29,0,25
dataset/preprocessed/test-data/natural_language_inference/15,These models mainly fall into two types of approaches : sentence - encoding - based models and models using also inter-sentence attention .,30,0,23
dataset/preprocessed/test-data/natural_language_inference/15,Sentence - encoding - based models use Siamese architecture .,31,0,10
dataset/preprocessed/test-data/natural_language_inference/15,The parametertied neural networks are applied to encode both the premise and the hypothesis .,32,0,15
dataset/preprocessed/test-data/natural_language_inference/15,Then a neural network classifier is applied to decide relationship between the two sentences .,33,0,15
dataset/preprocessed/test-data/natural_language_inference/15,"Different neural networks have been utilized for sentence encoding , such as LSTM , GRU , CNN , BiL - STM and its variants , self - attention network , and more complicated neural networks .",34,0,36
dataset/preprocessed/test-data/natural_language_inference/15,"Sentence - encoding - based models transform sentences into fixed - length vector representations , which may help a wide range of tasks .",35,0,24
dataset/preprocessed/test-data/natural_language_inference/15,The second set of models use inter-sentence attention .,36,0,9
dataset/preprocessed/test-data/natural_language_inference/15,"Among them , were among the first to propose neural attention - based models for NLI .",37,0,17
dataset/preprocessed/test-data/natural_language_inference/15,"proposed an enhanced sequential inference model ( ESIM ) , which is one of the best models so far and is used as one of our baselines in this paper .",38,0,31
dataset/preprocessed/test-data/natural_language_inference/15,In this paper we enrich neural - network - based NLI models with external knowledge .,39,0,16
dataset/preprocessed/test-data/natural_language_inference/15,"Unlike early work on NLI ) that explores external knowledge in conventional NLI models on relatively small NLI datasets , we aim to merge the advantage of powerful modeling ability of neural networks with extra external inference knowledge .",40,0,39
dataset/preprocessed/test-data/natural_language_inference/15,We show that the proposed model improves the state - of - the - art neural NLI models to achieve better performances on the SNLI and MultiNLI datasets .,41,0,29
dataset/preprocessed/test-data/natural_language_inference/15,"The advantage of using external knowledge is more significant when the size of training data is restricted , suggesting that if more knowledge can be obtained , it may have more benefit .",42,0,33
dataset/preprocessed/test-data/natural_language_inference/15,"In addition to attaining the state - of - the - art performance , we are also interested in understanding how external knowledge affect major components of neural - network - based NLI models .",43,0,35
dataset/preprocessed/test-data/natural_language_inference/15,"In general , external knowledge has shown to be effective in neural networks for other NLP tasks , including word embedding , machine translation , language modeling , and dialogue systems .",44,0,32
dataset/preprocessed/test-data/natural_language_inference/15,Neural - Network - Based NLI,45,0,6
dataset/preprocessed/test-data/natural_language_inference/15,Models with External Knowledge,46,0,4
dataset/preprocessed/test-data/natural_language_inference/15,"In this section we propose neural - network - based NLI models to incorporate external inference knowledge , which , as we will show later in Section 5 , achieve the state - of - the - art performance .",47,0,40
dataset/preprocessed/test-data/natural_language_inference/15,In addition to attaining the leading performance we are also interested in investigating the effects of external knowledge on major components of neural - network - based NLI modeling .,48,0,30
dataset/preprocessed/test-data/natural_language_inference/15,shows a high - level general view of the proposed framework .,49,0,12
dataset/preprocessed/test-data/natural_language_inference/8,Learning Natural Language Inference using Bidirectional LSTM model and Inner- Attention,2,1,11
dataset/preprocessed/test-data/natural_language_inference/8,"In this paper , we proposed a sentence encoding - based model for recognizing text entailment .",4,1,17
dataset/preprocessed/test-data/natural_language_inference/8,"In our approach , the encoding of sentence is a two - stage process .",5,0,15
dataset/preprocessed/test-data/natural_language_inference/8,"Firstly , average pooling was used over word - level bidirectional LSTM ( biLSTM ) to generate a firststage sentence representation .",6,0,22
dataset/preprocessed/test-data/natural_language_inference/8,"Secondly , attention mechanism was employed to replace average pooling on the same sentence for better representations .",7,0,18
dataset/preprocessed/test-data/natural_language_inference/8,"Instead of using target sentence to attend words in source sentence , we utilized the sentence 's first - stage representation to attend words appeared in itself , which is called "" Inner-Attention "" in our paper .",8,0,38
dataset/preprocessed/test-data/natural_language_inference/8,Experiments conducted on Stanford Natural Language Inference ( SNLI ),9,0,10
dataset/preprocessed/test-data/natural_language_inference/8,"Corpus has proved the effectiveness of "" Inner-Attention "" mechanism .",10,0,11
dataset/preprocessed/test-data/natural_language_inference/8,"With less number of parameters , our model outperformed the existing best sentence encoding - based approach by a large margin .",11,0,22
dataset/preprocessed/test-data/natural_language_inference/8,"Given a pair of sentences , the goal of recognizing text entailment ( RTE ) is to determine whether the hypothesis can reasonably be inferred from the premises .",13,1,29
dataset/preprocessed/test-data/natural_language_inference/8,"There were three types of relation in RTE , Entailment ( inferred to be true ) , Contradiction ( inferred to be false ) and Neutral ( truth unknown ) .",14,1,31
dataset/preprocessed/test-data/natural_language_inference/8,A few examples were given in .,15,0,7
dataset/preprocessed/test-data/natural_language_inference/8,"Traditional methods to RTE has been the dominion of classifiers employing hand engineered features , which heavily relied on natural language processing pipelines and external resources .",16,0,27
dataset/preprocessed/test-data/natural_language_inference/8,Formal reasoning methods were P The boy is running through a grassy area .,17,0,14
dataset/preprocessed/test-data/natural_language_inference/8,The boy is in his room .,18,0,7
dataset/preprocessed/test-data/natural_language_inference/8,C H A boy is running outside .,19,0,8
dataset/preprocessed/test-data/natural_language_inference/8,E The boy is in a park .,20,0,8
dataset/preprocessed/test-data/natural_language_inference/8,"N also explored by many researchers , but not been widely used because of its complexity and domain limitations .",21,0,20
dataset/preprocessed/test-data/natural_language_inference/8,Recently published Stanford Natural Language Inference ( SNLI 1 ) corpus makes it possible to use deep learning methods to solve RTE problems .,22,0,24
dataset/preprocessed/test-data/natural_language_inference/8,So far proposed deep learning approaches can be roughly categorized into two groups : sentence encoding - based models and matching encodingbased models .,23,0,24
dataset/preprocessed/test-data/natural_language_inference/8,"As the name implies , the encoding of sentence is the core of former methods , while the latter methods directly model the relation between two sentences and did n't generate sentence representations at all .",24,0,36
dataset/preprocessed/test-data/natural_language_inference/8,"In view of universality , we focused our efforts on sentence encoding - based model .",25,0,16
dataset/preprocessed/test-data/natural_language_inference/8,"Existing methods of this kind including : LSTMs - based model , GRUsbased model , TBCNN - based model and SPINNbased model .",26,0,23
dataset/preprocessed/test-data/natural_language_inference/8,Single directional LSTMs and GRUs suffer a weakness of not utilizing the contextual information from the future tokens and Convolutional Neural Networks did n't make full use of information contained in word order .,27,0,34
dataset/preprocessed/test-data/natural_language_inference/8,Bidirectional LSTM utilizes both the previous and future context by processing the sequence on two directions which helps to address the drawbacks mentioned above .,28,0,25
dataset/preprocessed/test-data/natural_language_inference/8,A recent work by improved the performance by applying a neural attention model that did n't yield sentence embeddings .,29,0,20
dataset/preprocessed/test-data/natural_language_inference/8,"In this paper , we proposed a unified deep learning framework for recognizing textual entailment which dose not require any feature engineering , or external resources .",30,0,27
dataset/preprocessed/test-data/natural_language_inference/8,The basic model is based on building biL - STM models on both premises and hypothesis .,31,0,17
dataset/preprocessed/test-data/natural_language_inference/8,The basic mean pooling encoder can roughly form a intuition about what this sentence is talking about .,32,0,18
dataset/preprocessed/test-data/natural_language_inference/8,"Obtained this representation , we extended this model by utilize an Inner - Attention mechanism on both sides .",33,0,19
dataset/preprocessed/test-data/natural_language_inference/8,This mechanism helps generate more accurate and focused sentence representations for classification .,34,0,13
dataset/preprocessed/test-data/natural_language_inference/8,"In addition , we introduced a simple effective input strategy that get ride of same words in hypothesis and premise , which further boosts our performance .",35,0,27
dataset/preprocessed/test-data/natural_language_inference/8,"Without parameter tuning , we improved the art - of - the - state performance of sentence encodingbased model by nearly 2 % .",36,0,24
dataset/preprocessed/test-data/natural_language_inference/8,"In our work , we treated RTE task as a supervised three - way classification problem .",38,0,17
dataset/preprocessed/test-data/natural_language_inference/8,The overall architecture of our model is shown in .,39,0,10
dataset/preprocessed/test-data/natural_language_inference/8,"The design of this model we follow the idea of Siamese Network , that the two identical sentence encoders share the same set of weights during training , and the two sentence representations then combined together to generated a "" relation vector "" for classification .",40,0,46
dataset/preprocessed/test-data/natural_language_inference/8,"As we can see from the figure , the model mainly consists of three parts .",41,0,16
dataset/preprocessed/test-data/natural_language_inference/8,From top to bottom were : ( A ) .,42,0,10
dataset/preprocessed/test-data/natural_language_inference/8,The sentence input module ; ( B ) .,43,0,9
dataset/preprocessed/test-data/natural_language_inference/8,The sentence encoding module ; ( C ) .,44,0,9
dataset/preprocessed/test-data/natural_language_inference/8,The sentence matching module .,45,0,5
dataset/preprocessed/test-data/natural_language_inference/8,We will explain the last two parts in detail in the following subsection .,46,0,14
dataset/preprocessed/test-data/natural_language_inference/8,And the sentence input module will be introduced in Section 3.3 .,47,0,12
dataset/preprocessed/test-data/natural_language_inference/8,Sentence Encoding Module,48,0,3
dataset/preprocessed/test-data/natural_language_inference/8,Sentence encoding module is the fundamental part of this model .,49,0,11
dataset/preprocessed/test-data/natural_language_inference/6,Dynamic Self - Attention : Computing Attention over Words Dynamically for Sentence Embedding,2,1,13
dataset/preprocessed/test-data/natural_language_inference/6,"In this paper , we propose Dynamic Self - Attention ( DSA ) , a new self - attention mechanism for sentence embedding .",4,1,24
dataset/preprocessed/test-data/natural_language_inference/6,"We design DSA by modifying dynamic routing in capsule network ( Sabour et al. , 2017 ) for natural language processing .",5,1,22
dataset/preprocessed/test-data/natural_language_inference/6,DSA attends to informative words with a dynamic weight vector .,6,0,11
dataset/preprocessed/test-data/natural_language_inference/6,"We achieve new state - of - the - art results among sentence encoding methods in Stanford Natural Language Inference ( SNLI ) dataset with the least number of parameters , while showing comparative results in Stanford Sentiment Treebank ( SST ) dataset .",7,0,44
dataset/preprocessed/test-data/natural_language_inference/6,"In Natural Language Process ( NLP ) , most neural network - based models contain a sentence encoder to map a sequence of words into a vector .",9,0,28
dataset/preprocessed/test-data/natural_language_inference/6,"The vector is then used for various downstream tasks , e.g. , sentiment analysis , natural language inference , etc .",10,0,21
dataset/preprocessed/test-data/natural_language_inference/6,The key part of a sentence encoder is a computation across a variable - length input sequence fora fixed size vector .,11,0,22
dataset/preprocessed/test-data/natural_language_inference/6,One of the common approaches is the max - pooling in CNN or RNN .,12,0,15
dataset/preprocessed/test-data/natural_language_inference/6,Self - attention is another approach fora fixed size vector .,13,0,11
dataset/preprocessed/test-data/natural_language_inference/6,"Self - attention derived from the attention mechanism , originally designed for neural machine translation , is utilized in various tasks .",14,0,22
dataset/preprocessed/test-data/natural_language_inference/6,Self - attention computes attention weights by the inner product between words and the learnable weight vector .,15,0,18
dataset/preprocessed/test-data/natural_language_inference/6,"The weight vector is important in that it detects informative words , yet it is static during inference .",16,0,19
dataset/preprocessed/test-data/natural_language_inference/6,The significance of the role of the weight vector casts doubt on whether its being static is an optimal status .,17,0,21
dataset/preprocessed/test-data/natural_language_inference/6,* Equal Contribution .,18,0,4
dataset/preprocessed/test-data/natural_language_inference/6,"In parallel , recently proposed capsule network for image classification .",19,0,11
dataset/preprocessed/test-data/natural_language_inference/6,"In capsule network , dynamic routing iteratively computes weights over inputs by the inner product between inputs and a weighted sum of inputs .",20,0,24
dataset/preprocessed/test-data/natural_language_inference/6,"Varying with the inputs , the weighted sum detects informative inputs ; therefore it can be interpreted as a dynamic weight vector from the perspective of self - attention .",21,0,30
dataset/preprocessed/test-data/natural_language_inference/6,We expect the dynamic weight vector to give rise to flexibility in self - attention since it can adapt to given sentences even after training .,22,0,26
dataset/preprocessed/test-data/natural_language_inference/6,"Motivated by dynamic routing ) , we propose a new self - attention mechanism for sentence embedding , namely Dynamic Self - Attention ( DSA ) .",23,0,27
dataset/preprocessed/test-data/natural_language_inference/6,"To this end , we modify dynamic routing such that it functions as self - attention with the dynamic weight vector .",24,0,22
dataset/preprocessed/test-data/natural_language_inference/6,"DSA , which is stacked on CNN with Dense Connection , achieves new state - of - the - art results among the sentence encoding methods in Stanford Natural Language Inference ( SNLI ) dataset with the least number of parameters , while obtaining comparative results in Stanford Sentiment Treebank ( SST ) dataset .",25,0,55
dataset/preprocessed/test-data/natural_language_inference/6,It also outperforms recent models in terms of time efficiency due to its simplicity and highly parallelized computations .,26,0,19
dataset/preprocessed/test-data/natural_language_inference/6,Our technical contributions are as follows :,27,0,7
dataset/preprocessed/test-data/natural_language_inference/6,"We design and implement Dynamic Self - Attention ( DSA ) , a new self - attention mechanism for sentence embedding .",28,0,22
dataset/preprocessed/test-data/natural_language_inference/6,We devise the dynamic weight vector with which DSA computes attention weights .,29,0,13
dataset/preprocessed/test-data/natural_language_inference/6,"We achieve new state - of - the - art results in SNLI dataset , while showing comparative results in SST dataset .",30,0,23
dataset/preprocessed/test-data/natural_language_inference/6,"In self - attention , attention weights are computed as follows :",32,0,12
dataset/preprocessed/test-data/natural_language_inference/6,where X ?,33,0,3
dataset/preprocessed/test-data/natural_language_inference/6,"R dwn is an input sequence , W ?",34,0,9
dataset/preprocessed/test-data/natural_language_inference/6,R dvdw is a projection matrix and v ?,35,0,9
dataset/preprocessed/test-data/natural_language_inference/6,R dv is the learnable weight vector of self - attention .,36,0,12
dataset/preprocessed/test-data/natural_language_inference/6,"The weight vector v plays an important role , since attention weights are computed by the inner product between v and the projection of the input sequence X .",37,0,29
dataset/preprocessed/test-data/natural_language_inference/6,The weight vector v is static with respect to the input sequence X during inference .,38,0,16
dataset/preprocessed/test-data/natural_language_inference/6,Replacing the weight vector v with a weight matrix enables multiple attentions .,39,0,13
dataset/preprocessed/test-data/natural_language_inference/6,"Our architecture , shown in , is built on CNN with Dense Connection .",41,0,14
dataset/preprocessed/test-data/natural_language_inference/6,"Dynamic Self - Attention ( DSA ) , which is stacked on CNN with Dense Connection , computes attention weights over words .",42,0,23
dataset/preprocessed/test-data/natural_language_inference/6,CNN with Dense Connection,43,0,4
dataset/preprocessed/test-data/natural_language_inference/6,The goal of this module is to encode each word into a meaningful representation space while capturing local information .,44,0,20
dataset/preprocessed/test-data/natural_language_inference/6,"We do not add any positional encoding , as suggested by ; deep convolution layers capture relative position information .",45,0,20
dataset/preprocessed/test-data/natural_language_inference/6,We also enforce every output of layers to have the same number of columns by using appropriate zero padding .,46,0,20
dataset/preprocessed/test-data/natural_language_inference/6,We denote a sequence of word embeddings as,47,0,8
dataset/preprocessed/test-data/natural_language_inference/6,"is a composite function of the l th layer , composed of 1 D Convolution , dropout , and leaky rectified linear unit ( Leaky ReLU ) .",48,0,28
dataset/preprocessed/test-data/natural_language_inference/6,We feed a sequence of word embeddings into h 1 ( ) with kernel size 1 :,49,0,17
dataset/preprocessed/test-data/natural_language_inference/11,Published as a conference paper at ICLR 2017 DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING,2,1,14
dataset/preprocessed/test-data/natural_language_inference/11,Several deep learning models have been proposed for question answering .,4,0,11
dataset/preprocessed/test-data/natural_language_inference/11,"However , due to their single - pass nature , they have noway to recover from local maxima corresponding to incorrect answers .",5,0,23
dataset/preprocessed/test-data/natural_language_inference/11,"To address this problem , we introduce the Dynamic Coattention Network ( DCN ) for question answering .",6,0,18
dataset/preprocessed/test-data/natural_language_inference/11,The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both .,7,0,22
dataset/preprocessed/test-data/natural_language_inference/11,Then a dynamic pointing decoder iterates over potential answer spans .,8,0,11
dataset/preprocessed/test-data/natural_language_inference/11,This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers .,9,0,17
dataset/preprocessed/test-data/natural_language_inference/11,"On the Stanford question answering dataset , a single DCN model improves the previous state of the art from 71.0 % F1 to 75. 9 % , while a DCN ensemble obtains 80.4 % F1 .",10,0,36
dataset/preprocessed/test-data/natural_language_inference/11,Question answering ( QA ) is a crucial task in natural language processing that requires both natural language understanding and world knowledge .,12,1,23
dataset/preprocessed/test-data/natural_language_inference/11,"Previous QA datasets tend to be high in quality due to human annotation , but small in size .",13,1,19
dataset/preprocessed/test-data/natural_language_inference/11,"Hence , they did not allow for training data-intensive , expressive models such as deep neural networks .",14,0,18
dataset/preprocessed/test-data/natural_language_inference/11,"To address this problem , researchers have developed large - scale datasets through semi-automated techniques .",15,0,16
dataset/preprocessed/test-data/natural_language_inference/11,"Compared to their smaller , hand - annotated counterparts , these QA datasets allow the training of more expressive models .",16,0,21
dataset/preprocessed/test-data/natural_language_inference/11,"However , it has been shown that they differ from more natural , human annotated datasets in the types of reasoning required to answer the questions .",17,0,27
dataset/preprocessed/test-data/natural_language_inference/11,"Recently , released the Stanford Question Answering dataset ( SQuAD ) , which is orders of magnitude larger than all previous hand - annotated datasets and has a variety of qualities that culminate in a natural QA task .",18,0,39
dataset/preprocessed/test-data/natural_language_inference/11,SQuAD has the desirable quality that answers are spans in a reference document .,19,0,14
dataset/preprocessed/test-data/natural_language_inference/11,This constrains answers to the space of all possible spans .,20,0,11
dataset/preprocessed/test-data/natural_language_inference/11,"However , show that the dataset retains a diverse set of answers and requires different forms of logical reasoning , including multi-sentence reasoning .",21,0,24
dataset/preprocessed/test-data/natural_language_inference/11,"We introduce the Dynamic Coattention Network ( DCN ) , illustrated in , an end - to - end neural network for question answering .",22,0,25
dataset/preprocessed/test-data/natural_language_inference/11,"The model consists of a coattentive encoder that captures the interactions between the question and the document , as well as a dynamic pointing decoder that alternates between estimating the start and end of the answer span .",23,0,38
dataset/preprocessed/test-data/natural_language_inference/11,Our single model obtains an F1 of 75.9 % compared to the best published result of 71.0 % .,24,0,19
dataset/preprocessed/test-data/natural_language_inference/11,"In addition , our ensemble model obtains an F1 of 80.4 % compared to the second best result of 78.1 % on the official SQuAD leaderboard .",25,0,27
dataset/preprocessed/test-data/natural_language_inference/11,1 illustrates an overview of the DCN .,26,0,8
dataset/preprocessed/test-data/natural_language_inference/11,"We first describe the encoders for the document and the question , followed by the coattention mechanism and the dynamic decoder which produces the answer span .",27,0,27
dataset/preprocessed/test-data/natural_language_inference/11,What plants create most electric power ?,30,0,7
dataset/preprocessed/test-data/natural_language_inference/11,The weight of boilers and condensers generally makes the power - to - weight ...,32,0,15
dataset/preprocessed/test-data/natural_language_inference/11,"However , most electric power is generated using steam turbine plants , so that indirectly the world 's industry is ...",33,0,21
dataset/preprocessed/test-data/natural_language_inference/11,Dynamic pointer decoder,34,0,3
dataset/preprocessed/test-data/natural_language_inference/11,DOCUMENT AND QUESTION ENCODER,35,0,4
dataset/preprocessed/test-data/natural_language_inference/11,"Let ( x Q 1 , x Q 2 , . . . , x Q n ) denote the sequence of word vectors corresponding to words in the question and ( x D 1 , x D 2 , . . . , x D m ) denote the same for words in the document .",36,0,57
dataset/preprocessed/test-data/natural_language_inference/11,"Using an LSTM , we encode the document as : d t = LSTM enc d t?1 , x D t .",37,0,22
dataset/preprocessed/test-data/natural_language_inference/11,We define the document encoding matrix as D = [ d 1 . . . d m d ? ],38,0,20
dataset/preprocessed/test-data/natural_language_inference/11,R ( m + 1 ) .,40,0,7
dataset/preprocessed/test-data/natural_language_inference/11,"We also add a sentinel vector d ? , which we later show allows the model to not attend to any particular word in the input .",41,0,27
dataset/preprocessed/test-data/natural_language_inference/11,"The question embeddings are computed with the same LSTM to share representation power : qt = LSTM enc q t?1 , x Q t .",42,0,25
dataset/preprocessed/test-data/natural_language_inference/11,We define an intermediate question representation Q = [ q 1 . . . q n q ? ] ? R ( n+ 1 ) .,43,0,26
dataset/preprocessed/test-data/natural_language_inference/11,"To allow for variation between the question encoding space and the document encoding space , we introduce a non-linear projection layer on top of the question encoding .",44,0,28
dataset/preprocessed/test-data/natural_language_inference/11,The final representation for the question becomes :,45,0,8
dataset/preprocessed/test-data/natural_language_inference/11,"We propose a coattention mechanism that attends to the question and document simultaneously , similar to , and finally fuses both attention contexts .",47,0,24
dataset/preprocessed/test-data/natural_language_inference/11,provides an illustration of the coattention encoder .,48,0,8
dataset/preprocessed/test-data/natural_language_inference/11,"We first compute the affinity matrix , which contains affinity scores corresponding to all pairs of document words and question words : L = D Q ?",49,0,27
dataset/preprocessed/test-data/natural_language_inference/14,End - To - End Memory Networks,2,1,7
dataset/preprocessed/test-data/natural_language_inference/14,We introduce a neural network with a recurrent attention model over a possibly large external memory .,4,0,17
dataset/preprocessed/test-data/natural_language_inference/14,"The architecture is a form of Memory Network [ 23 ] but unlike the model in that work , it is trained end - to - end , and hence requires significantly less supervision during training , making it more generally applicable in realistic settings .",5,0,46
dataset/preprocessed/test-data/natural_language_inference/14,It can also be seen as an extension of RNNsearch [ 2 ] to the case where multiple computational steps ( hops ) are performed per output symbol .,6,0,29
dataset/preprocessed/test-data/natural_language_inference/14,The flexibility of the model allows us to apply it to tasks as diverse as ( synthetic ) question answering [ 22 ] and to language modeling .,7,0,28
dataset/preprocessed/test-data/natural_language_inference/14,"For the former our approach is competitive with Memory Networks , but with less supervision .",8,0,16
dataset/preprocessed/test-data/natural_language_inference/14,"For the latter , on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs .",9,0,21
dataset/preprocessed/test-data/natural_language_inference/14,In both cases we show that the key concept of multiple computational hops yields improved results .,10,0,17
dataset/preprocessed/test-data/natural_language_inference/14,"Two grand challenges in artificial intelligence research have been to build models that can make multiple computational steps in the service of answering a question or completing a task , and models that can describe long term dependencies in sequential data .",11,0,42
dataset/preprocessed/test-data/natural_language_inference/14,"Recently there has been a resurgence in models of computation using explicit storage and a notion of attention [ 23 , 8 , 2 ] ; manipulating such a storage offers an approach to both of these challenges .",12,0,39
dataset/preprocessed/test-data/natural_language_inference/14,"In [ 23 , 8 , 2 ] , the storage is endowed with a continuous representation ; reads from and writes to the storage , as well as other processing steps , are modeled by the actions of neural networks .",13,0,42
dataset/preprocessed/test-data/natural_language_inference/14,"In this work , we present a novel recurrent neural network ( RNN ) architecture where the recurrence reads from a possibly large external memory multiple times before outputting a symbol .",14,0,32
dataset/preprocessed/test-data/natural_language_inference/14,Our model can be considered a continuous form of the Memory Network implemented in [ 23 ] .,15,0,18
dataset/preprocessed/test-data/natural_language_inference/14,"The model in that work was not easy to train via backpropagation , and required supervision at each layer of the network .",16,0,23
dataset/preprocessed/test-data/natural_language_inference/14,"The continuity of the model we present here means that it can be trained end - to - end from input - output pairs , and so is applicable to more tasks , i.e. tasks where such supervision is not available , such as in language modeling or realistically supervised question answering tasks .",17,0,54
dataset/preprocessed/test-data/natural_language_inference/14,"Our model can also be seen as a version of RNNsearch [ 2 ] with multiple computational steps ( which we term "" hops "" ) per output symbol .",18,0,30
dataset/preprocessed/test-data/natural_language_inference/14,"We will show experimentally that the multiple hops over the long - term memory are crucial to good performance of our model on these tasks , and that training the memory representation can be integrated in a scalable manner into our end - to - end neural network model .",19,0,50
dataset/preprocessed/test-data/natural_language_inference/14,"Our model takes a discrete set of inputs x 1 , ... , x n that are to be stored in the memory , a query q , and outputs an answer a .",20,0,34
dataset/preprocessed/test-data/natural_language_inference/14,"Each of the x i , q , and a contains symbols coming from a dictionary with V words .",21,0,20
dataset/preprocessed/test-data/natural_language_inference/14,"The model writes all x to the memory up to a fixed buffer size , and then finds a continuous representation for the x and q.",22,0,26
dataset/preprocessed/test-data/natural_language_inference/14,The continuous representation is then processed via multiple hops to output a .,23,0,13
dataset/preprocessed/test-data/natural_language_inference/14,This allows backpropagation of the error signal through multiple memory accesses back to the input during training .,24,0,18
dataset/preprocessed/test-data/natural_language_inference/14,"We start by describing our model in the single layer case , which implements a single memory hop operation .",27,0,20
dataset/preprocessed/test-data/natural_language_inference/14,We then show it can be stacked to give multiple hops in memory .,28,0,14
dataset/preprocessed/test-data/natural_language_inference/14,Input memory representation :,29,0,4
dataset/preprocessed/test-data/natural_language_inference/14,"Suppose we are given an input set x 1 , .. , xi to be stored in memory .",30,0,19
dataset/preprocessed/test-data/natural_language_inference/14,"The entire set of {x i } are converted into memory vectors {m i } of dimension d computed by embedding each x i in a continuous space , in the simplest case , using an embedding matrix A ( of size dV ) .",31,0,45
dataset/preprocessed/test-data/natural_language_inference/14,"The query q is also embedded ( again , in the simplest case via another embedding matrix B with the same dimensions as A ) to obtain an internal state u .",32,0,32
dataset/preprocessed/test-data/natural_language_inference/14,"In the embedding space , we compute the match between u and each memory mi by taking the inner product followed by a softmax :",33,0,25
dataset/preprocessed/test-data/natural_language_inference/14,where Softmax ( z i ) = e zi / j e zj .,34,0,14
dataset/preprocessed/test-data/natural_language_inference/14,Defined in this way p is a probability vector over the inputs .,35,0,13
dataset/preprocessed/test-data/natural_language_inference/14,Output memory representation :,36,0,4
dataset/preprocessed/test-data/natural_language_inference/14,Each x i has a corresponding output vector c i ( given in the simplest case by another embedding matrix C ) .,37,0,23
dataset/preprocessed/test-data/natural_language_inference/14,"The response vector from the memory o is then a sum over the transformed inputs c i , weighted by the probability vector from the input :",38,0,27
dataset/preprocessed/test-data/natural_language_inference/14,"Because the function from input to output is smooth , we can easily compute gradients and backpropagate through it .",39,0,20
dataset/preprocessed/test-data/natural_language_inference/14,"Other recently proposed forms of memory or attention take this approach , notably Bahdanau et al. and Graves et al. , see also .",40,0,24
dataset/preprocessed/test-data/natural_language_inference/14,Generating the final prediction :,41,0,5
dataset/preprocessed/test-data/natural_language_inference/14,"In the single layer case , the sum of the output vector o and the input embedding u is then passed through a final weight matrix W ( of size V d ) and a softmax to produce the predicted label : = Softmax ( W ( o + u ) )",42,0,52
dataset/preprocessed/test-data/natural_language_inference/14,The overall model is shown in .,43,0,7
dataset/preprocessed/test-data/natural_language_inference/14,"During training , all three embedding matrices A , B and C , as well as Ware jointly learned by minimizing a standard cross - entropy loss between and the true label a .",44,0,34
dataset/preprocessed/test-data/natural_language_inference/14,Training is performed using stochastic gradient descent ( see Section 4.2 for more details ) .,45,0,16
dataset/preprocessed/test-data/natural_language_inference/14,We now extend our model to handle K hop operations .,47,0,11
dataset/preprocessed/test-data/natural_language_inference/14,The memory layers are stacked in the following way :,48,0,10
dataset/preprocessed/test-data/natural_language_inference/14,The input to layers above the first is the sum of the output o k and the input u k from layer k ( different ways to combine o k and u k are proposed later ) :,49,0,38
dataset/preprocessed/test-data/natural_language_inference/17,GLUE : A MULTI - TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTAND - ING,2,1,16
dataset/preprocessed/test-data/natural_language_inference/17,"For natural language understanding ( NLU ) technology to be maximally useful , it must be able to process language in away that is not exclusive to a single task , genre , or dataset .",4,1,36
dataset/preprocessed/test-data/natural_language_inference/17,"In pursuit of this objective , we introduce the General Language Understanding Evaluation ( GLUE ) benchmark , a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks .",5,0,37
dataset/preprocessed/test-data/natural_language_inference/17,"By including tasks with limited training data , GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks .",6,0,24
dataset/preprocessed/test-data/natural_language_inference/17,GLUE also includes a hand - crafted diagnostic test suite that enables detailed linguistic analysis of models .,7,0,18
dataset/preprocessed/test-data/natural_language_inference/17,We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task .,8,0,30
dataset/preprocessed/test-data/natural_language_inference/17,"However , the low absolute performance of our best model indicates the need for improved general NLU systems .",9,1,19
dataset/preprocessed/test-data/natural_language_inference/17,Published as a conference paper at ICLR 2019,10,0,8
dataset/preprocessed/test-data/natural_language_inference/17,"The human ability to understand language is general , flexible , and robust .",12,0,14
dataset/preprocessed/test-data/natural_language_inference/17,"In contrast , most NLU models above the word level are designed fora specific task and struggle with out - of - domain data .",13,0,25
dataset/preprocessed/test-data/natural_language_inference/17,"If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs , then it is critical to develop a more unified model that can learn to execute a range of different linguistic tasks in different domains .",14,0,44
dataset/preprocessed/test-data/natural_language_inference/17,"To facilitate research in this direction , we present the General Language Understanding Evaluation ( GLUE ) benchmark : a collection of NLU tasks including question answering , sentiment analysis , and textual entailment , and an associated online platform for model evaluation , comparison , and analysis .",15,0,49
dataset/preprocessed/test-data/natural_language_inference/17,GLUE does not place any constraints on model architecture beyond the ability to process single - sentence and sentence - pair inputs and to make corresponding predictions .,16,0,28
dataset/preprocessed/test-data/natural_language_inference/17,"For some GLUE tasks , training data is plentiful , but for others it is limited or fails to match the genre of the test set .",17,0,27
dataset/preprocessed/test-data/natural_language_inference/17,GLUE therefore favors models that can learn to represent linguistic knowledge in away that facilitates sample - efficient learning and effective knowledge - transfer across tasks .,18,0,27
dataset/preprocessed/test-data/natural_language_inference/17,None of the datasets in GLUE were created from scratch for the benchmark ; we rely on preexisting datasets because they have been implicitly agreed upon by the NLP community as challenging and interesting .,19,0,35
dataset/preprocessed/test-data/natural_language_inference/17,"Four of the datasets feature privately - held test data , which will be used to ensure that the benchmark is used fairly .",20,0,24
dataset/preprocessed/test-data/natural_language_inference/17,"To understand the types of knowledge learned by models and to encourage linguistic - meaningful solution strategies , GLUE also includes a set of hand - crafted analysis examples for probing trained models .",22,0,34
dataset/preprocessed/test-data/natural_language_inference/17,"This dataset is designed to highlight common challenges , such as the use of world knowledge and logical operators , that we expect models must handle to robustly solve the tasks . :",23,0,33
dataset/preprocessed/test-data/natural_language_inference/17,Task descriptions and statistics .,24,0,5
dataset/preprocessed/test-data/natural_language_inference/17,"All tasks are single sentence or sentence pair classification , except STS - B , which is a regression task .",25,0,21
dataset/preprocessed/test-data/natural_language_inference/17,MNLI has three classes ; all other classification tasks have two .,26,0,12
dataset/preprocessed/test-data/natural_language_inference/17,Test sets shown in bold use labels that have never been made public in any form .,27,0,17
dataset/preprocessed/test-data/natural_language_inference/17,"To better understand the challenged posed by GLUE , we conduct experiments with simple baselines and state - of - the - art sentence representation models .",28,0,27
dataset/preprocessed/test-data/natural_language_inference/17,We find that unified multi-task trained models slightly outperform comparable models trained on each task separately .,29,0,17
dataset/preprocessed/test-data/natural_language_inference/17,"Our best multi-task model makes use of ELMo , a recently proposed pre-training technique .",30,0,15
dataset/preprocessed/test-data/natural_language_inference/17,"However , this model still achieves a fairly low absolute score .",31,0,12
dataset/preprocessed/test-data/natural_language_inference/17,Analysis with our diagnostic dataset reveals that our baseline models deal well with strong lexical signals but struggle with deeper logical structure .,32,0,23
dataset/preprocessed/test-data/natural_language_inference/17,"In summary , we offer : ( i ) A suite of nine sentence or sentence - pair NLU tasks , built on established annotated datasets and selected to cover a diverse range of text genres , dataset sizes , and degrees of difficulty .",33,0,45
dataset/preprocessed/test-data/natural_language_inference/17,"( ii ) An online evaluation platform and leaderboard , based primarily on privately - held test data .",34,0,19
dataset/preprocessed/test-data/natural_language_inference/17,"The platform is model - agnostic , and can evaluate any method capable of producing results on all nine tasks .",35,0,21
dataset/preprocessed/test-data/natural_language_inference/17,( iii ) An expert - constructed diagnostic evaluation dataset .,36,0,11
dataset/preprocessed/test-data/natural_language_inference/17,( iv ) Baseline results for several major existing approaches to sentence representation learning .,37,0,15
dataset/preprocessed/test-data/natural_language_inference/17,"Collobert et al. ( 2011 ) used a multi - task model with a shared sentence understanding component to jointly learn POS tagging , chunking , named entity recognition , and semantic role labeling .",39,0,35
dataset/preprocessed/test-data/natural_language_inference/17,More recent work has explored using labels from core NLP tasks to supervise training of lower levels of deep neural networks ) and automatically learning cross - task sharing mechanisms for multi-task learning ) .,40,0,35
dataset/preprocessed/test-data/natural_language_inference/17,"Beyond multi-task learning , much work in developing general NLU systems has focused on sentence - to - vector encoders , leveraging unlabeled data , labeled data , and combinations of these .",41,0,33
dataset/preprocessed/test-data/natural_language_inference/17,"In this line of work , a standard evaluation practice has emerged , recently codified as SentEval .",42,0,18
dataset/preprocessed/test-data/natural_language_inference/17,"Like GLUE , SentEval relies on a set of existing classification tasks involving either one or two sentences as inputs .",43,0,21
dataset/preprocessed/test-data/natural_language_inference/17,"Unlike GLUE , SentEval only evaluates sentenceto - vector encoders , making it well - suited for evaluating models on tasks involving sentences in isolation .",44,0,26
dataset/preprocessed/test-data/natural_language_inference/17,"However , cross - sentence contextualization and alignment are instrumental in achieving state - of - the - art performance on tasks such as machine translation , question answering , and natural language inference .",45,0,35
dataset/preprocessed/test-data/natural_language_inference/17,GLUE is designed to facilitate the development of these methods :,46,0,11
dataset/preprocessed/test-data/natural_language_inference/17,"It is model - agnostic , allowing for any kind of representation or contextualization , including models that use no explicit vector or symbolic representations for sentences whatsoever .",47,0,29
dataset/preprocessed/test-data/natural_language_inference/17,GLUE also diverges from SentEval in the selection of evaluation tasks that are included in the suite .,48,0,18
dataset/preprocessed/test-data/natural_language_inference/17,"Many of the SentEval tasks are closely related to sentiment analysis , such as MR , SST , , and SUBJ .",49,0,22
dataset/preprocessed/test-data/natural_language_inference/29,PHASE CONDUCTOR ON MULTI - LAYERED ATTEN - TIONS FOR MACHINE COMPREHENSION,2,1,12
dataset/preprocessed/test-data/natural_language_inference/29,Attention models have been intensively studied to improve NLP tasks such as machine comprehension via both question - aware passage attention model and selfmatching attention model .,4,0,27
dataset/preprocessed/test-data/natural_language_inference/29,Our research proposes phase conductor ( PhaseCond ) for attention models in two meaningful ways .,5,0,16
dataset/preprocessed/test-data/natural_language_inference/29,"First , PhaseCond , an architecture of multi-layered attention models , consists of multiple phases each implementing a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow .",6,0,39
dataset/preprocessed/test-data/natural_language_inference/29,"Second , we extend and improve the dot-product attention function for PhaseCond by simultaneously encoding multiple question and passage embedding layers from different perspectives .",7,0,25
dataset/preprocessed/test-data/natural_language_inference/29,"We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset , showing that our model significantly outperforms both stateof - the - art single - layered and multiple - layered attention models .",8,0,36
dataset/preprocessed/test-data/natural_language_inference/29,We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models .,9,0,24
dataset/preprocessed/test-data/natural_language_inference/29,* Authors ' contributions are equally important to this work .,10,0,11
dataset/preprocessed/test-data/natural_language_inference/29,"Attention - based neural networks have demonstrated success in a wide range of NLP tasks ranging from neural machine translation , image captioning , and speech recognition .",12,0,28
dataset/preprocessed/test-data/natural_language_inference/29,"Benefiting from the availability of large - scale benchmark datasets such as SQuAD , the attention - based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors .",13,1,40
dataset/preprocessed/test-data/natural_language_inference/29,uses attention mechanism in Pointer Network to detect an answer boundary by predicting the start and the end indices in the passage .,14,0,23
dataset/preprocessed/test-data/natural_language_inference/29,introduces a bi-directional attention flow network that attention models are decoupled from the recurrent neural networks .,15,0,17
dataset/preprocessed/test-data/natural_language_inference/29,employs a coattention mechanism that attends to the question and document together .,16,0,13
dataset/preprocessed/test-data/natural_language_inference/29,uses a gated attention network that includes both question and passage match and self - matching attentions .,17,0,18
dataset/preprocessed/test-data/natural_language_inference/29,Both and employs the structure of multi-hops or iterative aligner to repeatedly fuse the passage representation with the question representation as well as the passage representation itself .,18,0,28
dataset/preprocessed/test-data/natural_language_inference/29,"Inspired by the above - mentioned works , we are proposing to introduce a general framework PhaseCond for the use of multiple attention layers .",19,0,25
dataset/preprocessed/test-data/natural_language_inference/29,There are two motivations .,20,0,5
dataset/preprocessed/test-data/natural_language_inference/29,"First , previous research on the self - attention model is to purely capture long - distance dependencies , and therefore a multi-hops architecture is used to alternatively captures question - aware passage representations and refines the results by using a self - attention model .",21,0,46
dataset/preprocessed/test-data/natural_language_inference/29,"In contrast to the multi-hops and interactive architecture , our motivation of using the self - attention model for machine comprehension is to propagate answer evidence which is derived from the preceding question - passage representation layers .",22,0,38
dataset/preprocessed/test-data/natural_language_inference/29,"This perspective leads to a different attention - based architecture containing two sequential phases , question - aware passage representation phase and evidence propagation phase. , RNET , MReader , and PhaseCond ( our proposed model ) .",23,0,38
dataset/preprocessed/test-data/natural_language_inference/29,"Second , unlike the domains such as machine translation which jointly align and translate words , question - passage attention models for machine comprehension and question answering calculate the alignment matrix corresponding to all question and passage word pairs .",24,0,40
dataset/preprocessed/test-data/natural_language_inference/29,"Despite the attention models ' success on the machine comprehension task , there has not been any other work exploring learning to encode multiple representations of question or passage from different perspectives for different parts of attention functions .",25,0,39
dataset/preprocessed/test-data/natural_language_inference/29,"More specifically , most approaches use two same question representations U for the question - passage attention model ?( H , U ) U , where H is the passage representation .",26,0,32
dataset/preprocessed/test-data/natural_language_inference/29,Our hypothesis is that attention models can be more effective by learning different encoders fora question representation U and a question representation V from different aspects .,27,0,27
dataset/preprocessed/test-data/natural_language_inference/29,The key differences between our proposed model and competing approaches are summarized at .,28,0,14
dataset/preprocessed/test-data/natural_language_inference/29,Our contributions are threefold :,29,0,5
dataset/preprocessed/test-data/natural_language_inference/29,"1 ) we proposed a phase conductor for attention models containing multiple phases , each with a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow , 2 ) we present an improved attention function for question - passage attention based on two kinds of encoders : an independent question encoder and a weight - sharing encoder jointly considering the question and the passage , as opposed to most previous works which only using the same encoder for one attention model , and 3 ) we provide both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models .",30,0,114
dataset/preprocessed/test-data/natural_language_inference/29,Experimental results show that our proposed PhaseCond lead to significant performance improvements over the state - of - the - art single - layered and multilayered attention models .,31,0,29
dataset/preprocessed/test-data/natural_language_inference/29,"Moreover , we observe several meaningful trends : a ) during the questionpassage attention phase , repeatedly attending the passage with the same question representation "" forces "" each passage word to become increasingly closer to the original question representation , and therefore increasing the number of layers has a risk of degrading the network performance , b ) during the self - attention phase , the self - attention 's alignment weights of the second layer become noticeably "" sharper "" than the first layer , suggesting the importance of fully propagating evidence through the passage itself .",32,0,99
dataset/preprocessed/test-data/natural_language_inference/29,"We proposed phased conductor model ( or PhaseCond ) , which consisting of multiple phases and each phase has two parts , a stack of attention layers",34,0,27
dataset/preprocessed/test-data/natural_language_inference/29,Land a stack of fusion layers F controlling information flow .,35,0,11
dataset/preprocessed/test-data/natural_language_inference/29,"In our model , a fusion layer F can bean inner fusion layer F inner inside of a stack of attention layers , or an outer fusion layer F outer immediately following a stack of attention layers .",36,0,38
dataset/preprocessed/test-data/natural_language_inference/29,"Without loss of generality , PhaseCond 's configurable computational path for two - phase , a question - passage attention phase containing N question - passage attention layers L Q , and a selfattention phase containing K self - attention layers L S , can be defined as question - attended passage representation is directly matching against itself , for the purpose of propagating information through the whole passage detailed in Section 2.3 .",37,0,74
dataset/preprocessed/test-data/natural_language_inference/29,"For each self - attention layer , we configure an inner fusion layer to obtain a gated representation that is learned to decide how much of the current output is fused by the input from the previous layer detailed in Section 2.3.1 .",38,0,43
dataset/preprocessed/test-data/natural_language_inference/29,"Finally , the fused vectors are sent to the output layer to predict the boundary of the answer span described in Section 2.4 .",39,0,24
dataset/preprocessed/test-data/natural_language_inference/29,The concatenation of raw features as inputs are processed in fusion layers followed by encoder layers to form more abstract representations .,41,0,22
dataset/preprocessed/test-data/natural_language_inference/29,Here we choose a bi-directional Long Short - Term Memory ( LSTM ) to obtain more abstract representations for words in passages and questions .,42,0,25
dataset/preprocessed/test-data/natural_language_inference/29,"Different from the commonly used approaches that every single model has exactly one question and passage encoder , our encoder layers simultaneously calculate multiple question and passage representations , for the purpose of serving different parts of attention functions of different phases .",43,0,43
dataset/preprocessed/test-data/natural_language_inference/29,"We use two types of encoders , independent encoder and shared encoder .",44,0,13
dataset/preprocessed/test-data/natural_language_inference/29,"In terms of independent encoder , a bi-directional LSTM is used to",45,0,12
dataset/preprocessed/test-data/natural_language_inference/29,where v Q j ?,46,0,5
dataset/preprocessed/test-data/natural_language_inference/29,R 2d are concatenated hidden states of two independent BiLSTM for the j - th question word and dis the hidden size .,47,0,23
dataset/preprocessed/test-data/natural_language_inference/29,"In terms of shared encoder , we jointly produce new representation h P 1 , . . . , h P n and u Q 1 , . . . , u Q m for the passage and question via a shared bi-directional LSTM ,",48,0,45
dataset/preprocessed/test-data/natural_language_inference/29,where h P i ?,49,0,5
dataset/preprocessed/test-data/natural_language_inference/24,Reading Wikipedia to Answer Open-Domain Questions,2,1,6
dataset/preprocessed/test-data/natural_language_inference/24,Wikipedia as the unique knowledge source : the answer to any factoid question is a text span in a Wikipedia article .,4,0,22
dataset/preprocessed/test-data/natural_language_inference/24,This task of machine reading at scale combines the challenges of document retrieval ( finding the relevant articles ) with that of machine comprehension of text ( identifying the answer spans from those articles ) .,5,0,36
dataset/preprocessed/test-data/natural_language_inference/24,Our approach combines a search component based on bigram hashing and TF - IDF matching with a multi - layer recurrent neural network model trained to detect answers in Wikipedia paragraphs .,6,0,32
dataset/preprocessed/test-data/natural_language_inference/24,Our experiments on multiple existing QA datasets indicate that ( 1 ) both modules are highly competitive with respect to existing counterparts and ( 2 ) multitask learning using distant supervision on their combination is an effective complete system on this challenging task .,7,0,44
dataset/preprocessed/test-data/natural_language_inference/24,"This paper considers the problem of answering factoid questions in an open - domain setting using Wikipedia as the unique knowledge source , such as one does when looking for answers in an encyclopedia .",9,1,35
dataset/preprocessed/test-data/natural_language_inference/24,Wikipedia is a constantly evolving source of detailed information that could facilitate intelligent machines - if they are able to leverage its power .,10,0,24
dataset/preprocessed/test-data/natural_language_inference/24,"Unlike knowledge bases ( KBs ) such as Freebase or DB - Pedia , which are easier for computers to process but too sparsely populated for open - domain question answering , Wikipedia contains up - to - date knowledge that humans are interested in .",11,1,46
dataset/preprocessed/test-data/natural_language_inference/24,"It is designed , however , for humans - not machines - to read .",12,0,15
dataset/preprocessed/test-data/natural_language_inference/24,Using Wikipedia articles as the knowledge source causes the task of question answering ( QA ) to combine the challenges of both large - scale open - domain QA and of machine comprehension of text .,13,0,36
dataset/preprocessed/test-data/natural_language_inference/24,"In order to answer any question , one must first retrieve the few relevant articles among more than 5 million items , and then scan them carefully to identify the answer .",14,0,32
dataset/preprocessed/test-data/natural_language_inference/24,"We term this setting , machine reading at scale ( MRS ) .",15,0,13
dataset/preprocessed/test-data/natural_language_inference/24,Our work treats Wikipedia as a collection of articles and does not rely on its internal graph structure .,16,0,19
dataset/preprocessed/test-data/natural_language_inference/24,"As a result , our approach is generic and could be switched to other collections of documents , books , or even daily updated newspapers .",17,0,26
dataset/preprocessed/test-data/natural_language_inference/24,"Large - scale QA systems like IBM 's Deep QA rely on multiple sources to answer : besides Wikipedia , it is also paired with KBs , dictionaries , and even news articles , books , etc .",18,0,38
dataset/preprocessed/test-data/natural_language_inference/24,"As a result , such systems heavily rely on information redundancy among the sources to answer correctly .",19,0,18
dataset/preprocessed/test-data/natural_language_inference/24,Having a single knowledge source forces the model to be very precise while searching for an answer as the evidence might appear only once .,20,0,25
dataset/preprocessed/test-data/natural_language_inference/24,"This challenge thus encourages research in the ability of a machine to read , a key motivation for the machine comprehension subfield and the creation of datasets such as SQuAD , and CBT .",21,0,34
dataset/preprocessed/test-data/natural_language_inference/24,"However , those machine comprehension resources typically assume that a short piece of relevant text is already identified and given to the model , which is not realistic for building an opendomain QA system .",22,0,35
dataset/preprocessed/test-data/natural_language_inference/24,"In sharp contrast , methods that use KBs or information retrieval over documents have to employ search as an integral part of the solution .",23,0,25
dataset/preprocessed/test-data/natural_language_inference/24,"Instead MRS is focused on simultaneously maintaining the challenge of machine comprehension , which requires the deep understanding of text , while keeping the realistic constraint of searching over a large open resource .",24,0,34
dataset/preprocessed/test-data/natural_language_inference/24,"In this paper , we show how multiple existing QA datasets can be used to evaluate MRS by requiring an open - domain system to perform well on all of them at once .",25,0,34
dataset/preprocessed/test-data/natural_language_inference/24,"We develop DrQA , a strong system for question answering from Wikipedia composed of : ( 1 ) Document Retriever , a module using bigram hashing and TF - IDF matching designed to , given a question , efficiently return a subset of relevant articles and ( 2 ) Document Reader , a multi - layer recurrent neural network machine comprehension model trained to detect answer spans in those few returned documents .",26,0,73
dataset/preprocessed/test-data/natural_language_inference/24,gives an illustration of DrQA .,27,0,6
dataset/preprocessed/test-data/natural_language_inference/24,Our experiments show that Document Retriever outperforms the built - in Wikipedia search engine and that Document Reader reaches state - of - theart results on the very competitive SQuAD benchmark .,28,0,32
dataset/preprocessed/test-data/natural_language_inference/24,"Finally , our full system is evaluated using multiple benchmarks .",29,0,11
dataset/preprocessed/test-data/natural_language_inference/24,"In particular , we show that performance is improved across all datasets through the use of multitask learning and distant supervision compared to single task training .",30,0,27
dataset/preprocessed/test-data/natural_language_inference/24,"Open-domain QA was originally defined as finding answers in collections of unstructured documents , following the setting of the annual TREC competitions",32,0,22
dataset/preprocessed/test-data/natural_language_inference/24,"With the development of KBs , many recent innovations have occurred in the context of QA from KBs with the creation of resources like WebQuestions and SimpleQuestions based on the Freebase KB , or on automatically extracted KBs , e.g. , OpenIE triples and NELL .",34,0,46
dataset/preprocessed/test-data/natural_language_inference/24,"However , KBs have inherent limitations ( incompleteness , fixed schemas ) that motivated researchers to return to the original setting of answering from raw text .",35,0,27
dataset/preprocessed/test-data/natural_language_inference/24,"A second motivation to cast afresh look at this problem is that of machine comprehension of text , i.e. , answering questions after reading a short text or story .",36,0,30
dataset/preprocessed/test-data/natural_language_inference/24,"That subfield has made considerable progress recently thanks to new deep learning architectures like attention - based and memory - 1 http://trec.nist.gov/data/qamain.html augmented neural networks and release of new training and evaluation datasets like QuizBowl , CNN / Daily Mail based on news articles , CBT based on children books , or SQuAD and WikiReading , both based on Wikipedia .",37,0,61
dataset/preprocessed/test-data/natural_language_inference/24,An objective of this paper is to test how such new methods can perform in an open - domain QA framework .,38,0,22
dataset/preprocessed/test-data/natural_language_inference/24,QA using Wikipedia as a resource has been explored previously .,39,0,11
dataset/preprocessed/test-data/natural_language_inference/24,perform opendomain QA using a Wikipedia - based knowledge model .,40,0,11
dataset/preprocessed/test-data/natural_language_inference/24,"They combine article content with multiple other answer matching modules based on different types of semi-structured knowledge such as infoboxes , article structure , category structure , and definitions .",41,0,30
dataset/preprocessed/test-data/natural_language_inference/24,"Similarly , also combine Wikipedia as a text resource with other resources , in this case with information retrieval over other documents .",42,0,23
dataset/preprocessed/test-data/natural_language_inference/24,also mine knowledge from Wikipedia for QA .,43,0,8
dataset/preprocessed/test-data/natural_language_inference/24,"Instead of using it as a resource for seeking answers to questions , they focus on validating answers returned by their QA system , and use Wikipedia categories for determining a set of patterns that should fit with the expected answer .",44,0,42
dataset/preprocessed/test-data/natural_language_inference/24,"In our work , we consider the comprehension of text only , and use Wikipedia text documents as the sole resource in order to emphasize the task of machine reading at scale , as described in the introduction .",45,0,39
dataset/preprocessed/test-data/natural_language_inference/24,"There area number of highly developed full pipeline QA approaches using either the Web , as does QuASE , or Wikipedia as a resource , as do Microsoft 's AskMSR , IBM 's DeepQA and YodaQA - the latter of which is open source and hence reproducible for comparison purposes .",46,0,51
dataset/preprocessed/test-data/natural_language_inference/24,"MSR is a search - engine based QA system that relies on "" data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers "" , i.e. , it does not focus on machine comprehension , as we do .",48,0,42
dataset/preprocessed/test-data/natural_language_inference/31,Scaling Memory - Augmented Neural Networks with Sparse Reads and Writes,2,1,11
dataset/preprocessed/test-data/natural_language_inference/31,Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks .,4,0,17
dataset/preprocessed/test-data/natural_language_inference/31,These models appear promising for applications such as language modeling and machine translation .,5,0,14
dataset/preprocessed/test-data/natural_language_inference/31,"However , they scale poorly in both space and time as the amount of memory grows - limiting their applicability to real - world domains .",6,0,26
dataset/preprocessed/test-data/natural_language_inference/31,"Here , we present an end - to - end differentiable memory access scheme , which we call Sparse Access Memory ( SAM ) , that retains the representational power of the original approaches whilst training efficiently with very large memories .",7,0,42
dataset/preprocessed/test-data/natural_language_inference/31,"We show that SAM achieves asymptotic lower bounds in space and time complexity , and find that an implementation runs 1,000 faster and with 3,000 less physical memory than non-sparse models .",8,0,32
dataset/preprocessed/test-data/natural_language_inference/31,"SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one - shot Omniglot character recognition , and can scale to tasks requiring 100,000s of time steps and memories .",9,0,36
dataset/preprocessed/test-data/natural_language_inference/31,"As well , we show how our approach can be adapted for models that maintain temporal associations between memories , as with the recently introduced Differentiable Neural Computer .",10,0,29
dataset/preprocessed/test-data/natural_language_inference/31,"Recurrent neural networks , such as the Long Short - Term Memory ( LSTM ) , have proven to be powerful sequence learning models .",12,0,25
dataset/preprocessed/test-data/natural_language_inference/31,"However , one limitation of the LSTM architecture is that the number of parameters grows proportionally to the square of the size of the memory , making them unsuitable for problems requiring large amounts of long - term memory .",13,0,40
dataset/preprocessed/test-data/natural_language_inference/31,"Recent approaches , such as Neural Turing Machines ( NTMs ) and Memory Networks , have addressed this issue by decoupling the memory capacity from the number of model parameters .",14,0,31
dataset/preprocessed/test-data/natural_language_inference/31,We refer to this class of models as memory augmented neural networks ( MANNs ) .,15,1,16
dataset/preprocessed/test-data/natural_language_inference/31,"External memory allows MANNs to learn algorithmic solutions to problems that have eluded the capabilities of traditional LSTMs , and to generalize to longer sequence lengths .",16,0,27
dataset/preprocessed/test-data/natural_language_inference/31,"Nonetheless , MANNs have had limited success in real world application .",17,0,12
dataset/preprocessed/test-data/natural_language_inference/31,"A significant difficulty in training these models results from their smooth read and write operations , which incur linear computational overhead on the number of memories stored per time step of training .",18,0,33
dataset/preprocessed/test-data/natural_language_inference/31,"Even worse , they require duplication of the entire memory at each time step to perform backpropagation through time ( BPTT ) .",19,0,23
dataset/preprocessed/test-data/natural_language_inference/31,"To deal with sufficiently complex problems , such as processing a book , or Wikipedia , this overhead becomes prohibitive .",20,0,21
dataset/preprocessed/test-data/natural_language_inference/31,"For example , to store 64 memories , a straightforward implementation of the NTM trained over a sequence of length 100 consumes ?",21,0,23
dataset/preprocessed/test-data/natural_language_inference/31,"30 MiB physical memory ; to store 64,000 memories the overhead exceeds 29 GiB ( see ) .",22,0,18
dataset/preprocessed/test-data/natural_language_inference/31,"In this paper , we present a MANN named SAM ( sparse access memory ) .",23,0,16
dataset/preprocessed/test-data/natural_language_inference/31,"By thresholding memory modifications to a sparse subset , and using efficient data structures for content - based read operations , our model is optimal in space and time with respect to memory size , while retaining end - to - end gradient based optimization .",24,0,46
dataset/preprocessed/test-data/natural_language_inference/31,"To test whether the model is able to learn with this sparse approximation , we examined its performance on a selection of synthetic and natural tasks : algorithmic tasks from the NTM work , Babi reasoning tasks used with Memory Networks and Omniglot one - shot classification .",25,0,48
dataset/preprocessed/test-data/natural_language_inference/31,We also tested several of these tasks scaled to longer sequences via curriculum learning .,26,0,15
dataset/preprocessed/test-data/natural_language_inference/31,"For large external memories we observed improvements in empirical run-time and memory overhead by up to three orders magnitude over vanilla NTMs , while maintaining near- identical data efficiency and performance .",27,0,32
dataset/preprocessed/test-data/natural_language_inference/31,"Further , in Supplementary",28,0,4
dataset/preprocessed/test-data/natural_language_inference/31,D we demonstrate the generality of our approach by describing how to construct a sparse version of the recently published Differentiable Neural Computer .,29,0,24
dataset/preprocessed/test-data/natural_language_inference/31,"This Sparse Differentiable Neural Computer ( SDNC ) is over 400 faster than the canonical dense variant fora memory size of 2,000 slots , and achieves the best reported result in the Babi tasks without supervising the memory access .",30,0,40
dataset/preprocessed/test-data/natural_language_inference/31,Attention and content - based addressing,32,0,6
dataset/preprocessed/test-data/natural_language_inference/31,An external memory M ?,33,0,5
dataset/preprocessed/test-data/natural_language_inference/31,"M is a collection of N real - valued vectors , or words , of fixed size M .",35,0,19
dataset/preprocessed/test-data/natural_language_inference/31,"A soft read operation is defined to be a weighted average over memory words ,",36,0,15
dataset/preprocessed/test-data/natural_language_inference/31,where w ?,37,0,3
dataset/preprocessed/test-data/natural_language_inference/31,RN is a vector of weights with non-negative entries that sum to one .,38,0,14
dataset/preprocessed/test-data/natural_language_inference/31,Attending to memory is formalized as the problem of computing w.,39,0,11
dataset/preprocessed/test-data/natural_language_inference/31,"A content addressable memory , proposed in , is an external memory with an addressing scheme which selects w based upon the similarity of memory words to a given query q .",40,0,32
dataset/preprocessed/test-data/natural_language_inference/31,"Specifically , for the ith read weight w ( i ) we define ,",41,0,14
dataset/preprocessed/test-data/natural_language_inference/31,"where dis a similarity measure , typically Euclidean distance or cosine similarity , and f is a differentiable monotonic transformation , typically a softmax .",42,0,25
dataset/preprocessed/test-data/natural_language_inference/31,We can think of this as an instance of kernel smoothing where the network learns to query relevant points q.,43,0,20
dataset/preprocessed/test-data/natural_language_inference/31,"Because the read operation ( 1 ) and content - based addressing scheme ( 2 ) are smooth , we can place them within a neural network , and train the full model using backpropagation .",44,0,36
dataset/preprocessed/test-data/natural_language_inference/31,"One recent architecture , Memory Networks , make use of a content addressable memory that is accessed via a series of read operations and has been successfully applied to a number of question answering tasks .",46,0,36
dataset/preprocessed/test-data/natural_language_inference/31,"In these tasks , the memory is pre-loaded using a learned embedding of the provided context , such as a paragraph of text , and then the controller , given an embedding of the question , repeatedly queries the memory by content - based reads to determine an answer .",47,0,50
dataset/preprocessed/test-data/natural_language_inference/31,Neural Turing Machine,48,0,3
dataset/preprocessed/test-data/natural_language_inference/31,"The Neural Turing Machine is a recurrent neural network equipped with a content - addressable memory , similar to Memory Networks , but with the additional capability to write to memory overtime .",49,0,33
dataset/preprocessed/test-data/natural_language_inference/0,Neural Models for Reasoning over Multiple Mentions using Coreference,2,1,9
dataset/preprocessed/test-data/natural_language_inference/0,Many problems in NLP require aggregating information from multiple mentions of the same entity which maybe far apart in the text .,4,0,22
dataset/preprocessed/test-data/natural_language_inference/0,Existing Recurrent Neural Network ( RNN ) layers are biased towards short - term dependencies and hence not suited to such tasks .,5,0,23
dataset/preprocessed/test-data/natural_language_inference/0,We present a recurrent layer which is instead biased towards coreferent dependencies .,6,0,13
dataset/preprocessed/test-data/natural_language_inference/0,The layer uses coreference annotations extracted from an external system to connect entity mentions belonging to the same cluster .,7,0,20
dataset/preprocessed/test-data/natural_language_inference/0,"Incorporating this layer into a state - of - the - art reading comprehension model improves performance on three datasets - Wikihop , LAMBADA and the b Abi AI tasks - with large gains when training data is scarce .",8,0,40
dataset/preprocessed/test-data/natural_language_inference/0,A long - standing goal of NLP is to build systems capable of reasoning about the information present in text .,10,0,21
dataset/preprocessed/test-data/natural_language_inference/0,One important form of reasoning for Question Answering ( QA ) models is the ability to aggregate information from multiple mentions of entities .,11,0,24
dataset/preprocessed/test-data/natural_language_inference/0,"We call this coreference - based reasoning since multiple pieces of information , which may lie across sentence , paragraph or document boundaries , are tied together with the help of referring expressions which denote the same real - world entity .",12,1,42
dataset/preprocessed/test-data/natural_language_inference/0,shows examples .,13,0,3
dataset/preprocessed/test-data/natural_language_inference/0,"QA models which directly read text to answer questions ( commonly known as Reading Comprehension systems ) , typically consist of RNN layers .",14,0,24
dataset/preprocessed/test-data/natural_language_inference/0,"RNN layers have a bias towards sequential recency , i.e. a tendency to favor short - term dependencies .",15,0,19
dataset/preprocessed/test-data/natural_language_inference/0,"Attention mechanisms alleviate part of the issue , but empirical studies suggest RNNs with attention also have difficulty modeling long - term dependencies .",16,0,24
dataset/preprocessed/test-data/natural_language_inference/0,"We conjecture that when training data is scarce , and inductive biases play an important role , RNN - based models would have trouble with coreference - based reasoning .",17,0,30
dataset/preprocessed/test-data/natural_language_inference/0,"At the same time , systems for coreference resolution have seen a gradual increase in accuracy over the years .",18,0,20
dataset/preprocessed/test-data/natural_language_inference/0,"Hence , in this work we use the annotations produced by such systems to adapt a standard RNN layer by introducing a bias towards coreferent recency .",19,0,27
dataset/preprocessed/test-data/natural_language_inference/0,"Specifically , given an input sequence and coreference clusters extracted from an external system , we introduce a term in the update equations for Gated Recurrent Units ( GRU ) which depends on the hidden state of the coreferent antecedent of the current token ( if it exists ) .",20,0,50
dataset/preprocessed/test-data/natural_language_inference/0,This way hidden states are propagated along coreference chains and the original sequence in parallel .,21,0,16
dataset/preprocessed/test-data/natural_language_inference/0,We compare our Coref - GRU layer with the regular GRU layer by incorporating it in a recent model for reading comprehension .,22,0,23
dataset/preprocessed/test-data/natural_language_inference/0,"On synthetic data specifically constructed to test coreferencebased reasoning , C - GRUs lead to a large improvement over regular GRUs .",23,0,22
dataset/preprocessed/test-data/natural_language_inference/0,We show that the structural bias introduced and coreference signals are both important to reach high performance in this case .,24,0,21
dataset/preprocessed/test-data/natural_language_inference/0,"On a more realistic dataset , with noisy coreference annotations , we see small but significant improvements over a state - of - the - art baseline .",25,0,28
dataset/preprocessed/test-data/natural_language_inference/0,"As we reduce the training data , the gains become larger .",26,0,12
dataset/preprocessed/test-data/natural_language_inference/0,"Lastly , we apply the same model to a broad - context language modeling task , where coreference resolution is an important factor , and show improved performance over state - of - the - art .",27,0,37
dataset/preprocessed/test-data/natural_language_inference/0,Entity - based models .,29,0,5
dataset/preprocessed/test-data/natural_language_inference/0,presented a generative model for jointly predicting the next word in the text and its gold - standard coreference annotation .,30,0,21
dataset/preprocessed/test-data/natural_language_inference/0,"The difference in our work is that we look at the task of reading comprehension , and also work in the more practical setting of system extracted coreferences .",31,0,29
dataset/preprocessed/test-data/natural_language_inference/0,"EntNets also maintain dynamic memory slots for entities , but do not use coreference signals and instead update all memories after reading each sentence , which leads to poor performance in the low - data regime ( c.f.. model references in text as explicit latent variables , but limit their work to text generation .",32,0,55
dataset/preprocessed/test-data/natural_language_inference/0,used a pooling operation to aggregate entity information across multiple mentions .,33,0,12
dataset/preprocessed/test-data/natural_language_inference/0,"also noted the importance of reference resolution for reading comprehension , and we compare our model to their one - hot pointer reader .",34,0,24
dataset/preprocessed/test-data/natural_language_inference/0,Syntactic - recency .,35,0,4
dataset/preprocessed/test-data/natural_language_inference/0,"Recent work has used syntax , in the form of dependency trees , to replace the sequential recency bias in RNNs with a syntactic recency bias .",36,0,27
dataset/preprocessed/test-data/natural_language_inference/0,"However , syntax only looks at dependencies within sentence boundaries , whereas our focus here is on longer ranges .",37,0,20
dataset/preprocessed/test-data/natural_language_inference/0,"Our resulting layer is structurally similar to Graph LSTMs , with an additional attention mechanism over the graph edges .",38,0,20
dataset/preprocessed/test-data/natural_language_inference/0,"However , while found that using coreference did not lead to any gains for the task of relation extraction , here we show that it has a positive impact on the reading comprehension task .",39,0,35
dataset/preprocessed/test-data/natural_language_inference/0,"Self - Attention models are becoming popular for modeling long - term dependencies , and may also benefit from coreference information to bias the learning of those dependencies .",40,0,29
dataset/preprocessed/test-data/natural_language_inference/0,Here we focus on recurrent layers and leave such an analysis to future work .,41,0,15
dataset/preprocessed/test-data/natural_language_inference/0,Part of this work was described in an unpub - lished preprint .,42,0,13
dataset/preprocessed/test-data/natural_language_inference/0,The current paper extends that version and focuses exclusively on coreference relations .,43,0,13
dataset/preprocessed/test-data/natural_language_inference/0,"We also report results on the WikiHop dataset , including the performance of the model in the low - data regime .",44,0,22
dataset/preprocessed/test-data/natural_language_inference/0,Coref - GRU ( C - GRU ) Layer .,46,0,10
dataset/preprocessed/test-data/natural_language_inference/0,"Suppose we are given an input sequence w 1 , w 2 , . . . , w T along with their word vectors x 1 , . . . , x T and annotations for the most recent coreferent antecedent for each token y 1 , . . . , y T , where y t ?",47,0,58
dataset/preprocessed/test-data/natural_language_inference/0,"{ 0 , . . . , t ?",48,0,9
dataset/preprocessed/test-data/natural_language_inference/0,1 } and y t = 0 denotes the null antecedent ( for tokens not belonging to any cluster ) .,49,0,21
dataset/preprocessed/test-data/natural_language_inference/12,FINDING REMO ( RELATED MEMORY OBJECT ) : A SIMPLE NEURAL ARCHITECTURE FOR TEXT BASED REASONING,2,1,16
dataset/preprocessed/test-data/natural_language_inference/12,"To solve the text - based question and answering task that requires relational reasoning , it is necessary to memorize a large amount of information and find out the question relevant information from the memory .",4,1,36
dataset/preprocessed/test-data/natural_language_inference/12,Most approaches were based on external memory and four components proposed by Memory Network .,5,0,15
dataset/preprocessed/test-data/natural_language_inference/12,The distinctive component among them was the way of finding the necessary information and it contributes to the performance .,6,0,20
dataset/preprocessed/test-data/natural_language_inference/12,"Recently , a simple but powerful neural network module for reasoning called Relation Network ( RN ) has been introduced .",7,0,21
dataset/preprocessed/test-data/natural_language_inference/12,"We analyzed RN from the view of Memory Network , and realized that its MLP component is able to reveal the complicate relation between question and object pair .",8,0,29
dataset/preprocessed/test-data/natural_language_inference/12,"Motivated from it , we introduce Relation Memory Network ( RMN ) which uses MLP to find out relevant information on Memory Network architecture .",9,0,25
dataset/preprocessed/test-data/natural_language_inference/12,It shows new state - of - the - art results in jointly trained b AbI - 10 k story - based question answering tasks and bAbI dialog - based question answering tasks .,10,0,34
dataset/preprocessed/test-data/natural_language_inference/12,* Code is publicly available at : https://github.com/juung/RMN,11,0,8
dataset/preprocessed/test-data/natural_language_inference/12,Neural network has made an enormous progress on the two major challenges in artificial intelligence : seeing and reading .,13,0,20
dataset/preprocessed/test-data/natural_language_inference/12,"In both areas , embedding methods have served as the main vehicle to process and analyze text and image data for solving classification problems .",14,0,25
dataset/preprocessed/test-data/natural_language_inference/12,"As for the task of logical reasoning , however , more complex and careful handling of features is called for .",15,0,21
dataset/preprocessed/test-data/natural_language_inference/12,A reasoning task requires the machine to answer a simple question upon the delivery of a series of sequential information .,16,0,21
dataset/preprocessed/test-data/natural_language_inference/12,"For example , imagine that the machine is given the following three sentences : "" Mary got the milk there . "" ,",17,0,23
dataset/preprocessed/test-data/natural_language_inference/12,""" John moved to the bedroom . "" , and "" Mary traveled to the hallway . """,18,0,18
dataset/preprocessed/test-data/natural_language_inference/12,"Once prompted with the question , "" Where is the milk ? "" , the machine then needs to sequentially focus on the two supporting sentences , "" Mary got the milk there . "" and "" Mary traveled to the hallway . "" in order to successfully determine that the milk is located in the hallway .",19,0,58
dataset/preprocessed/test-data/natural_language_inference/12,"Inspired by this reasoning mechanism , J. has introduced the memory network ( MemNN ) , which consists of an external memory and four components : input feature map ( I ) , generalization ( G ) , output feature map ( O ) , and response ( R ) .",20,0,51
dataset/preprocessed/test-data/natural_language_inference/12,The external memory enables the model to deal with a knowledge base without loss of information .,21,0,17
dataset/preprocessed/test-data/natural_language_inference/12,Input feature map embeds the incoming sentences .,22,0,8
dataset/preprocessed/test-data/natural_language_inference/12,Generalization updates old memories given the new input and output feature map finds relevant information from the memory .,23,0,19
dataset/preprocessed/test-data/natural_language_inference/12,"Finally , response produces the final output .",24,0,8
dataset/preprocessed/test-data/natural_language_inference/12,"Based on the memory network architecture , neural network based models like end - to - end memory network ( Mem N2N ) , gated end - to - end memory network ( GMe m N2N ) , dynamic memory network ( DMN ) , and dynamic memory network + ( DMN + ) are proposed .",25,0,57
dataset/preprocessed/test-data/natural_language_inference/12,"Since strong reasoning ability depends on whether the model is able to sequentially catching the right supporting sentences that lead to the answer , the most important thing that discriminates those models is the way of constructing the output feature map .",26,0,42
dataset/preprocessed/test-data/natural_language_inference/12,"As the output feature map becomes more complex , it is able to learn patterns for more complicate relations .",27,0,20
dataset/preprocessed/test-data/natural_language_inference/12,"For example , MemN2N , which has the lowest performance among the four models , measures the relatedness between question and sentence by the inner product , while the best performing DMN + uses inner product and absolute difference with two embedding matrices .",28,0,44
dataset/preprocessed/test-data/natural_language_inference/12,"Recently , a new architecture called Relation Network ( RN ) has been proposed as a general solution to relational reasoning .",29,0,22
dataset/preprocessed/test-data/natural_language_inference/12,The design philosophy behind it is to directly capture the supporting relation between the sentences through the multi - layer perceptron ( MLP ) .,30,0,25
dataset/preprocessed/test-data/natural_language_inference/12,"Despite its simplicity , RN achieves better performance than previous models without any catastrophic failure .",31,0,16
dataset/preprocessed/test-data/natural_language_inference/12,The interesting thing we found is that RN can also be interpreted in terms of MemNN .,32,0,17
dataset/preprocessed/test-data/natural_language_inference/12,It is composed of O and R where each corresponds to MLP which focuses on the related pair and another MLP which infers the answer .,33,0,26
dataset/preprocessed/test-data/natural_language_inference/12,RN does not need to have G because it directly finds all the supporting sentences at once .,34,0,18
dataset/preprocessed/test-data/natural_language_inference/12,"In this point of view , the significant component would be MLP - based output feature map .",35,0,18
dataset/preprocessed/test-data/natural_language_inference/12,"As MLP is enough to recognize highly non-linear pattern , RN could find the proper relation better than previous models to answer the given question .",36,0,26
dataset/preprocessed/test-data/natural_language_inference/12,"However , as RN considers a pair at a time unlike MemNN , the number of relations that RN learns is n 2 when the number of input sentence is n .",37,0,32
dataset/preprocessed/test-data/natural_language_inference/12,"When n is small , the cost of learning relation is reduced by n times compared to MemNN based models , which enables more data - efficient learning .",38,0,29
dataset/preprocessed/test-data/natural_language_inference/12,"However , when n increases , the performance becomes worse than the previous models .",39,0,15
dataset/preprocessed/test-data/natural_language_inference/12,"In this case , the pair - wise operation increases the number of non-related sentence pairs more than the related sentence pair , thereby confuses RN 's learning .",40,0,29
dataset/preprocessed/test-data/natural_language_inference/12,"has suggested attention mechanisms as a solution to filter out unimportant relations ; however , since it interrupts the reasoning operation , it may not be the most optimal solution to the problem .",41,0,34
dataset/preprocessed/test-data/natural_language_inference/12,"Our proposed model , "" Relation Memory Network "" ( RMN ) , is able to find complex relation even when a lot of information is given .",42,0,28
dataset/preprocessed/test-data/natural_language_inference/12,It uses MLP to find out relevant information with a new generalization which simply erase the information already used .,43,0,20
dataset/preprocessed/test-data/natural_language_inference/12,"In other words , RMN inherits RN 's MLP - based output feature map on Memory Network architecture .",44,0,19
dataset/preprocessed/test-data/natural_language_inference/12,Experiments show its state - of the - art result on the text - based question answering tasks .,45,0,19
dataset/preprocessed/test-data/natural_language_inference/12,"Relation Memory Network ( RMN ) is composed of four components - embedding , attention , updating , and reasoning .",46,0,21
dataset/preprocessed/test-data/natural_language_inference/12,"It takes as the inputs a set of sentences x 1 , x 2 , ... , x n and its related question u , and outputs an answer a .",47,0,31
dataset/preprocessed/test-data/natural_language_inference/12,"Each of the x i , u , and a is made up of one - hot representation of words , for example ,",48,0,24
dataset/preprocessed/test-data/natural_language_inference/12,RELATION MEMORY NETWORK,49,0,3
dataset/preprocessed/test-data/natural_language_inference/2,Stochastic Answer Networks for Natural Language Inference,2,1,7
dataset/preprocessed/test-data/natural_language_inference/2,We utilize a stochastic answer network ( SAN ) to explore multi-step inference strategies in Natural Language Inference .,4,0,19
dataset/preprocessed/test-data/natural_language_inference/2,"Rather than directly predicting the results given the inputs , the model maintains a state and iteratively refines its predictions .",5,0,21
dataset/preprocessed/test-data/natural_language_inference/2,This can potentially model more complex inferences than the existing single - step inference methods .,6,0,16
dataset/preprocessed/test-data/natural_language_inference/2,"Our experiments show that SAN achieves state - of - the - art results on four benchmarks : Stanford Natural Language Inference ( SNLI ) , MultiGenre Natural Language Inference ( MultiNLI ) , SciTail , and Quora Question Pairs datasets .",7,0,42
dataset/preprocessed/test-data/natural_language_inference/2,"The natural language inference task , also known as recognizing textual entailment ( RTE ) , is to infer the relation between a pair of sentences ( e.g. , premise and hypothesis ) .",9,1,34
dataset/preprocessed/test-data/natural_language_inference/2,"This task is challenging , since it requires a model to fully understand the sentence meaning , ( i.e. , lexical and compositional semantics ) .",10,0,26
dataset/preprocessed/test-data/natural_language_inference/2,"For instance , the following example from MultiNLI dataset illustrates the need fora form of multi-step synthesis of information between premise :",11,0,22
dataset/preprocessed/test-data/natural_language_inference/2,""" If you need this book , it is probably too late unless you are about to take an SAT or GRE . "" , and hypothesis :",12,0,28
dataset/preprocessed/test-data/natural_language_inference/2,""" It 's never too late , unless you 're about to take a test . """,13,0,17
dataset/preprocessed/test-data/natural_language_inference/2,"To predict the correct relation between these two sentences , the model needs to first infer that "" SAT or GRE "" is a "" test "" , and then pick the correct relation , e.g. , contradiction .",14,0,39
dataset/preprocessed/test-data/natural_language_inference/2,This kind of iterative process can be viewed as a form of multi-step inference .,15,0,15
dataset/preprocessed/test-data/natural_language_inference/2,"To best of our knowledge , all of works on NLI use a single step inference .",16,0,17
dataset/preprocessed/test-data/natural_language_inference/2,"Inspired by the recent success of multi-step inference on Machine Reading Comprehension ( MRC ) , we explore the multi-step inference strategies on NLI .",17,0,25
dataset/preprocessed/test-data/natural_language_inference/2,"Rather than directly predicting the results given the inputs , the model maintains a state and iteratively refines its predictions .",18,0,21
dataset/preprocessed/test-data/natural_language_inference/2,"We show that our model outperforms single - step inference and further achieves the state - of - the - art on SNLI , MultiNLI , SciTail , and Quora Question Pairs datasets .",19,0,34
dataset/preprocessed/test-data/natural_language_inference/2,Multi- step inference with SAN,20,0,5
dataset/preprocessed/test-data/natural_language_inference/2,"The natural language inference task as defined here involves a premise P = {p 0 , p 1 , ... , p m?1 } of m words and a hypothesis H = {h 0 , h 1 , ... , h n?1 } of n words , and aims to find a logic relationship R between P and H , which is one of labels in a close set : entailment , neutral and contradiction .",21,0,76
dataset/preprocessed/test-data/natural_language_inference/2,"The goal is to learn a model f ( P , H ) ?",22,0,14
dataset/preprocessed/test-data/natural_language_inference/2,"Ina single - step inference architecture , the model directly predicts R given P and H as input .",24,0,19
dataset/preprocessed/test-data/natural_language_inference/2,"In our multi-step inference architecture , we additionally incorporate a recurrent state st ; the model processes multiple passes through P and H , iteratively refining the state st , before finally generating the output at step t = T , where T is an a priori chosen limit on the number of inference steps .",25,0,56
dataset/preprocessed/test-data/natural_language_inference/2,describes in detail the architecture of the stochastic answer network ( SAN ) used in this study ; this model is adapted from the MRC multistep inference literature .,26,0,29
dataset/preprocessed/test-data/natural_language_inference/2,"Compared to the original SAN for MRC , in the SAN for NLI we simplify the bottom layers and Selfattention layers since the length of the premise and hypothesis is short ) .",27,0,33
dataset/preprocessed/test-data/natural_language_inference/2,We also modify the answer module from prediction a text span to an NLI classification label .,28,0,17
dataset/preprocessed/test-data/natural_language_inference/2,"Overall , it contains four different layers :",29,0,8
dataset/preprocessed/test-data/natural_language_inference/2,1 ) the lexicon encoding layer computes word representations ; 2 ) the contextual encoding layer modifies these representations in context ; 3 ) the memory generation layer gathers all information from the premise and hypothesis and forms a,30,0,39
dataset/preprocessed/test-data/natural_language_inference/2,Information Gathering Layer s t - 1 s t s t +1 s t - 1 s t s t+1,31,0,20
dataset/preprocessed/test-data/natural_language_inference/2,Memory : Architecture of the Stochastic Answer Network ( SAN ) for Natural Language Inference .,32,0,16
dataset/preprocessed/test-data/natural_language_inference/2,""" working memory "" for the final answer module ; 4 ) the final answer module , a type of multi-step network , predicts the relation between the premise and hypothesis .",33,0,32
dataset/preprocessed/test-data/natural_language_inference/2,Lexicon Encoding Layer .,34,0,4
dataset/preprocessed/test-data/natural_language_inference/2,First we concatenate word embeddings and character embeddings to handle the out - of - vocabulary words,35,0,17
dataset/preprocessed/test-data/natural_language_inference/2,"Following , we use two separate twolayer position - wise feedforward network to obtain the final lexicon embedings , E p ?",37,0,22
dataset/preprocessed/test-data/natural_language_inference/2,R dm and E h ?,38,0,6
dataset/preprocessed/test-data/natural_language_inference/2,"R dn , for the tokens in P and H , respectively .",39,0,13
dataset/preprocessed/test-data/natural_language_inference/2,"Here , dis the hidden size .",40,0,7
dataset/preprocessed/test-data/natural_language_inference/2,Contextual Encoding Layer .,41,0,4
dataset/preprocessed/test-data/natural_language_inference/2,"Two stacked BiL - STM layers are used on the lexicon encoding layer to encode the context information for each word in both P and H . Due to the bidirectional layer , it doubles the hidden size .",42,0,39
dataset/preprocessed/test-data/natural_language_inference/2,"We use a maxout layer ( Goodfellow et al. , 2013 ) on the BiLSTM to shrink its output into its original hidden size .",43,0,25
dataset/preprocessed/test-data/natural_language_inference/2,"By a concatenation of the outputs of two BiLSTM layers , we obtain C p ?",44,0,16
dataset/preprocessed/test-data/natural_language_inference/2,R 2 dm and Ch ?,45,0,6
dataset/preprocessed/test-data/natural_language_inference/2,"R 2 dn as representation of P and H , respectively .",46,0,12
dataset/preprocessed/test-data/natural_language_inference/2,We omit POS Tagging and Name Entity Features for simplicity Memory Layer .,47,0,13
dataset/preprocessed/test-data/natural_language_inference/2,We construct our working memory via an attention mechanism .,48,0,10
dataset/preprocessed/test-data/natural_language_inference/2,"First , a dotproduct attention is adopted like in to measure the similarity between the tokens in P and H. Instead of using a scalar to normalize the scores as in , we use a layer projection to transform the contextual information of both C p and Ch :",49,0,49
dataset/preprocessed/test-data/natural_language_inference/4,MemoReader : Large - Scale Reading Comprehension through Neural Memory Controller,2,1,11
dataset/preprocessed/test-data/natural_language_inference/4,Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text .,4,1,20
dataset/preprocessed/test-data/natural_language_inference/4,"Existing approaches made a significant progress comparable to human - level performance , but they are still limited in understanding , up to a few paragraphs , failing to properly comprehend lengthy document .",5,0,34
dataset/preprocessed/test-data/natural_language_inference/4,"In this paper , we propose a novel deep neural network architecture to handle a long - range dependency in RC tasks .",6,1,23
dataset/preprocessed/test-data/natural_language_inference/4,"In detail , our method has two novel aspects : ( 1 ) an advanced memory - augmented architecture and ( 2 ) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory .",7,0,41
dataset/preprocessed/test-data/natural_language_inference/4,Our proposed architecture is widely applicable to other models .,8,0,10
dataset/preprocessed/test-data/natural_language_inference/4,"We have performed extensive experiments with well - known benchmark datasets such as TriviaQA , QUASAR - T , and SQ u AD .",9,0,24
dataset/preprocessed/test-data/natural_language_inference/4,"The experimental results demonstrate that the proposed method outperforms existing methods , especially for lengthy documents .",10,0,17
dataset/preprocessed/test-data/natural_language_inference/4,Most of the human knowledge has been stored in the form of text .,12,0,14
dataset/preprocessed/test-data/natural_language_inference/4,Reading comprehension ( RC ) to understand this knowledge is a major challenge that can vastly increase the range of knowledge available to the machines .,13,1,26
dataset/preprocessed/test-data/natural_language_inference/4,"Many neural networkbased methods have been proposed , pushing performance close to a human level .",14,0,16
dataset/preprocessed/test-data/natural_language_inference/4,"Nonetheless , there still exists room to improve the performance especially in comprehending lengthy documents that involve complicated reasoning processes .",15,0,21
dataset/preprocessed/test-data/natural_language_inference/4,We identify the main bottleneck as the lack of the long - term memory and its improper controlling mechanism .,16,0,20
dataset/preprocessed/test-data/natural_language_inference/4,"Previously , several memory - augmenting methods have been proposed to solve the long - term de - *",17,0,19
dataset/preprocessed/test-data/natural_language_inference/4,To whom correspondence should be addressed .,18,0,7
dataset/preprocessed/test-data/natural_language_inference/4,pendency problem .,19,0,3
dataset/preprocessed/test-data/natural_language_inference/4,"For example , in relatively simple tasks such as bAbI tasks , ; proposed methods that handle the external memory to address long - term dependency .",20,0,27
dataset/preprocessed/test-data/natural_language_inference/4,"Inspired by these approaches , we develop a customized memory controller along with an external memory augmentation for complicated RC tasks .",21,0,22
dataset/preprocessed/test-data/natural_language_inference/4,"However , we found that the memory controller is susceptible to information distortion as neural networks become deeper , this distortion can hinder the performance .",22,0,26
dataset/preprocessed/test-data/natural_language_inference/4,"To overcome this issue , we propose two novel strategies that improve the memory - handling capability while mitigating the information distortion .",23,0,23
dataset/preprocessed/test-data/natural_language_inference/4,We extend the memory controller with a residual connection to alleviate the information distortion occurring in it .,24,0,18
dataset/preprocessed/test-data/natural_language_inference/4,We also expand the gated recurrent unit ( GRU ) with a dense connection that conveys enriched features to the next layer containing the original as well as the transformed information .,25,0,32
dataset/preprocessed/test-data/natural_language_inference/4,"We conducted extensive experiments through several benchmark datasets such as TriviaQA , QUASAR - T , and SQ u AD .",26,0,21
dataset/preprocessed/test-data/natural_language_inference/4,The results show that the proposed model outperforms all the published results .,27,0,13
dataset/preprocessed/test-data/natural_language_inference/4,We also integrated the proposed memory controller and the expanded GRU cell block with other existing methods to ensure that our proposed components are widely applicable .,28,0,27
dataset/preprocessed/test-data/natural_language_inference/4,The results show that our components consistently bring performance improvement across various state - of - the - art architectures .,29,0,21
dataset/preprocessed/test-data/natural_language_inference/4,The main contributions of this work include the following :,30,0,10
dataset/preprocessed/test-data/natural_language_inference/4,( 1 ) We propose an extended memory controller module for RC tasks .,31,0,14
dataset/preprocessed/test-data/natural_language_inference/4,"( 2 ) We propose a densely connected encoder block with self attention to provide rich representation of given data , reducing information loss due to deep layers of the network .",32,0,32
dataset/preprocessed/test-data/natural_language_inference/4,( 3 ) We present the state - of - the - art results in lengthy - document RC tasks such as TriviaQA and QUASAR - T as well as relatively short document RC tasks such as SQuAD .,33,0,39
dataset/preprocessed/test-data/natural_language_inference/4,"This section presents two of our proposed components in detail , as depicted in .",35,0,15
dataset/preprocessed/test-data/natural_language_inference/4,Our first proposed component is an advanced external memory controller module for solving RC tasks .,37,0,16
dataset/preprocessed/test-data/natural_language_inference/4,We modified the recently proposed memory controller by using our new encoder block and layer - wise residual connections .,38,0,20
dataset/preprocessed/test-data/natural_language_inference/4,"These modifications enable the memory controller to reason over a lengthy document , leading to the overall performance improvement .",39,0,20
dataset/preprocessed/test-data/natural_language_inference/4,"This layer takes input as a sequence of vector representations corresponding to individual tokens , d t ?",40,0,18
dataset/preprocessed/test-data/natural_language_inference/4,"R l , where l is the given vector dimension .",41,0,11
dataset/preprocessed/test-data/natural_language_inference/4,"For example , such input can be the output of the co-attention layer in Section 3 .",42,0,17
dataset/preprocessed/test-data/natural_language_inference/4,The operation of this layer is defined as,43,0,8
dataset/preprocessed/test-data/natural_language_inference/4,"That is , at time step t , the controller generates an interface vector it for read and write operations and an output vector o t based on the input vector d t and the external memory content from the previous time step , M t?1 ?",44,0,47
dataset/preprocessed/test-data/natural_language_inference/4,"R pq , where p is the memory size and q is the vector dimension of each memory .",45,0,19
dataset/preprocessed/test-data/natural_language_inference/4,"Through this controller , we encode an input D = {d t } n t =1 to {x t } n t = 1 by using the encoder block , i.e. ,",46,0,32
dataset/preprocessed/test-data/natural_language_inference/4,where k is the output dimension of the encoder block .,47,0,11
dataset/preprocessed/test-data/natural_language_inference/4,"In general , this block is implemented as a recurrent unit , e.g. , GRU .",48,0,16
dataset/preprocessed/test-data/natural_language_inference/4,"In our model , we replace it with our dense encoder block with self attention ( DEBS ) , as will be discussed in Section 2.2 .",49,0,27
dataset/preprocessed/test-data/natural_language_inference/13,Natural Language Comprehension with the EpiReader,2,1,6
dataset/preprocessed/test-data/natural_language_inference/13,"We present the EpiReader , a novel model for machine comprehension of text .",4,1,14
dataset/preprocessed/test-data/natural_language_inference/13,"Machine comprehension of unstructured , real - world text is a major research goal for natural language processing .",5,1,19
dataset/preprocessed/test-data/natural_language_inference/13,"Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text , and evaluate a model 's response to the questions .",6,1,27
dataset/preprocessed/test-data/natural_language_inference/13,"The EpiReader is an end - to - end neural model comprising two components : the first component proposes a small set of candidate answers after comparing a question to its supporting text , and the second component formulates hypotheses using the proposed candidates and the question , then reranks the hypotheses based on their estimated concordance with the supporting text .",7,0,62
dataset/preprocessed/test-data/natural_language_inference/13,"We present experiments demonstrating that the EpiReader sets anew state - of - the - art on the CNN and Children 's Book Test machine comprehension benchmarks , outperforming previous neural models by a significant margin .",8,0,37
dataset/preprocessed/test-data/natural_language_inference/13,"When humans reason about the world , we tend to formulate a variety of hypotheses and counterfactuals , then test them in turn by physical or thought experiments .",10,0,29
dataset/preprocessed/test-data/natural_language_inference/13,"The philosopher Epicurus first formalized this idea in his Principle of Multiple Explanations : if several theories are consistent with the observed data , retain them all until more data is observed .",11,0,33
dataset/preprocessed/test-data/natural_language_inference/13,"In this paper , we argue that the same principle can be applied to machine comprehension of natural language .",12,0,20
dataset/preprocessed/test-data/natural_language_inference/13,"We propose a deep , end - to - end , neural comprehension model that we call the EpiReader .",13,0,20
dataset/preprocessed/test-data/natural_language_inference/13,"Comprehension of natural language by machines , at a near- human level , is a prerequisite for an extremely broad class of useful applications of artificial intelligence .",14,0,28
dataset/preprocessed/test-data/natural_language_inference/13,"Indeed , most human knowledge is collected in the natural language of text .",15,0,14
dataset/preprocessed/test-data/natural_language_inference/13,Machine comprehension ( MC ) has therefore garnered significant attention from the machine learning research community .,16,0,17
dataset/preprocessed/test-data/natural_language_inference/13,"Machine comprehension is typically evaluated by posing a set of questions based on a supporting text passage , then scoring a system 's answers to those questions .",17,0,28
dataset/preprocessed/test-data/natural_language_inference/13,We all took similar tests in school .,18,0,8
dataset/preprocessed/test-data/natural_language_inference/13,"Such tests are objectively gradable and may assess a range of abilities , from basic understanding to causal reasoning to inference .",19,0,22
dataset/preprocessed/test-data/natural_language_inference/13,"In the past year , two large - scale MC datasets have been released : the CNN / Daily Mail corpus , consisting of news articles from those outlets , and the Children 's Book Test ( CBT ) , consisting of short excerpts from books available through Project Gutenberg .",20,0,51
dataset/preprocessed/test-data/natural_language_inference/13,The size of these datasets ( on the order of 10 5 distinct questions ) makes them amenable to data - intensive deep learning techniques .,21,0,26
dataset/preprocessed/test-data/natural_language_inference/13,"Both corpora use Clozestyle questions , which are formulated by replacing a word or phrase in a given sentence with a placeholder token .",22,0,24
dataset/preprocessed/test-data/natural_language_inference/13,"The task is then to find the answer that "" fills in the blank "" .",23,0,16
dataset/preprocessed/test-data/natural_language_inference/13,"In tandem with these corpora , a host of neural machine comprehension models has been developed .",24,0,17
dataset/preprocessed/test-data/natural_language_inference/13,We compare the EpiReader to these earlier models through training and evaluation on the CNN and CBT datasets .,25,0,19
dataset/preprocessed/test-data/natural_language_inference/13,The EpiReader factors into two components .,26,0,7
dataset/preprocessed/test-data/natural_language_inference/13,The first component extracts a small set of potential answers based on a shallow comparison of the question with its supporting text ; we call this the Extractor .,27,0,29
dataset/preprocessed/test-data/natural_language_inference/13,The second component reranks the proposed answers based on deeper semantic comparisons with the text ; we call this the Reasoner .,28,0,22
dataset/preprocessed/test-data/natural_language_inference/13,We can summarize this process as Extract ?,29,0,8
dataset/preprocessed/test-data/natural_language_inference/13,Test 2 .,31,0,3
dataset/preprocessed/test-data/natural_language_inference/13,"The semantic comparisons implemented by the Reasoner are based on the concept of recognizing textual entailment ( RTE ) , also known as natural language inference .",32,0,27
dataset/preprocessed/test-data/natural_language_inference/13,This process is computationally demanding .,33,0,6
dataset/preprocessed/test-data/natural_language_inference/13,"Thus , the Extractor serves the important function of filtering a large set of potential answers down to a small , tractable set of likely candidates for more thorough testing .",34,0,31
dataset/preprocessed/test-data/natural_language_inference/13,"The Extractor follows the form of a pointer network , and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question .",35,0,28
dataset/preprocessed/test-data/natural_language_inference/13,This approach was used ( on it s own ) for question answering with the Attention Sum Reader .,36,0,19
dataset/preprocessed/test-data/natural_language_inference/13,The Extractor outputs a small set of answer candidates along with their estimated probabilities of correctness .,37,0,17
dataset/preprocessed/test-data/natural_language_inference/13,"The Reasoner forms hypotheses by inserting the candidate answers into the question , then estimates the concordance of each hypothesis with each sentence in the supporting text .",38,0,28
dataset/preprocessed/test-data/natural_language_inference/13,"We use these estimates as a measure of the evidence for a hypothesis , and aggregate evidence overall sentences .",39,0,20
dataset/preprocessed/test-data/natural_language_inference/13,"In the end , we combine the Reasoner 's evidence with the Extractor 's probability estimates to produce a final ranking of the answer candidates .",40,0,26
dataset/preprocessed/test-data/natural_language_inference/13,This paper is organized as follows .,41,0,7
dataset/preprocessed/test-data/natural_language_inference/13,In Section 2 we formally define the problem to be solved and give some background on the datasets used in our tests .,42,0,23
dataset/preprocessed/test-data/natural_language_inference/13,"In Section 3 we describe the EpiReader , focusing on its two components and how they combine .",43,0,18
dataset/preprocessed/test-data/natural_language_inference/13,"Section 4 discusses related work , and Section 5 details our experimental results and analysis .",44,0,16
dataset/preprocessed/test-data/natural_language_inference/13,We conclude in Section 6 . The CNN and Daily Mail datasets were released together and have the same form .,45,0,21
dataset/preprocessed/test-data/natural_language_inference/13,"The Daily Mail dataset is significantly larger , and our tests with this data are still in progress .",46,0,19
dataset/preprocessed/test-data/natural_language_inference/13,"The Extractor performs extraction , while the Reasoner both hypothesizes and tests .",47,0,13
dataset/preprocessed/test-data/natural_language_inference/13,"Problem definition , notation , datasets",48,0,6
dataset/preprocessed/test-data/natural_language_inference/13,The task of the EpiReader is to answer a Cloze - style question by reading and comprehending a supporting passage of text .,49,0,23
dataset/preprocessed/test-data/natural_language_inference/20,Neural Tree Indexers for Text Understanding,2,1,6
dataset/preprocessed/test-data/natural_language_inference/20,Recurrent neural networks ( RNNs ) process input text sequentially and model the conditional transition between word tokens .,4,0,19
dataset/preprocessed/test-data/natural_language_inference/20,"In contrast , the advantages of recursive networks include that they explicitly model the compositionality and the recursive structure of natural language .",5,0,23
dataset/preprocessed/test-data/natural_language_inference/20,"However , the current recursive architecture is limited by its dependence on syntactic tree .",6,0,15
dataset/preprocessed/test-data/natural_language_inference/20,"In this paper , we introduce a robust syntactic parsing - independent tree structured model , Neural Tree Indexers ( NTI ) that provides a middle ground between the sequential RNNs and the syntactic treebased recursive models .",7,0,38
dataset/preprocessed/test-data/natural_language_inference/20,NTI constructs a full n-ary tree by processing the input text with its node function in a bottom - up fashion .,8,0,22
dataset/preprocessed/test-data/natural_language_inference/20,Attention mechanism can then be applied to both structure and node function .,9,0,13
dataset/preprocessed/test-data/natural_language_inference/20,"We implemented and evaluated a binarytree model of NTI , showing the model achieved the state - of - the - art performance on three different NLP tasks : natural language inference , answer sentence selection , and sentence classification , outperforming state - of - the - art recurrent and recursive neural networks 1 .",10,0,56
dataset/preprocessed/test-data/natural_language_inference/20,Recurrent neural networks ( RNNs ) have been successful for modeling sequence data .,12,0,14
dataset/preprocessed/test-data/natural_language_inference/20,"RNNs equipped with gated hidden units and internal short - term memories , such as long shortterm memories ( LSTM ) ) have achieved a notable success in several NLP tasks including named entity recognition , constituency parsing , textual entailment recognition ) , question answering , and machine translation .",13,0,51
dataset/preprocessed/test-data/natural_language_inference/20,"However , most LSTM models explored so far are sequential .",14,0,11
dataset/preprocessed/test-data/natural_language_inference/20,It encodes text sequentially from left to right or vice versa and do not naturally support compositionality of language .,15,0,20
dataset/preprocessed/test-data/natural_language_inference/20,Sequential LSTM models seem to learn syntactic structure from the natural language however their generalization on unseen text is relatively poor comparing with models that exploit syntactic tree structure .,16,0,30
dataset/preprocessed/test-data/natural_language_inference/20,"Unlike sequential models , recursive neural networks compose word phrases over syntactic tree structure and have shown improved performance in sentiment analysis .",17,0,23
dataset/preprocessed/test-data/natural_language_inference/20,However its dependence on a syntactic tree architecture limits practical NLP applications .,18,0,13
dataset/preprocessed/test-data/natural_language_inference/20,"In this study , we introduce Neural Tree Indexers ( NTI ) , a class of tree structured models for NLP tasks .",19,0,23
dataset/preprocessed/test-data/natural_language_inference/20,NTI takes a sequence of tokens and produces its representation by constructing a full n-ary tree in a bottom - up fashion .,20,0,23
dataset/preprocessed/test-data/natural_language_inference/20,Each node in NTI is associated with one of the node transformation functions : leaf node mapping and non-leaf node composition functions .,21,0,23
dataset/preprocessed/test-data/natural_language_inference/20,"Unlike previous recursive models , the tree structure for NTI is relaxed , i.e. , NTI does not require the input sequences to be parsed syntactically ; and therefore it is flexible and can be directly applied to a wide range of NLP tasks beyond sentence modeling .",22,0,48
dataset/preprocessed/test-data/natural_language_inference/20,"Furthermore , we propose different variants of node composition function and attention over tree for our NTI models .",23,0,19
dataset/preprocessed/test-data/natural_language_inference/20,"When a sequential leaf node transformer such as LSTM is chosen , the NTI network forms a sequence - tree hybrid model taking advantage of both conditional and compositional powers of sequential and recursive models . :",24,0,37
dataset/preprocessed/test-data/natural_language_inference/20,A binary tree form of Neural Tree Indexers ( NTI ) in the context of question answering and natural language inference .,25,0,22
dataset/preprocessed/test-data/natural_language_inference/20,We insert empty tokens ( denoted by ? ) to the input text to form a full binary tree .,26,0,20
dataset/preprocessed/test-data/natural_language_inference/20,( a ) NTI produces answer representation at the root node .,27,0,12
dataset/preprocessed/test-data/natural_language_inference/20,This representation along with the question is used to find the answer .,28,0,13
dataset/preprocessed/test-data/natural_language_inference/20,( b ) NTI learns representations for the premise and hypothesis sentences and then attentively combines them for classification .,29,0,20
dataset/preprocessed/test-data/natural_language_inference/20,Dotted lines indicate attention over premise - indexed tree .,30,0,10
dataset/preprocessed/test-data/natural_language_inference/20,1 shows a binary - tree model of NTI .,31,0,10
dataset/preprocessed/test-data/natural_language_inference/20,"Although the model does not follow the syntactic tree structure , we empirically show that it achieved the state - of the - art performance on three different NLP applications : natural language inference , answer sentence selection , and sentence classification .",32,0,43
dataset/preprocessed/test-data/natural_language_inference/20,2 Related Work,33,0,3
dataset/preprocessed/test-data/natural_language_inference/20,Recurrent Neural Networks and Attention Mechanism,34,0,6
dataset/preprocessed/test-data/natural_language_inference/20,RNNs model input text sequentially by taking a single token at each time step and producing a corresponding hidden state .,35,0,21
dataset/preprocessed/test-data/natural_language_inference/20,The hidden state is then passed along through the next time step to provide historical sequence information .,36,0,18
dataset/preprocessed/test-data/natural_language_inference/20,"Although a great success in a variety of tasks , RNNs have limitations .",37,0,14
dataset/preprocessed/test-data/natural_language_inference/20,"Among them , it is not efficient at memorizing long or distant sequence .",38,0,14
dataset/preprocessed/test-data/natural_language_inference/20,This is frequently called as information flow bottleneck .,39,0,9
dataset/preprocessed/test-data/natural_language_inference/20,Approaches have therefore been developed to overcome the limitations .,40,0,10
dataset/preprocessed/test-data/natural_language_inference/20,"For example , to mitigate the information flow bottleneck , extended RNNs with a soft attention mechanism in the context of neural machine translation , leading to improved the results in translating longer sentences .",41,0,35
dataset/preprocessed/test-data/natural_language_inference/20,RNNs are linear chain - structured ; this limits its potential for natural language which can be represented by complex structures including syntactic structure .,42,0,25
dataset/preprocessed/test-data/natural_language_inference/20,"In this study , we propose models to mitigate this limitation .",43,0,12
dataset/preprocessed/test-data/natural_language_inference/20,Recursive Neural Networks,44,0,3
dataset/preprocessed/test-data/natural_language_inference/20,"Unlike RNNs , recursive neural networks explicitly model the compositionality and the recursive structure of natural language over tree .",45,0,20
dataset/preprocessed/test-data/natural_language_inference/20,The tree structure can be predefined by a syntactic parser .,46,0,11
dataset/preprocessed/test-data/natural_language_inference/20,Each non - leaf tree node is associated with anode composition function which combines its children nodes and produces its own representation .,47,0,23
dataset/preprocessed/test-data/natural_language_inference/20,The model is then trained by back - propagating error through structures .,48,0,13
dataset/preprocessed/test-data/natural_language_inference/20,The node composition function can be varied .,49,0,8
