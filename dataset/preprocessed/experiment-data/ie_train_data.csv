sample_dir_path,rps,sentence
dataset/preprocessed/training-data/passage_re-ranking/1,['PASSAGE RE - RANKING'],PASSAGE RE - RANKING WITH BERT
dataset/preprocessed/training-data/passage_re-ranking/1,['query - based passage re-ranking'],"In this paper , we describe a simple re-implementation of BERT for query - based passage re-ranking ."
dataset/preprocessed/training-data/passage_re-ranking/0,['Document Expansion'],Document Expansion by Query Prediction
dataset/preprocessed/training-data/temporal_information_extraction/1,['Temporal Relation Extraction'],A Structured Learning Approach to Temporal Relation Extraction
dataset/preprocessed/training-data/temporal_information_extraction/1,['Identifying temporal relations between events'],Identifying temporal relations between events is an essential step towards natural language understanding .
dataset/preprocessed/training-data/temporal_information_extraction/1,['temporal processing'],"The fundamental tasks in temporal processing , as identified in the TE workshops , are 1 ) time expression ( the so - called "" timex "" ) extraction and normalization and 2 ) temporal relation ( also known as TLINKs ) extraction ."
dataset/preprocessed/training-data/temporal_information_extraction/0,['temporal and causal relation extraction and classification'],"We present CATENA , a sieve - based system to perform temporal and causal relation extraction and classification from English texts , exploiting the interaction between the temporal and the causal model ."
dataset/preprocessed/training-data/part-of-speech_tagging/7,['Multilingual Part - of - Speech Tagging'],Multilingual Part - of - Speech Tagging with Bidirectional Long Short - Term Memory Models and Auxiliary Loss
dataset/preprocessed/training-data/part-of-speech_tagging/7,['POS tagging'],"We address these issues and evaluate bi - LSTMs with word , character , and unicode byte embeddings for POS tagging ."
dataset/preprocessed/training-data/part-of-speech_tagging/3,['End - to - end Sequence Labeling'],End - to - end Sequence Labeling via Bi-directional LSTM-CNNs-CRF
dataset/preprocessed/training-data/part-of-speech_tagging/3,['sequence labeling'],State - of - the - art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing .
dataset/preprocessed/training-data/part-of-speech_tagging/3,['Linguistic sequence labeling'],"Linguistic sequence labeling , such as part - ofspeech ( POS ) tagging and named entity recognition ( NER ) , is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community ."
dataset/preprocessed/training-data/part-of-speech_tagging/1,['Sequence Labeling'],Learning Better Internal Structure of Words for Sequence Labeling
dataset/preprocessed/training-data/part-of-speech_tagging/5,['Morphosyntactic Tagging'],Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings
dataset/preprocessed/training-data/part-of-speech_tagging/6,['Joint POS Tagging and Graph - based Dependency Parsing'],A Novel Neural Network Model for Joint POS Tagging and Graph - based Dependency Parsing
dataset/preprocessed/training-data/part-of-speech_tagging/0,['Part - of - Speech Tagging'],Robust Multilingual Part - of - Speech Tagging via Adversarial Training
dataset/preprocessed/training-data/part-of-speech_tagging/0,['neural POS tagging'],"In this paper , we propose and analyze a neural POS tagging model that exploits AT ."
dataset/preprocessed/training-data/part-of-speech_tagging/2,['SEQUENCE TAGGING'],TRANSFER LEARNING FOR SEQUENCE TAGGING WITH HIERARCHICAL RECURRENT NETWORKS
dataset/preprocessed/training-data/part-of-speech_tagging/4,['Sequence Labeling'],Hierarchically - Refined Label Attention Network for Sequence Labeling
dataset/preprocessed/training-data/part-of-speech_tagging/4,['statistical sequence labeling'],CRF has been used as a powerful model for statistical sequence labeling .
dataset/preprocessed/training-data/prosody_prediction/0,['Predicting Prosodic Prominence from Text'],Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations
dataset/preprocessed/training-data/prosody_prediction/0,['predicting prosodic prominence from written text'],In this paper we introduce a new natural language processing dataset and benchmark for predicting prosodic prominence from written text .
dataset/preprocessed/training-data/prosody_prediction/0,['Prosody prediction'],Prosody prediction can be turned into a sequence labeling task by giving each word in a text a discrete prominence value based on the amount of emphasis the speaker gives to the word when reading the text .
dataset/preprocessed/training-data/text_generation/3,['Dialogue Generation'],An Auto - Encoder Matching Model for Learning Utterance - Level Semantic Dependency in Dialogue Generation
dataset/preprocessed/training-data/text_generation/3,['Automatic dialogue generation'],"Automatic dialogue generation task is of great importance to many applications , ranging from open - domain chatbots to goal - oriented technical support agents ."
dataset/preprocessed/training-data/text_generation/3,['conversation generation'],"However , conversation generation is a much more complex and flexible task as there are less "" word - to - words "" relations between inputs and responses ."
dataset/preprocessed/training-data/text_generation/1,['Language Generation'],Adversarial Ranking for Language Generation
dataset/preprocessed/training-data/text_generation/5,['Text Modeling'],Improved Variational Autoencoders for Text Modeling using Dilated Convolutions
dataset/preprocessed/training-data/text_generation/5,['generative text modeling'],"Recent work on generative text modeling has found that variational autoencoders ( VAE ) with LSTM decoders perform worse than simpler LSTM language models ( Bowman et al. , 2015 ) ."
dataset/preprocessed/training-data/text_generation/0,['generating real - valued data'],"As a new way of training generative models , Generative Adversarial Net ( GAN ) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real - valued data ."
dataset/preprocessed/training-data/text_generation/0,['generating sequences of discrete tokens'],"However , it has limitations when the goal is for generating sequences of discrete tokens ."
dataset/preprocessed/training-data/text_generation/0,['Generating sequential synthetic data'],Generating sequential synthetic data that mimics the real one is an important problem in unsupervised learning .
dataset/preprocessed/training-data/text_generation/2,['Text Generation'],Long Text Generation via Adversarial Training with Leaked Information
dataset/preprocessed/training-data/text_generation/2,['Automatically generating coherent and semantically meaningful text'],"Automatically generating coherent and semantically meaningful text has many applications in machine translation , dialogue systems , image captioning , etc ."
dataset/preprocessed/training-data/text_generation/4,['Generating Text'],Generating Text through Adversarial Training using Skip - Thought Vectors
dataset/preprocessed/training-data/text_generation/4,['text generation'],Attempts have been made for utilizing GANs with word embeddings for text generation .
dataset/preprocessed/training-data/text_generation/4,['natural language text generation'],Numerous efforts have been made in the field of natural language text generation for tasks such as sentiment analysis and machine translation .
dataset/preprocessed/training-data/sentence_compression/3,['Sentence Compression'],A Language Model based Evaluator for Sentence Compression
dataset/preprocessed/training-data/sentence_compression/3,['deletion - based sentence compression'],"We herein present a language - modelbased evaluator for deletion - based sentence compression , and viewed this task as a series of deletion - and - evaluation operations using the evaluator ."
dataset/preprocessed/training-data/sentence_compression/1,['Sentence Compression by Deletion'],Sentence Compression by Deletion with LSTMs
dataset/preprocessed/training-data/sentence_compression/1,['deletion - based sentence compression'],"We present an LSTM approach to deletion - based sentence compression where the task is to translate a sentence into a sequence of zeros and ones , corresponding to token deletion decisions ."
dataset/preprocessed/training-data/sentence_compression/1,['Sentence compression'],Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence .
dataset/preprocessed/training-data/sentence_compression/0,['Sentence Compression'],Can Syntax Help ? Improving an LSTM - based Sentence Compression Model for New Domains
dataset/preprocessed/training-data/sentence_compression/2,['sentence compression'],Improving sentence compression by learning to predict gaze
dataset/preprocessed/training-data/relation_extraction/7,['Neural Relation Extraction'],Neural Relation Extraction via Inner - Sentence Noise Reduction and Transfer Learning
dataset/preprocessed/training-data/relation_extraction/7,['Relation extraction'],Relation extraction aims to extract relations between pairs of marked entities in raw texts .
dataset/preprocessed/training-data/relation_extraction/7,['distant supervised relation extraction'],"In this paper , we propose a novel word - level approach for distant supervised relation extraction by reducing inner-sentence noise and improving robustness against noisy words ."
dataset/preprocessed/training-data/relation_extraction/3,['Extracting Multiple - Relations'],Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers
dataset/preprocessed/training-data/relation_extraction/3,['extracting multiple entity - relations'],The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .
dataset/preprocessed/training-data/relation_extraction/3,['multiple entityrelations extraction'],"This paper proposes a new solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve a new state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark ."
dataset/preprocessed/training-data/relation_extraction/3,['Relation extraction ( RE )'],Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .
dataset/preprocessed/training-data/relation_extraction/3,"['RE', 'multiplerelations extraction ( MRE )']",One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .
dataset/preprocessed/training-data/relation_extraction/3,['MRE'],"Because in real - world applications , whose input paragraphs dominantly contain multiple pairs of entities , an efficient and effective solution for MRE has more important and more practical implications ."
dataset/preprocessed/training-data/relation_extraction/10,['Relation Extraction'],Span - Level Model for Relation Extraction
dataset/preprocessed/training-data/relation_extraction/10,['Relation Extraction ( RE )'],"This paper focuses on Relation Extraction ( RE ) , which is the task of entity mention detection and classifying the relations between each pair of those mentions ."
dataset/preprocessed/training-data/relation_extraction/10,['RE'],"Since , work on RE has revolved around end - to - end systems : single models which first perform entity mention detection and then relation extraction ."
dataset/preprocessed/training-data/relation_extraction/9,['Relation Classification'],Relation Classification via Multi - Level Attention CNNs
dataset/preprocessed/training-data/relation_extraction/1,"['Relation Extraction', 'Semantic Role Labeling']",Simple BERT Models for Relation Extraction and Semantic Role Labeling
dataset/preprocessed/training-data/relation_extraction/1,['semantic role labeling ( SRL )'],Relation extraction and semantic role labeling ( SRL ) are two fundamental tasks in natural language understanding .
dataset/preprocessed/training-data/relation_extraction/1,['SRL'],"For SRL , the task is to extract the predicate - argument structure of a sentence , determining "" who did what to whom "" , "" when "" , "" where "" , etc ."
dataset/preprocessed/training-data/relation_extraction/5,['Relation Extraction'],Graph Convolution over Pruned Dependency Trees Improves Relation Extraction
dataset/preprocessed/training-data/relation_extraction/8,['Distant Supervision for Relation Extraction'],Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks
dataset/preprocessed/training-data/relation_extraction/8,['distant supervised relation extraction'],"To address the first problem , distant supervised relation extraction is treated as a multi-instance problem similar to previous studies ."
dataset/preprocessed/training-data/relation_extraction/8,['relation extraction'],"In relation extraction , one challenge that is faced when building a machine learning system is the generation of training examples ."
dataset/preprocessed/training-data/relation_extraction/6,['Knowledge Base Relation Extraction'],Context - Aware Representations for Knowledge Base Relation Extraction
dataset/preprocessed/training-data/relation_extraction/6,['sentence - level relation extraction'],We demonstrate that for sentence - level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation .
dataset/preprocessed/training-data/relation_extraction/6,['relation extraction'],The main goal of relation extraction is to determine a type of relation between two target entities that appear together in a text .
dataset/preprocessed/training-data/relation_extraction/6,['sentential relation extraction'],"In this paper , we consider the sentential relation extraction task : to each occurrence of the target entity pair e 1 , e 2 in some sentence s one has to assign a relation type r from a given set R. A triple e 1 , r , e 2 is called a relation instance and we refer to the relation of the target entity pair as target relation ."
dataset/preprocessed/training-data/relation_extraction/11,['Distantly - Supervised Neural Relation Extraction'],RESIDE : Improving Distantly - Supervised Neural Relation Extraction using Side Information
dataset/preprocessed/training-data/relation_extraction/11,['Distantly - supervised Relation Extraction ( RE )'],Distantly - supervised Relation Extraction ( RE ) methods train an extractor by automatically aligning relation instances in a Knowledge Base ( KB ) with unstructured text .
dataset/preprocessed/training-data/relation_extraction/11,['Relation Extraction ( RE )'],Relation Extraction ( RE ) attempts to fill this gap by extracting semantic relationships between entity pairs from plain text .
dataset/preprocessed/training-data/relation_extraction/11,['RE'],RE models usually ignore such readily available side information .
dataset/preprocessed/training-data/relation_extraction/0,['Joint Extraction of Entity Mentions and Relations'],Going out on a limb : Joint Extraction of Entity Mentions and Relations without Dependency Trees
dataset/preprocessed/training-data/relation_extraction/0,['Extraction of entities and their relations from text'],Extraction of entities and their relations from text belongs to a very well - studied family of structured prediction tasks in NLP .
dataset/preprocessed/training-data/relation_extraction/0,['entity mention and relation extraction at the sentencelevel'],Several methods have been proposed for entity mention and relation extraction at the sentencelevel .
dataset/preprocessed/training-data/relation_extraction/12,['Relation Extraction'],Attention Guided Graph Convolutional Networks for Relation Extraction
dataset/preprocessed/training-data/relation_extraction/2,['Relation Classification'],Enriching Pre-trained Language Model with Entity Information for Relation Classification
dataset/preprocessed/training-data/relation_extraction/4,['populate knowledge bases with facts'],"However , the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly ."
dataset/preprocessed/training-data/relation_extraction/4,['populate a knowledge base with relational facts'],A basic but highly important challenge in natural language understanding is being able to populate a knowledge base with relational facts contained in a piece of text .
dataset/preprocessed/training-data/relation_extraction/4,['relation extraction'],"Existing work on relation extraction ( e.g. , has been unable to achieve sufficient recall or precision for the results to be usable versus hand - constructed knowledge bases ."
dataset/preprocessed/training-data/relation_extraction/13,['Relation Learning'],Matching the Blanks : Distributional Similarity for Relation Learning
dataset/preprocessed/training-data/relation_extraction/13,['identify and extract relations between entities'],Reading text to identify and extract relations between entities has been along standing goal in natural language processing .
dataset/preprocessed/training-data/relation_extraction/13,['relation extraction'],Typically efforts in relation extraction fall into one of three groups .
dataset/preprocessed/training-data/sentence_classification/1,['Sequential Sentence Classification in Medical Scientific Abstracts'],Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts
dataset/preprocessed/training-data/sentence_classification/1,['sequential sentence classification'],"This hampers the traditional sentence classification approaches to the problem of sequential sentence classification , where structured prediction is needed for better over all classification performance ."
dataset/preprocessed/training-data/sentence_classification/0,['Citation Intent Classification in Scientific Publications'],Structural Scaffolds for Citation Intent Classification in Scientific Publications
dataset/preprocessed/training-data/sentence_classification/0,['Identifying the intent of a citation in scientific papers'],"Identifying the intent of a citation in scientific papers ( e.g. , background information , use of methods , comparing results ) is critical for machine reading of individual publications and automated analysis of the scientific literature ."
dataset/preprocessed/training-data/sentence_classification/0,['citation intent classification'],"In this work , we approach the problem of citation intent classification by modeling the language expressed in the citation context ."
dataset/preprocessed/training-data/sentence_classification/2,['Sentence Classification'],Translations as Additional Contexts for Sentence Classification
dataset/preprocessed/training-data/question_similarity/0,['Semantic Question Similarity in Arabic'],Tha3aroon at NSURL - 2019 Task 8 : Semantic Question Similarity in Arabic
dataset/preprocessed/training-data/question_similarity/0,['semantic text question similarity'],"In this paper , we describe our team 's effort on the semantic text question similarity task of NSURL 2019 ."
dataset/preprocessed/training-data/question_similarity/0,['Semantic Text Similarity ( STS )'],Semantic Text Similarity ( STS ) problems are both real - life and challenging .
dataset/preprocessed/training-data/question_similarity/0,['STS'],"For example , in the paraphrase identification task , STS is used to predict if one sentence is a paraphrase of the other or not ."
dataset/preprocessed/training-data/question_similarity/0,['Semantic Question Similarity ( SQS ) for the Arabic language'],A new task has been proposed by Mawdoo3 1 company with a new dataset provided by their data annotation team for Semantic Question Similarity ( SQS ) for the Arabic language .
dataset/preprocessed/training-data/question_similarity/0,['SQS'],"SQS is a variant of STS , which aims to compare a pair of questions and determine whether they have the same meaning or not ."
dataset/preprocessed/training-data/question_generation/1,['Visual Question Generation'],Multimodal Differential Network for Visual Question Generation
dataset/preprocessed/training-data/question_generation/1,['Generating natural questions from an image'],Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations .
dataset/preprocessed/training-data/question_generation/1,['generating natural questions for an image'],Here the au-thors have proposed the challenging task of generating natural questions for an image .
dataset/preprocessed/training-data/question_generation/0,['Neural Question Generation from Text'],Neural Question Generation from Text : A Preliminary Study
dataset/preprocessed/training-data/question_generation/0,['Automatic question generation'],Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub- spans of the given passage .
dataset/preprocessed/training-data/question_generation/0,['Automatic question generation from natural language text'],"Automatic question generation from natural language text aims to generate questions taking text as input , which has the potential value of education purpose ) ."
dataset/preprocessed/training-data/question_generation/0,['question generation'],"As the reverse task of question answering , question generation also has the potential for providing a large scale corpus of question - answer pairs ."
dataset/preprocessed/training-data/sarcasm_detection/1,['Contextual Sarcasm Detection'],CASCADE : Contextual Sarcasm Detection in Online Discussion Forums
dataset/preprocessed/training-data/sarcasm_detection/1,['automated sarcasm detection'],"The literature in automated sarcasm detection has mainly focused on lexical , syntactic and semantic - level analysis of text ."
dataset/preprocessed/training-data/sarcasm_detection/1,['sarcasm detection'],"In this paper , we propose CASCADE ( a ContextuAl SarCasm DEtector ) that adopts a hybrid approach of both content and context - driven modeling for sarcasm detection in online social media discussions ."
dataset/preprocessed/training-data/sarcasm_detection/0,['sarcasm detection'],"We introduce the Self - Annotated Reddit Corpus ( SARC ) , a large corpus for sarcasm research and for training and evaluating systems for sarcasm detection ."
dataset/preprocessed/training-data/semantic_parsing/1,['Semantic Parsing and Code Generation'],TRANX : A Transition - based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation
dataset/preprocessed/training-data/semantic_parsing/0,"['Complex and Cross - Domain Semantic Parsing', 'Text - to - SQL']",Spider : A Large - Scale Human - Labeled Dataset for Complex and Cross - Domain Semantic Parsing and Text - to - SQL Task
dataset/preprocessed/training-data/semantic_parsing/0,['Semantic parsing ( SP )'],Semantic parsing ( SP ) is one of the most important tasks in natural language processing ( NLP ) .
dataset/preprocessed/training-data/semantic_parsing/0,['SP'],Existing datasets for SP have two shortcomings .
dataset/preprocessed/training-data/semantic_parsing/2,['Neural Semantic Parsing'],Coarse - to - Fine Decoding for Neural Semantic Parsing
dataset/preprocessed/training-data/semantic_parsing/2,['Semantic parsing'],Semantic parsing aims at mapping natural language utterances into structured meaning representations .
dataset/preprocessed/training-data/negation_scope_resolution/0,['Negation Detection and Scope Resolution'],NegBERT : A Transfer Learning Approach for Negation Detection and Scope Resolution
dataset/preprocessed/training-data/question_answering/3,['Reading Comprehension'],Multi - Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension
dataset/preprocessed/training-data/question_answering/3,['reading comprehension ( RC )'],This paper presents a new compositional encoder for reading comprehension ( RC ) .
dataset/preprocessed/training-data/question_answering/3,['RC'],"We conduct experiments on three RC datasets , showing that our proposed encoder demonstrates very promising results both as a standalone encoder as well as a complementary building block ."
dataset/preprocessed/training-data/question_answering/1,['MACHINE COMPREHENSION'],BI - DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION
dataset/preprocessed/training-data/question_answering/1,['Machine comprehension ( MC )'],"Machine comprehension ( MC ) , answering a query about a given context paragraph , requires modeling complex interactions between the context and the query ."
dataset/preprocessed/training-data/question_answering/1,['MC'],"Recently , attention mechanisms have been successfully extended to MC ."
dataset/preprocessed/training-data/question_answering/1,['question answering ( QA )'],The tasks of machine comprehension ( MC ) and question answering ( QA ) have gained significant popularity over the past few years within the natural language processing and computer vision communities .
dataset/preprocessed/training-data/question_answering/5,['OPEN - DOMAIN QUESTION ANSWERING'],EVIDENCE AGGREGATION FOR ANSWER RE - RANKING IN OPEN - DOMAIN QUESTION ANSWERING
dataset/preprocessed/training-data/question_answering/5,['Open-domain question answering ( QA )'],Open-domain question answering ( QA ) aims to answer questions from a broad range of domains by effectively marshalling evidence from large open - domain knowledge sources .
dataset/preprocessed/training-data/question_answering/5,['open - domain QA'],Recent work on open - domain QA has focused on using unstructured text retrieved from the web to build machine comprehension models .
dataset/preprocessed/training-data/question_answering/0,['Knowledge Base Question Answering'],Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering
dataset/preprocessed/training-data/question_answering/0,['Knowledge base question answering ( QA )'],Knowledge base question answering ( QA ) is an important natural language processing problem .
dataset/preprocessed/training-data/question_answering/0,['QA'],QA requires precise modeling of the question semantics through the entities and relations available in the KB in order to retrieve the correct answer .
dataset/preprocessed/training-data/question_answering/0,['KB QA'],"In this paper , we describe a semantic parsing approach to the problem of KB QA ."
dataset/preprocessed/training-data/question_answering/2,['Visual Question Answering'],Focal Visual - Text Attention for Visual Question Answering
dataset/preprocessed/training-data/question_answering/2,['Visual question answering ( VQA )'],"Visual question answering ( VQA ) is a successful direction utilizing both computer vision and natural language processing techniques to solve an interesting problem : given a pair of image and a question ( in natural language ) , the goal is to learn an inference model that can the answer questions according to cues discovered from the image ."
dataset/preprocessed/training-data/question_answering/2,['VQA'],"Extending from VQA on a single image , this paper considers the following problem :"
dataset/preprocessed/training-data/question_answering/4,['Reading Comprehension'],Densely Connected Attention Propagation for Reading Comprehension
dataset/preprocessed/training-data/question_answering/4,['reading comprehension ( RC )'],"We propose DECAPROP ( Densely Connected Attention Propagation ) , a new densely connected neural architecture for reading comprehension ( RC ) ."
dataset/preprocessed/training-data/question_answering/4,['RC'],We conduct extensive experiments on four challenging RC benchmarks .
dataset/preprocessed/training-data/semantic_role_labeling/3,['Deep Semantic Role Labeling'],Deep Semantic Role Labeling with Self - Attention
dataset/preprocessed/training-data/semantic_role_labeling/3,['Semantic Role Labeling ( SRL )'],Semantic Role Labeling ( SRL ) is believed to be a crucial step towards natural language understanding and has been widely studied .
dataset/preprocessed/training-data/semantic_role_labeling/3,['SRL'],"In this paper , we present a simple and effective architecture for SRL which aims to address these problems ."
dataset/preprocessed/training-data/semantic_role_labeling/3,['Semantic Role Labeling'],"Semantic Role Labeling is a shallow semantic parsing task , whose goal is to determine essentially "" who did what to whom "" , "" when "" and "" where "" ."
dataset/preprocessed/training-data/semantic_role_labeling/1,['Semantic Role Labeling'],Linguistically - Informed Self - Attention for Semantic Role Labeling
dataset/preprocessed/training-data/semantic_role_labeling/1,['semantic role labeling ( SRL )'],Current state - of - the - art semantic role labeling ( SRL ) uses a deep neural network with no explicit linguistic features .
dataset/preprocessed/training-data/semantic_role_labeling/1,['SRL'],"However , prior work has shown that gold syntax trees can dramatically improve SRL decoding , suggesting the possibility of increased accuracy from explicit modeling of syntax ."
dataset/preprocessed/training-data/semantic_role_labeling/0,['Neural Semantic Role Labeling'],Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling
dataset/preprocessed/training-data/semantic_role_labeling/0,['Semantic role labeling ( SRL )'],"Semantic role labeling ( SRL ) captures predicateargument relations , such as "" who did what to whom . """
dataset/preprocessed/training-data/semantic_role_labeling/0,['SRL'],"Recent high - performing SRL models are BIO - taggers , labeling argument spans for a single predicate at a time ( as shown in ."
dataset/preprocessed/training-data/semantic_role_labeling/2,['Deep Semantic Role Labeling'],Deep Semantic Role Labeling : What Works and What 's Next
dataset/preprocessed/training-data/semantic_role_labeling/2,['semantic role labeling ( SRL )'],"We introduce a new deep learning model for semantic role labeling ( SRL ) that significantly improves the state of the art , along with detailed analyses to reveal its strengths and limitations ."
dataset/preprocessed/training-data/semantic_role_labeling/2,['SRL'],Recently breakthroughs involving end - to - end deep models for SRL without syntactic input seem to overturn the long - held belief that syntactic parsing is a prerequisite for this task .
dataset/preprocessed/training-data/semantic_role_labeling/4,['Semantic Role Labeling'],A Span Selection Model for Semantic Role Labeling
dataset/preprocessed/training-data/semantic_role_labeling/4,['semantic role labeling ( SRL )'],We present a simple and accurate span - based model for semantic role labeling ( SRL ) .
dataset/preprocessed/training-data/semantic_role_labeling/4,['SRL'],"Given a sentence and a target predicate , SRL systems have to predict semantic arguments of the predicate ."
dataset/preprocessed/training-data/paraphrase_generation/1,['Paraphrase Generation'],A Deep Generative Framework for Paraphrase Generation
dataset/preprocessed/training-data/paraphrase_generation/1,['generating paraphrases automatically'],"In this paper , we address the problem of generating paraphrases automatically ."
dataset/preprocessed/training-data/paraphrase_generation/0,['Learning Semantic Sentence Embeddings'],Learning Semantic Sentence Embeddings using Pair- wise Discriminator
dataset/preprocessed/training-data/paraphrase_generation/0,['obtaining sentence - level embeddings'],"In this paper , we propose a method for obtaining sentence - level embeddings ."
dataset/preprocessed/training-data/query_wellformedness/0,['Identifying Well - formed Natural Language Questions'],Identifying Well - formed Natural Language Questions
dataset/preprocessed/training-data/sentiment_analysis/28,['Targeted Sentiment Classification'],Attentional Encoder Network for Targeted Sentiment Classification
dataset/preprocessed/training-data/sentiment_analysis/28,['fine - grained sentiment analysis'],"Targeted sentiment classification is a fine - grained sentiment analysis task , which aims at determining the sentiment polarities ( e.g. , negative , neutral , or positive ) of a sentence over "" opinion targets "" that explicitly appear in the sentence ."
dataset/preprocessed/training-data/sentiment_analysis/28,['fine - grained targeted sentiment classification'],"However , these neural network models are still in infancy to deal with the fine - grained targeted sentiment classification task ."
dataset/preprocessed/training-data/sentiment_analysis/7,['MULTI - MODAL EMOTION RECOGNITION'],MULTI - MODAL EMOTION RECOGNITION ON IEMOCAP WITH NEURAL NETWORKS
dataset/preprocessed/training-data/sentiment_analysis/7,['Emotion recognition'],Emotion recognition has become an important field of research in human computer interactions and there is a growing need for automatic emotion recognition systems .
dataset/preprocessed/training-data/sentiment_analysis/49,['Multi-modal Sentiment Analysis'],Contextual Inter-modal Attention for Multi-modal Sentiment Analysis
dataset/preprocessed/training-data/sentiment_analysis/49,['sentiment analysis'],"Traditionally , sentiment analysis has been applied to a wide variety of texts ."
dataset/preprocessed/training-data/sentiment_analysis/3,['Emotion Detection in Textual Conversations'],Knowledge - Enriched Transformer for Emotion Detection in Textual Conversations
dataset/preprocessed/training-data/sentiment_analysis/3,['detecting emotions in textual conversations'],The task of detecting emotions in textual conversations leads to a wide range of applications such as opinion mining in social networks .
dataset/preprocessed/training-data/sentiment_analysis/3,"['detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations']","This work addresses the task of detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations , where the emotion of an utterance is detected in the conversational context ."
dataset/preprocessed/training-data/sentiment_analysis/25,['Aspect Based Sentiment Analysis'],Aspect Based Sentiment Analysis with Gated Convolutional Networks
dataset/preprocessed/training-data/sentiment_analysis/25,['Aspect based sentiment analysis ( ABSA )'],"Aspect based sentiment analysis ( ABSA ) can provide more detailed information than general sentiment analysis , because it aims to predict the sentiment polarities of the given aspects or entities in text ."
dataset/preprocessed/training-data/sentiment_analysis/25,"['aspect - category sentiment analysis ( ACSA )', 'aspect - term sentiment analysis ( ATSA )']",We summarize previous approaches into two subtasks : aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .
dataset/preprocessed/training-data/sentiment_analysis/25,['ABSA'],"A number of models have been developed for ABSA , but there are two different subtasks , namely aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) ."
dataset/preprocessed/training-data/sentiment_analysis/25,['ACSA'],"The goal of ACSA is to predict the sentiment polarity with regard to the given aspect , which is one of a few predefined categories ."
dataset/preprocessed/training-data/sentiment_analysis/25,['ATSA'],"On the other hand , the goal of ATSA is to identify the sentiment polarity concerning the target entities that appear in the text instead , which could be a multi-word phrase or a single word ."
dataset/preprocessed/training-data/sentiment_analysis/40,['Aspect Sentiment Analysis'],Recurrent Attention Network on Memory for Aspect Sentiment Analysis
dataset/preprocessed/training-data/sentiment_analysis/39,['Targeted Aspect Based Sentiment Analysis'],SentiHood : Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods
dataset/preprocessed/training-data/sentiment_analysis/39,['targeted aspect - based sentiment analysis'],"In this paper , we introduce the task of targeted aspect - based sentiment analysis ."
dataset/preprocessed/training-data/sentiment_analysis/39,['Sentiment analysis'],Sentiment analysis is an important task in natural language processing .
dataset/preprocessed/training-data/sentiment_analysis/39,['Aspect - based sentiment analysis ( ABSA )'],"Aspect - based sentiment analysis ( ABSA ) relates to the task of extracting fine - grained information by identifying the polarity towards different aspects of an entity in the same unit of text , and recognizing the polarity associated with each aspect separately ."
dataset/preprocessed/training-data/sentiment_analysis/39,['Targeted sentiment analysis'],Targeted sentiment analysis investigates the classification of opinion polarities towards certain target entity mentions in given sentences ( often a tweet ) .
dataset/preprocessed/training-data/sentiment_analysis/43,['Aspect - Level Sentiment Classification'],Multi - grained Attention Network for Aspect - Level Sentiment Classification
dataset/preprocessed/training-data/sentiment_analysis/43,['aspect level sentiment classification'],We propose a novel multi-grained attention network ( MGAN ) model for aspect level sentiment classification .
dataset/preprocessed/training-data/sentiment_analysis/16,['Aspect Sentiment Classification'],Target - Sensitive Memory Networks for Aspect Sentiment Classification
dataset/preprocessed/training-data/sentiment_analysis/16,"['Aspect sentiment classification ( ASC )', 'sentiment analysis']",Aspect sentiment classification ( ASC ) is a fundamental task in sentiment analysis .
dataset/preprocessed/training-data/sentiment_analysis/16,['ASC'],"However , we found an important problem with the current MNs in performing the ASC task ."
dataset/preprocessed/training-data/sentiment_analysis/10,['Emotion Detection in Conversations'],DialogueRNN : An Attentive RNN for Emotion Detection in Conversations
dataset/preprocessed/training-data/sentiment_analysis/46,['Sentiment Classification'],A Multi-sentiment - resource Enhanced Attention Network for Sentiment Classification
dataset/preprocessed/training-data/sentiment_analysis/26,['Deep Sentiment Analysis'],A Helping Hand : Transfer Learning for Deep Sentiment Analysis
dataset/preprocessed/training-data/sentiment_analysis/26,['sentiment analysis'],"Over the past decades , sentiment analysis has grown from an academic endeavour to an essential analytics tool ."
dataset/preprocessed/training-data/sentiment_analysis/26,['supervised sentiment polarity classification'],"In recent years , deep neural architectures based on convolutional or recurrent layers have become established as the preeminent models for supervised sentiment polarity classification ."
dataset/preprocessed/training-data/sentiment_analysis/21,['Sentiment Classification'],Sentiment Classification using Document Embeddings trained with Cosine Similarity
dataset/preprocessed/training-data/sentiment_analysis/21,['document - level sentiment classification'],"In document - level sentiment classification , each document must be mapped to a fixed length vector ."
dataset/preprocessed/training-data/sentiment_analysis/21,"['sentiment classification', 'binary sentiment classification of long movie reviews']","In document classification tasks such as sentiment classification ( in this paper we focus on binary sentiment classification of long movie reviews , i.e. determining whether each review is positive or negative ) , the choice of document representation is usually more important than the choice of classifier ."
dataset/preprocessed/training-data/sentiment_analysis/35,['Aspect - level Sentiment Classification'],Exploiting Coarse - to - Fine Task Transfer for Aspect - level Sentiment Classification
dataset/preprocessed/training-data/sentiment_analysis/35,['Aspect - level sentiment classification ( ASC )'],"Aspect - level sentiment classification ( ASC ) aims at identifying sentiment polarities towards aspects in a sentence , where the aspect can behave as a general Aspect Category ( AC ) or a specific Aspect Term ( AT ) ."
dataset/preprocessed/training-data/sentiment_analysis/35,['aspect - oriented sentiment analysis'],"To model aspect - oriented sentiment analysis , equipping Recurrent Neural Networks ( RNNs ) with the attention Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) ."
dataset/preprocessed/training-data/sentiment_analysis/30,['Targeted Aspect - based Sentiment Analysis'],Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect - based Sentiment Analysis
dataset/preprocessed/training-data/sentiment_analysis/30,['targeted aspect - based sentiment analysis ( TABSA )'],"While neural networks have been shown to achieve impressive results for sentence - level sentiment analysis , targeted aspect - based sentiment analysis ( TABSA ) - extraction of finegrained opinion polarity w.r.t. a pre-defined set of aspects - remains a difficult task ."
dataset/preprocessed/training-data/sentiment_analysis/23,['Discriminative Neural Sentence Modeling'],Discriminative Neural Sentence Modeling by Tree - Based Convolution
dataset/preprocessed/training-data/sentiment_analysis/34,['Arabic Sentiment Analyser'],Mazajak : An Online Arabic Sentiment Analyser
dataset/preprocessed/training-data/sentiment_analysis/34,['Sentiment analysis ( SA )'],Sentiment analysis ( SA ) is one of the most useful natural language processing applications .
dataset/preprocessed/training-data/sentiment_analysis/34,['Sentiment analysis'],Sentiment analysis is one of the vital approaches to extract public opinion from large corpora of text .
dataset/preprocessed/training-data/sentiment_analysis/34,['SA'],"Work on SA started in early 2000s , particularly with the work of , where they studied the sentiment of movies ' reviews ."
dataset/preprocessed/training-data/sentiment_analysis/32,['Aspect - Level Sentiment Classification'],Interactive Attention Networks for Aspect - Level Sentiment Classification
dataset/preprocessed/training-data/sentiment_analysis/32,['sentiment classification'],Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating target - specific representations .
dataset/preprocessed/training-data/sentiment_analysis/48,['Aspect - term Sentiment Analysis'],Variational Semi-supervised Aspect - term Sentiment Analysis via Transformer
dataset/preprocessed/training-data/sentiment_analysis/48,"['aspect - term sentiment analysis ( ATSA )', 'aspect - category sentiment analysis ( ACSA )']","Aspect based sentiment analysis ( ABSA ) has two sub - tasks , namely aspect - term sentiment analysis ( ATSA ) and aspect - category sentiment analysis ( ACSA ) ."
dataset/preprocessed/training-data/sentiment_analysis/48,['ACSA'],"ACSA is to infer the sentiment polarity with regard to the predefined categories , e.g. , the aspect food , price , ambience ."
dataset/preprocessed/training-data/sentiment_analysis/48,['ATSA'],"On the other hand , ATSA aims at classifying the sentiment polarity of a given aspect word or phrase in the text ."
dataset/preprocessed/training-data/sentiment_analysis/44,['Sentence Level Discourse Parsing and Sentiment Analysis'],Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment Analysis
dataset/preprocessed/training-data/sentiment_analysis/38,['Representation learning'],Representation learning ) plays a critical role in many modern machine learning systems .
dataset/preprocessed/training-data/sentiment_analysis/38,['sentiment analysis'],We focus in on the task of sentiment analysis and attempt to learn an unsupervised representation that accurately contains this concept .
dataset/preprocessed/training-data/sentiment_analysis/9,['Aspect Polarity Classification and Aspect Term Extraction'],Graphical Abstract A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction Highlights A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction
dataset/preprocessed/training-data/sentiment_analysis/9,"['Aspect - based sentiment analysis ( ABSA )', 'aspect term extraction ( ATE )', 'aspect polarity classification ( APC )']",Aspect - based sentiment analysis ( ABSA ) task is a multi - grained task of natural language processing and consists of two subtasks : aspect term extraction ( ATE ) and aspect polarity classification ( APC ) .
dataset/preprocessed/training-data/sentiment_analysis/9,"['aspect term polarity inferring', 'aspect term extraction']",Most of the existing work focuses on the subtask of aspect term polarity inferring and ignores the significance of aspect term extraction .
dataset/preprocessed/training-data/sentiment_analysis/9,['aspect polarity classification'],"By integrating the domain - adapted BERT model , the LCF - ATEPC model achieved the state - of the - art performance of aspect term extraction and aspect polarity classification in four Chinese review datasets ."
dataset/preprocessed/training-data/sentiment_analysis/9,['Aspect - based sentiment analysis'],"Aspect - based sentiment analysis ; Pontiki , Galanis , Papageorgiou , Androutsopoulos , Manandhar , AL - Smadi , Al - Ayyoub , Zhao , Qin , De Clercq , Hoste , Apidianaki , Tannier , Loukachevitch , Kotelnikov , Bel , Jimnez - Zafra and Eryigit ( 2016 ) ( ABSA ) is a fine - grained task compared with traditional sentiment analysis , which requires the model to be able to automatic extract the aspects and predict the polarities of all the aspects ."
dataset/preprocessed/training-data/sentiment_analysis/9,['APC'],The APC task is a kind of classification problem .
dataset/preprocessed/training-data/sentiment_analysis/9,['ATE'],"The researches concerning APC tasks is more abundant than the ATE task , and a large number of deep learning - based models have been proposed to solve APC problems , such as the models ; ; ; based on long short - term memory ( LSTM ) and the methodologies based on transformer ."
dataset/preprocessed/training-data/sentiment_analysis/19,['Improved Sentence Modeling'],Improved Sentence Modeling using Suffix Bidirectional LSTM
dataset/preprocessed/training-data/sentiment_analysis/19,['computing representations of sequential data'],"Recurrent neural networks have become ubiquitous in computing representations of sequential data , especially textual data in natural language processing ."
dataset/preprocessed/training-data/sentiment_analysis/19,"['fine - grained sentiment classification', 'question classification']",Using SuBiLSTM we achieve new state - of - the - art results for fine - grained sentiment classification and question classification .
dataset/preprocessed/training-data/sentiment_analysis/19,['modeling sequential data'],Recurrent Neural Networks ( RNN ) ) have emerged as a powerful tool for modeling sequential data .
dataset/preprocessed/training-data/sentiment_analysis/22,['Aspect-level Sentiment Analysis'],Hierarchical Attention Based Position-aware Network for Aspect-level Sentiment Analysis
dataset/preprocessed/training-data/sentiment_analysis/22,['sentiment analysis'],"Aspect - level sentiment analysis is a fine - grained task in sentiment analysis , which aims to identify the sentiment polarity ( i.e. , negative , neutral , or positive ) of a specific opinion target expressed in a comment / review by a reviewer ."
dataset/preprocessed/training-data/sentiment_analysis/1,['EEG - Based Emotion Recognition'],EEG - Based Emotion Recognition Using Regularized Graph Neural Networks
dataset/preprocessed/training-data/sentiment_analysis/1,['E MOTION recognition'],"E MOTION recognition is an important subarea of affective computing , which focuses on recognizing human emotions based on a variety of modalities , such as audio- visual expressions , body language , physiological signals , etc ."
dataset/preprocessed/training-data/sentiment_analysis/5,['EEG Emotion Recognition'],A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition
dataset/preprocessed/training-data/sentiment_analysis/5,['electroencephalograph ( EEG ) emotion recognition'],"Inspired by this study , in this paper , we propose a novel bi-hemispheric discrepancy model ( BiHDM ) to learn the asymmetric differences between two hemispheres for electroencephalograph ( EEG ) emotion recognition ."
dataset/preprocessed/training-data/sentiment_analysis/5,['emotion recognition'],"As the first step to make machines capture human emotions , emotion recognition has received substantial attention from human - machine - interaction ( HMI ) and pattern recognition research communities in recent years , , ."
dataset/preprocessed/training-data/sentiment_analysis/27,['Aspect - level Sentiment Classification'],Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect - level Sentiment Classification
dataset/preprocessed/training-data/sentiment_analysis/27,['sentiment analysis'],"It is a fine - grained task in sentiment analysis , which aims to infer the sentiment polarities of aspects in their context ."
dataset/preprocessed/training-data/sentiment_analysis/47,['Aspect - based Sentiment Analysis'],Left - Center - Right Separated Neural Network for Aspect - based Sentiment Analysis with Rotatory Attention
dataset/preprocessed/training-data/sentiment_analysis/47,['sentiment analysis'],"Aspect - based sentiment analysis is a fine - grained classification task in sentiment analysis , identifying sentiment polarity of a sentence expressed toward a target ."
dataset/preprocessed/training-data/sentiment_analysis/47,['sentiment classification'],"In the early studies , methods for the aspect - based sentiment classification task were similar as that used in standard sentiment classification task ."
dataset/preprocessed/training-data/sentiment_analysis/18,['Aspect - Level Sentiment Classification'],Effective Attention Modeling for Aspect - Level Sentiment Classification
dataset/preprocessed/training-data/sentiment_analysis/18,['fine - grained sentiment analysis'],Aspect - level sentiment classification is an important task in fine - grained sentiment analysis .
dataset/preprocessed/training-data/sentiment_analysis/15,['Sentiment Treebank'],Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
dataset/preprocessed/training-data/sentiment_analysis/15,['richer supervised training and evaluation resources'],Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition .
dataset/preprocessed/training-data/sentiment_analysis/8,['Multimodal Speech Emotion Recognition and Ambiguity Resolution'],Multimodal Speech Emotion Recognition and Ambiguity Resolution
dataset/preprocessed/training-data/sentiment_analysis/8,['Identifying emotion from speech'],Identifying emotion from speech is a nontrivial task pertaining to the ambiguous definition of emotion itself .
dataset/preprocessed/training-data/sentiment_analysis/8,['Speech Emotion Recognition ( SER )'],"With the rise of deep learning algorithms , there have been multiple attempts to tackle the task of Speech Emotion Recognition ( SER ) as in [ 2 ] and ."
dataset/preprocessed/training-data/sentiment_analysis/8,['SER'],"In this work , we explore the implication of hand - crafted features for SER and compare the performance of lighter machine learning models with the heavily data - reliant deep learning models ."
dataset/preprocessed/training-data/sentiment_analysis/6,['Context - Dependent Sentiment Analysis'],Context - Dependent Sentiment Analysis in User- Generated Videos
dataset/preprocessed/training-data/sentiment_analysis/6,['identification of sentiments in videos'],"Multimodal sentiment analysis is a developing area of research , which involves the identification of sentiments in videos ."
dataset/preprocessed/training-data/sentiment_analysis/6,['Sentiment analysis'],"Sentiment analysis is a ' suitcase ' research problem that requires tackling many NLP sub - tasks , e.g. , aspect extraction , named entity recognition , concept extraction , sarcasm detection , personality recognition , and more ."
dataset/preprocessed/training-data/sentiment_analysis/6,['Emotion recognition'],"Emotion recognition further breaks down the inferred polarity into a set of emotions conveyed by the subjective data , e.g. , positive sentiment can be caused by joy or anticipation , while negative sentiment can be caused by fear or disgust ."
dataset/preprocessed/training-data/sentiment_analysis/6,['multimodal sentiment analysis'],"Recently , a number of approaches to multimodal sentiment analysis , producing interesting results , have been proposed ."
dataset/preprocessed/training-data/sentiment_analysis/41,['Aspect - level Sentiment Classification'],Attention - based LSTM for Aspect - level Sentiment Classification
dataset/preprocessed/training-data/sentiment_analysis/41,['sentiment analysis'],Aspect - level sentiment classification is a finegrained task in sentiment analysis .
dataset/preprocessed/training-data/sentiment_analysis/41,['aspect - level sentiment classification'],"In this paper , we deal with aspect - level sentiment classification and we find that the sentiment polarity of a sentence is highly dependent on both content and aspect ."
dataset/preprocessed/training-data/sentiment_analysis/11,['Emotion Recognition in Dyadic Dialogue Videos'],Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos
dataset/preprocessed/training-data/sentiment_analysis/11,['Emotion recognition in conversations'],Emotion recognition in conversations is crucial for the development of empathetic machines .
dataset/preprocessed/training-data/sentiment_analysis/11,['emotion detection in videos of dyadic conversations'],"In this paper , we analyze emotion detection in videos of dyadic conversations ."
dataset/preprocessed/training-data/sentiment_analysis/11,['Emotion detection'],"Emotion detection from such resources can benefit numerous fields like counseling , public opinion mining , financial forecasting , and intelligent systems such as smart homes and chatbots ."
dataset/preprocessed/training-data/sentiment_analysis/36,['Target - Oriented Sentiment Classification'],Transformation Networks for Target - Oriented Sentiment Classification
dataset/preprocessed/training-data/sentiment_analysis/14,['Learning Generalized Emotion Representation'],Emo2 Vec : Learning Generalized Emotion Representation by Multi- task Training
dataset/preprocessed/training-data/sentiment_analysis/17,['Improved Semantic Representations'],Improved Semantic Representations From Tree - Structured Long Short - Term Memory Networks
dataset/preprocessed/training-data/sentiment_analysis/17,"['predicting the semantic relatedness of two sentences', 'sentiment classification']","Tree - LSTMs outperform all existing systems and strong LSTM baselines on two tasks : predicting the semantic relatedness of two sentences ( Sem Eval 2014 , Task 1 ) and sentiment classification ( Stanford Sentiment Treebank ) ."
dataset/preprocessed/training-data/sentiment_analysis/29,['Aspect Level Sentiment Classification'],Aspect Level Sentiment Classification with Attention - over - Attention Neural Networks
dataset/preprocessed/training-data/sentiment_analysis/29,['Aspect - level sentiment classification'],Aspect - level sentiment classification aims to identify the sentiment expressed towards some aspects given context sentences .
dataset/preprocessed/training-data/sentiment_analysis/24,['Aspect - Based Sentiment Analysis'],An Interactive Multi - Task Learning Network for End - to - End Aspect - Based Sentiment Analysis
dataset/preprocessed/training-data/sentiment_analysis/24,['Aspect - based sentiment analysis ( ABSA )'],Aspect - based sentiment analysis ( ABSA ) aims to determine people 's attitude towards specific aspects in a review .
dataset/preprocessed/training-data/sentiment_analysis/24,"['aspect term extraction ( AE )', 'aspect - level sentiment classification ( AS )']","This is done by extracting explicit aspect mentions , referred to as aspect term extraction ( AE ) , and detecting the sentiment orientation towards each extracted aspect term , referred to as aspect - level sentiment classification ( AS ) ."
dataset/preprocessed/training-data/sentiment_analysis/42,['Aspect Level Sentiment Classification'],Aspect Level Sentiment Classification with Deep Memory Network
dataset/preprocessed/training-data/sentiment_analysis/42,['sentiment analysis'],Aspect level sentiment classification is a fundamental task in the field of sentiment analysis .
dataset/preprocessed/training-data/sentiment_analysis/31,['Aspect Level Sentiment Classification'],Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification
dataset/preprocessed/training-data/sentiment_analysis/31,['sentiment classification'],Continuous growing of user generated text in social media platforms such as Twitter drives sentiment classification increasingly popular .
dataset/preprocessed/training-data/sentiment_analysis/31,['general sentiment classification'],"Differing from general sentiment classification , aspect level sentiment classification identifies opinions from text about specific entities and their aspects ."
dataset/preprocessed/training-data/sentiment_analysis/37,['Aspect - Based Sentiment Analysis'],IARM : Inter-Aspect Relation Modeling with Memory Networks in Aspect - Based Sentiment Analysis
dataset/preprocessed/training-data/sentiment_analysis/37,['Sentiment analysis'],Sentiment analysis has immense implications in modern businesses through user-feedback mining .
dataset/preprocessed/training-data/sentiment_analysis/37,['Aspect - based sentiment analysis ( ABSA )'],Aspect - based sentiment analysis ( ABSA ) caters to these needs .
dataset/preprocessed/training-data/sentiment_analysis/37,['ABSA'],The aim of the ABSA classifier is to learn these connections between the aspects and their sentiment bearing phrases .
dataset/preprocessed/training-data/sentiment_analysis/33,['Convolutional Neural Networks with Recurrent Neural Filters'],Convolutional Neural Networks with Recurrent Neural Filters
dataset/preprocessed/training-data/sentiment_analysis/33,['model convolution filters with RNNs'],"In this work , we model convolution filters with RNNs that naturally capture compositionality and long - term dependencies in language ."
dataset/preprocessed/training-data/sentiment_analysis/0,['SPEECH EMOTION RECOGNITION'],MULTIMODAL SPEECH EMOTION RECOGNITION USING AUDIO AND TEXT
dataset/preprocessed/training-data/sentiment_analysis/50,['Aspect - level Sentiment Classification'],Exploiting Document Knowledge for Aspect - level Sentiment Classification
dataset/preprocessed/training-data/sentiment_analysis/45,['Aspect - Based Sentiment Analysis'],Utilizing BERT for Aspect - Based Sentiment Analysis via Constructing Auxiliary Sentence
dataset/preprocessed/training-data/sentiment_analysis/45,"['Aspect - based sentiment analysis ( ABSA )', 'sentiment analysis ( SA )']","Aspect - based sentiment analysis ( ABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect , is a challenging subtask of sentiment analysis ( SA ) ."
dataset/preprocessed/training-data/sentiment_analysis/45,"['SA', 'ABSA']","Both SA and ABSA are sentence - level or document - level tasks , but one comment may refer to more than one object , and sentence - level tasks can not handle sentences with multiple targets ."
dataset/preprocessed/training-data/sentiment_analysis/45,['targeted aspect - based sentiment analysis ( TABSA )'],"Therefore , introduce the task of targeted aspect - based sentiment analysis ( TABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect associated with a given target ."
dataset/preprocessed/training-data/sentiment_analysis/12,['Aspect - Level Sentiment Analysis'],Progressive Self - Supervised Attention Learning for Aspect - Level Sentiment Analysis
dataset/preprocessed/training-data/sentiment_analysis/12,['aspect - level sentiment classification ( ASC )'],"In aspect - level sentiment classification ( ASC ) , it is prevalent to equip dominant neural models with attention mechanisms , for the sake of acquiring the importance of each context word on the given aspect ."
dataset/preprocessed/training-data/sentiment_analysis/12,['neural ASC'],"In this paper , we propose a progressive self - supervised attention learning approach for neural ASC models , which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms ."
dataset/preprocessed/training-data/sentiment_analysis/12,"['Aspect - level sentiment classification ( ASC )', 'sentiment analysis']","Aspect - level sentiment classification ( ASC ) , as an indispensable task in sentiment analysis , aims at inferring the sentiment polarity of an input sentence in a certain aspect ."
dataset/preprocessed/training-data/sentiment_analysis/12,['ASC'],"However , the existing attention mechanism in ASC suffers from a major drawback ."
dataset/preprocessed/training-data/sentiment_analysis/2,['Aspect - level Sentiment Analysis'],A Position - aware Bidirectional Attention Network for Aspect - level Sentiment Analysis
dataset/preprocessed/training-data/sentiment_analysis/2,['Sentiment analysis'],"Sentiment analysis , also known as opinion mining , is a vital task in Natural Language Processing ( NLP ) ."
dataset/preprocessed/training-data/sentiment_analysis/51,['Fine - grained Sentiment Classification'],Fine - grained Sentiment Classification using BERT
dataset/preprocessed/training-data/sentiment_analysis/51,['Sentiment classification'],"Sentiment classification is an important process in understanding people 's perception towards a product , service , or topic ."
dataset/preprocessed/training-data/sentiment_analysis/4,['Multimodal Emotion Detection'],ICON : Interactive Conversational Memory Network for Multimodal Emotion Detection
dataset/preprocessed/training-data/sentiment_analysis/4,['Emotion recognition in conversations'],Emotion recognition in conversations is crucial for building empathetic machines .
dataset/preprocessed/training-data/sentiment_analysis/4,['Analyzing emotional dynamics in conversations'],"Analyzing emotional dynamics in conversations , however , poses complex challenges ."
dataset/preprocessed/training-data/sentiment_analysis/13,['Aspect - based Sentiment Analysis'],BERT Post - Training for Review Reading Comprehension and Aspect - based Sentiment Analysis
dataset/preprocessed/training-data/sentiment_analysis/13,"['aspect extraction', 'aspect sentiment classification', 'aspect - based sentiment analysis']","To show the generality of the approach , the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis ."
dataset/preprocessed/training-data/sentiment_analysis/20,['Message - level and Topic - based Sentiment Analysis'],DataStories at SemEval-2017 Task 4 : Deep LSTM with Attention for Message - level and Topic - based Sentiment Analysis
dataset/preprocessed/training-data/sentiment_analysis/20,['Sentiment Analysis in Twitter'],"In this paper we present two deep - learning systems that competed at SemEval - 2017 Task 4 "" Sentiment Analysis in Twitter "" ."
dataset/preprocessed/training-data/sentiment_analysis/20,['Sentiment analysis'],"Sentiment analysis is an area in Natural Language Processing ( NLP ) , studying the identification and quantification of the sentiment expressed in text ."
dataset/preprocessed/training-data/text-to-speech_synthesis/1,['Text to Speech'],"FastSpeech : Fast , Robust and Controllable Text to Speech"
dataset/preprocessed/training-data/text-to-speech_synthesis/1,['Neural network based end - to - end text to speech ( TTS )'],Neural network based end - to - end text to speech ( TTS ) has significantly improved the quality of synthesized speech .
dataset/preprocessed/training-data/text-to-speech_synthesis/1,['Text to speech ( TTS )'],Text to speech ( TTS ) has attracted a lot of attention in recent years due to the advance in deep learning .
dataset/preprocessed/training-data/text-to-speech_synthesis/1,['Neural network based TTS'],Neural network based TTS has outperformed conventional concatenative and statistical parametric approaches in terms of speech quality .
dataset/preprocessed/training-data/text-to-speech_synthesis/0,['Grapheme - to - Phoneme Conversion'],Token - Level Ensemble Distillation for Grapheme - to - Phoneme Conversion
dataset/preprocessed/training-data/text-to-speech_synthesis/0,['Grapheme - to - phoneme ( G2P ) conversion'],Grapheme - to - phoneme ( G2P ) conversion is an important task in automatic speech recognition and text - to - speech systems .
dataset/preprocessed/training-data/text-to-speech_synthesis/2,['Text - To - Speech Synthesis'],Transfer Learning from Speaker Verification to Multispeaker Text - To - Speech Synthesis
dataset/preprocessed/training-data/text-to-speech_synthesis/2,['text - to - speech ( TTS ) synthesis'],"We describe a neural network - based system for text - to - speech ( TTS ) synthesis that is able to generate speech audio in the voice of different speakers , including those unseen during training ."
dataset/preprocessed/training-data/text-to-speech_synthesis/2,['build a TTS system'],The goal of this work is to build a TTS system which can generate natural speech for a variety of speakers in a data efficient manner .
dataset/preprocessed/training-data/phrase_grounding/0,['Image - Phrase Grounding'],Multi - level Multimodal Common Semantic Space for Image - Phrase Grounding
dataset/preprocessed/training-data/phrase_grounding/0,['phrase grounding'],We address the problem of phrase grounding by learning a multi - level common semantic space shared by the textual and visual modalities .
dataset/preprocessed/training-data/natural_language_inference/56,['Recurrent Relational Networks'],Recurrent Relational Networks
dataset/preprocessed/training-data/natural_language_inference/56,['recurrent relational network'],"We introduce the recurrent relational network , a general purpose module that operates on a graph representation of objects ."
dataset/preprocessed/training-data/natural_language_inference/54,['Machine Comprehension'],Structural Embedding of Syntactic Trees for Machine Comprehension
dataset/preprocessed/training-data/natural_language_inference/54,['Reading comprehension'],"Reading comprehension such as SQuAD or News QA requires identifying a span from a given context , which is an extension to the traditional question answering task , aiming at responding questions posed by human with natural language ."
dataset/preprocessed/training-data/natural_language_inference/82,['Machine Reading'],Dynamic Entity Representation with Max - pooling Improves Machine Reading
dataset/preprocessed/training-data/natural_language_inference/28,['Recurrent Neural Networks ( RNN )'],The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks ( RNN ) to state - of - the - art performance in a variety of sequential tasks .
dataset/preprocessed/training-data/natural_language_inference/28,['RNN'],"However , RNN still have a limited capacity to manipulate long - term memory ."
dataset/preprocessed/training-data/natural_language_inference/7,['EXTRACTIVE QUESTION ANSWERING'],LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING
dataset/preprocessed/training-data/natural_language_inference/7,['answer extraction'],"In this paper , we focus on this answer extraction task , presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network ."
dataset/preprocessed/training-data/natural_language_inference/60,['Improved Sentence Representations'],Dynamic Meta - Embeddings for Improved Sentence Representations
dataset/preprocessed/training-data/natural_language_inference/60,['sentence representations'],"In this work , we explore the supervised learning of task - specific , dynamic meta-embeddings , and apply the technique to sentence representations ."
dataset/preprocessed/training-data/natural_language_inference/88,['Machine Reading'],Long Short - Term Memory - Networks for Machine Reading
dataset/preprocessed/training-data/natural_language_inference/62,['Machine Reading Comprehension'],Explicit Utilization of General Knowledge in Machine Reading Comprehension
dataset/preprocessed/training-data/natural_language_inference/62,"['Machine Reading Comprehension ( MRC )', 'MRC']","To bridge the gap between Machine Reading Comprehension ( MRC ) models and human beings , which is mainly reflected in the hunger for data and the robustness to noise , in this paper , we explore how to integrate the neural networks of MRC models with the general knowledge of human beings ."
dataset/preprocessed/training-data/natural_language_inference/49,['NATURAL LANGUAGE INFERENCE'],Published as a conference paper at ICLR 2018 NATURAL LANGUAGE INFERENCE OVER INTERACTION SPACE
dataset/preprocessed/training-data/natural_language_inference/49,"['NLI', 'recognizing textual entiailment', 'RTE']","Natural Language Inference ( NLI also known as recognizing textual entiailment , or RTE ) task requires one to determine whether the logical relationship between two sentences is among entailment ( if the premise is true , then the hypothesis must be true ) , contradiction ( if the premise is true , then the hypothesis must be false ) and neutral ( neither entailment nor contradiction ) ."
dataset/preprocessed/training-data/natural_language_inference/71,['recurrent neural network ( RNN )'],We present a novel recurrent neural network ( RNN ) based model that combines the remembering ability of unitary RNNs with the ability of gated RNNs to effectively forget redundant / irrelevant information in its memory .
dataset/preprocessed/training-data/natural_language_inference/71,['Recurrent Neural Networks with gating units'],Recurrent Neural Networks with gating units - such as Long Short Term Memory ( LSTMs ) and Gated Recurrent Units ( GRUs ) )
dataset/preprocessed/training-data/natural_language_inference/71,['gating units for Recurrent Neural Networks'],These works have proven the importance of gating units for Recurrent Neural Networks .
dataset/preprocessed/training-data/natural_language_inference/71,['RNNs'],The main advantage of using these gated units in RNNs is primarily due to the ease of optimization of the models using them and to reduce the learning degeneracies such as vanishing gradients that can cripple conventional RNNs .
dataset/preprocessed/training-data/natural_language_inference/3,['Modelling Interaction of Sentence Pair'],Modelling Interaction of Sentence Pair with Coupled- LSTMs
dataset/preprocessed/training-data/natural_language_inference/3,['modelling the interactions of two sentences'],"Recently , there is rising interest in modelling the interactions of two sentences with deep neural networks ."
dataset/preprocessed/training-data/natural_language_inference/3,"['modelling the relevance / similarity of the sentence pair', 'text semantic matching']","Among these tasks , a common problem is modelling the relevance / similarity of the sentence pair , which is also called text semantic matching ."
dataset/preprocessed/training-data/natural_language_inference/25,['Reading Comprehension'],Contextualized Word Representations for Reading Comprehension
dataset/preprocessed/training-data/natural_language_inference/25,['Reading comprehension ( RC )'],Reading comprehension ( RC ) is a high - level task in natural language understanding that requires reading a document and answering questions about its content .
dataset/preprocessed/training-data/natural_language_inference/25,['RC'],"RC has attracted substantial attention over the last few years with the advent of large annotated datasets , computing resources , and neural network models and optimization procedures ."
dataset/preprocessed/training-data/natural_language_inference/40,['Recurrent Neural Networks'],Linguistic Knowledge as Memory for Recurrent Neural Networks
dataset/preprocessed/training-data/natural_language_inference/40,['Training recurrent neural networks to model long term dependencies'],Training recurrent neural networks to model long term dependencies is difficult .
dataset/preprocessed/training-data/natural_language_inference/39,['Semantic Similarity Measurement'],Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement
dataset/preprocessed/training-data/natural_language_inference/39,['Textual similarity measurement'],"Textual similarity measurement is a challenging problem , as it requires understanding the semantics of input sentences ."
dataset/preprocessed/training-data/natural_language_inference/39,['semantic textual similarity ( STS )'],"Given two pieces of text , measuring their semantic textual similarity ( STS ) remains a fundamental problem in language research and lies at the core of many language processing tasks , including question answering , query ranking , and paraphrase generation ."
dataset/preprocessed/training-data/natural_language_inference/73,['attention mechanisms'],Equipping deep neural networks ( DNN ) with attention mechanisms provides an effective and parallelizable approach for context fusion and sequence compression .
dataset/preprocessed/training-data/natural_language_inference/74,['Natural Language Inference'],Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference
dataset/preprocessed/training-data/natural_language_inference/74,"['Natural Language Inference ( NLI )', 'Recognizing Textual Entailment ( RTE )']","Natural Language Inference ( NLI ) , also known as Recognizing Textual Entailment ( RTE ) , is one of the most important problems in natural language processing ."
dataset/preprocessed/training-data/natural_language_inference/74,['NLI'],"While current approaches mostly focus on the interaction architectures of the sentences , in this paper , we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model ."
dataset/preprocessed/training-data/natural_language_inference/43,['Semantic Parsing'],Evaluating Semantic Parsing against a Simple Web - based Question Answering Model
dataset/preprocessed/training-data/natural_language_inference/16,['Natural language sentence matching'],Natural language sentence matching is a fundamental technology for a variety of tasks .
dataset/preprocessed/training-data/natural_language_inference/16,['Natural language sentence matching ( NLSM )'],Natural language sentence matching ( NLSM ) is the task of comparing two sentences and identifying the relationship between them .
dataset/preprocessed/training-data/natural_language_inference/16,['NLSM'],"For example , in a paraphrase identification task , NLSM is used to determine whether two sentences are paraphrase or not ."
dataset/preprocessed/training-data/natural_language_inference/10,['Task - Specific Tree Structures'],Learning to Compose Task - Specific Tree Structures
dataset/preprocessed/training-data/natural_language_inference/10,['task - specific tree structures only from plain text data'],"In this paper , we propose Gumbel Tree - LSTM , a novel tree - structured long short - term memory architecture that learns how to compose task - specific tree structures only from plain text data efficiently ."
dataset/preprocessed/training-data/natural_language_inference/86,['Machine Comprehension'],Multi - Perspective Context Matching for Machine Comprehension
dataset/preprocessed/training-data/natural_language_inference/86,"['machine comprehension ( MC )', 'MC']","Previous machine comprehension ( MC ) datasets are either too small to train endto - end deep learning models , or not difficult enough to evaluate the ability of current MC techniques ."
dataset/preprocessed/training-data/natural_language_inference/46,['Reading Comprehension'],The NarrativeQA Reading Comprehension Challenge
dataset/preprocessed/training-data/natural_language_inference/46,['Reading comprehension ( RC )'],"Reading comprehension ( RC ) - in contrast to information retrieval - requires integrating information and reasoning about events , entities , and their relations across a full document ."
dataset/preprocessed/training-data/natural_language_inference/46,['RC'],"Question answering is conventionally used to assess RC ability , in both artificial agents and children learning to read ."
dataset/preprocessed/training-data/natural_language_inference/26,['language representations'],"The latest work on language representations carefully integrates contextualized features into language model training , which enables a series of success especially in various machine reading comprehension and natural language inference tasks ."
dataset/preprocessed/training-data/natural_language_inference/26,['learning universal language representations'],"Recently , deep contextual language model ( LM ) has been shown effective for learning universal language representations , achieving state - of - the - art results in a series of flagship natural language understanding ( NLU ) tasks ."
dataset/preprocessed/training-data/natural_language_inference/100,['question answering'],"As an alternative to question answering methods based on feature engineering , deep learning approaches such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) have recently been proposed for semantic matching of questions and answers ."
dataset/preprocessed/training-data/natural_language_inference/100,['Question answering ( QA )'],"Question answering ( QA ) , which returns exact answers as either short facts or long passages to natural language questions issued by users , is a challenging task and plays a central role in the next generation of advanced web search ."
dataset/preprocessed/training-data/natural_language_inference/100,['QA'],"Many of current QA systems use a learning to rank approach that encodes question / answer pairs with complex linguistic features including lexical , syntactic and semantic features ."
dataset/preprocessed/training-data/natural_language_inference/21,['RNN / CNN - Free Language Understanding'],DiSAN : Directional Self - Attention Network for RNN / CNN - Free Language Understanding
dataset/preprocessed/training-data/natural_language_inference/35,['Generative Reading Comprehension'],Multi - Style Generative Reading Comprehension
dataset/preprocessed/training-data/natural_language_inference/35,['generative reading comprehension ( RC )'],"This study tackles generative reading comprehension ( RC ) , which consists of answering questions based on textual evidence and natural language generation ( NLG ) ."
dataset/preprocessed/training-data/natural_language_inference/35,['reading comprehension ( RC )'],"Recently , reading comprehension ( RC ) , a challenge to answer a question given textual evidence provided in a document set , has received much attention ."
dataset/preprocessed/training-data/natural_language_inference/35,['RC'],"Current mainstream studies have treated RC as a process of extracting an answer span from one passage or multiple passages , which is usually done by predicting the start and end positions of the answer ."
dataset/preprocessed/training-data/natural_language_inference/35,['multi-passage RC'],"In this study , we propose Masque , a generative model for multi-passage RC ."
dataset/preprocessed/training-data/natural_language_inference/97,['Machine Comprehension'],A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data
dataset/preprocessed/training-data/natural_language_inference/97,['Understanding unstructured text'],Understanding unstructured text is a major goal within natural language processing .
dataset/preprocessed/training-data/natural_language_inference/97,['Machine comprehension ( MC )'],Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .
dataset/preprocessed/training-data/natural_language_inference/30,['Open Question Answering'],Open Question Answering with Weakly Supervised Embedding Models
dataset/preprocessed/training-data/natural_language_inference/30,['open - domain question answering'],"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain ."
dataset/preprocessed/training-data/natural_language_inference/30,['Question answering'],Question answering is then defined as the task of retrieving the correct entity or set of entities from a KB given a query expressed as a question in natural language .
dataset/preprocessed/training-data/natural_language_inference/52,['Answer Justification'],Tell Me Why : Using Question Answering as Distant Supervision for Answer Justification
dataset/preprocessed/training-data/natural_language_inference/52,['Developing interpretable machine learning ( ML ) models'],"Developing interpretable machine learning ( ML ) models , that is , models where a human user can understand what the model is learning , is considered by many to be crucial for ensuring usability and accelerating progress ."
dataset/preprocessed/training-data/natural_language_inference/68,['MACHINE COMPREHENSION'],MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER
dataset/preprocessed/training-data/natural_language_inference/68,['Machine comprehension of text'],Machine comprehension of text is an important problem in natural language processing .
dataset/preprocessed/training-data/natural_language_inference/69,['Multi-hop Reading Comprehension'],Constructing Datasets for Multi-hop Reading Comprehension Across Documents
dataset/preprocessed/training-data/natural_language_inference/69,['Reading Comprehension'],"Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence , paragraph , or document ."
dataset/preprocessed/training-data/natural_language_inference/69,['Reading Comprehension ( RC )'],Contemporary end - to - end Reading Comprehension ( RC ) methods can learn to extract the correct answer span within a given text and approach human - level performance .
dataset/preprocessed/training-data/natural_language_inference/99,['Machine Comprehension ( MC )'],"Machine Comprehension ( MC ) is a challenging task in Natural Language Processing field , which aims to guide the machine to comprehend a passage and answer the given question ."
dataset/preprocessed/training-data/natural_language_inference/99,['MC'],"Many existing approaches on MC task are suffering the inefficiency in some bottlenecks , such as insufficient lexical understanding , complex question - passage interaction , incorrect answer extraction and soon ."
dataset/preprocessed/training-data/natural_language_inference/99,['machine comprehension'],Recently machine comprehension task accumulates much concern among NLP researchers .
dataset/preprocessed/training-data/natural_language_inference/23,['MACHINE COMPREHENSION'],FUSIONNET : FUSING VIA FULLY - AWARE ATTENTION WITH APPLICATION TO MACHINE COMPREHENSION
dataset/preprocessed/training-data/natural_language_inference/23,"['Teaching machines to read , process and comprehend text and then answer questions']","Teaching machines to read , process and comprehend text and then answer questions is one of key problems in artificial intelligence ."
dataset/preprocessed/training-data/natural_language_inference/23,['machine reading comprehension ( MRC )'],Many neural network models have been proposed for this challenge and they generally frame this problem as a machine reading comprehension ( MRC ) task .
dataset/preprocessed/training-data/natural_language_inference/23,"['language understanding', 'MRC']",We argue that this hypothesis also holds in language understanding and MRC .
dataset/preprocessed/training-data/natural_language_inference/58,['Machine Comprehension'],ReasoNet : Learning to Stop Reading in Machine Comprehension
dataset/preprocessed/training-data/natural_language_inference/91,"['Parsing', 'Sentence Understanding']",A Fast Unified Model for Parsing and Sentence Understanding
dataset/preprocessed/training-data/natural_language_inference/98,['Natural Language Inference'],Recurrent Neural Network - Based Sentence Encoder with Gated Attention for Natural Language Inference
dataset/preprocessed/training-data/natural_language_inference/98,['natural language inference ( NLI )'],"Task aims to evaluate language understanding models for sentence representation with natural language inference ( NLI ) tasks , where a sentence is represented as a fixedlength vector ."
dataset/preprocessed/training-data/natural_language_inference/98,['NLI'],"Specifically , NLI is concerned with determining whether a hypothesis sentence h can be inferred from a premise sentence p."
dataset/preprocessed/training-data/natural_language_inference/34,['Multi-hop Reasoning'],Dynamically Fused Graph Network for Multi-hop Reasoning
dataset/preprocessed/training-data/natural_language_inference/34,['Text - based question answering ( TBQA )'],Text - based question answering ( TBQA ) has been studied extensively in recent years .
dataset/preprocessed/training-data/natural_language_inference/34,['Question answering ( QA )'],Question answering ( QA ) has been a popular topic in natural language processing .
dataset/preprocessed/training-data/natural_language_inference/34,['QA'],QA provides a quantifiable way to evaluate an NLP system 's capability on language understanding and reasoning .
dataset/preprocessed/training-data/natural_language_inference/32,['CONVERSATIONAL MACHINE COMPREHENSION'],FLOWQA : GRASPING FLOW IN HISTORY FOR CONVERSATIONAL MACHINE COMPREHENSION
dataset/preprocessed/training-data/natural_language_inference/32,['conversational machine comprehension ( MC )'],Recently proposed conversational machine comprehension ( MC ) datasets aim to enable models to assist in such information seeking dialogs .
dataset/preprocessed/training-data/natural_language_inference/48,['Machine Reading'],Iterative Alternating Neural Attention for Machine Reading
dataset/preprocessed/training-data/natural_language_inference/48,['machine comprehension'],"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document ."
dataset/preprocessed/training-data/natural_language_inference/85,['Natural Language Inference'],Enhanced LSTM for Natural Language Inference
dataset/preprocessed/training-data/natural_language_inference/85,['Reasoning and inference'],Reasoning and inference are central to human and artificial intelligence .
dataset/preprocessed/training-data/natural_language_inference/85,['natural language inference ( NLI )'],"Specifically , natural language inference ( NLI ) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a premise p , as depicted in the following example from MacCartney ( 2009 ) , where the hypothesis is regarded to be entailed from the premise ."
dataset/preprocessed/training-data/natural_language_inference/85,['NLI'],Exploring syntax for NLI is very attractive to us .
dataset/preprocessed/training-data/natural_language_inference/87,['Machine Reading Comprehension'],SG - Net : Syntax - Guided Machine Reading Comprehension
dataset/preprocessed/training-data/natural_language_inference/87,['machine reading comprehension ( MRC )'],"Understanding the meaning of a sentence is a prerequisite to solve many natural language understanding ( NLU ) problems , such as machine reading comprehension ( MRC ) based question answering ."
dataset/preprocessed/training-data/natural_language_inference/87,['MRC'],We observe that the accuracy of MRC models decreases when answering long questions ( shown in Section 5.1 ) .
dataset/preprocessed/training-data/natural_language_inference/44,['Question Answering'],Efficient and Robust Question Answering from Minimal Context over Documents
dataset/preprocessed/training-data/natural_language_inference/44,['question answering ( QA )'],Neural models for question answering ( QA ) over documents have achieved significant performance improvements .
dataset/preprocessed/training-data/natural_language_inference/44,['QA'],"Inspired by this observation , we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model ."
dataset/preprocessed/training-data/natural_language_inference/38,['Machine Comprehension'],MEMEN : Multi-layer Embedding with Memory Networks for Machine Comprehension
dataset/preprocessed/training-data/natural_language_inference/38,['Machine comprehension ( MC ) style question answering'],Machine comprehension ( MC ) style question answering is a representative problem in natural language processing .
dataset/preprocessed/training-data/natural_language_inference/38,['Machine comprehension ( MC )'],Machine comprehension ( MC ) has gained significant popularity over the past few years and it is a coveted goal in the field of natural language processing and artificial intelligence .
dataset/preprocessed/training-data/natural_language_inference/9,['QUESTION ANSWERING'],Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING
dataset/preprocessed/training-data/natural_language_inference/9,['question answering when reasoning over multiple facts'],"In this paper , we study the problem of question answering when reasoning over multiple facts is required ."
dataset/preprocessed/training-data/natural_language_inference/19,['Natural Language Inference'],Distance - based Self - Attention Network for Natural Language Inference
dataset/preprocessed/training-data/natural_language_inference/19,['NLI'],"Our model shows good performance with NLI data , and it records the new state - of - the - art result with SNLI data ."
dataset/preprocessed/training-data/natural_language_inference/19,['Natural Language Inference ( NLI )'],"More recently , models incorporating attention mechanisms have shown good performance in machine translation , Natural Language Inference ( NLI ) , and Question Answering ( QA ) etc ."
dataset/preprocessed/training-data/natural_language_inference/22,['machine comprehension ( MC )'],"To answer the question in machine comprehension ( MC ) task , the models need to establish the interaction between the question and the context ."
dataset/preprocessed/training-data/natural_language_inference/22,['question answering ( QA )'],Machine comprehension ( MC ) - especially in the form of question answering ( QA ) - is therefore attracting a significant amount of attention from the machine learning community .
dataset/preprocessed/training-data/natural_language_inference/1,['Large - scale Simple Question Answering'],Large - scale Simple Question Answering with Memory Networks
dataset/preprocessed/training-data/natural_language_inference/1,['large - scale question answering'],Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .
dataset/preprocessed/training-data/natural_language_inference/1,['simple question answering'],"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions ."
dataset/preprocessed/training-data/natural_language_inference/1,['Simple Question Answering'],"However , while most recent efforts have focused on designing systems with higher reasoning capabilities , that could jointly retrieve and use multiple facts to answer , the simpler problem of answering questions that refer to a single fact of the KB , which we call Simple Question Answering in this paper , is still far from solved ."
dataset/preprocessed/training-data/natural_language_inference/83,['Story Comprehension'],Story Comprehension for Predicting What Happens Next
dataset/preprocessed/training-data/natural_language_inference/83,['Automatic story comprehension'],"Automatic story comprehension is a fundamental challenge in Natural Language Understanding , and can enable computers to learn about social norms , human behavior and commonsense ."
dataset/preprocessed/training-data/natural_language_inference/83,['automatically understanding stories'],"For these reasons , automatically understanding stories is an interesting but challenging task for Computational Linguists ."
dataset/preprocessed/training-data/natural_language_inference/83,['story - cloze'],"Recently , introduced the story - cloze task for testing this ability , albeit without the aspect of language generation ."
dataset/preprocessed/training-data/natural_language_inference/5,['Answer Selection'],A Compare - Aggregate Model with Latent Clustering for Answer Selection
dataset/preprocessed/training-data/natural_language_inference/5,['sentence - level answer- selection'],"In this paper , we propose a novel method for a sentence - level answer- selection task that is a fundamental problem in natural language processing ."
dataset/preprocessed/training-data/natural_language_inference/5,['Automatic question answering ( QA )'],Automatic question answering ( QA ) is a primary objective of artificial intelligence .
dataset/preprocessed/training-data/natural_language_inference/27,['READING COMPRE - HENSION'],COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF - ATTENTION FOR READING COMPRE - HENSION
dataset/preprocessed/training-data/natural_language_inference/27,['machine reading and question answering ( Q&A )'],Current end - to - end machine reading and question answering ( Q&A ) models are primarily based on recurrent neural networks ( RNNs ) with attention .
dataset/preprocessed/training-data/natural_language_inference/27,"['machine reading comprehension', 'automated question answering']",There is growing interest in the tasks of machine reading comprehension and automated question answering .
dataset/preprocessed/training-data/natural_language_inference/27,['machine comprehension'],"In this paper , aiming to make the machine comprehension fast , we propose to remove the recurrent nature of these models ."
dataset/preprocessed/training-data/natural_language_inference/47,['natural language inference'],A large annotated corpus for learning natural language inference
dataset/preprocessed/training-data/natural_language_inference/47,['natural language inference ( NLI )'],"Thus , natural language inference ( NLI ) - characterizing and using these relations in computational systems ) - is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning ."
dataset/preprocessed/training-data/natural_language_inference/47,['NLI'],"NLI has been addressed using a variety of techniques , including those based on symbolic logic , knowledge bases , and neural networks ."
dataset/preprocessed/training-data/natural_language_inference/77,['Neural QA'],Making Neural QA as Simple as Possible but not Simpler
dataset/preprocessed/training-data/natural_language_inference/77,"['question answering ( QA )', 'QA']",Recent development of large - scale question answering ( QA ) datasets triggered a substantial amount of research into end - toend neural architectures for QA .
dataset/preprocessed/training-data/natural_language_inference/77,['Question answering'],Question answering is an important end - user task at the intersection of natural language processing ( NLP ) and information retrieval ( IR ) .
dataset/preprocessed/training-data/natural_language_inference/64,['Answer Sentence Selection'],TANDA : Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection
dataset/preprocessed/training-data/natural_language_inference/64,['Question Answering'],"We demonstrate the benefits of our approach for answer sentence selection , which is a well - known inference task in Question Answering ."
dataset/preprocessed/training-data/natural_language_inference/64,['Question Answering ( QA )'],"This has renewed the research interest in Question Answering ( QA ) and , in particular , in two main tasks :"
dataset/preprocessed/training-data/natural_language_inference/64,['answer sentence selection ( AS2 )'],"( i ) answer sentence selection ( AS2 ) , which , given a question and a set of answer sentence candidates , consists in selecting sentences ( e.g. , retrieved by a search engine ) correctly answering the question ; and ( ii ) machine reading ( MR ) or reading comprehension , which , given a question and a reference text , consists in finding a text span answering it ."
dataset/preprocessed/training-data/natural_language_inference/64,"['AS2', 'QA']","Even though the latter is gaining more and more popularity , AS2 is more relevant to a production scenario since , a combination of a search engine and an AS2 model already implements an initial QA system ."
dataset/preprocessed/training-data/natural_language_inference/18,['Neural Variational Inference'],Neural Variational Inference for Text Processing Phil Blunsom 12
dataset/preprocessed/training-data/natural_language_inference/15,['Semantic Sentence Matching'],Semantic Sentence Matching with Densely - connected Recurrent and Co - attentive Information
dataset/preprocessed/training-data/natural_language_inference/15,['Sentence matching'],"Sentence matching is widely used in various natural language tasks such as natural language inference , paraphrase identification , and question answering ."
dataset/preprocessed/training-data/natural_language_inference/57,['ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE'],ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE
dataset/preprocessed/training-data/natural_language_inference/57,"['single visual - semantic hierarchy over words , sentences , and images']","Hypernymy , textual entailment , and image captioning can be seen as special cases of a single visual - semantic hierarchy over words , sentences , and images ."
dataset/preprocessed/training-data/natural_language_inference/57,['visualsemantic hierarchy'],"In fact , all three relations can be seen as special cases of a partial order over images and language , illustrated in , which we refer to as the visualsemantic hierarchy ."
dataset/preprocessed/training-data/natural_language_inference/61,['natural language inference'],"is paper proposes an a ention boosted natural language inference model named a ESIM by adding word a ention and adaptive direction - oriented a ention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM . is makes the inference model a ESIM has the ability toe ectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis ."
dataset/preprocessed/training-data/natural_language_inference/61,['Natural language inference ( NLI )'],Natural language inference ( NLI ) is an important and signicant task in natural language processing ( NLP ) .
dataset/preprocessed/training-data/natural_language_inference/61,['NLI'],"In the literature , the task of NLI is usually viewed as a relation classi cation ."
dataset/preprocessed/training-data/natural_language_inference/53,['Supervised Learning of Universal Sentence Representations'],Supervised Learning of Universal Sentence Representations from Natural Language Inference Data
dataset/preprocessed/training-data/natural_language_inference/8,['Answer Sentence Selection'],Deep Learning for Answer Sentence Selection
dataset/preprocessed/training-data/natural_language_inference/89,['Machine Reading Comprehension'],Read + Verify : Machine Reading Comprehension with Unanswerable Questions
dataset/preprocessed/training-data/natural_language_inference/84,['Learning representations for rare words'],"Learning representations for rare words is a well - known challenge of natural language understanding , since the standard end - to - end supervised learning methods require many occurrences of each word to generalize well ."
dataset/preprocessed/training-data/natural_language_inference/84,['COMPUTE WORD EMBEDDINGS ON THE FLY'],LEARNING TO COMPUTE WORD EMBEDDINGS ON THE FLY
dataset/preprocessed/training-data/natural_language_inference/6,['Massively Multilingual Sentence Embeddings'],Massively Multilingual Sentence Embeddings for Zero - Shot Cross - Lingual Transfer and Beyond
dataset/preprocessed/training-data/natural_language_inference/6,['joint multilingual sentence representations'],"We introduce an architecture to learn joint multilingual sentence representations for 93 languages , belonging to more than 30 different families and written in 28 different scripts ."
dataset/preprocessed/training-data/natural_language_inference/90,['Natural Language Inference'],A Decomposable Attention Model for Natural Language Inference
dataset/preprocessed/training-data/natural_language_inference/90,['Natural language inference ( NLI )'],Natural language inference ( NLI ) refers to the problem of determining entailment and contradiction relationships between a premise and a hypothesis .
dataset/preprocessed/training-data/natural_language_inference/90,['NLI'],NLI is a central problem in language understanding ) and recently the large SNLI corpus of 570K sentence pairs was created for this task .
dataset/preprocessed/training-data/natural_language_inference/41,['Sequence - to - Sequence Pre-training'],"BART : Denoising Sequence - to - Sequence Pre-training for Natural Language Generation , Translation , and Comprehension"
dataset/preprocessed/training-data/natural_language_inference/41,['pretraining sequence - to - sequence models'],"We present BART , a denoising autoencoder for pretraining sequence - to - sequence models ."
dataset/preprocessed/training-data/natural_language_inference/55,['Question Answering'],Question Answering with Subgraph Embeddings
dataset/preprocessed/training-data/natural_language_inference/55,['answer questions on a broad range of topics'],This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features .
dataset/preprocessed/training-data/natural_language_inference/55,['automatically answer questions asked in natural language on any topic or in any domain'],Teaching machines how to automatically answer questions asked in natural language on any topic or in any domain has always been along standing goal in Artificial Intelligence .
dataset/preprocessed/training-data/natural_language_inference/55,['open - domain question answering ( or open QA )'],"With the rise of large scale structured knowledge bases ( KBs ) , this problem , known as open - domain question answering ( or open QA ) , boils down to being able to query efficiently such databases with natural language ."
dataset/preprocessed/training-data/natural_language_inference/55,['open QA'],"These KBs , such as Freebase encompass huge ever growing amounts of information and ease open QA by organizing a great variety of answers in a structured format ."
dataset/preprocessed/training-data/natural_language_inference/76,['Recognizing textual entailment ( RTE )'],"Recognizing textual entailment ( RTE ) is the task of determining whether two natural language sentences are ( i ) contradicting each other , ( ii ) not related , or whether ( iii ) the first sentence ( called premise ) entails the second sentence ( called hypothesis ) ."
dataset/preprocessed/training-data/natural_language_inference/76,['RTE'],"This task is important since many natural language processing ( NLP ) problems , such as information extraction , relation extraction , text summarization or machine translation , rely on it explicitly or implicitly and could benefit from more accurate RTE systems ."
dataset/preprocessed/training-data/natural_language_inference/96,['Grounded Commonsense Inference'],Swag : A Large - Scale Adversarial Dataset for Grounded Commonsense Inference
dataset/preprocessed/training-data/natural_language_inference/11,['Neural NLU'],Dynamic Integration of Background Knowledge in Neural NLU Systems
dataset/preprocessed/training-data/natural_language_inference/11,['neural natural language understanding ( NLU )'],"Common- sense and background knowledge is required to understand natural language , but in most neural natural language understanding ( NLU ) systems , this knowledge must be acquired from training corpora during learning , and then it is static at test time ."
dataset/preprocessed/training-data/natural_language_inference/36,['Text Comprehension'],Explicit Contextual Semantics for Text Comprehension
dataset/preprocessed/training-data/natural_language_inference/36,"['text comprehension ( TC )', 'machine reading comprehension ( MRC )', 'textual entailment ( TE )']","This paper focuses on two core text comprehension ( TC ) tasks , machine reading comprehension ( MRC ) and textual entailment ( TE ) ."
dataset/preprocessed/training-data/natural_language_inference/78,['Natural Language Inference'],"Compare , Compress and Propagate : Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference"
dataset/preprocessed/training-data/natural_language_inference/78,['Natural Language Inference ( NLI )'],This paper presents a new deep learning architecture for Natural Language Inference ( NLI ) .
dataset/preprocessed/training-data/natural_language_inference/78,['NLI'],"More concretely , given a premise and hypothesis , NLI aims to detect whether the latter entails or contradicts the former ."
dataset/preprocessed/training-data/natural_language_inference/14,['Question Answering'],CODAH : An Adversarially - Authored Question Answering Dataset for Common Sense
dataset/preprocessed/training-data/natural_language_inference/14,['Commonsense reasoning'],"Commonsense reasoning is a critical AI capability , but it is difficult to construct challenging datasets that test commonsense ."
dataset/preprocessed/training-data/natural_language_inference/14,['commonsense reasoning over text'],The rise of datadriven methods has led to interest in developing large datasets for commonsense reasoning over text .
dataset/preprocessed/training-data/natural_language_inference/14,['commonsense question answering'],"In this work , we introduce the COmmonsense Dataset Adversarially - authored by Humans ( CODAH ) for commonsense question answering in the style of SWAG multiple choice sentence completion ."
dataset/preprocessed/training-data/natural_language_inference/17,['Machine Reading Comprehension'],Reinforced Mnemonic Reader for Machine Reading Comprehension
dataset/preprocessed/training-data/natural_language_inference/81,['MULTI - EVIDENCE QUESTION ANSWERING'],COARSE - GRAIN FINE - GRAIN COATTENTION NET - WORK FOR MULTI - EVIDENCE QUESTION ANSWERING
dataset/preprocessed/training-data/natural_language_inference/81,['question answering'],"End - to - end neural models have made significant progress in question answering , however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document ."
dataset/preprocessed/training-data/natural_language_inference/81,['question answering ( QA )'],A requirement of scalable and practical question answering ( QA ) systems is the ability to reason over multiple documents and combine their information to answer questions .
dataset/preprocessed/training-data/natural_language_inference/81,['neural question answering'],"Although existing datasets enabled the development of effective end - to - end neural question answering systems , they tend to focus on reasoning over localized sections of a single document ."
dataset/preprocessed/training-data/natural_language_inference/29,['Product - Aware Answer Generation'],Product - Aware Answer Generation in E - Commerce Question - Answering
dataset/preprocessed/training-data/natural_language_inference/29,"['question - answering ( QA )', 'reading comprehension']","In recent years , the explosive popularity of question - answering ( QA ) is revitalizing the task of reading comprehension with promising results ."
dataset/preprocessed/training-data/natural_language_inference/24,['Machine Reading Comprehension'],Stochastic Answer Networks for Machine Reading Comprehension
dataset/preprocessed/training-data/natural_language_inference/24,['Machine reading comprehension ( MRC )'],Machine reading comprehension ( MRC ) is a challenging task : the goal is to have machines read a text passage and then answer any question about the passage .
dataset/preprocessed/training-data/natural_language_inference/24,['MRC'],It has been hypothesized that difficult MRC problems require some form of multi-step synthesis and reasoning .
dataset/preprocessed/training-data/natural_language_inference/42,['Information Retriever'],A Fully Attention - Based Information Retriever
dataset/preprocessed/training-data/natural_language_inference/42,['Question - answering ( QA )'],Question - answering ( QA ) systems that can answer queries expressed in natural language have been a perennial goal of the artificial intelligence community .
dataset/preprocessed/training-data/natural_language_inference/42,['open - domain QA'],"That is , in fact , the proposed focus of recent open - domain QA datasets , such as SQuAD ."
dataset/preprocessed/training-data/natural_language_inference/42,['QA'],"In SQuAD , each problem instance consists of a passage P and a question Q. A QA system must then provide an answer A by selecting a snippet from P ."
dataset/preprocessed/training-data/natural_language_inference/93,['Reading Comprehension and Question Answering'],Gated Self - Matching Networks for Reading Comprehension and Question Answering
dataset/preprocessed/training-data/natural_language_inference/93,['reading comprehension style question answering'],"In this paper , we present the gated selfmatching networks for reading comprehension style question answering , which aims to answer questions from a given passage ."
dataset/preprocessed/training-data/natural_language_inference/59,['Neural Paraphrase Identification of Questions'],Neural Paraphrase Identification of Questions with Noisy Pretraining
dataset/preprocessed/training-data/natural_language_inference/59,['paraphrase identification of questions'],We present a solution to the problem of paraphrase identification of questions .
dataset/preprocessed/training-data/natural_language_inference/59,['Question paraphrase identification'],Question paraphrase identification is a widely useful NLP application .
dataset/preprocessed/training-data/natural_language_inference/80,['Natural Language Inference'],DR- BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference
dataset/preprocessed/training-data/natural_language_inference/80,['natural language inference ( NLI )'],We present a novel deep learning architecture to address the natural language inference ( NLI ) task .
dataset/preprocessed/training-data/natural_language_inference/80,['NLI'],"The goal of NLI is to identify the logical relationship ( entailment , neutral , or contradiction ) between a premise and a corresponding hypothesis ."
dataset/preprocessed/training-data/natural_language_inference/31,['Text Matching'],Simple and Effective Text Matching with Richer Alignment Features
dataset/preprocessed/training-data/natural_language_inference/65,['Neural networks ( NN ) with attention mechanisms'],"Neural networks ( NN ) with attention mechanisms have recently proven to be successful at different computer vision ( CV ) and natural language processing ( NLP ) tasks such as image captioning , machine translation and factoid question answering ."
dataset/preprocessed/training-data/natural_language_inference/65,['attention mechanisms'],"However , most recent work on neural attention models have focused on one - way attention mechanisms based on recurrent neural networks designed . for generation tasks ."
dataset/preprocessed/training-data/natural_language_inference/92,['Weakly Supervised Question Answering'],A Discrete Hard EM Approach for Weakly Supervised Question Answering
dataset/preprocessed/training-data/natural_language_inference/92,['question answering ( QA )'],Many question answering ( QA ) tasks only provide weak supervision for how the answer should be computed .
dataset/preprocessed/training-data/natural_language_inference/92,['QA'],"Despite its simplicity , we show that this approach significantly outperforms previous methods on six QA tasks , including absolute gains of 2 - 10 % , and achieves the stateof - the - art on five of them ."
dataset/preprocessed/training-data/natural_language_inference/67,['Reading Comprehension'],End - to - End Answer Chunk Extraction and Ranking for Reading Comprehension
dataset/preprocessed/training-data/natural_language_inference/67,['neural reading comprehension ( RC )'],"This paper proposes dynamic chunk reader ( DCR ) , an end - toend neural reading comprehension ( RC ) model that is able to extract and rank a set of answer candidates from a given document to answer questions ."
dataset/preprocessed/training-data/natural_language_inference/67,['Reading comprehension - based question answering ( RCQA )'],Reading comprehension - based question answering ( RCQA ) is the task of answering a question with a chunk of text taken from related document ( s ) .
dataset/preprocessed/training-data/natural_language_inference/67,['RCQA'],"Different from the above two assumptions for RCQA , in the real - world QA scenario , people may ask questions about both entities ( factoid ) and non-entities such as explanations and reasons ( non -factoid ) ( see for examples ) ."
dataset/preprocessed/training-data/natural_language_inference/37,['Multi - Paragraph Reading Comprehension'],Simple and Effective Multi - Paragraph Reading Comprehension
dataset/preprocessed/training-data/natural_language_inference/37,['neural paragraph - level question answering'],We consider the problem of adapting neural paragraph - level question answering models to the case where entire documents are given as input .
dataset/preprocessed/training-data/natural_language_inference/37,['answering questions given a related paragraph'],The recent success of neural models at answering questions given a related paragraph suggests neural models have the potential to be a key part of a solution to this problem .
dataset/preprocessed/training-data/natural_language_inference/33,['LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS'],Published as a conference paper at ICLR 2018 LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS VIA LARGE SCALE MULTI - TASK LEARNING
dataset/preprocessed/training-data/natural_language_inference/33,['distributed vector representations of words'],A lot of the recent success in natural language processing ( NLP ) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner .
dataset/preprocessed/training-data/natural_language_inference/33,['learning representations of sequences of words'],"However , extending this success to learning representations of sequences of words , such as sentences , remains an open problem ."
dataset/preprocessed/training-data/natural_language_inference/33,['learning general - purpose sentence representations'],Some recent work has addressed this by learning general - purpose sentence representations .
dataset/preprocessed/training-data/natural_language_inference/94,['Generative Multi - Hop Question Answering'],Commonsense for Generative Multi - Hop Question Answering Tasks
dataset/preprocessed/training-data/natural_language_inference/94,['Reading comprehension QA'],"Reading comprehension QA tasks have seen a recent surge in popularity , yet most works have focused on fact - finding extractive QA ."
dataset/preprocessed/training-data/natural_language_inference/94,['machine reading comprehension ( MRC ) based QA'],"In this paper , we explore the task of machine reading comprehension ( MRC ) based QA ."
dataset/preprocessed/training-data/natural_language_inference/94,['reasoning - based MRC - QA'],"Much progress has been made in reasoning - based MRC - QA on the bAbI dataset , which contains questions that require the combination of multiple disjoint pieces of evidence in the context ."
dataset/preprocessed/training-data/natural_language_inference/63,['Machine Reading Comprehension'],A Multi - Stage Memory Augmented Neural Network for Machine Reading Comprehension
dataset/preprocessed/training-data/natural_language_inference/63,['Reading Comprehension ( RC )'],Reading Comprehension ( RC ) of text is one of the fundamental tasks in natural language processing .
dataset/preprocessed/training-data/natural_language_inference/63,['RC'],"In recent years , several end - to - end neural network models have been proposed to solve RC tasks ."
dataset/preprocessed/training-data/natural_language_inference/63,['answer span prediction style Question Answering ( QA )'],"One possible way of measuring RC is by formulating it as answer span prediction style Question Answering ( QA ) task , which is finding an answer to the question based on the given document ( s ) ."
dataset/preprocessed/training-data/natural_language_inference/63,['QA'],"Recently , influential deep learning approaches have been proposed to solve this QA task . ; propose the attention mechanism between question and context for question - aware contextual representation ."
dataset/preprocessed/training-data/natural_language_inference/0,['Text Comprehension'],Gated - Attention Readers for Text Comprehension
dataset/preprocessed/training-data/natural_language_inference/0,['machine reading'],A recent trend to measure progress towards machine reading is to test a system 's ability to answer questions about a document it has to comprehend .
dataset/preprocessed/training-data/natural_language_inference/66,['Natural Language Inference'],Learning Natural Language Inference with LSTM
dataset/preprocessed/training-data/natural_language_inference/66,['Natural language inference ( NLI )'],Natural language inference ( NLI ) is a fundamentally important task in natural language processing that has many applications .
dataset/preprocessed/training-data/natural_language_inference/66,['NLI'],"In this paper , we propose a special long short - term memory ( LSTM ) architecture for NLI ."
dataset/preprocessed/training-data/natural_language_inference/50,['Neural Question Answering'],Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering
dataset/preprocessed/training-data/natural_language_inference/72,['Machine Reading Comprehension'],CliCR : A Dataset of Clinical Case Reports for Machine Reading Comprehension *
dataset/preprocessed/training-data/natural_language_inference/72,['machine comprehension'],We present a new dataset for machine comprehension in the medical domain .
dataset/preprocessed/training-data/natural_language_inference/45,['READING COMPREHENSION'],Published as a conference paper at ICLR 2017 WORDS OR CHARACTERS ? FINE - GRAINED GATING FOR READING COMPREHENSION
dataset/preprocessed/training-data/natural_language_inference/70,['Community Question Answering ( c QA )'],This paper describes the KeLP system participating in the SemEval - 2016 Community Question Answering ( c QA ) task .
dataset/preprocessed/training-data/natural_language_inference/70,['c QA'],"In this task , participants are asked to automatically provide good answers in a c QA setting ."
dataset/preprocessed/training-data/natural_language_inference/79,['Distributed Representations of Sentences and Documents'],Distributed Representations of Sentences and Documents
dataset/preprocessed/training-data/natural_language_inference/95,['Multi - Passage Machine Reading Comprehension'],Multi - Passage Machine Reading Comprehension with Cross - Passage Answer Verification
dataset/preprocessed/training-data/natural_language_inference/95,['Machine reading comprehension ( MRC )'],Machine reading comprehension ( MRC ) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine .
dataset/preprocessed/training-data/natural_language_inference/95,['MRC'],"Compared with MRC on a single passage , multi-passage MRC is more challenging , since we are likely to get multiple confusing answer candidates from different passages ."
dataset/preprocessed/training-data/natural_language_inference/12,['Multi- Domain Inference'],Shortcut - Stacked Sentence Encoders for Multi- Domain Inference
dataset/preprocessed/training-data/natural_language_inference/12,['multi-domain natural language inference'],We present a simple sequential sentence encoder for multi-domain natural language inference .
dataset/preprocessed/training-data/natural_language_inference/12,"['Natural language inference ( NLI )', 'recognizing textual entailment ( RTE )']",Natural language inference ( NLI ) or recognizing textual entailment ( RTE ) is a fundamental semantic task in the field of natural language processing .
dataset/preprocessed/training-data/natural_language_inference/2,['Question Answering'],A Question - Focused Multi- Factor Attention Network for Question Answering
dataset/preprocessed/training-data/natural_language_inference/2,['question answering ( QA )'],Neural network models recently proposed for question answering ( QA ) primarily focus on capturing the passagequestion relation .
dataset/preprocessed/training-data/natural_language_inference/2,['QA'],They also do not explicitly focus on the question and answer type which often plays a critical role in QA .
dataset/preprocessed/training-data/natural_language_inference/2,['machine comprehension - based ( MC ) question answering ( QA )'],"In machine comprehension - based ( MC ) question answering ( QA ) , a machine is expected to provide an answer for a given question by understanding texts ."
dataset/preprocessed/training-data/natural_language_inference/75,['Directly reading documents'],Directly reading documents and being able to answer questions from them is an unsolved challenge .
dataset/preprocessed/training-data/natural_language_inference/75,['question answering ( QA )'],"To avoid its inherent difficulty , question answering ( QA ) has been directed towards using Knowledge Bases ( KBs ) instead , which has proven effective ."
dataset/preprocessed/training-data/natural_language_inference/75,['QA'],"To compare using KBs , information extraction or Wikipedia documents directly in a single framework we construct an analysis tool , WIKIMOVIES , a QA dataset that contains raw text alongside a preprocessed KB , in the domain of movies ."
dataset/preprocessed/training-data/natural_language_inference/51,['Neural Stored - program Memory'],Neural Stored - program Memory
dataset/preprocessed/training-data/natural_language_inference/51,['stored - program memory'],"In this paper , we introduce a new memory to store weights for the controller , analogous to the stored - program memory in modern computer architectures ."
dataset/preprocessed/training-data/natural_language_inference/4,['QUESTION ANSWERING'],DCN + : MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING
dataset/preprocessed/training-data/natural_language_inference/13,['Machine Comprehension'],Multi-range Reasoning for Machine Comprehension
dataset/preprocessed/training-data/natural_language_inference/13,['machine comprehension ( MC )'],"We propose MRU ( Multi - Range Reasoning Units ) , a new fast compositional encoder for machine comprehension ( MC ) ."
dataset/preprocessed/training-data/natural_language_inference/13,['MC'],"While the usage of recurrent encoder is often regarded as indispensable in highly complex MC tasks , there are still several challenges and problems pertaining to it 's usage in modern MC tasks ."
dataset/preprocessed/training-data/natural_language_inference/20,['Sentence Embeddings'],Sentence Embeddings in NLI with Iterative Refinement Encoders
dataset/preprocessed/training-data/natural_language_inference/20,['Sentence - level representations'],Sentence - level representations are necessary for various NLP tasks .
dataset/preprocessed/training-data/topic_models/0,['Learning document embeddings'],Learning document embeddings along with their uncertainties
dataset/preprocessed/training-data/topic_models/0,['topic identification'],We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .
dataset/preprocessed/training-data/topic_models/0,['L EARNING word and document embeddings'],"L EARNING word and document embeddings have proven to be useful in wide range of information retrieval , speech and natural language processing applications -."
dataset/preprocessed/training-data/text_summarization/7,['Neural Abstractive Summarization'],Cutting - off Redundant Repeating Generations for Neural Abstractive Summarization
dataset/preprocessed/training-data/text_summarization/7,['abstractive summarization ( ABS )'],"The RNN - based encoder - decoder ( EncDec ) approach has recently been providing significant progress in various natural language generation ( NLG ) tasks , i.e. , machine translation ( MT ) and abstractive summarization ( ABS ) ."
dataset/preprocessed/training-data/text_summarization/3,['Abstractive Summarization'],Concept Pointer Network for Abstractive Summarization
dataset/preprocessed/training-data/text_summarization/3,['Abstractive summarization ( ABS )'],Abstractive summarization ( ABS ) has gained overwhelming success owing to a tremendous development of sequence - to - sequence ( seq2seq ) model and its variants .
dataset/preprocessed/training-data/text_summarization/10,['Multi - Task Summarization'],Soft Layer - Specific Multi - Task Summarization with Entailment and Question Generation
dataset/preprocessed/training-data/text_summarization/10,['abstractive summarization'],"We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation , where the former teaches the summarization model how to look for salient questioning - worthy details , and the latter teaches the model how to rewrite a summary which is a directed - logical subset of the input document ."
dataset/preprocessed/training-data/text_summarization/10,['abstractive text summarization'],"In this work , we improve abstractive text summarization via soft , high - level ( semantic ) layerspecific multi-task learning with two relevant auxiliary tasks ."
dataset/preprocessed/training-data/text_summarization/9,['Abstractive Sentence Summarization'],Abstractive Sentence Summarization with Attentive Recurrent Neural Networks
dataset/preprocessed/training-data/text_summarization/9,['text summarization'],Generating a condensed version of a passage while preserving its meaning is known as text summarization .
dataset/preprocessed/training-data/text_summarization/1,['Diverse Sequence Generation'],Mixture Content Selection for Diverse Sequence Generation
dataset/preprocessed/training-data/text_summarization/1,['Generating diverse sequences'],Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one - to - many relationships between source and the target sequences .
dataset/preprocessed/training-data/text_summarization/1,['Generating target sequences given a source sequence'],Generating target sequences given a source sequence has applications in a wide range of problems in NLP with different types of relationships between the source and target sequences .
dataset/preprocessed/training-data/text_summarization/1,['sequence generation'],"Encoder - decoder models are widely used for sequence generation , most notably in machine translation where neural models are now often almost as good as human translators in some language pairs ."
dataset/preprocessed/training-data/text_summarization/5,['Neural Summarization'],"Retrieve , Rerank and Rewrite : Soft Template Based Neural Summarization"
dataset/preprocessed/training-data/text_summarization/5,['abstractive sentence summarization'],"In this paper , we focus on an increasingly intriguing task , i.e. , abstractive sentence summarization , which generates a shorter version of a given sentence while attempting to preserve its original meaning ."
dataset/preprocessed/training-data/text_summarization/8,['Abstractive Summarization'],Bottom - Up Abstractive Summarization
dataset/preprocessed/training-data/text_summarization/8,['Text summarization'],Text summarization systems aim to generate natural language summaries that compress the information in a longer text .
dataset/preprocessed/training-data/text_summarization/8,['neural abstractive summarization'],Current state - of - the - art neural abstractive summarization models combine extractive and abstractive techniques by using pointergenerator style models which can copy words from the source document .
dataset/preprocessed/training-data/text_summarization/6,['Abstractive Text Summarization'],Deep Recurrent Generative Decoder for Abstractive Text Summarization
dataset/preprocessed/training-data/text_summarization/6,['Automatic summarization'],Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document .
dataset/preprocessed/training-data/text_summarization/11,['Abstractive Summarization'],Global Encoding for Abstractive Summarization
dataset/preprocessed/training-data/text_summarization/11,['neural abstractive summarization'],"Therefore , sequence - to - sequence learning can be applied to neural abstractive summarization , whose model consists of an encoder and a decoder ."
dataset/preprocessed/training-data/text_summarization/14,['Abstractive Sentence Summarization'],Ensure the Correctness of the Summary : Incorporate Entailment Knowledge into Abstractive Sentence Summarization
dataset/preprocessed/training-data/text_summarization/14,['sentence summarization'],"In this paper , we investigate the sentence summarization task that produces a summary from a source sentence ."
dataset/preprocessed/training-data/text_summarization/0,['Abstractive Text Summarization'],Abstractive Text Summarization by Incorporating Reader Comments
dataset/preprocessed/training-data/text_summarization/0,['neural abstractive summarization'],"In neural abstractive summarization field , conventional sequence - to - sequence based models often suffer from summarizing the wrong aspect of the document with respect to the main aspect ."
dataset/preprocessed/training-data/text_summarization/0,['reader - aware abstractive summary generation'],"To tackle this problem , we propose the task of reader - aware abstractive summary generation , which utilizes the reader comments to help the model produce better summary about the main aspect ."
dataset/preprocessed/training-data/text_summarization/0,['abstractive summarization'],"Unlike traditional abstractive summarization task , reader - aware summarization confronts two main challenges :"
dataset/preprocessed/training-data/text_summarization/12,['Abstractive Sentence Summarization'],Selective Encoding for Abstractive Sentence Summarization
dataset/preprocessed/training-data/text_summarization/12,['sentence summarization'],"The second level representation is tailored for sentence summarization task , which leads to better performance ."
dataset/preprocessed/training-data/text_summarization/2,['Abstractive Summarization'],Structure - Infused Copy Mechanisms for Abstractive Summarization
dataset/preprocessed/training-data/text_summarization/2,['summarization'],Seq2seq learning has produced promising results on summarization .
dataset/preprocessed/training-data/text_summarization/4,['Abstractive Summarization'],Entity Commonsense Representation for Neural Abstractive Summarization
dataset/preprocessed/training-data/text_summarization/4,['Text summarization'],Text summarization is a task to generate a shorter and concise version of a text while preserving the meaning of the original text .
dataset/preprocessed/training-data/text_summarization/13,['Document Summarization'],Coarse-to-Fine Attention Models for Document Summarization
dataset/preprocessed/training-data/smile_recognition/0,['Smile Recognition'],Deep Learning For Smile Recognition
dataset/preprocessed/training-data/smile_recognition/0,['facial expression recognition'],"Inspired by recent successes of deep learning in computer vision , we propose a novel application of deep convolutional neural networks to facial expression recognition , in particular smile recognition ."
