sample_dir_path,text,index,label,text-len
dataset/preprocessed/training-data/passage_re-ranking/1,PASSAGE RE - RANKING WITH BERT,2,1,6
dataset/preprocessed/training-data/passage_re-ranking/1,"Recently , neural models pretrained on a language modeling task , such as ELMo ( Peters et al. , 2017 ) , OpenAI GPT ( Radford et al. , 2018 ) , and BERT ( Devlin et al. , 2018 ) , have achieved impressive results on various natural language processing tasks such as question - answering and natural language inference .",4,0,62
dataset/preprocessed/training-data/passage_re-ranking/1,"In this paper , we describe a simple re-implementation of BERT for query - based passage re-ranking .",5,1,18
dataset/preprocessed/training-data/passage_re-ranking/1,"Our system is the state of the art on the TREC - CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task , outperforming the previous state of the art by 27 % ( relative ) in MRR@ 10 .",6,0,46
dataset/preprocessed/training-data/passage_re-ranking/1,The code to reproduce our results is available at https://github.com/nyu-dl/,7,0,10
dataset/preprocessed/training-data/passage_re-ranking/1,"We have seen rapid progress in machine reading compression in recent years with the introduction of large - scale datasets , such as SQuAD , MS MARCO , SearchQA , TriviaQA , and QUASAR - T , and the broad adoption of neural models , such as BiDAF , DrQA , DocumentQA , and QAnet .",10,0,56
dataset/preprocessed/training-data/passage_re-ranking/1,"The information retrieval ( IR ) community has also experienced a flourishing development of neural ranking models , such as DRMM , KNRM , Co - PACRR , and DUET .",11,0,31
dataset/preprocessed/training-data/passage_re-ranking/1,"However , until recently , there were only a few large datasets for passage ranking , with the notable exception of the TREC - CAR .",12,0,26
dataset/preprocessed/training-data/passage_re-ranking/1,"This , at least in part , prevented the neural ranking models from being successful when compared to more classical IR techniques .",13,0,23
dataset/preprocessed/training-data/passage_re-ranking/1,We argue that the same two ingredients that made possible much progress on the reading comprehension task are now available for passage ranking task .,14,0,25
dataset/preprocessed/training-data/passage_re-ranking/1,"Namely , the MS MARCO passage ranking dataset , which contains one million queries from real users and their respective relevant passages annotated by humans , and BERT , a powerful general purpose natural language processing model .",15,0,38
dataset/preprocessed/training-data/passage_re-ranking/1,"In this paper , we describe in detail how we have re-purposed BERT as a passage re-ranker and achieved state - of - the - art results on the MS MARCO passage re-ranking task .",16,0,35
dataset/preprocessed/training-data/passage_re-ranking/1,PASSAGE RE - RANKING WITH BERT,17,1,6
dataset/preprocessed/training-data/passage_re-ranking/1,Task A simple question - answering pipeline consists of three main stages .,18,0,13
dataset/preprocessed/training-data/passage_re-ranking/1,"First , a large number ( for example , a thousand ) of possibly relevant documents to a given question are retrieved from a corpus by a standard mechanism , such as BM25 .",19,0,34
dataset/preprocessed/training-data/passage_re-ranking/1,"In the second stage , passage re-ranking , each of these documents is scored and re-ranked by a more computationally - intensive method .",20,0,24
dataset/preprocessed/training-data/passage_re-ranking/1,"Finally , the top tenor fifty of these documents will be the source for the candidate answers by an answer generation module .",21,0,23
dataset/preprocessed/training-data/passage_re-ranking/1,"In this paper , we describe how we implemented the second stage of this pipeline , passage re-ranking .",22,0,19
dataset/preprocessed/training-data/passage_re-ranking/1,The job of the re-ranker is to estimate a score s i of how relevant a candidate passage d i is to a query q .,24,0,26
dataset/preprocessed/training-data/passage_re-ranking/1,We use BERT as our re-ranker .,25,0,7
dataset/preprocessed/training-data/passage_re-ranking/1,"Using the same notation used by , we feed the query as sentence A and the passage text as sentence B .",26,0,22
dataset/preprocessed/training-data/passage_re-ranking/1,We truncate the query to have at most 64 tokens .,27,0,11
dataset/preprocessed/training-data/passage_re-ranking/1,"We also truncate the passage text such that the concatenation of query , passage , and separator tokens have the maximum length of 512 tokens .",28,0,26
dataset/preprocessed/training-data/passage_re-ranking/1,"We use a BERT LARGE model as a binary classification model , that is , we use the [ CLS ] vector as input to a single layer neural network to obtain the probability of the passage being relevant .",29,0,40
dataset/preprocessed/training-data/passage_re-ranking/1,We compute this probability for each passage independently and obtain the final list of passages by ranking them with respect to these probabilities .,30,0,24
dataset/preprocessed/training-data/passage_re-ranking/1,We start training from a pre-trained BERT model and fine - tune it to our re-ranking task using the cross - entropy loss :,31,0,24
dataset/preprocessed/training-data/passage_re-ranking/1,"where J pos is the set of indexes of the relevant passages and J neg is the set of indexes of non-relevant passages in top - 1,000 documents retrieved with BM25 .",32,0,32
dataset/preprocessed/training-data/passage_re-ranking/1,"We train and evaluate our models on two passage - ranking datasets , MS MARCO and TREC - CAR .",34,0,20
dataset/preprocessed/training-data/passage_re-ranking/1,"The training set contains approximately 400M tuples of a query , relevant and non-relevant passages .",36,0,16
dataset/preprocessed/training-data/passage_re-ranking/1,"The development set contains approximately 6,900 queries , each paired with the top 1,000 passages retrieved with BM25 from the MS MARCO corpus .",37,0,24
dataset/preprocessed/training-data/passage_re-ranking/1,"On average , each query has one relevant passage .",38,0,10
dataset/preprocessed/training-data/passage_re-ranking/1,"However , some have no relevant passage because the corpus was initially constructed by retrieving the top - 10 passages from the Bing search engine and then annotated .",39,0,29
dataset/preprocessed/training-data/passage_re-ranking/1,"Hence , some of the relevant passages might not be retrieved by BM25 .",40,0,14
dataset/preprocessed/training-data/passage_re-ranking/1,"An evaluation set with approximately 6,800 queries and their top 1,000 retrieved passages without relevance annotations is also provided .",41,0,20
dataset/preprocessed/training-data/passage_re-ranking/1,"We fine - tune the model using TPUs 1 with a batch size of 32 ( 32 sequences * 512 tokens = 16,384 tokens / batch ) for 400 k iterations , which takes approximately 70 hours .",43,0,38
dataset/preprocessed/training-data/passage_re-ranking/1,This corresponds to training on 12.8M ( 400 k * 32 ) query - passage pairs or less than 2 % of the full training set .,44,0,27
dataset/preprocessed/training-data/passage_re-ranking/1,"We could not see any improvement in the dev set when training for another 10 days , which equivalent to seeing 50M pairs in total .",45,0,26
dataset/preprocessed/training-data/passage_re-ranking/1,"We use ADAM ( Kingma & Ba , 2014 ) with the initial learning rate set to 3 10 ?6 , ? 1 = 0.9 , ? 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10,000 steps , and linear decay of the learning rate .",46,0,54
dataset/preprocessed/training-data/passage_re-ranking/1,We use a dropout probability of 0.1 on all layers .,47,0,11
dataset/preprocessed/training-data/passage_re-ranking/1,TREC - CAR,48,0,3
dataset/preprocessed/training-data/passage_re-ranking/1,"Introduced by , in this dataset , the input query is the concatenation of a Wikipedia article title with the title of one of its section .",49,0,27
dataset/preprocessed/training-data/passage_re-ranking/0,Document Expansion by Query Prediction,2,1,5
dataset/preprocessed/training-data/passage_re-ranking/0,One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms thatare related or representative of the documents ' content .,4,0,27
dataset/preprocessed/training-data/passage_re-ranking/0,"From the perspective of a question answering system , this might comprise questions the document can potentially answer .",5,0,19
dataset/preprocessed/training-data/passage_re-ranking/0,"Following this observation , we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence - to - sequence model , trained using datasets consisting of pairs of query and relevant documents .",6,0,49
dataset/preprocessed/training-data/passage_re-ranking/0,"By combining our method with a highly - effective re-ranking component , we achieve the state of the art in two retrieval tasks .",7,0,24
dataset/preprocessed/training-data/passage_re-ranking/0,"In a latencycritical regime , retrieval results alone ( without re-ranking ) approach the effectiveness of more computationally expensive neural re-rankers but are much faster .",8,0,26
dataset/preprocessed/training-data/passage_re-ranking/0,Code to reproduce experiments and trained models can be found at https://github.,9,0,12
dataset/preprocessed/training-data/passage_re-ranking/0,"The "" vocabulary mismatch "" problem , where users use query terms that differ from those used in relevant documents , is one of the central challenges in information retrieval .",12,0,31
dataset/preprocessed/training-data/passage_re-ranking/0,"Prior to the advent of neural retrieval models , this problem has most often been tackled using query expansion techniques , where an initial round of retrieval can provide useful terms to augment the original query .",13,0,37
dataset/preprocessed/training-data/passage_re-ranking/0,"Continuous vector space representations and neural networks , however , no longer depend on discrete onehot representations , and thus offer an exciting new approach to tackling this challenge .",14,0,30
dataset/preprocessed/training-data/passage_re-ranking/0,"Despite the potential of neural models to match documents at the semantic level for improved ranking , most scalable search engines use exact Researchers are finding that cinnamon reduces blood sugar levels naturally when taken daily ...",15,0,37
dataset/preprocessed/training-data/passage_re-ranking/0,does cinnamon lower blood sugar ?,16,0,6
dataset/preprocessed/training-data/passage_re-ranking/0,does cinnamon lower blood sugar ?,17,0,6
dataset/preprocessed/training-data/passage_re-ranking/0,Researchers are finding that cinnamon reduces blood sugar levels naturally when taken daily ...,18,0,14
dataset/preprocessed/training-data/passage_re-ranking/0,Input : Document,19,0,3
dataset/preprocessed/training-data/passage_re-ranking/0,"Output : Predicted Query : Given a document , our Doc2query model predicts a query , which is appended to the document .",20,0,23
dataset/preprocessed/training-data/passage_re-ranking/0,"Expansion is applied to all documents in the corpus , which are then indexed and searched as before .",21,0,19
dataset/preprocessed/training-data/passage_re-ranking/0,term match between queries and documents to perform initial retrieval .,22,0,11
dataset/preprocessed/training-data/passage_re-ranking/0,Query expansion is about enriching the query representation while holding the document representation static .,23,0,15
dataset/preprocessed/training-data/passage_re-ranking/0,"In this paper , we explore an alternative approach based on enriching the document representation ( prior to indexing ) .",24,0,21
dataset/preprocessed/training-data/passage_re-ranking/0,"Focusing on question answering , we train a sequence - to - sequence model , that given a document , generates possible questions that the document might answer .",25,0,29
dataset/preprocessed/training-data/passage_re-ranking/0,An overview of the proposed method is shown in .,26,0,10
dataset/preprocessed/training-data/passage_re-ranking/0,We view this work as having several contributions :,27,0,9
dataset/preprocessed/training-data/passage_re-ranking/0,This is the first successful application of document expansion using neural networks that we are aware of .,28,0,18
dataset/preprocessed/training-data/passage_re-ranking/0,"On the recent MS MARCO dataset , our approach is competitive with the best results on the official leaderboard , and we report the best - known results on TREC CAR .",29,0,32
dataset/preprocessed/training-data/passage_re-ranking/0,We further show that document expansion is more effective than query expansion on these two datasets .,30,0,17
dataset/preprocessed/training-data/passage_re-ranking/0,"We accomplish this with relatively simple models using existing open - source toolkits , which allows easy replication of our results .",31,0,22
dataset/preprocessed/training-data/passage_re-ranking/0,"Document expansion also presents another major advantage , since the enrichment is performed prior to indexing :",32,0,17
dataset/preprocessed/training-data/passage_re-ranking/0,"Although retrieved output can be further re-ranked using a neural model to greatly enhance effectiveness , the output can also be returned as - is .",33,0,26
dataset/preprocessed/training-data/passage_re-ranking/0,"These results already yield a noticeable improvement in effectiveness over a "" bag of words "" baseline without the need to apply expensive and slow neural network inference at retrieval time .",34,0,32
dataset/preprocessed/training-data/passage_re-ranking/0,"Prior to the advent of continuous vector space representations and neural ranking models , information retrieval techniques were mostly limited to keyword matching ( i.e. , "" one - hot "" representations ) .",36,0,34
dataset/preprocessed/training-data/passage_re-ranking/0,Alternatives such as latent semantic indexing ) and its various successors never really gained significant traction .,37,0,17
dataset/preprocessed/training-data/passage_re-ranking/0,"Approaches to tackling the vocabulary mismatch problem within these constraints include relevance feedback , query expansion , and modeling term relationships using statistical translation .",38,0,25
dataset/preprocessed/training-data/passage_re-ranking/0,These techniques share in their focus on enhancing query representations to better match documents .,39,0,15
dataset/preprocessed/training-data/passage_re-ranking/0,"In this work , we adopt the alternative approach of enriching document representations , which works particularly well for speech ( Singhal and Pereira , 1999 ) and multi-lingual retrieval , where terms are noisy .",40,0,36
dataset/preprocessed/training-data/passage_re-ranking/0,Document expansion techniques have been less popular with IR researchers because they are less amenable to rapid experimentation .,41,0,19
dataset/preprocessed/training-data/passage_re-ranking/0,"The corpus needs to be re-indexed every time the expansion technique changes ( typically , a costly process ) ; in contrast , manipulations to query representations can happen at retrieval time ( and hence are much faster ) .",42,0,40
dataset/preprocessed/training-data/passage_re-ranking/0,"The success of document expansion has also been mixed ; for example , explore both query expansion and document expansion in the same framework and conclude that the former is consistently more effective .",43,0,34
dataset/preprocessed/training-data/passage_re-ranking/0,A new generation of neural ranking models offer solutions to the vocabulary mismatch problem based on continuous word representations and the ability to learn highly non-linear models of relevance ; see recent overviews by and .,44,0,36
dataset/preprocessed/training-data/passage_re-ranking/0,"However , due to the size of most corpora and the impractical - ity of applying inference over every document in response to a query , nearly all implementations today deploy neural networks as re-rankers over initial candidate sets retrieved using standard inverted indexes and a term - based ranking model such as BM25 .",45,0,55
dataset/preprocessed/training-data/passage_re-ranking/0,"Our work fits into this broad approach , where we take advantage of neural networks to augment document representations prior to indexing ; term - based retrieval then happens exactly as before .",46,0,33
dataset/preprocessed/training-data/passage_re-ranking/0,"Of course , retrieved results can still be re-ranked by a stateof - the - art neural model , but the output of term - based ranking already appears to be quite good .",47,0,34
dataset/preprocessed/training-data/passage_re-ranking/0,"In other words , our document expansion approach can leverage neural networks without their high inference - time costs .",48,0,20
dataset/preprocessed/training-data/passage_re-ranking/0,Method : Doc2query,49,0,3
dataset/preprocessed/training-data/temporal_information_extraction/1,A Structured Learning Approach to Temporal Relation Extraction,2,1,8
dataset/preprocessed/training-data/temporal_information_extraction/1,Identifying temporal relations between events is an essential step towards natural language understanding .,4,1,14
dataset/preprocessed/training-data/temporal_information_extraction/1,"However , the temporal relation between two events in a story depends on , and is often dictated by , relations among other events .",5,0,25
dataset/preprocessed/training-data/temporal_information_extraction/1,"Consequently , effectively identifying temporal relations between events is a challenging problem even for human annotators .",6,0,17
dataset/preprocessed/training-data/temporal_information_extraction/1,This paper suggests that it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge .,7,0,30
dataset/preprocessed/training-data/temporal_information_extraction/1,"As a byproduct , this provides a new perspective on handling missing relations , a known issue that hurts existing methods .",8,0,22
dataset/preprocessed/training-data/temporal_information_extraction/1,"As we show , the proposed approach results in significant improvements on the two commonly used data sets for this problem .",9,0,22
dataset/preprocessed/training-data/temporal_information_extraction/1,"Understanding temporal information described in natural language text is a key component of natural language understanding and , following a series of TempEval ( TE ) workshops , it has drawn increased attention .",11,0,34
dataset/preprocessed/training-data/temporal_information_extraction/1,"Time - slot filling , storyline construction , clinical narratives processing , and temporal question answering are all explicit examples of temporal processing .",12,0,24
dataset/preprocessed/training-data/temporal_information_extraction/1,"The fundamental tasks in temporal processing , as identified in the TE workshops , are 1 ) time expression ( the so - called "" timex "" ) extraction and normalization and 2 ) temporal relation ( also known as TLINKs ) extraction .",13,1,44
dataset/preprocessed/training-data/temporal_information_extraction/1,"While the first task has now been well handled by the state - of - the - art systems ( Heidel - Time , SUTime , IllinoisTime , NavyTime , UWTime , etc. ) with end - to - end F 1 scores being around 80 % , the second task has long been a challenging one ; even the top systems only achieved F 1 scores of around 35 % in the TE workshops .",14,0,76
dataset/preprocessed/training-data/temporal_information_extraction/1,"The goal of the temporal relation task is to generate a directed temporal graph whose nodes represent temporal entities ( i.e. , events or timexes ) and edges represent the TLINKs between them .",15,0,34
dataset/preprocessed/training-data/temporal_information_extraction/1,"The task is challenging because it often requires global considerations - considering the entire graph , the TLINK annotation is quadratic in the number of nodes and thus very expensive , and an overwhelming fraction of the temporal relations are missing in human annotation .",16,0,45
dataset/preprocessed/training-data/temporal_information_extraction/1,"In this paper , we propose a structured learning approach to temporal relation extraction , where local models are updated based on feedback from global inferences .",17,0,27
dataset/preprocessed/training-data/temporal_information_extraction/1,"The structured approach also gives rise to a semisupervised method , making it possible to take advantage of the readily available unlabeled data .",18,0,24
dataset/preprocessed/training-data/temporal_information_extraction/1,"As a byproduct , this approach further provides a new , effective perspective on handling those missing relations .",19,0,19
dataset/preprocessed/training-data/temporal_information_extraction/1,"In the common formulations , temporal relations are categorized into three types : the E -E TLINKs ( those between a pair of events ) , the T - T TLINKs ( those between a pair of timexes ) , and the E - T TLINKs ( those between an event and a timex ) .",20,0,56
dataset/preprocessed/training-data/temporal_information_extraction/1,"While the proposed approach can be generally applied to all three types , this paper focuses on the majority type , i.e. , the E -E TLINKs .",21,0,28
dataset/preprocessed/training-data/temporal_information_extraction/1,"For example , consider the following snippet taken from the training set provided in the TE3 workshop .",22,0,18
dataset/preprocessed/training-data/temporal_information_extraction/1,We want to construct a temporal graph as in for the events in boldface in Ex1 .,23,0,17
dataset/preprocessed/training-data/temporal_information_extraction/1,"Ex1 . . . tons of earth cascaded down a hillside , ripping two houses from their foundations .",24,0,19
dataset/preprocessed/training-data/temporal_information_extraction/1,"No one was hurt , but firefighters ordered the evacuation of nearby homes and said they 'll monitor the shifting ground .. . .",25,0,24
dataset/preprocessed/training-data/temporal_information_extraction/1,"As discussed in existing work , the structure of a temporal graph is constrained by some rather simple rules :",26,0,20
dataset/preprocessed/training-data/temporal_information_extraction/1,B must be after A.,27,0,5
dataset/preprocessed/training-data/temporal_information_extraction/1,"For example , if A is before B and B is before C , then A must be before C.",30,0,20
dataset/preprocessed/training-data/temporal_information_extraction/1,"This particular structure of a temporal graph ( especially the transitivity structure ) makes its nodes highly interrelated , as can be seen from .",31,0,25
dataset/preprocessed/training-data/temporal_information_extraction/1,"It is thus very challenging to identify the TLINKs between them , even for human annotators :",32,0,17
dataset/preprocessed/training-data/temporal_information_extraction/1,The inter-annotator agreement on TLINKs is usually about 50 % - 60 % .,33,0,14
dataset/preprocessed/training-data/temporal_information_extraction/1,shows the actual human annotations provided by TE3 .,34,0,9
dataset/preprocessed/training-data/temporal_information_extraction/1,"Among all the ten possible pairs of nodes , only three TLINKs were annotated .",35,0,15
dataset/preprocessed/training-data/temporal_information_extraction/1,"Even if we only look at main events in consecutive sentences and at events in the same sentence , there are still quite a few missing TLINKs , e.g. , the one between hurt and cascaded and the one between monitor and ordered .",36,0,44
dataset/preprocessed/training-data/temporal_information_extraction/1,Early attempts by ; ; studied local methods - learning models that make pairwise decisions between each pair of events .,37,0,21
dataset/preprocessed/training-data/temporal_information_extraction/1,"State - of - the - art local methods , including ClearTK , UTTime ( , and NavyTime , use better designed rules or more features such as syntactic tree paths and achieve better results .",38,0,36
dataset/preprocessed/training-data/temporal_information_extraction/1,"However , the decisions made by these ( local ) models are often globally inconsistent ( i.e. , the symmetry and / or transitivity constraints are not satisfied for the entire temporal graph ) .",39,0,35
dataset/preprocessed/training-data/temporal_information_extraction/1,"Integer linear programming ( ILP ) methods were used in this domain to enforce global consistency by several authors including ; ; , which formulated TLINK extraction as an ILP and showed that it improves over local methods for densely connected graphs .",40,0,43
dataset/preprocessed/training-data/temporal_information_extraction/1,"Since these methods perform inference ( "" I "" ) on top of pre-trained local classifiers ( "" L "" ) , they are often referred to as L+I .",41,0,30
dataset/preprocessed/training-data/temporal_information_extraction/1,"In a state - of - the - art method , CAEVO , many hand - crafted rules and machine learned classifiers ( called sieves therein ) form a pipeline .",42,0,31
dataset/preprocessed/training-data/temporal_information_extraction/1,The global consistency is enforced by inferring all possible relations before passing the graph to the next sieve .,43,0,19
dataset/preprocessed/training-data/temporal_information_extraction/1,"This best - first architecture is conceptually similar to L+ I but the inference is greedy , similar to ; .",44,0,21
dataset/preprocessed/training-data/temporal_information_extraction/1,"Although L+I methods impose global constraints in the inference phase , this paper argues that global considerations are necessary in the learning phase as well ( i.e. , structured learning ) .",45,0,32
dataset/preprocessed/training-data/temporal_information_extraction/1,"In parallel to the work presented here , Leeuwenberg and Moens ( 2017 ) also proposed a structured learning approach to extracting the temporal relations .",46,0,26
dataset/preprocessed/training-data/temporal_information_extraction/1,"Their work focuses on a domain - specific dataset from Clinical TempEval , so their work does not need to address some of the difficulties of the general problem that our work addresses .",47,0,34
dataset/preprocessed/training-data/temporal_information_extraction/1,"More importantly , they compared structured learning to local baselines , while we find that the comparison between structured learning and L+ I is more interesting and important for understanding the effect of global considerations in the learning phase .",48,0,40
dataset/preprocessed/training-data/temporal_information_extraction/1,"In difference from existing methods , we also discuss how to effectively use unlabeled data and how to handle the overwhelming fraction of missing relations in a principled way .",49,0,30
dataset/preprocessed/training-data/temporal_information_extraction/0,CATENA : CAusal and TEmporal relation extraction from NAtural language texts,2,0,11
dataset/preprocessed/training-data/temporal_information_extraction/0,"We present CATENA , a sieve - based system to perform temporal and causal relation extraction and classification from English texts , exploiting the interaction between the temporal and the causal model .",4,1,33
dataset/preprocessed/training-data/temporal_information_extraction/0,"We evaluate the performance of each sieve , showing that the rule - based , the machinelearned and the reasoning components all contribute to achieving state - of - the - art performance on TempEval - 3 and TimeBank - Dense data .",5,0,43
dataset/preprocessed/training-data/temporal_information_extraction/0,"Although causal relations are much sparser than temporal ones , the architecture and the selected features are mostly suitable to serve both tasks .",6,0,24
dataset/preprocessed/training-data/temporal_information_extraction/0,"The effects of the interaction between the temporal and the causal components , although limited , yield promising results and confirm the tight connection between the temporal and the causal dimension of texts .",7,0,34
dataset/preprocessed/training-data/temporal_information_extraction/0,: System architecture of CATENA,9,0,5
dataset/preprocessed/training-data/temporal_information_extraction/0,"The problem of detecting causality between events is as challenging as recognizing their temporal order , but less analyzed from an NLP perspective .",10,0,24
dataset/preprocessed/training-data/temporal_information_extraction/0,"Besides , previous works have mostly focused on specific types of event pairs and causal expressions in text .",11,0,19
dataset/preprocessed/training-data/temporal_information_extraction/0,"Several works , relying on corpus of parallel temporal and causal relations developed with specific connectives in mind , have presented analyses on the interaction between temporal and causal relations .",12,0,31
dataset/preprocessed/training-data/temporal_information_extraction/0,Exploiting gold temporal labels as features for the causal relation classifier is shown to be beneficial .,13,0,17
dataset/preprocessed/training-data/temporal_information_extraction/0,"presented some annotation guidelines to capture explicit causality between event pairs , inspired by Time ML .",14,0,17
dataset/preprocessed/training-data/temporal_information_extraction/0,"The resulting corpus , Causal - TimeBank , is then used to build supervised classification models for extracting causal relations .",15,0,21
dataset/preprocessed/training-data/temporal_information_extraction/0,None of the above systems presents a hybrid approach in a sieve - based architecture to deal with this task .,16,0,21
dataset/preprocessed/training-data/temporal_information_extraction/0,CATENA is at present the first integrated system available performing temporal and causal relation extraction .,17,0,16
dataset/preprocessed/training-data/temporal_information_extraction/0,3 System architecture,18,0,3
dataset/preprocessed/training-data/temporal_information_extraction/0,"The CATENA system includes two main classification modules , one for temporal and the other for causal relations between events .",19,0,21
dataset/preprocessed/training-data/temporal_information_extraction/0,"As shown in , they both take as input a document annotated with the so - called temporal entities according to TimeML guidelines , including the document creation time ( DCT ) , events and time expressions ( timexes ) .",20,0,41
dataset/preprocessed/training-data/temporal_information_extraction/0,"The output is the same document with temporal links ( TLINKs ) set between pairs of temporal entities , each assigned to one of the TimeML temporal relation types , such as or SIMULTANEOUS , which denotes the temporal ordering .",21,0,41
dataset/preprocessed/training-data/temporal_information_extraction/0,The document is also annotated with causal relations ( CLINKs ) between event pairs .,22,0,15
dataset/preprocessed/training-data/temporal_information_extraction/0,"The modules for temporal and causal relation classification rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a rule - based component and / or a transitive reasoner are fed into a supervised classifier .",23,0,44
dataset/preprocessed/training-data/temporal_information_extraction/0,"Although some steps can be run in parallel , the two modules interact , based on the assumption that the notion of causality is tightly connected with the temporal dimension and that information from one module can be used to improve or check the consistency of the other .",24,0,49
dataset/preprocessed/training-data/temporal_information_extraction/0,"In particular , ( i ) TLINK labels for event - event ( E -E ) pairs , resulting from the rule - based sieve + temporal reasoner modules , are used as features for the CLINK classifier ; and ( ii ) CLINK labels ( i.e. CLINK and are used as a post-editing method for correcting the wrong labelled event pairs by the TLINK classifier .",25,0,67
dataset/preprocessed/training-data/temporal_information_extraction/0,"This step relies on a set of rules based on the temporal constraint of causality , i.e. ( i ) CLINK ( e 1 , e 2 ) ? BEFORE ( e 1 , e 2 ) and ( ii ) CLINK - R ( e 1 , e 2 ) ? AFTER ( e 1 , e 2 ) .",26,0,61
dataset/preprocessed/training-data/temporal_information_extraction/0,The modules for temporal and causal relation extraction are detailed in Section 4 and 5 respectively .,27,0,17
dataset/preprocessed/training-data/temporal_information_extraction/0,Temporal Relation Extraction System,28,0,4
dataset/preprocessed/training-data/temporal_information_extraction/0,"The module for the extraction of temporal relations contains two main components , one for ( i ) temporal relation identification , which is based on a set of rules , and the other for ( ii ) temporal relation type classification , which is a combination of rule - based and supervised classification modules , with a temporal reasoning component in between .",29,0,64
dataset/preprocessed/training-data/temporal_information_extraction/0,The three steps for temporal relation type classification are ordered based on their individual precisions .,30,0,16
dataset/preprocessed/training-data/temporal_information_extraction/0,"This mechanism allows the system to first label few links with high precision using rules , then to infer new links through the reasoner , and finally to increase recall through supervised classification , based on the output of the previous steps .",31,0,43
dataset/preprocessed/training-data/temporal_information_extraction/0,Temporal Relation Identification,32,0,3
dataset/preprocessed/training-data/temporal_information_extraction/0,"All pairs of temporal entities satisfying one of the following rules , inspired by the TempEval - 3 task description , are considered as having temporal links ( TLINK s ) : ( i ) two main events of consecutive sentences , ( ii ) two events in the same sentence , ( iii ) an event and a timex in the same sentence , ( iv ) an event and a document creation time and ( v ) pairs of all possible timexes ( including document creation time ) linked with each other .",33,0,95
dataset/preprocessed/training-data/temporal_information_extraction/0,"These pairs are then grouped together into four different groups : timex - timex ( T - T ) , event - DCT ( E - D ) , event - timex ( E - T ) and event - event ( E - E ) .",35,0,47
dataset/preprocessed/training-data/temporal_information_extraction/0,Temporal Relation Type Classification,36,0,4
dataset/preprocessed/training-data/temporal_information_extraction/0,"Our sieve - based architecture is inspired by CAEVO , although we significantly reduce the system complexity as follows :",37,0,20
dataset/preprocessed/training-data/temporal_information_extraction/0,"We merge all rule - based classifiers into one sieve component ( rule - based sieve ) , and all Support Vector Machine ( SVM ) classifiers in the machine - learned sieve .",38,0,34
dataset/preprocessed/training-data/temporal_information_extraction/0,"Instead of running transitive inference after each classifier , we run our temporal reasoner module on the output of the rule - based sieve , only once .",39,0,28
dataset/preprocessed/training-data/temporal_information_extraction/0,"Furthermore , we use the output of the rule - based sieve ( Section 4.2.1 ) as features for the machinelearned sieve ( Section 4.2.3 ) , specifically : ( i ) the timex - DCT link label proposed by the timex - timex rules are used as a feature in the event - timex SVM , and ( ii ) the event - DCT link label proposed by the event - DCT rules are used as a feature in the event - event SVM .",40,0,86
dataset/preprocessed/training-data/temporal_information_extraction/0,Temporal Rule - Based Sieve,41,0,5
dataset/preprocessed/training-data/temporal_information_extraction/0,"The temporal rule - based sieve relies on specific hand - crafted rules designed for each type of temporal entity pairs , and takes as input the entity pairs identified in the previous step .",42,0,35
dataset/preprocessed/training-data/temporal_information_extraction/0,Timex - timex Rules,43,0,4
dataset/preprocessed/training-data/temporal_information_extraction/0,"For timex - timex relations , we take into account temporal expressions of types DATE and TIME , and determine the relation types based on their normalized values .",44,0,29
dataset/preprocessed/training-data/temporal_information_extraction/0,"For example , "" 7 PM tonight "" ( 2015-12-12T19:00 ) IS INCLUDED in "" today "" ( 2015 - 12-12 ) .",45,0,23
dataset/preprocessed/training-data/temporal_information_extraction/0,Event - DCT,46,0,3
dataset/preprocessed/training-data/temporal_information_extraction/0,The rules for labelling E - D pairs are based on the tense and / or aspect of the event word .,48,0,22
dataset/preprocessed/training-data/temporal_information_extraction/0,"For example , for the event mention "" ( had ) fallen "" , which is in the past tense with perfective aspect , its relation with the DCT is labelled as BEFORE .",49,0,34
dataset/preprocessed/training-data/part-of-speech_tagging/7,Multilingual Part - of - Speech Tagging with Bidirectional Long Short - Term Memory Models and Auxiliary Loss,2,1,18
dataset/preprocessed/training-data/part-of-speech_tagging/7,"Bidirectional long short - term memory ( bi - LSTM ) networks have recently proven successful for various NLP sequence modeling tasks , but little is known about their reliance to input representations , target languages , data set size , and label noise .",4,0,45
dataset/preprocessed/training-data/part-of-speech_tagging/7,"We address these issues and evaluate bi - LSTMs with word , character , and unicode byte embeddings for POS tagging .",5,1,22
dataset/preprocessed/training-data/part-of-speech_tagging/7,We compare bi - LSTMs to traditional POS taggers across languages and data sizes .,6,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/7,"We also present a novel bi - LSTM model , which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words .",7,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/7,"The model obtains state - of - the - art performance across 22 languages , and works especially well for morphologically complex languages .",8,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/7,Our analysis suggests that bi - LSTMs are less sensitive to training data size and label corruptions ( at small noise levels ) than previously assumed .,9,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/7,"Recently , bidirectional long short - term memory networks ( bi - LSTM ) have been used for language modelling , POS tagging , transition - based dependency parsing , fine - grained sentiment analysis , syntactic chunking , and semantic role labeling .",11,0,44
dataset/preprocessed/training-data/part-of-speech_tagging/7,LSTMs are recurrent neural networks ( RNNs ) in which layers are designed to prevent vanishing gradients .,12,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/7,Bidirectional LSTMs make a backward and forward pass through the sequence before passing onto the next layer .,13,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/7,"For further details , see .",14,0,6
dataset/preprocessed/training-data/part-of-speech_tagging/7,We consider using bi - LSTMs for POS tagging .,15,0,10
dataset/preprocessed/training-data/part-of-speech_tagging/7,Previous work on using deep learning - based methods for POS tagging has focused either on a single language or a small set of languages .,16,0,26
dataset/preprocessed/training-data/part-of-speech_tagging/7,Instead we evaluate our models across 22 languages .,17,0,9
dataset/preprocessed/training-data/part-of-speech_tagging/7,"In addition , we compare performance with representations at different levels of granularity ( words , characters , and bytes ) .",18,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/7,"These levels of representation were previously introduced in different efforts , but a comparative evaluation was missing .",19,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/7,"Moreover , deep networks are often said to require large volumes of training data .",20,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/7,We investigate to what extent bi - LSTMs are more sensitive to the amount of training data and label noise than standard POS taggers .,21,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/7,"Finally , we introduce a novel model , a bi - LSTM trained with auxiliary loss .",22,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/7,The model jointly predicts the POS and the log frequency of the word .,23,0,14
dataset/preprocessed/training-data/part-of-speech_tagging/7,"The intuition behind this model is that the auxiliary loss , being predictive of word frequency , helps to differentiate the representations of rare and common words .",24,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/7,We indeed observe performance gains on rare and out - of - vocabulary words .,25,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/7,These performance gains transfer into general improvements for morphologically rich languages .,26,0,12
dataset/preprocessed/training-data/part-of-speech_tagging/7,"In this paper , we a ) evaluate the effectiveness of different representations in bi - LSTMs , b ) compare these models across a large set of languages and under varying conditions ( data size , label noise ) and c) propose a novel bi - LSTM model with auxiliary loss ( LOGFREQ ) .",28,0,56
dataset/preprocessed/training-data/part-of-speech_tagging/7,Tagging with bi - LSTMs,29,0,5
dataset/preprocessed/training-data/part-of-speech_tagging/7,Recurrent neural networks ( RNNs ) allow the computation of fixed - size vector representations for word sequences of arbitrary length .,30,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/7,"An RNN is a function that reads inn vectors x 1 , ... , x n and produces an output vector h n , that depends on the entire sequence x 1 , ... , x n .",31,0,38
dataset/preprocessed/training-data/part-of-speech_tagging/7,"The vector h n is then fed as an input to some classifier , or higher - level RNNs in stacked / hierarchical models .",32,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/7,The entire network is trained jointly such that the hidden representation captures the important information from the sequence for the prediction task .,33,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/7,"A bidirectional recurrent neural network ( bi - RNN ) is an extension of an RNN that reads the input sequence twice , from left to right and right to left , and the encodings are concatenated .",34,0,38
dataset/preprocessed/training-data/part-of-speech_tagging/7,"The literature uses the term bi - RNN to refer to two related architectures , which we refer to here as "" context bi - RNN "" and "" sequence bi - RNN "" .",35,0,35
dataset/preprocessed/training-data/part-of-speech_tagging/7,"In a sequence bi - RNN ( b i - RNN seq ) , the input is a sequence of vectors x 1:n and the output is a concatenation ( ) of a forward ( f ) and reverse ( r ) RNN each reading the sequence in a different directions : v = bi - RNN seq ( x 1:n ) = RNN f ( x 1:n ) RNN r ( x n:1 )",36,0,75
dataset/preprocessed/training-data/part-of-speech_tagging/7,"In a context bi - RNN ( bi - RNN ctx ) , we get an additional input i indicating a sequence position , and the resulting vectors vi result from concatenating the RNN encodings up to i:",37,0,38
dataset/preprocessed/training-data/part-of-speech_tagging/7,"Thus , the state vector vi in this bi - RNN encodes information at position i and its entire sequential context .",38,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/7,Another view of the context bi - RNN is of taking a sequence x 1:n and returning the corresponding sequence of state vectors v 1 :n .,39,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/7,LSTMs ) are a variant of RNNs that replace the cells of RNNs with LSTM cells that were designed to prevent vanishing gradients .,40,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/7,Bidirectional LSTMs are the bi - RNN counterpart based on LSTMs .,41,0,12
dataset/preprocessed/training-data/part-of-speech_tagging/7,Our basic bi - LSTM tagging model is a context bi - LSTM taking as input word embeddings w .,42,0,20
dataset/preprocessed/training-data/part-of-speech_tagging/7,We incorporate subtoken information using an hierarchical bi - LSTM architecture .,43,0,12
dataset/preprocessed/training-data/part-of-speech_tagging/7,We compute subtokenlevel ( either characters cor unicode byte b) embeddings of words using a sequence bi - LSTM at the lower level .,44,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/7,This representation is then concatenated with the ( learned ) word embeddings vector w which forms the input to the context bi - LSTM at the next layer .,45,0,29
dataset/preprocessed/training-data/part-of-speech_tagging/7,"This model , illustrated in ( lower part in left figure ) , is inspired by .",46,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/7,"We also test models in which we only keep sub-token information , e.g. , either both byte and character embeddings , right ) or a single ( sub - ) token representation alone .",47,0,34
dataset/preprocessed/training-data/part-of-speech_tagging/7,"In our novel model , cf. left , we train the bi - LSTM tagger to predict both the tags of the sequence , as well as a label that represents the log frequency of the token as estimated from the training data .",48,0,44
dataset/preprocessed/training-data/part-of-speech_tagging/7,"Our combined cross - entropy loss is now : L ( ? t , y t ) + L ( ? a , ya ) , where t stands for a POS tag and a is the log frequency label , i.e. , a = int ( log ( f req train ( w ) ) .",49,0,57
dataset/preprocessed/training-data/part-of-speech_tagging/3,End - to - end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,2,1,10
dataset/preprocessed/training-data/part-of-speech_tagging/3,State - of - the - art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing .,4,1,27
dataset/preprocessed/training-data/part-of-speech_tagging/3,"In this paper , we introduce a novel neutral network architecture that benefits from both word - and character - level representations automatically , by using combination of bidirectional LSTM , CNN and CRF .",5,0,35
dataset/preprocessed/training-data/part-of-speech_tagging/3,"Our system is truly end - to - end , requiring no feature engineering or data preprocessing , thus making it applicable to a wide range of sequence labeling tasks .",6,0,31
dataset/preprocessed/training-data/part-of-speech_tagging/3,We evaluate our system on two data sets for two sequence labeling tasks - Penn Treebank WSJ corpus for part - of - speech ( POS ) tagging and CoNLL 2003 corpus for named entity recognition ( NER ) .,7,0,40
dataset/preprocessed/training-data/part-of-speech_tagging/3,We obtain state - of - the - art performance on both datasets - 97.55 % accuracy for POS tagging and 91.21 % F1 for NER .,8,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/3,"Linguistic sequence labeling , such as part - ofspeech ( POS ) tagging and named entity recognition ( NER ) , is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community .",10,1,45
dataset/preprocessed/training-data/part-of-speech_tagging/3,"Natural language processing ( NLP ) systems , like syntactic parsing and entity coreference resolution , are becoming more sophisticated , in part because of utilizing output information of POS tagging or NER systems .",11,0,35
dataset/preprocessed/training-data/part-of-speech_tagging/3,"Most traditional high performance sequence labeling models are linear statistical models , including Hidden Markov Models ( HMM ) and Conditional Random Fields ( CRF ) , which rely heavily on hand - crafted features and taskspecific resources .",12,0,39
dataset/preprocessed/training-data/part-of-speech_tagging/3,"For example , English POS taggers benefit from carefully designed word spelling features ; orthographic features and external resources such as gazetteers are widely used in NER .",13,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/3,"However , such task - specific knowledge is costly to develop , making sequence labeling models difficult to adapt to new tasks or new domains .",14,0,26
dataset/preprocessed/training-data/part-of-speech_tagging/3,"In the past few years , non-linear neural networks with as input distributed word representations , also known as word embeddings , have been broadly applied to NLP problems with great success .",15,0,33
dataset/preprocessed/training-data/part-of-speech_tagging/3,proposed a simple but effective feed - forward neutral network that independently classifies labels for each word by using contexts within a window with fixed size .,16,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/3,"Recently , recurrent neural networks ( RNN ) , together with its variants such as long - short term memory ( LSTM ) and gated recurrent unit ( GRU ) , have shown great success in modeling sequential data .",17,0,40
dataset/preprocessed/training-data/part-of-speech_tagging/3,"Several RNN - based neural network models have been proposed to solve sequence labeling tasks like speech recognition , POS tagging and , achieving competitive performance against traditional models .",18,0,30
dataset/preprocessed/training-data/part-of-speech_tagging/3,"However , even systems that have utilized distributed representations as inputs have used these to augment , rather than replace , hand - crafted features ( e.g. word spelling and capitalization patterns ) .",19,0,34
dataset/preprocessed/training-data/part-of-speech_tagging/3,Their performance drops rapidly when the models solely depend on neural embeddings .,20,0,13
dataset/preprocessed/training-data/part-of-speech_tagging/3,"In this paper , we propose a neural network architecture for sequence labeling .",21,0,14
dataset/preprocessed/training-data/part-of-speech_tagging/3,"It is a truly endto - end model requiring no task - specific resources , feature engineering , or data pre-processing beyond pre-trained word embeddings on unlabeled corpora .",22,0,29
dataset/preprocessed/training-data/part-of-speech_tagging/3,"Thus , our model can be easily applied to a wide range of sequence labeling tasks on different languages and domains .",23,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/3,We first use convolutional neural networks ( CNNs ) to encode character - level information of a word into its character - level representation .,24,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/3,Then we combine character - and word - level representations and feed them into bi-directional LSTM ( BLSTM ) to model context information of each word .,25,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/3,"On top of BLSTM , we use a sequential CRF to jointly decode labels for the whole sentence .",26,0,19
dataset/preprocessed/training-data/part-of-speech_tagging/3,"We evaluate our model on two linguistic sequence labeling tasks - POS tagging on Penn Treebank WSJ , and NER on English data from the CoNLL 2003 shared task .",27,0,30
dataset/preprocessed/training-data/part-of-speech_tagging/3,"Our end - to - end model outperforms previous stateof - the - art systems , obtaining 97. 55 % accuracy for POS tagging and 91.21 % F1 for NER .",28,0,31
dataset/preprocessed/training-data/part-of-speech_tagging/3,The contributions of this work are ( i ) proposing a novel neural network architecture for linguistic sequence labeling .,29,0,20
dataset/preprocessed/training-data/part-of-speech_tagging/3,( ii ) giving empirical evaluations of this model on benchmark data sets for two classic NLP tasks .,30,0,19
dataset/preprocessed/training-data/part-of-speech_tagging/3,( iii ) achieving state - of - the - art performance with this truly end - to - end system .,31,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/3,Neural Network Architecture,32,0,3
dataset/preprocessed/training-data/part-of-speech_tagging/3,"In this section , we describe the components ( layers ) of our neural network architecture .",33,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/3,We introduce the neural layers in our neural network oneby - one from bottom to top .,34,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/3,CNN for Character - level Representation,35,0,6
dataset/preprocessed/training-data/part-of-speech_tagging/3,Previous studies have shown that CNN is an effective approach to extract morphological information ( like the prefix or suffix of a word ) from characters of words and encode it into neural representations .,36,0,35
dataset/preprocessed/training-data/part-of-speech_tagging/3,shows the CNN we use to extract character - level representation of a given word .,37,0,16
dataset/preprocessed/training-data/part-of-speech_tagging/3,"The CNN is similar to the one in , except that we use only character embeddings as the inputs to CNN , without character type features .",38,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/3,A dropout layer is applied before character embeddings are input to CNN .,39,0,13
dataset/preprocessed/training-data/part-of-speech_tagging/3,Dashed arrows indicate a dropout layer applied before character embeddings are input to CNN .,40,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/3,Recurrent neural networks ( RNNs ) are a powerful family of connectionist models that capture time dynamics via cycles in the graph .,45,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/3,"Though , in theory , RNNs are capable to capturing long - distance dependencies , in practice , they fail due to the gradient vanishing / exploding problems .",46,0,29
dataset/preprocessed/training-data/part-of-speech_tagging/3,"LSTMs ( Hochreiter and Schmidhuber , 1997 ) are variants of RNNs designed to cope with these gradient vanishing problems .",47,0,21
dataset/preprocessed/training-data/part-of-speech_tagging/3,"Basically , a LSTM unit is composed of three multiplicative gates which control the proportions of information to forget and to pass onto the next time step .",48,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/3,"Formally , the formulas to update an LSTM unit at time t are :",49,0,14
dataset/preprocessed/training-data/part-of-speech_tagging/1,Learning Better Internal Structure of Words for Sequence Labeling,2,1,9
dataset/preprocessed/training-data/part-of-speech_tagging/1,Character - based neural models have recently proven very useful for many NLP tasks .,4,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/1,"However , there is a gap of sophistication between methods for learning representations of sentences and words .",5,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/1,"While most character models for learning representations of sentences are deep and complex , models for learning representations of words are shallow and simple .",6,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/1,"Also , in spite of considerable research on learning character embeddings , it is still not clear which kind of architecture is the best for capturing character - to - word representations .",7,0,33
dataset/preprocessed/training-data/part-of-speech_tagging/1,"To address these questions , we first investigate the gaps between methods for learning word and sentence representations .",8,0,19
dataset/preprocessed/training-data/part-of-speech_tagging/1,"We conduct detailed experiments and comparisons of different state - of - the - art convolutional models , and also investigate the advantages and dis advantages of their constituents .",9,0,30
dataset/preprocessed/training-data/part-of-speech_tagging/1,"Furthermore , we propose IntNet , a funnel - shaped wide convolutional neural architecture with no down - sampling for learning representations of the internal structure of words by composing their characters from limited , supervised training corpora .",10,0,39
dataset/preprocessed/training-data/part-of-speech_tagging/1,"We evaluate our proposed model on six sequence labeling datasets , including named entity recognition , part - of - speech tagging , and syntactic chunking .",11,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/1,Our in - depth analysis shows that IntNet significantly outperforms other character embedding models and obtains new state - of - the - art performance without relying on any external knowledge or resources .,12,0,34
dataset/preprocessed/training-data/part-of-speech_tagging/1,"Sequence labeling is the task of assigning a label or class to each element of a sequence of data , and is one of the first stages in many natural language processing ( NLP ) tasks .",14,0,37
dataset/preprocessed/training-data/part-of-speech_tagging/1,"For example , named entity recognition ( NER ) aims to classify words in a sentence into several predefined categories of interest such as person , organization , location , etc .",15,0,32
dataset/preprocessed/training-data/part-of-speech_tagging/1,Part - of - speech ( POS ) tagging assigns apart of speech to each word in an input sentence .,16,0,21
dataset/preprocessed/training-data/part-of-speech_tagging/1,"Syntactic chunking divides text into syntactically related , non-overlapping groups of words .",17,0,13
dataset/preprocessed/training-data/part-of-speech_tagging/1,Sequence labeling is a challenging problem because human annotation is very expensive and typically only a small amount of tagging data is available .,18,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/1,Most traditional sequence labeling systems have been dominated by linear statistical models which heavily rely on feature engineering .,19,0,19
dataset/preprocessed/training-data/part-of-speech_tagging/1,"As a result , carefully constructed hand - crafted features and domain - specific knowledge are widely used for solving these tasks .",20,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/1,"Unfortunately , it is costly to develop domain specific knowledge and hand - crafted features .",21,0,16
dataset/preprocessed/training-data/part-of-speech_tagging/1,"Recently , neural networks using character - level information have been used successfully for minimizing the need of feature engineering .",22,0,21
dataset/preprocessed/training-data/part-of-speech_tagging/1,"There are basically two threads of character - based modeling , one focuses on learning representations of sentences for semantics and syntax ; the other focuses on learning representations of words for the purpose of eliminating handcrafted features for word shape information .",23,0,43
dataset/preprocessed/training-data/part-of-speech_tagging/1,Two main state - of - the - art approaches of learning character representations for sequence labeling emerged from the latter thread .,24,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/1,One is based on RNNs and uses bidirectional LSTMs or GRUs to learn forward and backward character information .,25,0,19
dataset/preprocessed/training-data/part-of-speech_tagging/1,The other approach is based on CNNs with a fixed - size window around each word to create character - level representations .,26,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/1,"However , there is a gap in the sophistication between character - based methods for learning representations of sentences compared to that of words .",27,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/1,"We found that most of the stateof - the - art character - based CNN models for words use a convolution followed by max pooling as a shallow feature extractor , which is very different from the CNN models with deep and complex architecture for sentences .",28,0,47
dataset/preprocessed/training-data/part-of-speech_tagging/1,"In spite of considerable research on learning character embeddings , it is still not clear which kind of architecture is the best for capturing character - to - word representations .",29,0,31
dataset/preprocessed/training-data/part-of-speech_tagging/1,"Therefore , a number of questions remain open :",30,0,9
dataset/preprocessed/training-data/part-of-speech_tagging/1,Why is there a gap between methods for learning representations of sentences and words ?,31,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/1,How can this gap be bridged ?,32,0,7
dataset/preprocessed/training-data/part-of-speech_tagging/1,How do state - of - the - art character embedding models differ in term of performance ?,33,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/1,What kind of neural network architecture is better for learning the internal structure of a word ?,34,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/1,Deep or shallow ?,35,0,4
dataset/preprocessed/training-data/part-of-speech_tagging/1,Narrow or wide ?,36,0,4
dataset/preprocessed/training-data/part-of-speech_tagging/1,"To answer these questions , we first investigate the gap between learning word representations and sentence representations for convolutional architectures .",37,0,21
dataset/preprocessed/training-data/part-of-speech_tagging/1,The most straightforward idea is to add more convolutional layers which follows the approaches from learning representations of sentences .,38,0,20
dataset/preprocessed/training-data/part-of-speech_tagging/1,"Interestingly , we observe the accuracy does not increase much and found that accuracy drops when we increased the depth of the network .",39,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/1,"This observation shows that learning character representations for the internal structure of words is very different than sentences , and also might explain one of the reasons there has been a gap in character - based CNN models for representing words and sentences .",40,0,44
dataset/preprocessed/training-data/part-of-speech_tagging/1,"In this paper , we present detailed experiments and comparisons across different state - of - the - art convolutional models from natural language processing and computer vision .",41,0,29
dataset/preprocessed/training-data/part-of-speech_tagging/1,We also investigate the advantages and dis advantages of some of their constituents on different convolutional architectures .,42,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/1,"Furthermore , we propose IntNet , a funnel - shaped wide convolutional neural network for learning the internal structure of words by composing their characters .",43,0,26
dataset/preprocessed/training-data/part-of-speech_tagging/1,"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture with no down - sampling for learning character - to - word representations from limited supervised training corpora .",44,0,38
dataset/preprocessed/training-data/part-of-speech_tagging/1,"Lastly , we combine our IntNet model with LSTM - CRF , which captures both word shape and context information , and jointly decode tags for sequence labeling .",45,0,29
dataset/preprocessed/training-data/part-of-speech_tagging/1,The main contributions of this paper are the following :,46,0,10
dataset/preprocessed/training-data/part-of-speech_tagging/1,We conduct detailed studies on investigating the gap between learning word representations and sentence representations .,47,0,16
dataset/preprocessed/training-data/part-of-speech_tagging/1,We provide in - depth experiments and empirical comparisons of different convolutional models and explore the advantages and dis advantages of their components for learning character - to - word representations .,48,0,32
dataset/preprocessed/training-data/part-of-speech_tagging/1,We propose a funnel - shaped wide convolutional neural architecture with no downsampling that focuses on learning a better internal structure of words .,49,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/5,Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings,2,1,11
dataset/preprocessed/training-data/part-of-speech_tagging/5,"The rise of neural networks , and particularly recurrent neural networks , has produced significant advances in part - ofspeech tagging accuracy ( Zeman et al. , 2017 ) .",4,0,30
dataset/preprocessed/training-data/part-of-speech_tagging/5,One characteristic common among these models is the presence of rich initial word encodings .,5,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/5,These encodings typically are composed of a recurrent character - based representation with learned and pre-trained word embeddings .,6,0,19
dataset/preprocessed/training-data/part-of-speech_tagging/5,"However , these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts .",7,0,29
dataset/preprocessed/training-data/part-of-speech_tagging/5,"In this paper , we investigate models that use recurrent neural networks with sentence - level context for initial character and word - based representations .",8,0,26
dataset/preprocessed/training-data/part-of-speech_tagging/5,In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states .,9,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/5,We present results on part - of - speech and morphological tagging with state - of - the - art performance on a number of languages .,10,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/5,Morphosyntactic tagging accuracy has seen dramatic improvements through the adoption of recurrent neural networks - specifically BiLSTMs to create sentence - level context sensitive encodings of words .,12,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/5,"A successful recipe is to first create an initial context insensitive word representation , which usually has three main parts :",13,0,21
dataset/preprocessed/training-data/part-of-speech_tagging/5,"1 ) A dynamically trained word embedding ; 2 ) a fixed pre-trained word - embedding , induced from a large corpus ; and 3 ) a sub-word character model , which itself is usually the final state of a recurrent model that ingests one character at a time .",14,0,50
dataset/preprocessed/training-data/part-of-speech_tagging/5,Such word / sub - word models originated with .,15,0,10
dataset/preprocessed/training-data/part-of-speech_tagging/5,"Recently , used precisely such a context insensitive word representation as input to a BiLSTM in order to obtain context sensitive word encodings used to predict partof - speech tags .",16,0,31
dataset/preprocessed/training-data/part-of-speech_tagging/5,The had the highest accuracy of all participating systems in the CoNLL 2017 shared task .,17,0,16
dataset/preprocessed/training-data/part-of-speech_tagging/5,"In such a model , sub - word character - based representations only interact indirectly via subsequent recurrent layers .",18,0,20
dataset/preprocessed/training-data/part-of-speech_tagging/5,"For example , consider the sentence I had shingles , which is a painful disease .",19,0,16
dataset/preprocessed/training-data/part-of-speech_tagging/5,"Context insensitive character and word representations may have learned that for unknown or infrequent words like ' shingles ' , ' s ' and more so ' es ' is a common way to end a plural noun .",20,0,39
dataset/preprocessed/training-data/part-of-speech_tagging/5,It is up to the subsequent BiLSTM layer to override this once it sees the singular verb is to the right .,21,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/5,Note that this differs from traditional linear models where word and sub-word representations are directly concatenated with similar features in the surrounding context .,22,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/5,In this paper we aim to investigate to what extent having initial sub - word and word context insensitive representations affects performance .,23,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/5,We propose a novel model where we learn context sensitive initial character and word representations through two separate sentence - level recurrent models .,24,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/5,These are then combined via a meta-BiLSTM model that builds a unified representation of each word that is then used for syntactic tagging .,25,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/5,"Critically , while each of these three models - character , word and meta - are trained synchronously , they are ultimately separate models using different network configurations , training hyperparameters and loss functions .",26,0,35
dataset/preprocessed/training-data/part-of-speech_tagging/5,"Empirically , we found this optimal as it allowed control over the fact that each representation has a different learning capacity .",27,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/5,We tested the system on the 2017 CoNLL shared task data sets and gain improvements compared to the top performing systems for the majority of languages for part - of - speech and morphological tagging .,28,0,36
dataset/preprocessed/training-data/part-of-speech_tagging/5,"As we will see , a pattern emerged where gains were largest for morphologically rich languages , especially those in the Slavic family group .",29,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/5,"We also applied the approach to the benchmark English PTB data , where our model achieved 97.9 using the standard train / dev / test split , which constitutes a relative reduction in error of 12 % over the previous best system .",30,0,43
dataset/preprocessed/training-data/part-of-speech_tagging/5,"While sub- word representations are often attributed to the advent of deep learning in NLP , it was , in fact , commonplace for linear featurized machine learning methods to incorporate such representations .",32,0,34
dataset/preprocessed/training-data/part-of-speech_tagging/5,"While the literature is too large to enumerate , is a good example of an accurate linear model that uses both word and sub-word features .",33,0,26
dataset/preprocessed/training-data/part-of-speech_tagging/5,"Specifically , like most systems of the time , they use ngram affix features , which were made context sensitive via manually constructed conjunctions with features from other words in a fixed window .",34,0,34
dataset/preprocessed/training-data/part-of-speech_tagging/5,was perhaps the first modern neural network for tagging .,35,0,10
dataset/preprocessed/training-data/part-of-speech_tagging/5,"While this first study used only word embeddings , a subsequent model extended the representation to include suffix embeddings .",36,0,20
dataset/preprocessed/training-data/part-of-speech_tagging/5,"The seminal dependency parsing paper of led to a number of tagging papers that used their basic architecture of highly featurized ( and embedded ) feed - forward neural networks. , for example , studied this architecture in a low resource setting using word , sub - word ( prefix / suffix ) and induced cluster features to obtain competitive accuracy with the state - of - the - art. , and extended the work of Chen et al. to a structured prediction setting , the later two use again a mix of word and sub-word features .",37,0,98
dataset/preprocessed/training-data/part-of-speech_tagging/5,The idea of using a recurrent layer over characters to induce a complementary view of a word has occurred in numerous papers .,38,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/5,Perhaps the earliest is Santos and Zadrozny ( 2014 ) who compare character - based LSTM encodings to tradi-tional word - based embeddings .,39,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/5,take this a step further and combine the word embeddings with a recurrent character encoding of the word - instead of just relying on one or the other .,40,0,29
dataset/preprocessed/training-data/part-of-speech_tagging/5,use a sentencelevel character LSTM encoding for parsing .,41,0,9
dataset/preprocessed/training-data/part-of-speech_tagging/5,show that contextual embeddings using character convolutions improve accuracy for number of NLP tasks .,42,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/5,is probably the jumping - off point for most current architectures for tagging models with recurrent neural networks .,43,0,19
dataset/preprocessed/training-data/part-of-speech_tagging/5,"Specifically , they used a combined word embedding and recurrent character encoding as the initial input to a BiLSTM that generated context sensitive word encodings .",44,0,26
dataset/preprocessed/training-data/part-of-speech_tagging/5,"Though , like most previous studies , these initial encodings were context insensitive and relied on subsequent layers to encode sentence - level interactions .",45,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/5,"Finally , showed that subword / word combination representations lead to state - of - the - art morphosyntactic tagging accuracy across a number of languages in the CoNLL 2017 shared task .",46,0,33
dataset/preprocessed/training-data/part-of-speech_tagging/5,Their word representation consisted of three parts :,47,0,8
dataset/preprocessed/training-data/part-of-speech_tagging/5,1 ) A dynamically trained word embedding ; 2 ) a fixed pretrained word embedding ; 3 ) a character LSTM encoding that summed the final state of the recurrent model with vector constructed using an attention mechanism over all character states .,48,0,43
dataset/preprocessed/training-data/part-of-speech_tagging/5,"Again , the initial representations are all context insensitive .",49,0,10
dataset/preprocessed/training-data/part-of-speech_tagging/6,A Novel Neural Network Model for Joint POS Tagging and Graph - based Dependency Parsing,2,1,15
dataset/preprocessed/training-data/part-of-speech_tagging/6,We present a novel neural network model that learns POS tagging and graph - based dependency parsing jointly .,4,0,19
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Our model uses bidirectional LSTMs to learn feature representations shared for both POS tagging and dependency parsing tasks , thus handling the feature - engineering problem .",5,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Our extensive experiments , on 19 languages from the Universal Dependencies project , show that our model outperforms the state - of - the - art neural networkbased Stack - propagation model for joint POS tagging and transition - based dependency parsing , resulting in a new state of the art .",6,0,52
dataset/preprocessed/training-data/part-of-speech_tagging/6,Our code is open - source and available together with pre-trained models at : https://github.com/ datquocnguyen/jPTDP .,7,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Dependency parsing has become a key research topic in NLP in the last decade , boosted by the success of the shared tasks on multilingual dependency parsing .",9,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/6,identify two types of data - driven methodologies for dependency parsing : graph - based approaches and transition - based approaches .,10,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/6,Most traditional graph - or transition - based parsing approaches manually define a set of core and combined features associated with one - hot representations .,11,0,26
dataset/preprocessed/training-data/part-of-speech_tagging/6,Recent work shows that using deep learning in dependency parsing has obtained state - of - the - art performances .,12,0,21
dataset/preprocessed/training-data/part-of-speech_tagging/6,Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural network - based classifiers .,13,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/6,"In addition , others propose novel neural architectures for parsing to handle feature - engineering .",14,0,16
dataset/preprocessed/training-data/part-of-speech_tagging/6,Part - of - speech ( POS ) tags are essential features used in most dependency parsers .,15,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/6,"In real - world parsing , those dependency parsers rely heavily on the use of automatically predicted POS tags , thus encountering error propagation problems. , and show that parsing accuracies drop by 5 + % when utilizing automatic POS tags instead of gold ones .",16,0,46
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Some attempts have been made to avoid using POS tags during dependency parsing , however , these approaches still additionally use the automatic POS tags to achieve the best accuracy .",17,0,31
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Alternatively , joint learning both POS tagging and dependency parsing has gained more attention because : i ) more accurate POS tags could lead to improved parsing performance and ii ) the the syntactic context of a parse tree could help resolve POS ambiguities .",18,0,45
dataset/preprocessed/training-data/part-of-speech_tagging/6,"In this paper , we propose a novel neural architecture for joint POS tagging and graph - based dependency parsing .",19,0,21
dataset/preprocessed/training-data/part-of-speech_tagging/6,Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTMthe bidirectional LSTM .,20,0,21
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Not using any external resources such as pre-trained word embeddings , experimental results on 19 languages from the Universal Dependencies project show that : our joint model performs better than strong baselines and especially outperforms the neural network - based Stack - propagation model for joint POS tagging and transition - based dependency parsing , achieving a new state of the art .",21,0,63
dataset/preprocessed/training-data/part-of-speech_tagging/6,Our joint model,22,0,3
dataset/preprocessed/training-data/part-of-speech_tagging/6,"In this section , we describe our new model for joint POS tagging and dependency parsing , which we call jPTDP .",23,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/6,illustrates the architecture of our new model .,24,0,8
dataset/preprocessed/training-data/part-of-speech_tagging/6,We learn shared latent feature vectors representing word tokens in an input sentence by using BiLSTMs .,25,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/6,Then these shared feature vectors are further used to make the predic - tion of POS tags as well as fed into a multi - layer perceptron with one hidden layer ( MLP ) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs .,26,0,51
dataset/preprocessed/training-data/part-of-speech_tagging/6,BiLSTM - based latent feature representations :,27,0,7
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Given an input sentence s consisting of n word tokens w 1 , w 2 , ... , w n , we represent each word w i in s by an embedding e ( ) w i . and show that character - based representations of words help improve POS tagging and dependency parsing performances .",28,0,56
dataset/preprocessed/training-data/part-of-speech_tagging/6,"So , we also use a sequence BiLSTM ( BiLSTM seq ) to compute a character - based vector representation for each word w i in s .",29,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/6,"For a word type w consisting of k characters w = c 1 c 2 ... c k , the input to the sequence BiLSTM consists of k character embeddings c 1:k in which each embedding vector c j represents the j th character c j in w ; and the output is the character - based embedding e ( * )",30,0,62
dataset/preprocessed/training-data/part-of-speech_tagging/6,"w of the word type w , computed as :",31,0,10
dataset/preprocessed/training-data/part-of-speech_tagging/6,"For the i th word w i in the input sentence s , we create an input vector e i which is a concatenation ( ) of the corresponding word embedding and character - based embedding vectors :",32,0,38
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Then , we feed the sequence of input vectors e 1:n with an additional index i corresponding to a context position into another BiLSTM ( BiLSTM ctx ) , resulting in shared feature vectors vi representing the i th words w i in the sentence s: vi = BiLSTM ctx ( e 1:n , i ) POS tagging :",33,0,59
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Using shared BiLSTM - based latent feature vector representations , then we follow a common approach to compute the cross - entropy objective loss L POS ( t , t ) , in whicht and tare the sequence of predicted POS tags and sequence of gold POS tags of words in the input sentence s , respectively .",34,0,58
dataset/preprocessed/training-data/part-of-speech_tagging/6,Arc - factored graph - based parsing :,35,0,8
dataset/preprocessed/training-data/part-of-speech_tagging/6,Dependency trees can be formalized as directed graphs .,36,0,9
dataset/preprocessed/training-data/part-of-speech_tagging/6,An arc - factored parsing approach learns the scores of the arcs in a graph .,37,0,16
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Then , using an efficient decoding algorithm ( in particular , we use the Eisner ( 1996 ) 's algorithm ) , we can find a maximum spanning tree - the highest scoring parse tree - of the graph from those arc scores :",38,0,44
dataset/preprocessed/training-data/part-of-speech_tagging/6,"where Y ( s ) is the set of all possible dependency trees for the input sentence s while score arc ( h , m ) measures the score of the arc between the head h th word and the modifier m th word in s .",39,0,47
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Following , we score an arc by using a MLP with one - node output layer ( MLP arc ) on top of the BiLSTM ctx :",40,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/6,"where v hand v mare the shared BiLSTM - based feature vectors representing the h th and m th words in s , respectively .",41,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/6,We then compute a marginbased hinge loss L arc with loss - augmented inference to maximize the margin between the gold unlabeled parse tree and the highest scoring incorrect tree .,42,0,31
dataset/preprocessed/training-data/part-of-speech_tagging/6,Dependency relation types are predicted in a similar manner .,43,0,10
dataset/preprocessed/training-data/part-of-speech_tagging/6,We use another MLP on top of the BiLSTM ctx for predicting relation type of an head - modifier arc .,44,0,21
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Here , the number of the nodes in the output layer of this MLP ( MLP rel ) is the number of relation types .",45,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Given an arc ( h , m ) , we compute a corresponding output vector as :",46,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/6,"Then , based on MLP output vectors v ( h , m ) , we also compute another margin - based hinge loss L rel for relation type prediction , using only the gold labeled parse tree .",47,0,38
dataset/preprocessed/training-data/part-of-speech_tagging/6,Joint model training :,48,0,4
dataset/preprocessed/training-data/part-of-speech_tagging/6,"The final training objective function of our joint model is the sum of the POS tagging loss L POS , the structure loss L arc and the relation labeling loss L rel .",49,0,33
dataset/preprocessed/training-data/part-of-speech_tagging/0,Robust Multilingual Part - of - Speech Tagging via Adversarial Training,2,1,11
dataset/preprocessed/training-data/part-of-speech_tagging/0,"Adversarial training ( AT ) 1 is a powerful regularization method for neural networks , aiming to achieve robustness to input perturbations .",4,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/0,"Yet , the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing .",5,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/0,"In this paper , we propose and analyze a neural POS tagging model that exploits AT .",6,1,17
dataset/preprocessed/training-data/part-of-speech_tagging/0,"In our experiments on the Penn Treebank WSJ corpus and the Universal Dependencies ( UD ) dataset ( 27 languages ) , we find that AT not only improves the over all tagging accuracy , but also 1 ) prevents over - fitting well in low resource languages and 2 ) boosts tagging accuracy for rare / unseen words .",7,0,60
dataset/preprocessed/training-data/part-of-speech_tagging/0,"We also demonstrate that 3 ) the improved tagging performance by AT contributes to the downstream task of dependency parsing , and that 4 ) AT helps the model to learn cleaner word representations .",8,0,35
dataset/preprocessed/training-data/part-of-speech_tagging/0,5 ) The proposed AT model is generally effective in different sequence labeling tasks .,9,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/0,These positive results motivate further use of AT for natural language tasks .,10,0,13
dataset/preprocessed/training-data/part-of-speech_tagging/0,"Recently , neural network - based approaches have become popular in many natural language processing ( NLP ) tasks including tagging , parsing , and translation .",12,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/0,"However , it has been shown that neural networks tend to be locally unstable and even tiny perturbations to the original inputs can mislead the models .",13,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/0,Such maliciously perturbed inputs are called adversarial examples .,14,0,9
dataset/preprocessed/training-data/part-of-speech_tagging/0,Adversarial training aims to improve the robustness of a model to input perturbations by training on both unmodified examples and adversarial examples .,15,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/0,We distinguish AT from Generative Adversarial Networks ( GANs ) .,17,0,11
dataset/preprocessed/training-data/part-of-speech_tagging/0,Figure 1 : Illustration of our architecture for adversarial POS tagging .,18,0,12
dataset/preprocessed/training-data/part-of-speech_tagging/0,"Given a sentence , we input the normalized word embeddings ( w 1 , w 2 , w 3 ) and character embeddings ( showing c 1 , c 2 , c 3 for w 1 ) .",19,0,38
dataset/preprocessed/training-data/part-of-speech_tagging/0,Each word is represented by concatenating its word embedding and its character - level BiLSTM output .,20,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/0,They are fed into the main BiLSTM - CRF network for POS tagging .,21,0,14
dataset/preprocessed/training-data/part-of-speech_tagging/0,"In adversarial training , we compute and add the worst - case perturbation ?",22,0,14
dataset/preprocessed/training-data/part-of-speech_tagging/0,to all the input embeddings for regularization .,23,0,8
dataset/preprocessed/training-data/part-of-speech_tagging/0,on image recognition has demonstrated the enhanced robustness of their models to unseen images via adversarial training and has provided theoretical explanations of the regularization effects .,24,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/0,"Despite its potential as a powerful regularizer , adversarial training ( AT ) has yet to be explored extensively in natural language tasks .",25,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/0,"Recently , applied AT on text classification , achieving state - of - the - art accuracy .",26,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/0,"Yet , the specific effects of the robustness obtained from AT are still unclear in the context of NLP .",27,0,20
dataset/preprocessed/training-data/part-of-speech_tagging/0,"For example , research studies have yet to answer questions such as 1 ) how can we interpret perturbations or robustness on natural language inputs ?",28,0,26
dataset/preprocessed/training-data/part-of-speech_tagging/0,2 ) how are they related to linguistic factors like vocabulary statistics ?,29,0,13
dataset/preprocessed/training-data/part-of-speech_tagging/0,3 ) are the effects of AT language - dependent ?,30,0,11
dataset/preprocessed/training-data/part-of-speech_tagging/0,Answering such questions is crucial to understand and motivate the application of adversarial training on natural language tasks .,31,0,19
dataset/preprocessed/training-data/part-of-speech_tagging/0,"In this paper , spotlighting a well - studied core problem of NLP , we propose and carefully analyze a neural part - of - speech ( POS ) tagging model that exploits adversarial training .",32,0,36
dataset/preprocessed/training-data/part-of-speech_tagging/0,"With a BiLSTM - CRF model as our baseline POS tagger , we apply adversarial training by considering perturbations to input word / character embeddings .",33,0,26
dataset/preprocessed/training-data/part-of-speech_tagging/0,"In order to demystify the effects of adversarial training in the context of NLP , we conduct POS tagging experiments on multiple languages using the Penn Treebank WSJ corpus ( Englsih ) and the Universal Dependencies dataset ( 27 languages ) , with thorough analyses of the following points :",34,0,50
dataset/preprocessed/training-data/part-of-speech_tagging/0,Effects on different target languages Vocabulary statistics and tagging accuracy Influence on downstream tasks Representation learning of words,35,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/0,"In our experiments , we find that our adversarial training model consistently outperforms the baseline POS tagger , and even achieves state - of - the - art results on 22 languages .",36,0,33
dataset/preprocessed/training-data/part-of-speech_tagging/0,"Furthermore , our analyses reveal the following insights into adversarial training in the context of NLP :",37,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/0,The regularization effects of adversarial training ( AT ) are general across different languages .,38,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/0,"AT can prevent overfitting especially well when training examples are scarce , providing an effective tool to process low resource languages .",39,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/0,AT can boost the tagging performance for rare / unseen words and increase the sentence - level accuracy .,40,0,19
dataset/preprocessed/training-data/part-of-speech_tagging/0,"This positively affects the performance of down - stream tasks such as dependency parsing , where low sentence - level POS accuracy can be a bottleneck .",41,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/0,"AT helps the network learn cleaner word embeddings , showing stronger correlations with their POS tags .",42,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/0,We argue that the effects of AT can be interpreted from the perspective of natural language .,43,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/0,"Finally , we demonstrate that the proposed AT model is generally effective across different sequence labeling tasks .",44,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/0,This work therefore provides a strong motivation and basis for utilizing adversarial training in NLP tasks .,45,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/0,2 Related Work,46,0,3
dataset/preprocessed/training-data/part-of-speech_tagging/0,Part - of - speech ( POS ) tagging is a fundamental NLP task that facilitates downstream tasks such as syntactic parsing .,49,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/2,TRANSFER LEARNING FOR SEQUENCE TAGGING WITH HIERARCHICAL RECURRENT NETWORKS,2,1,9
dataset/preprocessed/training-data/part-of-speech_tagging/2,Recent papers have shown that neural networks obtain state - of - the - art performance on several different sequence tagging tasks .,4,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/2,"One appealing property of such systems is their generality , as excellent performance can be achieved with a unified architecture and without task - specific feature engineering .",5,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/2,"However , it is unclear if such systems can be used for tasks without large amounts of training data .",6,0,20
dataset/preprocessed/training-data/part-of-speech_tagging/2,"In this paper we explore the problem of transfer learning for neural sequence taggers , where a source task with plentiful annotations ( e.g. , POS tagging on Penn Treebank ) is used to improve performance on a target task with fewer available annotations ( e.g. , POS tagging for microblogs ) .",7,0,53
dataset/preprocessed/training-data/part-of-speech_tagging/2,"We examine the effects of transfer learning for deep hierarchical recurrent networks across domains , applications , and languages , and show that significant improvement can often be obtained .",8,0,30
dataset/preprocessed/training-data/part-of-speech_tagging/2,These improvements lead to improvements over the current state - of the - art on several well - studied tasks .,9,0,21
dataset/preprocessed/training-data/part-of-speech_tagging/2,1 Code is available at https://github.com/kimiyoung/transfer 1 ar Xiv:1703.06345v1 [ cs.CL ],11,0,12
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Sequence tagging is an important problem in natural language processing , which has wide applications including part - of - speech ( POS ) tagging , text chunking , and named entity recognition ( NER ) .",13,0,37
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Given a sequence of words , sequence tagging aims to predict a linguistic tag for each word such as the POS tag .",14,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/2,"An important challenge for sequence tagging is how to transfer knowledge from one task to another , which is often referred to as transfer learning .",15,0,26
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Transfer learning can be used in several settings , notably for low - resource languages and low - resource domains such as biomedical corpora and Twitter corpora ) .",16,0,29
dataset/preprocessed/training-data/part-of-speech_tagging/2,"In these cases , transfer learning can improve performance by taking advantage of more plentiful labels from related tasks .",17,0,20
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Even on datasets with relatively abundant labels , multi-task transfer can sometimes achieve improvement over state - of - the - art results .",18,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Recently , a number of approaches based on deep neural networks have addressed the problem of sequence tagging in an end - to - end manner .",19,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/2,These neural networks consist of multiple layers of neurons organized in a hierarchy and can transform the input tokens to the output labels without explicit hand - engineered feature extraction .,20,0,31
dataset/preprocessed/training-data/part-of-speech_tagging/2,The aforementioned neural networks require minimal assumptions about the task at hand and thus demonstrate significant generality - one single model can be applied to multiple applications in multiple languages without changing the architecture .,21,0,35
dataset/preprocessed/training-data/part-of-speech_tagging/2,A natural question is whether the representation learned from one task can be useful for another task .,22,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/2,"In other words , is there away we can exploit the generality of neural networks to improve task performance by sharing model parameters and feature representations with another task ?",23,0,30
dataset/preprocessed/training-data/part-of-speech_tagging/2,"To address the above question , we study the transfer learning setting , which aims to improve the performance on a target task by joint training with a source task .",24,0,31
dataset/preprocessed/training-data/part-of-speech_tagging/2,"We present a transfer learning approach based on a deep hierarchical recurrent neural network , which shares the hidden feature repre-sentation and part of the model parameters between the source task and the target task .",25,0,36
dataset/preprocessed/training-data/part-of-speech_tagging/2,Our approach combines the objectives of the two tasks and uses gradient - based methods for efficient training .,26,0,19
dataset/preprocessed/training-data/part-of-speech_tagging/2,"We study cross - domain , cross - application , and cross-lingual transfer , and present a parameter - sharing architecture for each case .",27,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/2,Experimental results show that our approach can significantly improve the performance of the target task when the the target task has few labels and is more related to the source task .,28,0,32
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Furthermore , we show that transfer learning can improve performance over state - of the - art results even if the amount of labels is relatively abundant .",29,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/2,We have novel contributions in two folds .,30,0,8
dataset/preprocessed/training-data/part-of-speech_tagging/2,"First , our work is , to the best of our knowledge , the first that focuses on studying the transferability of different layers of representations for hierarchical RNNs .",31,0,30
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Second , different from previous transfer learning methods that usually focus on one specific transfer setting , our framework exploits different levels of representation sharing and provides a unified framework to handle cross-application , cross - lingual , and cross-domain transfer .",32,0,42
dataset/preprocessed/training-data/part-of-speech_tagging/2,"There are two common paradigms for transfer learning for natural language processing ( NLP ) tasks , resource - based transfer and model - based transfer .",34,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Resource - based transfer utilizes additional linguistic annotations as weak supervision for transfer learning , such as cross - lingual dictionaries , corpora , and word alignments .",35,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Resource - based methods demonstrate considerable success in cross - lingual transfer , but are quite sensitive to the scale and quality of the additional resources .",36,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Resource - based transfer is mostly limited to cross - lingual transfer in previous works , and there is not extensive research on extending resource - based methods to cross - domain and cross-application settings .",37,0,36
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Model - based transfer , on the other hand , does not require additional resources .",38,0,16
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Model - based transfer exploits the similarity and relatedness between the source task and the target task by adaptively modifying the model architectures , training algorithms , or feature representation .",39,0,31
dataset/preprocessed/training-data/part-of-speech_tagging/2,"For example , proposed a transfer learning framework that shares structural parameters across multiple tasks , and improve the performance on various tasks including NER ; presented a task - independent convolutional neural network and employed joint training to transfer knowledge from NER and POS tagging to chunking ; studied transfer learning between named entity recognition and word segmentation in Chinese based on recurrent neural networks .",40,0,67
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Cross - domain transfer , or domain adaptation , is also a well - studied branch of model - based transfer in NLP .",41,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/2,"Techniques in cross - domain transfer include the design of robust feature representations , co-training , hierarchical Bayesian prior , and canonical component analysis .",42,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/2,"While our approach falls into the paradigm of model - based transfer , in contrast to the above methods , our method focuses on exploiting the generality of deep recurrent neural networks and is applicable to transfer between domains , applications , and languages .",43,0,45
dataset/preprocessed/training-data/part-of-speech_tagging/2,Our work builds on previous work on sequence tagging based on deep neural networks .,44,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/2,develop end - to - end neural networks for sequence tagging without hand - engineered features .,45,0,17
dataset/preprocessed/training-data/part-of-speech_tagging/2,Later architectures based on different combinations of convolutional networks and recurrent networks have achieved state - of - the - art results on many tasks .,46,0,26
dataset/preprocessed/training-data/part-of-speech_tagging/2,These models demonstrate significant generality since they can be applied to multiple applications in multiple languages with a unified network architecture and without task - specific feature extraction .,47,0,29
dataset/preprocessed/training-data/part-of-speech_tagging/2,"In this section , we introduce our transfer learning approach .",49,0,11
dataset/preprocessed/training-data/part-of-speech_tagging/4,Hierarchically - Refined Label Attention Network for Sequence Labeling,2,1,9
dataset/preprocessed/training-data/part-of-speech_tagging/4,CRF has been used as a powerful model for statistical sequence labeling .,4,1,13
dataset/preprocessed/training-data/part-of-speech_tagging/4,"For neural sequence labeling , however , BiLSTM - CRF does not always lead to better results compared with BiLSTM - softmax local classification .",5,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/4,This can be because the simple Markov label transition model of CRF does not give much information gain over strong neural encoding .,6,0,23
dataset/preprocessed/training-data/part-of-speech_tagging/4,"For better representing label sequences , we investigate a hierarchically - refined label attention network , which explicitly leverages label embeddings and captures potential long - term label dependency by giving each word incrementally refined label distributions with hierarchical attention .",7,0,41
dataset/preprocessed/training-data/part-of-speech_tagging/4,"Results on POS tagging , NER and CCG supertagging show that the proposed model not only improves the over all tagging accuracy with similar number of parameters , but also significantly speeds up the training and testing compared to BiLSTM - CRF .",8,0,43
dataset/preprocessed/training-data/part-of-speech_tagging/4,Conditional random fields ( CRF ) ) is a state - of - the - art model for statistical sequence labeling .,10,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/4,"Recently , CRF has been integrated with neural encoders as an output layer to capture label transition patterns .",11,0,19
dataset/preprocessed/training-data/part-of-speech_tagging/4,"This , however , sees mixed results .",12,0,8
dataset/preprocessed/training-data/part-of-speech_tagging/4,"For example , previous work has shown that BiLSTM - softmax gives better accuracies compared to BiLSTM - CRF for part - of - speech ( POS ) tagging .",13,0,30
dataset/preprocessed/training-data/part-of-speech_tagging/4,"In addition , the state - of - the - art neural Combinatory Categorial Grammar ( CCG ) supertaggers do not use CRF .",14,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/4,"One possible reason is that the strong representation power of neural sentence encoders such as BiLSTMs allow models to capture implicit long - range label dependencies from input word sequences alone , thereby allowing the output layer to make local predictions .",15,0,42
dataset/preprocessed/training-data/part-of-speech_tagging/4,"In contrast , though explicitly capturing output label dependencies , CRF can be limited by its Markov assumptions , particularly when being used on top of neural encoders .",16,0,29
dataset/preprocessed/training-data/part-of-speech_tagging/4,"In addition , CRF can be computationally expensive when the number of labels is large , due to the use of Viterbi decoding .",17,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/4,"One interesting research question is whether there is neural alternative to CRF for neural sequence labeling , which is both faster and more accurate .",18,0,25
dataset/preprocessed/training-data/part-of-speech_tagging/4,"To this question , we investigate a neural network model for output label sequences .",19,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/4,"In particular , we represent each possible label using an embedding vector , and aim to encode sequences of label distributions using a recurrent neural network .",20,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/4,"One main challenge , however , is that the number of possible label sequences is exponential to the size of input .",21,0,22
dataset/preprocessed/training-data/part-of-speech_tagging/4,This makes our task essentially to represent a full - exponential search space without making Markov assumptions .,22,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/4,We tackle this challenge using a hierarchicallyrefined representation of marginal label distributions .,23,0,13
dataset/preprocessed/training-data/part-of-speech_tagging/4,"As shown in , our model consists of a multi - layer neural network .",24,0,15
dataset/preprocessed/training-data/part-of-speech_tagging/4,"In each layer , each input words is represented together with its marginal label probabilities , and a sequence neural network is employed to model unbounded dependencies .",25,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/4,"The marginal distributions space are refined hierarchically bottom - up , where a higher layer learns a more informed label sequence distribution based on information from a lower layer .",26,0,30
dataset/preprocessed/training-data/part-of-speech_tagging/4,"For instance , given a sentence "" They 1 can 2 fish 3 and 4 also 5 tomatoes 6 here 7 "" , the label distributions of the words can 2 and fish 3 in the first layer of have higher probabilities on the tags MD ( modal verb ) and VB ( base form verb ) , respectively , though not fully confidently .",27,0,65
dataset/preprocessed/training-data/part-of-speech_tagging/4,"The initial label distributions are then fed as the inputs to the next layer , so that long - range label dependencies can be considered .",28,0,26
dataset/preprocessed/training-data/part-of-speech_tagging/4,"In the second layer , the network can learn to assign a noun tag to fish 3 by taking into account the highly confident tagging information of tomatoes 6 ( NN ) , resulting in the pattern "" can 2 ( VB ) fish 3 ( NN ) "" . , our model consists of stacked attentive BiLSTM layers , each of which takes a sequence of vectors as input and yields a sequence of hidden state vectors together with a sequence of label distributions .",29,0,86
dataset/preprocessed/training-data/part-of-speech_tagging/4,"The model performs attention over label embeddings for deriving a marginal label distributions , which are in turn used to calculate a weighted sum of label embeddings .",30,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/4,"Finally , the resulting packed label vector is used together with input word vectors as the hidden state vector for the current layer .",31,0,24
dataset/preprocessed/training-data/part-of-speech_tagging/4,Thus our model is named label attention network ( LAN ) .,32,0,12
dataset/preprocessed/training-data/part-of-speech_tagging/4,"For sequence labeling , the input to the whole model is a sentence and the output is the label distributions of each word in the final layer .",33,0,28
dataset/preprocessed/training-data/part-of-speech_tagging/4,As shown in,34,0,3
dataset/preprocessed/training-data/part-of-speech_tagging/4,BiLSTM- LAN can be viewed as a form of multi - layered BiLSTM - softmax sequence labeler .,35,0,18
dataset/preprocessed/training-data/part-of-speech_tagging/4,"In particular , a single - layer BiLSTM - LAN is identical to a single - layer BiLSTM - softmax model , where the label embedding table serves as the softmax weights in BiLSTM - softmax , and the label attention distribution is the softmax distribution in BiLSTM - softmax .",36,0,51
dataset/preprocessed/training-data/part-of-speech_tagging/4,"The traditional way of making a multi - layer extention to BiLSTM - softmax is to stack multiple BiLSTM encoder layers before the softmax output layer , which learns a deeper input representation .",37,0,34
dataset/preprocessed/training-data/part-of-speech_tagging/4,"In contrast , a multi - layer BiLSTM - LAN stacks both the BiLSTM encoder layer and the softmax output layer , learning a deeper representation of both the input and candidate output sequences .",38,0,35
dataset/preprocessed/training-data/part-of-speech_tagging/4,"On standard benchmarks for POS tagging , NER and CCG supertagging , our model achieves significantly better accuracies and higher efficiencies than BiLSTM - CRF and BiLSTM - softmax with similar number of parameters .",39,0,35
dataset/preprocessed/training-data/part-of-speech_tagging/4,"It gives highly competitive results compared with topperformance systems on WSJ , OntoNotes 5.0 and CCGBank without external training .",40,0,20
dataset/preprocessed/training-data/part-of-speech_tagging/4,"In addition to accuracy and efficiency , BiLSTM - LAN is also more interpretable than BiLSTM - CRF thanks to visualizable label embeddings and label distributions .",41,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/4,Our code and models are released at https://github.com/Nealcly/LAN .,42,0,9
dataset/preprocessed/training-data/part-of-speech_tagging/4,Neural Attention .,44,0,3
dataset/preprocessed/training-data/part-of-speech_tagging/4,"Attention has been shown useful in neural machine translation , sentiment classification , relation classification , read comprehension , sentence summarization , parsing , question answering .",45,0,27
dataset/preprocessed/training-data/part-of-speech_tagging/4,"Self - attention network ( SAN ) has been used for semantic role labeling , text classification and other tasks .",46,0,21
dataset/preprocessed/training-data/part-of-speech_tagging/4,Our work is similar to in the sense that we also build a hierarchical attentive neural network for sequence representation .,47,0,21
dataset/preprocessed/training-data/part-of-speech_tagging/4,"The difference lies in that our main goal is to investigate the encoding of exponential label sequences , whereas their work focuses on encoding of a word sequence only .",48,0,30
dataset/preprocessed/training-data/part-of-speech_tagging/4,Label Embeddings .,49,0,3
dataset/preprocessed/training-data/prosody_prediction/0,Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations,2,1,10
dataset/preprocessed/training-data/prosody_prediction/0,In this paper we introduce a new natural language processing dataset and benchmark for predicting prosodic prominence from written text .,4,1,21
dataset/preprocessed/training-data/prosody_prediction/0,To our knowledge this will be the largest publicly available dataset with prosodic labels .,5,0,15
dataset/preprocessed/training-data/prosody_prediction/0,We describe the dataset construction and the resulting benchmark dataset in detail and train a number of different models ranging from feature - based classifiers to neural network systems for the prediction of discretized prosodic prominence .,6,0,37
dataset/preprocessed/training-data/prosody_prediction/0,We show that pre-trained contextualized word representations from BERT outperform the other models even with less than 10 % of the training data .,7,0,24
dataset/preprocessed/training-data/prosody_prediction/0,Finally we discuss the dataset in light of the results and point to future research and plans for further improving both the dataset and methods of predicting prosodic prominence from text .,8,0,32
dataset/preprocessed/training-data/prosody_prediction/0,The dataset and the code for the models are publicly available .,9,0,12
dataset/preprocessed/training-data/prosody_prediction/0,"Prosodic prominence , i.e. , the amount of emphasis that a speaker gives to a word , has been widely studied in phonetics and speech processing .",11,0,27
dataset/preprocessed/training-data/prosody_prediction/0,"However , the research on text - based natural language processing ( NLP ) methods for predicting prosodic prominence is somewhat limited .",12,0,23
dataset/preprocessed/training-data/prosody_prediction/0,"Even in the text - to - speech synthesis domain , with many recent methodological advances , work on symbolic prosody prediction has lagged behind .",13,0,26
dataset/preprocessed/training-data/prosody_prediction/0,We believe that this is mainly due to the lack of suitable datasets .,14,0,14
dataset/preprocessed/training-data/prosody_prediction/0,"Existing , publicly available annotated speech corpora , are very small by current standards .",15,0,15
dataset/preprocessed/training-data/prosody_prediction/0,"In this paper we introduce a new NLP dataset and benchmark for predicting prosodic prominence from text which is based on the recently published Libri TTS corpus , containing automatically generated prosodic prominence labels for over 260 hours or 2.8 million words of English audio books , read by 1230 different speakers .",16,0,53
dataset/preprocessed/training-data/prosody_prediction/0,To our knowledge this will be the largest publicly available dataset with prosodic annotations .,17,0,15
dataset/preprocessed/training-data/prosody_prediction/0,We first give some background about prosodic prominence and related research in Section 2 .,18,0,15
dataset/preprocessed/training-data/prosody_prediction/0,We then describe the dataset construction and annotation method in Section 3 .,19,0,13
dataset/preprocessed/training-data/prosody_prediction/0,Prosody prediction can be turned into a sequence labeling task by giving each word in a text a discrete prominence value based on the amount of emphasis the speaker gives to the word when reading the text .,20,1,38
dataset/preprocessed/training-data/prosody_prediction/0,In Section 4 we explain the experiments and the experimental results using a number of different sequence labeling approaches and show that pre-trained contextualized word representations from BERT outperform our other baselines even with less than 10 % of the training data .,21,0,43
dataset/preprocessed/training-data/prosody_prediction/0,"Although BERT has been previously applied in various sequence labeling tasks , like named entity recognition , to the best of our knowledge , this is the first application of BERT in the task of predicting prosodic prominence .",22,0,39
dataset/preprocessed/training-data/prosody_prediction/0,"We analyse the results in Section 5 , comparing BERT to a bidirectional long shortterm memory ( BiLSTM ) model and looking at the types of errors made by these selected models .",23,0,33
dataset/preprocessed/training-data/prosody_prediction/0,We find that BERT outperforms the BiLSTM model across all the labels .,24,0,13
dataset/preprocessed/training-data/prosody_prediction/0,Finally in Section 6 we discuss the methods in light of the experimental results and highlight are as thatare known to negatively impact the results .,25,0,26
dataset/preprocessed/training-data/prosody_prediction/0,We also discuss the relevance of pre-training for the task of predicting prosodic prominence .,26,0,15
dataset/preprocessed/training-data/prosody_prediction/0,We conclude by pointing to future research both in developing better methods for predicting prosodic prominence but also to further improve the quality of the dataset .,27,0,27
dataset/preprocessed/training-data/prosody_prediction/0,The dataset and the PyTorch code for the models are available on GitHub : https://github.com/,28,0,15
dataset/preprocessed/training-data/prosody_prediction/0,Helsinki - NLP / prosody .,29,0,6
dataset/preprocessed/training-data/prosody_prediction/0,Every word and utterance in speech encompasses phonetic and phonological properties thatare not resulting from the choice of the underlying lexical items and that encode meaning in addition to that of the individual lexemes .,32,0,35
dataset/preprocessed/training-data/prosody_prediction/0,"These properties are referred to as prosody and they depend on a variety of factors such as the semantic and syntactic relations between these items , and their rhythmic grouping .",33,0,31
dataset/preprocessed/training-data/prosody_prediction/0,Prosodic variation in speech contributes to a large extend to the perception of natural sounding speech .,34,0,17
dataset/preprocessed/training-data/prosody_prediction/0,Prosodic prominence represents one type of prosodic phenomenon that manifests through the subjective impression of emphasis in speech where certain words are interpreted as more salient within their lexical surrounding context .,35,0,32
dataset/preprocessed/training-data/prosody_prediction/0,"Due to the inherent difficulty in determining prominence - even for human subjects , see , e.g. , ) - the development of automatic tools for the annotation of prominent units has been a difficult task .",36,0,37
dataset/preprocessed/training-data/prosody_prediction/0,This is exemplified from the large degree of discrepancy observed between human annotators when labeling prominence where the inter-transcriber agreement can vary substantially based on a multitude of factors such as the choice of annotators or annotation method .,37,0,39
dataset/preprocessed/training-data/prosody_prediction/0,"Similarly , in prominence production , certain degree of freedom in prominence placement and large variability between styles and speakers , renders the task of prominence prediction from text very difficult compared to most NLP tasks involving text only .",38,0,40
dataset/preprocessed/training-data/prosody_prediction/0,Generating Prominence Annotations,39,0,3
dataset/preprocessed/training-data/prosody_prediction/0,Throughout the literature a number of methods have been proposed for the labeling of prosodic prominence .,40,0,17
dataset/preprocessed/training-data/prosody_prediction/0,"These methods can be roughly categorized on the basis of the need for training data ( manual prosodic annotations ) into supervised and unsupervised , but crucially , on the basis of the information they utilize from speech and language to generate their predictions ( prominence labels ) .",41,0,49
dataset/preprocessed/training-data/prosody_prediction/0,"As prominence perception has been found to correlate with acoustic - phonetic features , with the constituent syntactic structure of an utterance , with the frequency of occurrence of individual lexical items , and with the probabilities of contiguous lexical sequences , automatic methods have been developed utilizing these features either in combination or independently .",42,0,56
dataset/preprocessed/training-data/prosody_prediction/0,"Overall , these features can be largely divided into two categories : ( i ) acoustic ( derived from the sound pressure waveform of the speech signal ) and ( ii ) language ( extracted by studying the form of the language ; for instance , semantic or syntactic factors in the language ) .",43,0,55
dataset/preprocessed/training-data/prosody_prediction/0,"Both acoustic and language - based features have been shown to provide good over all performance in detecting prominence ( in both supervised and unsupervised cases ) , where , however , the methods utilizing acoustic features seem to provide better performance for the unsupervised detection of prominences in speech , with state - of - the - art results reaching high level of accuracy , close to that of the inter-annotator agreement for the data .",44,0,77
dataset/preprocessed/training-data/prosody_prediction/0,"While the top - down linguistic information is known to correlate with perceptual prominence , in this paper we want to make a clear distinction between data labelling and textbased prediction .",45,0,32
dataset/preprocessed/training-data/prosody_prediction/0,"Thus , in this work , we utilize purely acoustic prominence annotations of the speech data using the method developed by as the prosodic reference .",46,0,26
dataset/preprocessed/training-data/prosody_prediction/0,Predicting Prosodic Prominence from Text,47,0,5
dataset/preprocessed/training-data/prosody_prediction/0,To what extent prosodic prominence can be predicted from textual input only has been a topic of inquiry in linguistics for a longtime .,48,0,24
dataset/preprocessed/training-data/prosody_prediction/0,"In traditional generative phonology , accent placement was considered to be fully determined by linguistic structure , whereas a seminal work by emphasized the importance and relevance of the lexical semantic context as well as the speakers ' intention , positing that , in general , a mind reading ability maybe necessary to determine prominent words in a sentence .",49,0,60
dataset/preprocessed/training-data/text_generation/3,An Auto - Encoder Matching Model for Learning Utterance - Level Semantic Dependency in Dialogue Generation,2,1,16
dataset/preprocessed/training-data/text_generation/3,Generating semantically coherent responses is still a major challenge in dialogue generation .,4,0,13
dataset/preprocessed/training-data/text_generation/3,"Different from conventional text generation tasks , the mapping between inputs and responses in conversations is more complicated , which highly demands the understanding of utterance - level semantic dependency , a relation between the whole meanings of inputs and outputs .",5,0,42
dataset/preprocessed/training-data/text_generation/3,"To address this problem , we propose an Auto - Encoder Matching ( AEM ) model to learn such dependency .",6,0,21
dataset/preprocessed/training-data/text_generation/3,The model contains two auto - encoders and one mapping module .,7,0,12
dataset/preprocessed/training-data/text_generation/3,"The auto - encoders learn the semantic representations of inputs and responses , and the mapping module learns to connect the utterance - level representations .",8,0,26
dataset/preprocessed/training-data/text_generation/3,Experimental results from automatic and human evaluations demonstrate that our model is capable of generating responses of high coherence and fluency compared to baseline models .,9,0,26
dataset/preprocessed/training-data/text_generation/3,"Automatic dialogue generation task is of great importance to many applications , ranging from open - domain chatbots to goal - oriented technical support agents .",12,1,26
dataset/preprocessed/training-data/text_generation/3,"Recently there is an increasing amount of studies about purely datadriven dialogue models , which learn from large corpora of human conversations without handcrafted rules or templates .",13,0,28
dataset/preprocessed/training-data/text_generation/3,Most of them are based on the sequence - to - sequence ( Seq2Seq ) framework that maximizes the probability of gold responses given the previous dialogue turn .,14,0,29
dataset/preprocessed/training-data/text_generation/3,Although such methods offer great * Equal Contribution 1 The code is available at https://github.com/lancopku/AMM,15,0,15
dataset/preprocessed/training-data/text_generation/3,"promise for generating fluent responses , they still suffer from the poor semantic relevance between inputs and responses .",16,0,19
dataset/preprocessed/training-data/text_generation/3,"For example , given "" What 's your name "" as the input , the models generate "" I like it "" as the output .",17,0,26
dataset/preprocessed/training-data/text_generation/3,"Recently , the neural attention mechanism has been proved successful in many tasks including neural machine translation and abstractive summarization , for its ability of capturing word - level dependency by associating a generated word with relevant words in the source - side context .",18,0,45
dataset/preprocessed/training-data/text_generation/3,Recent studies have applied the attention mechanism to dialogue generation to improve the dialogue coherence .,19,0,16
dataset/preprocessed/training-data/text_generation/3,"However , conversation generation is a much more complex and flexible task as there are less "" word - to - words "" relations between inputs and responses .",20,1,29
dataset/preprocessed/training-data/text_generation/3,"For example , given "" Try not to take on more than you can handle "" as the input and "" You are right "" as the response , each response word can not find any aligned words from the input .",21,0,42
dataset/preprocessed/training-data/text_generation/3,"In fact , this task requires the model to understand the utterance - level dependency , a relation between the whole meanings of inputs and outputs .",22,0,27
dataset/preprocessed/training-data/text_generation/3,"Due to the lack of utterance - level semantic dependency , the conventional attention - based methods that simply capture the word - level dependency achieve less satisfying performance in generating high - quality responses .",23,0,36
dataset/preprocessed/training-data/text_generation/3,"To address this problem , we propose a novel Auto - Encoder Matching model to learn utterance - level dependency .",24,0,21
dataset/preprocessed/training-data/text_generation/3,"First , motivated by , we use two auto- encoders to learn the semantic representations of inputs and responses in an unsupervised style .",25,0,24
dataset/preprocessed/training-data/text_generation/3,"Second , given the utterance - level representations , the mapping module is taught to learn the utterance - level dependency .",26,0,22
dataset/preprocessed/training-data/text_generation/3,"The advantage is that by explicitly sep - arating representation learning and dependency learning , the model has a stronger modeling ability compared to traditional Seq2Seq models .",27,0,28
dataset/preprocessed/training-data/text_generation/3,Experimental results show that our model substantially outperforms baseline methods in generating highquality responses .,28,0,15
dataset/preprocessed/training-data/text_generation/3,Our contributions are listed as follows :,29,0,7
dataset/preprocessed/training-data/text_generation/3,"To promote coherence in dialogue generation , we propose a novel Auto - Encoder Matching model to learn the utterance - level dependency .",30,0,24
dataset/preprocessed/training-data/text_generation/3,"In our proposed model , we explicitly separate utterance representation learning and dependency learning for a better expressive ability .",31,0,20
dataset/preprocessed/training-data/text_generation/3,Experimental results on automatic evaluation and human evaluation show that our model can generate much more coherent text compared to baseline models .,32,0,23
dataset/preprocessed/training-data/text_generation/3,"In this section , we introduce our proposed model .",34,0,10
dataset/preprocessed/training-data/text_generation/3,An overview is presented in Section 2.1 .,35,0,8
dataset/preprocessed/training-data/text_generation/3,"The details of the modules are shown in Sections 2.2 , 2.3 and 2.4 .",36,0,15
dataset/preprocessed/training-data/text_generation/3,The training method is introduced in Section 2.5 .,37,0,9
dataset/preprocessed/training-data/text_generation/3,"The proposed model contains three modules : an encoder , a decoder , and a mapping module , as shown in .",39,0,22
dataset/preprocessed/training-data/text_generation/3,"In general , our model is different from the conventional sequence - to - sequence models .",40,0,17
dataset/preprocessed/training-data/text_generation/3,The encoder and decoder are both implemented as autoencoders .,41,0,10
dataset/preprocessed/training-data/text_generation/3,"They learn the internal representations of inputs and target responses , respectively .",42,0,13
dataset/preprocessed/training-data/text_generation/3,"In addition , a mapping module is built to map the internal representations of the input and the response .",43,0,20
dataset/preprocessed/training-data/text_generation/3,The encoder E ? is an unsupervised auto - encoder based on Long Short Term Memory Networks ( LSTM ) .,45,0,21
dataset/preprocessed/training-data/text_generation/3,"As it is essentially a LSTM - based Seq2Seq model , we name the encoder and decoder of the autoencoder "" source - encoder "" and "" source - decoder "" .",46,0,32
dataset/preprocessed/training-data/text_generation/3,"To be specific , the encoder E ?",47,0,8
dataset/preprocessed/training-data/text_generation/3,"receives the source text x = {x 1 , x 2 , ... , x n } , and encodes it to an internal representation h , and then decodes h to a new sequencex = {x 1 ,x 2 , ... ,x n } for the reconstruction of the input .",48,0,52
dataset/preprocessed/training-data/text_generation/3,We extract the hidden state h as the semantic representation .,49,0,11
dataset/preprocessed/training-data/text_generation/1,Adversarial Ranking for Language Generation,2,1,5
dataset/preprocessed/training-data/text_generation/1,Generative adversarial networks ( GANs ) have great successes on synthesizing data .,4,0,13
dataset/preprocessed/training-data/text_generation/1,"However , the existing GANs restrict the discriminator to be a binary classifier , and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions .",5,0,36
dataset/preprocessed/training-data/text_generation/1,"In this paper , we propose a novel generative adversarial network , RankGAN , for generating highquality language descriptions .",6,0,20
dataset/preprocessed/training-data/text_generation/1,"Rather than training the discriminator to learn and assign absolute binary predicate for individual data sample , the proposed RankGAN is able to analyze and rank a collection of human- written and machinewritten sentences by giving a reference group .",7,0,40
dataset/preprocessed/training-data/text_generation/1,"By viewing a set of data samples collectively and evaluating their quality through relative ranking scores , the discriminator is able to make better assessment which in turn helps to learn a better generator .",8,0,35
dataset/preprocessed/training-data/text_generation/1,The proposed RankGAN is optimized through the policy gradient technique .,9,0,11
dataset/preprocessed/training-data/text_generation/1,Experimental results on multiple public datasets clearly demonstrate the effectiveness of the proposed approach .,10,0,15
dataset/preprocessed/training-data/text_generation/1,"Language generation plays an important role in natural language processing , which is essential to many applications such as machine translation , image captioning , and dialogue systems .",12,0,29
dataset/preprocessed/training-data/text_generation/1,Recent studies show that the recurrent neural networks ( RNNs ) and the long shortterm memory networks ( LSTMs ) can achieve impressive performances for the task of language generation .,13,0,31
dataset/preprocessed/training-data/text_generation/1,"Evaluation metrics such as BLEU , METEOR , and CIDEr are reported in the literature .",14,0,16
dataset/preprocessed/training-data/text_generation/1,Generative adversarial networks ( GANs ) have drawn great attentions since Goodfellow et al. introduced the framework for generating the synthetic data that is similar to the real one .,15,0,30
dataset/preprocessed/training-data/text_generation/1,"The main idea behind GANs is to have two neural network models , the discriminator and the generator , competing against each other during learning .",16,0,26
dataset/preprocessed/training-data/text_generation/1,"The discriminator aims to distinguish the synthetic from the real data , while the generator is trained to confuse the discriminator by generating high quality synthetic data .",17,0,28
dataset/preprocessed/training-data/text_generation/1,"During learning , the gradient of the training loss from the discriminator is then used as the guidance for updating the parameters of the generator .",18,0,26
dataset/preprocessed/training-data/text_generation/1,"Since then , GANs achieve great performance in computer vision tasks such as image synthesis .",19,0,16
dataset/preprocessed/training-data/text_generation/1,"Their successes are mainly attributed to training the discriminator to estimate the statistical properties of the continuous real - valued data ( e.g. , pixel values ) .",20,0,28
dataset/preprocessed/training-data/text_generation/1,The adversarial learning framework provides a possible way to synthesize language descriptions in high quality .,21,0,16
dataset/preprocessed/training-data/text_generation/1,"However , GANs have limited progress with natural language processing .",22,0,11
dataset/preprocessed/training-data/text_generation/1,"Primarily , the GANs have difficulties in dealing with discrete data ( e.g. , text sequences ) .",23,0,18
dataset/preprocessed/training-data/text_generation/1,"In natural languages processing , the text sequences are evaluated as the discrete tokens whose values are nondifferentiable .",24,0,19
dataset/preprocessed/training-data/text_generation/1,"Therefore , the optimization of GANs is challenging .",25,0,9
dataset/preprocessed/training-data/text_generation/1,"Secondly , most of the existing GANs assume the output of the discriminator to be a binary predicate indicating whether the given sentence is written by human or machine .",26,0,30
dataset/preprocessed/training-data/text_generation/1,"For a large variety of natural language expressions , this binary predication is too restrictive , since the diversity and richness inside the sentences are constrained by the degenerated distribution due to binary classification .",27,0,35
dataset/preprocessed/training-data/text_generation/1,"In this paper , we propose a novel adversarial learning framework , RankGAN , for generating highquality language descriptions .",28,0,20
dataset/preprocessed/training-data/text_generation/1,RankGAN learns the model from the relative ranking information between the machine - written and the human - written sentences in an adversarial framework .,29,0,25
dataset/preprocessed/training-data/text_generation/1,"In the proposed RankGAN , we relax the training of the discriminator to a learning - to - rank optimization problem .",30,0,22
dataset/preprocessed/training-data/text_generation/1,"Specifically , the proposed new adversarial network consists of two neural network models , a generator and a ranker .",31,0,20
dataset/preprocessed/training-data/text_generation/1,"As opposed to performing a binary classification task , we propose to train the ranker to rank the machine - written sentences lower than human - written sentences with respect to a reference sentence which is human-written .",32,0,38
dataset/preprocessed/training-data/text_generation/1,"Accordingly , we train the generator to synthesize sentences which confuse the ranker so that machine - written sentences are ranked higher than human - written sentences in regard to the reference .",33,0,33
dataset/preprocessed/training-data/text_generation/1,"During learning , we adopt the policy gradient technique to overcome the non-differentiable problem .",34,0,15
dataset/preprocessed/training-data/text_generation/1,"Consequently , by viewing a set of data samples collectively and evaluating their quality through relative ranking , the discriminator is able to make better assessment of the quality of the samples , which in turn helps the generator to learn better .",35,0,43
dataset/preprocessed/training-data/text_generation/1,Our method is suitable for language learning in comparison to conventional GANs .,36,0,13
dataset/preprocessed/training-data/text_generation/1,Experimental results clearly demonstrate that our proposed method outperforms the state - of - the - art methods .,37,0,19
dataset/preprocessed/training-data/text_generation/1,"GANs : Recently , GANs have been widely explored due to its nature of unsupervised deep learning .",39,0,18
dataset/preprocessed/training-data/text_generation/1,"Though GANs achieve great successes on computer vision applications , there are only a few progresses in natural language processing because the discrete sequences are not differentiable .",40,0,28
dataset/preprocessed/training-data/text_generation/1,"To tackle the non-differentiable problem , SeqGAN addresses this issue by the policy gradient inspired from the reinforcement learning .",41,0,20
dataset/preprocessed/training-data/text_generation/1,"The approach considers each word selection in the sentence as an action , and computes the reward of the sequence with the Monte Carlo ( MC ) search .",42,0,29
dataset/preprocessed/training-data/text_generation/1,"Their method back - propagates the reward from the discriminator , and encourages the generator to create human - like language sentences .",43,0,23
dataset/preprocessed/training-data/text_generation/1,Li et al.,44,0,3
dataset/preprocessed/training-data/text_generation/1,apply GANs with the policy gradient method to dialogue generation .,45,0,11
dataset/preprocessed/training-data/text_generation/1,"They train a Seq2Seq model as the generator , and build the discriminator using a hierarchical encoder followed by a 2 - way softmax function .",46,0,26
dataset/preprocessed/training-data/text_generation/1,Dai et al .,47,0,4
dataset/preprocessed/training-data/text_generation/1,show that it is possible to enhance the diversity of the generated image captions with conditional GANs .,48,0,18
dataset/preprocessed/training-data/text_generation/1,Yang et al .,49,0,4
dataset/preprocessed/training-data/text_generation/5,Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,2,1,9
dataset/preprocessed/training-data/text_generation/5,"Recent work on generative text modeling has found that variational autoencoders ( VAE ) with LSTM decoders perform worse than simpler LSTM language models ( Bowman et al. , 2015 ) .",4,1,32
dataset/preprocessed/training-data/text_generation/5,"This negative result is so far poorly understood , but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder .",5,0,27
dataset/preprocessed/training-data/text_generation/5,"In this paper , we experiment with a new type of decoder for VAE : a dilated CNN .",6,0,19
dataset/preprocessed/training-data/text_generation/5,"By changing the decoder 's dilation architecture , we control the size of context from previously generated words .",7,0,19
dataset/preprocessed/training-data/text_generation/5,"In experiments , we find that there is a trade - off between contextual capacity of the decoder and effective use of encoding information .",8,0,25
dataset/preprocessed/training-data/text_generation/5,"We show that when carefully managed , VAEs can outperform LSTM language models .",9,0,14
dataset/preprocessed/training-data/text_generation/5,"We demonstrate perplexity gains on two datasets , representing the first positive language modeling result with VAE .",10,0,18
dataset/preprocessed/training-data/text_generation/5,"Further , we conduct an in - depth investigation of the use of VAE ( with our new decoding architecture ) for semi-supervised and unsupervised labeling tasks , demonstrating gains over several strong baselines .",11,0,35
dataset/preprocessed/training-data/text_generation/5,"Generative models play an important role in NLP , both in their use as language models and because of their ability to effectively learn from unlabeled data .",13,0,28
dataset/preprocessed/training-data/text_generation/5,"By parameterzing generative models using neural nets , recent work has proposed model classes thatare particularly expressive and can pontentially model a wide range of phenomena in language and other modalities .",14,0,32
dataset/preprocessed/training-data/text_generation/5,We focus on a specific instance 1 Carnegie Mellon University .,15,0,11
dataset/preprocessed/training-data/text_generation/5,Correspondence to : Zichao Yang < zichaoy@cs.cmu.edu >.,16,0,8
dataset/preprocessed/training-data/text_generation/5,"Proceedings of the 34 th International Conference on Machine Learning , Sydney , Australia , PMLR 70 , 2017 .",17,0,20
dataset/preprocessed/training-data/text_generation/5,Copyright 2017 by the author ( s ) .,18,0,9
dataset/preprocessed/training-data/text_generation/5,of this class : the variational autoencoder 1 ( VAE ) .,19,0,12
dataset/preprocessed/training-data/text_generation/5,The generative story behind the VAE ( to be described in detail in the next section ) is simple :,20,0,20
dataset/preprocessed/training-data/text_generation/5,"First , a continuous latent representation is sampled from a multivariate Gaussian .",21,0,13
dataset/preprocessed/training-data/text_generation/5,"Then , an output is sampled from a distribution parameterized by a neural decoder , conditioned on the latent representation .",22,0,21
dataset/preprocessed/training-data/text_generation/5,"The latent representation ( treated as a latent variable during training ) is intended to give the model more expressive capacity when compared with simpler neural generative models - for example , conditional language models .",23,0,36
dataset/preprocessed/training-data/text_generation/5,"The choice of decoding architecture and final output distribution , which connect the latent representation to output , depends on the kind of data being modeled .",24,0,27
dataset/preprocessed/training-data/text_generation/5,The VAE owes it s name to an accompanying variational technique ) that has been successfully used to train such models on image data .,25,0,25
dataset/preprocessed/training-data/text_generation/5,The application of VAEs to text data has been far less successful .,26,0,13
dataset/preprocessed/training-data/text_generation/5,"The obvious choice for decoding architecture for a textual VAE is an LSTM , a typical workhorse in NLP .",27,0,20
dataset/preprocessed/training-data/text_generation/5,"However , found that using an LSTM - VAE for text modeling yields higher perplexity on held - out data than using an LSTM language model .",28,0,27
dataset/preprocessed/training-data/text_generation/5,"In particular , they observe that the LSTM decoder in VAE does not make effective use of the latent representation during training and , as a result , VAE collapses into a simple language model .",29,0,36
dataset/preprocessed/training-data/text_generation/5,Related work has used simpler decoders that model text as a bag of words .,30,0,15
dataset/preprocessed/training-data/text_generation/5,"Their results indicate better use of latent representations , but their decoders can not effectively model longer - range dependencies in text and thus underperform in terms of final perplexity .",31,0,31
dataset/preprocessed/training-data/text_generation/5,"Motivated by these observations , we hypothesize that the contextual capacity of the decoder plays an important role in whether VAEs effectively condition on the latent representation when trained on text data .",32,0,33
dataset/preprocessed/training-data/text_generation/5,"We propose the use of a dilated CNN as a decoder in VAE , inspired by the recent success of using CNNs for audio , image and language modeling ( van den .",33,0,33
dataset/preprocessed/training-data/text_generation/5,"In contrast with prior work where extremely large CNNs are used , we exploit the dilated CNN for its flexibility in varying the amount of conditioning context .",34,0,28
dataset/preprocessed/training-data/text_generation/5,"In the two extremes , depending on the choice of dilation , the CNN decoder can reproduce a simple MLP using a bags of words representation of text , or can reproduce the long - range dependence of recurrent architectures ( like an LSTM ) by conditioning on the entire history .",35,0,52
dataset/preprocessed/training-data/text_generation/5,"Thus , by choosing a dilated CNN as the decoder , we are able to conduct experiments where we vary contextual capacity , finding a sweet spot where the decoder can accurately model text but does not yet overpower the latent representation .",36,0,43
dataset/preprocessed/training-data/text_generation/5,"We demonstrate that when this trade - off is correctly managed , textual VAEs can perform substantially better than simple LSTM language models , a finding consistent with recent image modeling experiments using variational lossy autoencoders .",37,0,37
dataset/preprocessed/training-data/text_generation/5,"We goon to show that VAEs with carefully selected CNN decoders can be quite effective for semi-supervised classification and unsupervised clustering , outperforming several strong baselines ( from ) on both text categorization and sentiment analysis .",38,0,37
dataset/preprocessed/training-data/text_generation/5,Our contributions are as follows :,39,0,6
dataset/preprocessed/training-data/text_generation/5,"First , we propose the use of a dilated CNN as a new decoder for VAE .",40,0,17
dataset/preprocessed/training-data/text_generation/5,"We then empirically evaluate several dilation architectures with different capacities , finding that reduced contextual capacity leads to stronger reliance on latent representations .",41,0,24
dataset/preprocessed/training-data/text_generation/5,"By picking a decoder with suitable contextual capacity , we find our VAE performs better than LSTM language models on two data sets .",42,0,24
dataset/preprocessed/training-data/text_generation/5,We also explore the use of dilated CNN VAEs for semi-supervised classification and find they perform better than strong baselines from .,43,0,22
dataset/preprocessed/training-data/text_generation/5,"Finally , we verify that the same framework can be used effectively for unsupervised clustering .",44,0,16
dataset/preprocessed/training-data/text_generation/5,"In this section , we begin by providing background on the use of variational autoencoders for language modeling .",46,0,19
dataset/preprocessed/training-data/text_generation/5,Then we introduce the dilated CNN architecture that we will use as a new decoder for VAE in experiments .,47,0,20
dataset/preprocessed/training-data/text_generation/5,"Finally , we describe the generalization of VAE that we will use to conduct experiments on semi-supervised classification .",48,0,19
dataset/preprocessed/training-data/text_generation/5,Background on Variational Autoencoders,49,0,4
dataset/preprocessed/training-data/text_generation/0,SeqGAN : Sequence Generative Adversarial Nets with Policy Gradient,2,0,9
dataset/preprocessed/training-data/text_generation/0,"As a new way of training generative models , Generative Adversarial Net ( GAN ) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real - valued data .",4,1,39
dataset/preprocessed/training-data/text_generation/0,"However , it has limitations when the goal is for generating sequences of discrete tokens .",5,1,16
dataset/preprocessed/training-data/text_generation/0,A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model .,6,0,30
dataset/preprocessed/training-data/text_generation/0,"Also , the discriminative model can only assess a complete sequence , while for a partially generated sequence , it is nontrivial to balance its current score and the future one once the entire sequence has been generated .",7,0,39
dataset/preprocessed/training-data/text_generation/0,"In this paper , we propose a sequence generation framework , called SeqGAN , to solve the problems .",8,0,19
dataset/preprocessed/training-data/text_generation/0,"Modeling the data generator as a stochastic policy in reinforcement learning ( RL ) , SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update .",9,0,28
dataset/preprocessed/training-data/text_generation/0,"The RL reward signal comes from the GAN discriminator judged on a complete sequence , and is passed back to the intermediate state - action steps using Monte Carlo search .",10,0,31
dataset/preprocessed/training-data/text_generation/0,Extensive experiments on synthetic data and real - world tasks demonstrate significant improvements over strong baselines .,11,0,17
dataset/preprocessed/training-data/text_generation/0,Generating sequential synthetic data that mimics the real one is an important problem in unsupervised learning .,13,1,17
dataset/preprocessed/training-data/text_generation/0,"Recently , recurrent neural networks ( RNNs ) with long shortterm memory ( LSTM ) cells have shown excellent performance ranging from natural language generation to handwriting generation .",14,0,29
dataset/preprocessed/training-data/text_generation/0,The most common approach to training an RNN is to maximize the log predictive likelihood of each true token in the training sequence given the previous observed tokens ) .,15,0,30
dataset/preprocessed/training-data/text_generation/0,"However , as argued in , the maximum likelihood approaches suffer from so - called exposure bias in the inference stage : the model generates a sequence iteratively and predicts next token conditioned on its previously predicted ones that maybe never observed in the training data .",16,0,47
dataset/preprocessed/training-data/text_generation/0,Such a discrepancy between training and inference can incur accumulatively along with the sequence and will become prominent as the length of sequence increases .,17,0,25
dataset/preprocessed/training-data/text_generation/0,"To address this problem , proposed a training strategy called scheduled sampling ( SS ) , where the generative model is partially fed with its own synthetic data as prefix ( observed tokens ) rather than the true data when deciding the next token in the training stage .",18,0,49
dataset/preprocessed/training-data/text_generation/0,"Nevertheless , showed that SS is an inconsistent training strategy and fails to address the problem fundamentally .",19,0,18
dataset/preprocessed/training-data/text_generation/0,Another possible solution of the training / inference discrepancy problem is to build the loss function on the entire generated sequence instead of each transition .,20,0,26
dataset/preprocessed/training-data/text_generation/0,"For instance , in the application of machine translation , a task specific sequence score / loss , bilingual evaluation understudy ( BLEU ) , can be adopted to guide the sequence generation .",21,0,34
dataset/preprocessed/training-data/text_generation/0,"However , in many other practical applications , such as poem generation and chatbot ( Hingston 2009 ) , a task specific loss may not be directly available to score a generated sequence accurately .",22,0,35
dataset/preprocessed/training-data/text_generation/0,General adversarial net ( GAN ) proposed by is a promising framework for alleviating the above problem .,23,0,18
dataset/preprocessed/training-data/text_generation/0,"Specifically , in GAN a discriminative net D learns to distinguish whether a given data instance is real or not , and a generative net G learns to confuse D by generating high quality data .",24,0,36
dataset/preprocessed/training-data/text_generation/0,This approach has been successful and been mostly applied in computer vision tasks of generating samples of natural images .,25,0,20
dataset/preprocessed/training-data/text_generation/0,"Unfortunately , applying GAN to generating sequences has two problems .",26,0,11
dataset/preprocessed/training-data/text_generation/0,"Firstly , GAN is designed for generating real - valued , continuous data but has difficulties in directly generating sequences of discrete tokens , such as texts .",27,0,28
dataset/preprocessed/training-data/text_generation/0,"The reason is that in GANs , the generator starts with random sampling first and then a determistic transform , govermented by the model parameters .",28,0,26
dataset/preprocessed/training-data/text_generation/0,"As such , the gradient of the loss from D w.r.t. the outputs by G is used to guide the generative model G ( paramters ) to slightly change the generated value to make it more realistic .",29,0,38
dataset/preprocessed/training-data/text_generation/0,"If the generated data is based on discrete tokens , the "" slight change "" guidance from the discriminative net makes little sense because there is probably no corresponding token for such slight change in the limited dictionary space .",30,0,40
dataset/preprocessed/training-data/text_generation/0,"Secondly , GAN can only give the score / loss for an entire sequence when it has been generated ; for a partially generated sequence , it is non-trivial to balance how good as it is now and the future score as the entire sequence .",31,0,46
dataset/preprocessed/training-data/text_generation/0,"In this paper , to address the above two issues , we follow ) and consider the sequence generation procedure as a sequential decision making process .",32,0,27
dataset/preprocessed/training-data/text_generation/0,The generative model is treated as an agent of reinforcement learning ( RL ) ; the state is the generated tokens so far and the action is the next token to be generated .,33,0,34
dataset/preprocessed/training-data/text_generation/0,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model .",34,0,47
dataset/preprocessed/training-data/text_generation/0,"To solve the problem that the gradient can not pass back to the generative model when the output is discrete , we regard the generative model as a stochastic parametrized policy .",35,0,32
dataset/preprocessed/training-data/text_generation/0,"In our policy gradient , we employ Monte Carlo ( MC ) search to approximate the state - action value .",36,0,21
dataset/preprocessed/training-data/text_generation/0,"We directly train the policy ( generative model ) via policy gradient , which naturally avoids the differentiation difficulty for discrete data in a conventional GAN .",37,0,27
dataset/preprocessed/training-data/text_generation/0,Extensive experiments based on synthetic and real data are conducted to investigate the efficacy and properties of the proposed SeqGAN .,38,0,21
dataset/preprocessed/training-data/text_generation/0,"In our synthetic data environment , SeqGAN significantly outperforms the maximum likelihood methods , scheduled sampling and PG - BLEU .",39,0,21
dataset/preprocessed/training-data/text_generation/0,"In three realworld tasks , i.e. poem generation , speech language generation and music generation , SeqGAN significantly outperforms the compared baselines in various metrics including human expert judgement .",40,0,30
dataset/preprocessed/training-data/text_generation/0,"Deep generative models have recently drawn significant attention , and the ability of learning overlarge data endows them with more potential and vitality .",42,0,24
dataset/preprocessed/training-data/text_generation/0,first proposed to use the contrastive divergence algorithm to efficiently training deep belief nets ( DBN ) .,43,0,18
dataset/preprocessed/training-data/text_generation/0,proposed denoising autoencoder ( DAE ) that learns the data distribution in a supervised learning fashion .,44,0,17
dataset/preprocessed/training-data/text_generation/0,Both DBN and DAE learn a low dimensional representation ( encoding ) for each data instance and generate it from a decoding network .,45,0,24
dataset/preprocessed/training-data/text_generation/0,"Recently , variational autoencoder ( VAE ) that combines deep learning with statistical inference intended to represent a data instance in a latent hidden space ( Kingma and Welling 2014 ) , while still utilizing ( deep ) neural networks for non-linear mapping .",46,0,44
dataset/preprocessed/training-data/text_generation/0,The inference is done via variational methods .,47,0,8
dataset/preprocessed/training-data/text_generation/0,"All these generative models are trained by maximizing ( the lower bound of ) training data likelihood , which , as mentioned by ( Goodfellow and others 2014 ) , suffers from the difficulty of approximating intractable probabilistic computations .",48,0,40
dataset/preprocessed/training-data/text_generation/0,"proposed an alternative training methodology to generative models , i.e. GANs , where the training procedure is a minimax game between a generative model and a discriminative model .",49,0,29
dataset/preprocessed/training-data/text_generation/2,Long Text Generation via Adversarial Training with Leaked Information,2,1,9
dataset/preprocessed/training-data/text_generation/2,"Automatically generating coherent and semantically meaningful text has many applications in machine translation , dialogue systems , image captioning , etc .",4,1,22
dataset/preprocessed/training-data/text_generation/2,"Recently , by combining with policy gradient , Generative Adversarial Nets ( GAN ) that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation .",5,0,40
dataset/preprocessed/training-data/text_generation/2,"However , the scalar guiding signal is only available after the entire text has been generated and lacks intermediate information about text structure during the generative process .",6,0,28
dataset/preprocessed/training-data/text_generation/2,"As such , it limits its success when the length of the generated text samples is long ( more than 20 words ) .",7,0,24
dataset/preprocessed/training-data/text_generation/2,"In this paper , we propose a new framework , called LeakGAN , to address the problem for long text generation .",8,0,22
dataset/preprocessed/training-data/text_generation/2,We allow the discriminative net to leak its own high - level extracted features to the generative net to further help the guidance .,9,0,24
dataset/preprocessed/training-data/text_generation/2,"The generator incorporates such informative signals into all generation steps through an additional MANAGER module , which takes the extracted features of current generated words and outputs a latent vector to guide the WORKER module for next - word generation .",10,0,41
dataset/preprocessed/training-data/text_generation/2,Our extensive experiments on synthetic data and various realworld tasks with Turing test demonstrate that LeakGAN is highly effective in long text generation and also improves the performance in short text generation scenarios .,11,0,34
dataset/preprocessed/training-data/text_generation/2,"More importantly , without any supervision , LeakGAN would be able to implicitly learn sentence structures only through the interaction between MANAGER and WORKER .",12,0,25
dataset/preprocessed/training-data/text_generation/2,"The ability to generate coherent and semantically meaningful text plays a key role in many natural language processing applications such as machine translation , dialogue generation , and image captioning .",14,0,31
dataset/preprocessed/training-data/text_generation/2,"While most previous work focuses on task - specific applications in supervised settings , the generic unsupervised text generation , which aims to mimic the distribution over real text from a corpus , has recently drawn much attention ; * Correspondence to Weinan Zhang .",15,0,45
dataset/preprocessed/training-data/text_generation/2,This work is financially supported by NSFC ( 61702327 ) and Shanghai Sailing Program ( 17YF1428200 ) .,16,0,18
dataset/preprocessed/training-data/text_generation/2,"Copyright c 2018 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",17,0,15
dataset/preprocessed/training-data/text_generation/2,All rights reserved ..,18,0,4
dataset/preprocessed/training-data/text_generation/2,"A typical approach is to train a recurrent neural network ( RNN ) to maximize the log-likelihood of each ground - truth word given prior observed words , which , however , suffers from so - called exposure bias due to the discrepancy between training and inference stage : the model sequentially generates the next word based on previously generated words during inference but itself is trained to generate words given ground - truth words .",19,0,76
dataset/preprocessed/training-data/text_generation/2,"A scheduled sampling approach is proposed to addressed this problem , but is proved to be fundamentally inconsistent .",20,0,19
dataset/preprocessed/training-data/text_generation/2,"Generative Adversarial Nets ( GAN ) , which is firstly proposed for continous data ( image generation etc. ) , is then extended to discrete , sequential data to alleviate the above problem and has shown promising results ) .",21,0,40
dataset/preprocessed/training-data/text_generation/2,"Due to the discrete nature of text samples , text generation is modeled as a sequential decision making process , where the state is previously generated words , the action is the next word to be generated , and the generative net G is a stochastic policy that maps current state to a distribution over the action space .",22,0,59
dataset/preprocessed/training-data/text_generation/2,"After the whole text generation is done , the generated text samples are then fed to the discriminative net D , a classifier that is trained to distinguish real and generated text samples , to get reward signals for updating G.",23,0,41
dataset/preprocessed/training-data/text_generation/2,"Since then , various methods have been proposed in text generation via GAN .",24,0,14
dataset/preprocessed/training-data/text_generation/2,"Nonetheless , the reported results are limited to the cases that the generated text samples are short ( say , fewer than 20 words ) while more challenging long text generation is hardly studied , which is necessary for practical tasks such as auto-generation of news articles or product descriptions .",25,0,51
dataset/preprocessed/training-data/text_generation/2,A main drawback of existing methods to long text generation is that the binary guiding signal from Dis sparse as it is only available when the whole text sample is generated .,26,0,32
dataset/preprocessed/training-data/text_generation/2,"Also , the scalar guiding signal for a whole text is non-informative as it does not necessarily preserve the picture about the intermediate syntactic structure and semantics of the text that is being generated for G to sufficiently learn .",27,0,40
dataset/preprocessed/training-data/text_generation/2,"On one hand , to make the guiding signals more informative , discriminator D could potentially provide more guidance beside the final reward value , since Dis a trained model , e.g. a convolutional neural network ( CNN ) , rather than an unknown black box .",28,0,47
dataset/preprocessed/training-data/text_generation/2,"With that idea , proposed to train generator G via forcing learned feature representations of real and generated text by D to be matched , instead of directly training G to maximize the reward from D. Such a method can be effective in short text generation , but the guiding signals are still absent until the end of the text ) .",29,0,62
dataset/preprocessed/training-data/text_generation/2,"On the other hand , to alleviate the sparsity problem of the guiding signal , the idea of hierarchy naturally arises in text generation , since the real text samples are generated following some kinds of hierarchy such as the semantic structure and the part - of - speech .",30,0,50
dataset/preprocessed/training-data/text_generation/2,"By decomposing the whole generation task into various sub - tasks according to the hierarchical structure , it becomes much easier for the model to learn .",31,0,27
dataset/preprocessed/training-data/text_generation/2,"Early efforts have been made to incorporate the hierarchy idea in text generation ) but all use a predefined sub - task set from domain knowledge , which makes them unable to adapt to arbitrary sequence generation tasks .",32,0,39
dataset/preprocessed/training-data/text_generation/2,"In this paper , we propose a new algorithmic framework called Leak GAN to address both the non-informativeness and the sparsity issues .",33,0,23
dataset/preprocessed/training-data/text_generation/2,LeakGAN is a new way of providing richer information from the discriminator to the generator by borrowing the recent advances in hierarchical reinforcement learning .,34,0,25
dataset/preprocessed/training-data/text_generation/2,"As illustrated in , we specifically introduce a hierarchical generator G , which consists of a high - level MANAGER module and a low - level WORKER module .",35,0,29
dataset/preprocessed/training-data/text_generation/2,The MANAGER is along shortterm memory network ( LSTM ) and serves as a mediator .,36,0,16
dataset/preprocessed/training-data/text_generation/2,"In each step , it receives generator D 's high - level feature representation , e.g. , the feature map of the CNN , and uses it to form the guiding goal for the WORKER module in that timestep .",37,0,40
dataset/preprocessed/training-data/text_generation/2,As the information from D is internally - maintained and in an adversarial game it is not supposed to provide G with such information .,38,0,25
dataset/preprocessed/training-data/text_generation/2,We thus call it a leakage of information from D.,39,0,10
dataset/preprocessed/training-data/text_generation/2,"Next , given the goal embedding produced by the MAN - AGER , the WORKER first encodes current generated words with another LSTM , then combines the output of the LSTM and the goal embedding to take a final action at current state .",40,0,44
dataset/preprocessed/training-data/text_generation/2,"As such , the guiding signals from D are not only available to G at the end in terms of the scalar reward signals , but also available in terms of a goal embedding vector during the generation process to guide G how to get improved .",41,0,47
dataset/preprocessed/training-data/text_generation/2,We conduct extensive experiments based on synthetic and real data .,42,0,11
dataset/preprocessed/training-data/text_generation/2,"For synthetic data , Leak GAN obtains much lower negative log - likelihood than previous models with sequence length set to 20 and 40 .",43,0,25
dataset/preprocessed/training-data/text_generation/2,"For real data , we use the text in EMNLP 2017 WMT News , COCO Image Caption and Chinese Poems as the long , mid-length and short text corpus , respectively .",44,0,32
dataset/preprocessed/training-data/text_generation/2,"In all those cases , LeakGAN shows significant improvements compared to previous models in terms of BLEU statistics and human Turing test .",45,0,23
dataset/preprocessed/training-data/text_generation/2,"We further provide a deep investigation on the interaction between MAN - AGER and WORKER , which indicates Leak GAN implicitly learns sentence structures , such as punctuation , clause structure and long suffix without any supervision . :",46,0,39
dataset/preprocessed/training-data/text_generation/2,An overview of our Leak GAN text generation framework .,47,0,10
dataset/preprocessed/training-data/text_generation/2,"While the generator is responsible to generate the next word , the discriminator adversarially judges the generated sentence once it is complete .",48,0,23
dataset/preprocessed/training-data/text_generation/2,"The chief novelty lies in that , unlike conventional adversarial training , during the process , the discriminator reveals its internal state ( feature ft ) in order to guide the generator more informatively and frequently .",49,0,37
dataset/preprocessed/training-data/text_generation/4,Generating Text through Adversarial Training using Skip - Thought Vectors,2,1,10
dataset/preprocessed/training-data/text_generation/4,"In the past few years , various advancements have been made in generative models owing to the formulation of Generative Adversarial Networks ( GANs ) .",4,0,26
dataset/preprocessed/training-data/text_generation/4,GANs have been shown to perform exceedingly well on a wide variety of tasks pertaining to image generation and style transfer .,5,0,22
dataset/preprocessed/training-data/text_generation/4,"In the field of Natural Language Processing , word embeddings such as word2vec and GLoVe are state - of - the - art methods for applying neural network models on textual data .",6,0,33
dataset/preprocessed/training-data/text_generation/4,Attempts have been made for utilizing GANs with word embeddings for text generation .,7,1,14
dataset/preprocessed/training-data/text_generation/4,This work presents an approach to text generation using Skip - Thought sentence embeddings in conjunction with GANs based on gradient penalty functions and f-measures .,8,0,26
dataset/preprocessed/training-data/text_generation/4,The results of using sentence embeddings with GANs for generating text conditioned on input information are comparable to the approaches where word embeddings are used .,9,0,26
dataset/preprocessed/training-data/text_generation/4,Numerous efforts have been made in the field of natural language text generation for tasks such as sentiment analysis and machine translation .,11,1,23
dataset/preprocessed/training-data/text_generation/4,"Early techniques for generating text conditioned on some input information were template or rule - based engines , or probabilistic models such as n-gram .",12,0,25
dataset/preprocessed/training-data/text_generation/4,"In recent times , state - of - the - art results on these tasks have been achieved by recurrent and convolutional neural network models trained for likelihood maximization .",13,0,30
dataset/preprocessed/training-data/text_generation/4,This work pro-Code available at : https://github.com/enigmaeth/skip-thought-gan poses an approach for text generation using Generative Adversarial Networks with Skip - Thought vectors .,14,0,23
dataset/preprocessed/training-data/text_generation/4,GANs are a class of neural networks that explicitly train a generator to produce high - quality samples by pitting against an adversarial discriminative model .,15,0,26
dataset/preprocessed/training-data/text_generation/4,GANs output differentiable values and hence the task of discrete text generation has to use vectors as differentiable inputs .,16,0,20
dataset/preprocessed/training-data/text_generation/4,"This is achieved by training the GAN with sentence embedding vectors produced by Skip - Thought , a neural network model for learning fixed length representations of sentences .",17,0,29
dataset/preprocessed/training-data/text_generation/4,Deep neural network architectures have demonstrated strong results on natural language generation tasks .,19,0,14
dataset/preprocessed/training-data/text_generation/4,Recurrent neural networks using combinations of shared parameter matrices across time - steps with different gating mechanisms for easing optimization have found some success in modeling natural language .,20,0,29
dataset/preprocessed/training-data/text_generation/4,Another approach is to use convolutional neural networks that reuse kernels across time - steps with attention mechanism to perform language generation tasks .,21,0,24
dataset/preprocessed/training-data/text_generation/4,Supervised learning with deep neural networks in the framework of encoder - decoder models has become the state - of - the - art methods for approaching NLP problems .,22,0,30
dataset/preprocessed/training-data/text_generation/4,Recent text generation models use a wide variety of GANs such as gradient policy based sequence generation framework and an actor-critic conditional GAN to fill missing text conditioned on surrounding text for performing natural language generation tasks .,23,0,38
dataset/preprocessed/training-data/text_generation/4,Other architectures such as those proposed in with RNN and variational autoencoder generator with CNN discriminator and in arXiv : 1808.08703v2 [ cs. CL ] 13 Nov 2018 with leaky discriminator to guide generator through high - level extracted features have also shown great results .,24,0,46
dataset/preprocessed/training-data/text_generation/4,Using adversarial examples of word and character level embeddings for natural language text generation has been explored in .,25,0,19
dataset/preprocessed/training-data/text_generation/4,Models trained using generative adversarial networks or variational autoencoders have been shown to learn representations of continuous structures by leveraging deep latent variables such as text embeddings .,26,0,28
dataset/preprocessed/training-data/text_generation/4,This work explores injecting sentence embeddings produced using the Skip Thought architecture into GANs with different setups .,27,0,18
dataset/preprocessed/training-data/text_generation/4,Skip - Thought Generative,28,0,4
dataset/preprocessed/training-data/text_generation/4,Adversarial Network ( STGAN ),29,0,5
dataset/preprocessed/training-data/text_generation/4,"In literature corpora ( eg : fantasy novels , sci - fi novels ) , the vocabulary does not vary significantly across the authors , but the manner of expression does , which is best captured at the level of sentences than words .",30,0,44
dataset/preprocessed/training-data/text_generation/4,The approach that this work takes in generating sentences with the writing style of one author is to make the adversarial model approximate the distribution of all sentences ( rather than words or characters ) in a latent space using skip - thought architecture .,31,0,45
dataset/preprocessed/training-data/text_generation/4,"The previous attempts on text generation have used the character and word - level embeddings instead with GANs , for example , in .",32,0,24
dataset/preprocessed/training-data/text_generation/4,This section introduces Skip - Thought Generative Adversarial Network with a background on models that it is based on .,33,0,20
dataset/preprocessed/training-data/text_generation/4,The Skip - Thought model induces embedding vectors for sentences present in training corpus .,34,0,15
dataset/preprocessed/training-data/text_generation/4,These vectors constitute the real distribution for the discriminator network .,35,0,11
dataset/preprocessed/training-data/text_generation/4,The generator network produces sentence vectors similar to those from the encoded real distribution .,36,0,15
dataset/preprocessed/training-data/text_generation/4,The generated vectors are sampled over training and decoded to produce sentences using a Skip - Thought decoder conditioned on the same text corpus .,37,0,25
dataset/preprocessed/training-data/text_generation/4,Skip - Thought Vectors,38,0,4
dataset/preprocessed/training-data/text_generation/4,"Skip - Thought is an encoder - decoder framework with an unsupervised approach to train a generic , distributed sentence encoder .",39,0,22
dataset/preprocessed/training-data/text_generation/4,The encoder maps sentences sharing semantic and syntactic properties to similar vector representations and the decoder reconstructs the surrounding sentences of an encoded passage .,40,0,25
dataset/preprocessed/training-data/text_generation/4,The sentence encoding approach draws inspiration from the skip - gram model in producing vector representations using previous and next sentences .,41,0,22
dataset/preprocessed/training-data/text_generation/4,"The Skip - Thought model uses an RNN encoder with GRU activations and an RNN decoder with conditional GRU , the combination being identical to the RNN encoder - decoder of ( Cho et al. , 2014 ) used in neural machine translation .",42,0,44
dataset/preprocessed/training-data/text_generation/4,Skip - Thought Architecture,43,0,4
dataset/preprocessed/training-data/text_generation/4,"For a given sentence tuple ( s i?1 , s i , s i + 1 ) , let wt i denote the t- th word for sentence s i , and let x ti denote it s word embedding .",44,0,41
dataset/preprocessed/training-data/text_generation/4,The model has three components : Encoder .,45,0,8
dataset/preprocessed/training-data/text_generation/4,"Encoded vectors for a sentence s i with N words w i , w i + 1 ,... , w n are computed by iterating over the following sequence of equations :",46,0,32
dataset/preprocessed/training-data/text_generation/4,"where ht i is a hidden state at each time step and interpreted as a sequence of words w 1 i , ... , w n i , t is the proposed state update at time t , z t is the update gate and rt is the reset gate .",47,0,51
dataset/preprocessed/training-data/text_generation/4,Both update gates take values between zero and one .,48,0,10
dataset/preprocessed/training-data/sentence_compression/3,A Language Model based Evaluator for Sentence Compression,2,1,8
dataset/preprocessed/training-data/sentence_compression/3,"We herein present a language - modelbased evaluator for deletion - based sentence compression , and viewed this task as a series of deletion - and - evaluation operations using the evaluator .",4,1,33
dataset/preprocessed/training-data/sentence_compression/3,"More specifically , the evaluator is a syntactic neural language model that is first built by learning the syntactic and structural collocation among words .",5,0,25
dataset/preprocessed/training-data/sentence_compression/3,"Subsequently , a series of trial - and - error deletion operations are conducted on the source sentences via a reinforcement learning framework to obtain the best target compression .",6,0,30
dataset/preprocessed/training-data/sentence_compression/3,"An empirical study shows that the proposed model can effectively generate more readable compression , comparable or superior to several strong baselines .",7,0,23
dataset/preprocessed/training-data/sentence_compression/3,"Furthermore , we introduce a 200 - sentence test set for a largescale dataset , setting a new baseline for the future research .",8,0,24
dataset/preprocessed/training-data/sentence_compression/3,Deletion - based sentence compression aims to delete unnecessary words from source sentence to form a short sentence ( compression ) while retaining grammatical and faithful to the underlying meaning of the source sentence .,10,0,35
dataset/preprocessed/training-data/sentence_compression/3,Previous works used either machine - learning - based approach or syntactic - tree - based approaches to yield most readable and informative compression .,11,0,25
dataset/preprocessed/training-data/sentence_compression/3,"For example , proposed a syntactictree - based method that considers the sentence compression task as an optimization problem by using integer linear programming , whereas viewed the sentence compression task as a sequence labeling problem using the recurrent neural network ( RNN ) , using maximum likelihood as the objective function for optimization .",12,0,55
dataset/preprocessed/training-data/sentence_compression/3,The latter sets a relatively strong baseline by training the model on a large - scale parallel corpus .,13,0,19
dataset/preprocessed/training-data/sentence_compression/3,"Although an RNN ( e.g. , Long short - term memory networks ) can implicitly model syntactic information , it still produces ungrammatical sentences .",14,0,25
dataset/preprocessed/training-data/sentence_compression/3,We argue that this is because ( i ) the labels ( or compressions ) are automatically yielded by employing the syntactic - tree - pruning method .,15,0,28
dataset/preprocessed/training-data/sentence_compression/3,"It thus contains some errors caused by syntactic tree parsing error , ( ii ) more importantly , the optimization objective of an RNN is the likelihood function that is based on individual words instead of readability ( or informativeness ) of the whole compressed sentence .",16,0,47
dataset/preprocessed/training-data/sentence_compression/3,A gap exists between optimization objective and evaluation .,17,0,9
dataset/preprocessed/training-data/sentence_compression/3,"As such , we are of great interest that : ( i ) can we take the readability of the whole compressed sentence as a learning objective and ( ii ) can grammar errors be recovered through a language - model - based evaluator to yield compression with better quality ?",18,0,51
dataset/preprocessed/training-data/sentence_compression/3,"To answer the above questions , a syntax - based neural language model is trained on large - scale datasets as a readability evaluator .",19,0,25
dataset/preprocessed/training-data/sentence_compression/3,The neural language model is supposed to learn the correct word collocations in terms of both syntax and semantics .,20,0,20
dataset/preprocessed/training-data/sentence_compression/3,"Subsequently , we formulate the deletionbased sentence compression as a series of trialand - error deletion operations through a reinforcement learning framework .",21,0,23
dataset/preprocessed/training-data/sentence_compression/3,"The policy network performs either RETAIN or REMOVE action to form a compression , and receives a reward ( e.g. , readability score ) to update the network .",22,0,29
dataset/preprocessed/training-data/sentence_compression/3,"The empirical study shows that the proposed method can produce more readable sentences that preserve the source sentences , comparable or superior to several strong baselines .",23,0,27
dataset/preprocessed/training-data/sentence_compression/3,"In short , our contributions are two - fold : ( i ) an effective syntaxbased evaluator is built as a post - hoc checker , yielding compression with better quality based upon the evaluation metrics ; ( ii ) a large scale news dataset with 1.02 million sentence compression pairs are compiled for this task in addition to 200 manually created sentences .",24,0,64
dataset/preprocessed/training-data/sentence_compression/3,We made it publicly available .,25,0,6
dataset/preprocessed/training-data/sentence_compression/3,Task and Framework,27,0,3
dataset/preprocessed/training-data/sentence_compression/3,"Formally , deletion - based sentence compression translates word tokens , ( w 1 , w 2 , ... , w n ) into a series of ones and zeros , ( l 1 , l 2 , ... , l n ) , where n refers to the length of the original sentence and l i ?",28,0,58
dataset/preprocessed/training-data/sentence_compression/3,"{ 0 , 1 }.",29,0,5
dataset/preprocessed/training-data/sentence_compression/3,"Here , "" 1 "" refers to RETAIN and "" 0 "" refers to REMOVE .",30,0,16
dataset/preprocessed/training-data/sentence_compression/3,"We first converted the word sequence into a dense vector representation through the parameter matrix E. Except for word embedding , ( e ( w 1 ) , e ( w 2 ) , ... , e ( w n ) ) , we also considered the part - of - speech tag and the dependency relation between w i and its headword as extra features .",31,0,67
dataset/preprocessed/training-data/sentence_compression/3,"Each part - ofspeech tag was mapped into a vector representation , ( p ( w 1 ) , p ( w 2 ) , ... , p (w n ) ) through the parameter matrix P , while each dependency relation was mapped into a vector representation , ( d ( w 1 ) , d ( w 2 ) , ... , d ( w n ) ) through the parameter matrix D .",32,0,76
dataset/preprocessed/training-data/sentence_compression/3,"Three vector representations are concatenated , [ e ( w i ) ; p ( w i ) ; d ( w i ) ] as the input to the next part , policy network .",33,0,36
dataset/preprocessed/training-data/sentence_compression/3,shows the graphical illustration of our model .,34,0,8
dataset/preprocessed/training-data/sentence_compression/3,"The policy network is a bi-directional RNN that uses the input [ e ( w i ) ; p ( w i ) ; d ( w i ) ] and yields the hidden states in the forward direction ,",35,0,40
dataset/preprocessed/training-data/sentence_compression/3,".. , hf n ) , and hidden states in the backward direction , ( h b 1 , h b 2 , ... , h b n ) .",36,0,30
dataset/preprocessed/training-data/sentence_compression/3,"Then , concatenation of hidden states in both directions , [h f i ; h bi ] are followed by a nonlinear layer to turn the output into a binary probability distribution ,",37,0,33
dataset/preprocessed/training-data/sentence_compression/3,"is a nonlinear function sigmoid , and Wis a parameter matrix .",39,0,12
dataset/preprocessed/training-data/sentence_compression/3,The policy network continues to sample actions from the binary probability distribution above until the whole action sequence is yielded .,40,0,21
dataset/preprocessed/training-data/sentence_compression/3,"In this task , binary actions space is { RETAIN , RE - MOVE } .",41,0,16
dataset/preprocessed/training-data/sentence_compression/3,"We turn the action sequence into the predicted compression , ( w 1 , w 2 , ... , w m ) , by deleting the words whose current action is REMOVE .",42,0,33
dataset/preprocessed/training-data/sentence_compression/3,"Then the ( w 1 , w 2 , ... , w m ) is fed into a pre-trained evaluator which will be described in the next section .",43,0,29
dataset/preprocessed/training-data/sentence_compression/3,Syntax - based Evaluator,44,0,4
dataset/preprocessed/training-data/sentence_compression/3,"The syntax - based evaluator should assess the degree to which the compressed sentence is grammatical , through being used as a reward function during the reinforcement learning phase .",45,0,30
dataset/preprocessed/training-data/sentence_compression/3,"It needs to satisfy three conditions : ( i ) grammatical compressions should obtain a higher score than ungrammatical compressions , ( ii ) for two ungrammatical compressions , it should be able to discriminate them through the score despite the ungrammaticality , ( iii ) lack of important parts ( such as the primary subject or verb ) in the original sentence should receive a greater penalty .",46,0,69
dataset/preprocessed/training-data/sentence_compression/3,"We therefore considered an ad - hoc evaluator , i.e. , the syntax - based language model ( evaluator - SLM ) for these requirements .",47,0,26
dataset/preprocessed/training-data/sentence_compression/3,"It integrates the part - of - speech tags and the dependency relations in the input , while the output to be predicted is the next word token .",48,0,29
dataset/preprocessed/training-data/sentence_compression/3,"We observed that the prediction of the next word could not only be based on the previous word but also the syntactic components , e.g. , for the part - of - speech tag , the noun is often followed by a verb instead of an adjective or adverb and the integration of the part - ofspeech tag allows the model to learn such correct word collocations .",49,0,68
dataset/preprocessed/training-data/sentence_compression/1,Sentence Compression by Deletion with LSTMs,2,1,6
dataset/preprocessed/training-data/sentence_compression/1,"We present an LSTM approach to deletion - based sentence compression where the task is to translate a sentence into a sequence of zeros and ones , corresponding to token deletion decisions .",4,1,33
dataset/preprocessed/training-data/sentence_compression/1,"We demonstrate that even the most basic version of the system , which is given no syntactic information ( no PoS or NE tags , or dependencies ) or desired compression length , performs surprisingly well : around 30 % of the compressions from a large test set could be regenerated .",5,0,52
dataset/preprocessed/training-data/sentence_compression/1,We compare the LSTM system with a competitive baseline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features .,6,0,29
dataset/preprocessed/training-data/sentence_compression/1,In an experiment with human raters the LSTMbased model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness .,7,0,21
dataset/preprocessed/training-data/sentence_compression/1,Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence .,9,1,20
dataset/preprocessed/training-data/sentence_compression/1,"Dozens of systems have been introduced in the past two decades and most of them are deletion - based : generated compressions are token subsequences of the input sentences , to name a few ) .",10,0,36
dataset/preprocessed/training-data/sentence_compression/1,Existing compression systems heavily use syntactic information to minimize chances of introducing grammatical mistakes in the output .,11,0,18
dataset/preprocessed/training-data/sentence_compression/1,A common approach is to use only some syntactic information or use syntactic features as signals in a statistical model .,12,0,21
dataset/preprocessed/training-data/sentence_compression/1,It is probably even more common to operate on syntactic trees directly ( dependency or constituency ) and generate compressions by pruning them .,13,0,24
dataset/preprocessed/training-data/sentence_compression/1,"Unfortunately , this makes such systems vulnerable to error propagation as there is noway to recover from an incorrect parse tree .",14,0,22
dataset/preprocessed/training-data/sentence_compression/1,"With the state - of - the - art parsing systems achieving about 91 points in labeled attachment accuracy , the problem is not a negligible one .",15,0,28
dataset/preprocessed/training-data/sentence_compression/1,"To our knowledge , there is no competitive compression system so far which does not require any linguistic preprocessing but tokenization .",16,0,22
dataset/preprocessed/training-data/sentence_compression/1,In this paper we research the following question : can a robust compression model be built which only uses tokens and has no access to syntactic or other linguistic information ?,17,0,31
dataset/preprocessed/training-data/sentence_compression/1,"While phenomena like long - distance relations may seem to make generation of grammatically correct compressions impossible , we are going to present an evidence to the contrary .",18,0,29
dataset/preprocessed/training-data/sentence_compression/1,"In particular , we will present a model which benefits from the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models ( LSTMs ) to output surprisingly readable and informative compressions .",19,0,39
dataset/preprocessed/training-data/sentence_compression/1,"Trained on a corpus of less than two million automatically extracted parallel sentences and using a standard tool to obtain word embeddings , in its best and most simple configuration it achieves 4.5 points out of 5 in readability and 3.8 points in informativeness in an extensive evaluation with human judges .",20,0,52
dataset/preprocessed/training-data/sentence_compression/1,"We believe that this is an important result as it may suggest a new direction for sentence compression research which is less tied to modeling linguistic structures , especially syntactic ones , than the compression work so far .",21,0,39
dataset/preprocessed/training-data/sentence_compression/1,The paper is organized as follows : Section 3 presents a competitive baseline which implements the system of for large training sets .,22,0,23
dataset/preprocessed/training-data/sentence_compression/1,The LSTM model and its three configurations are introduced in Section 4 .,23,0,13
dataset/preprocessed/training-data/sentence_compression/1,The evaluation set - up and a discussion on wins and losses with examples are presented in Section 5 which is followed by the conclusions .,24,0,26
dataset/preprocessed/training-data/sentence_compression/1,"The problem formulation we adopt in this paper is very simple : for every token in the input sentence we ask whether it should be kept or dropped , which translates into a sequence labeling problem with just two labels : one and zero .",26,0,45
dataset/preprocessed/training-data/sentence_compression/1,"The deletion approach is a standard one in compression research , although the problem is often formulated over the syntactic structure and not the raw token sequence .",27,0,28
dataset/preprocessed/training-data/sentence_compression/1,"That is , one usually drops constituents or prunes dependency edges .",28,0,12
dataset/preprocessed/training-data/sentence_compression/1,"Thus , the relation to existing compression work is that we also use the deletion approach .",29,0,17
dataset/preprocessed/training-data/sentence_compression/1,Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence .,30,0,34
dataset/preprocessed/training-data/sentence_compression/1,"Even though many of these models were proposed more than a decade ago , it is not until recently that they have empirically been shown to perform well .",31,0,29
dataset/preprocessed/training-data/sentence_compression/1,"Indeed , core problems in natural language processing such as translation , parsing , image captioning , or learning to execute small programs ) employed virtually the same principles - the use of Recurrent Neural Networks ( RNNs ) .",32,0,40
dataset/preprocessed/training-data/sentence_compression/1,"Thus , with regard to this line of research , our work comes closest to the recent machine translation work .",33,0,21
dataset/preprocessed/training-data/sentence_compression/1,An important difference is that we do not aim at building a model that generates compressions directly but rather a model which generates a sequence of deletion decisions .,34,0,29
dataset/preprocessed/training-data/sentence_compression/1,"A more complex translation model is also conceivable and may significantly advance work on compression by paraphrasing , of which there have not been many examples yet .",35,0,28
dataset/preprocessed/training-data/sentence_compression/1,"However , in this paper our goal is to demonstrate that a simple but robust deletionbased system can be built without using any linguistic features other than token boundaries .",36,0,30
dataset/preprocessed/training-data/sentence_compression/1,We leave experiments with paraphrasing models to future work .,37,0,10
dataset/preprocessed/training-data/sentence_compression/1,We compare our model against the system of which also formulates sentence compression as a binary sequence labeling problem .,39,0,20
dataset/preprocessed/training-data/sentence_compression/1,"In contrast to our proposal , it makes use of a large set of syntactic features which are treated as soft evidence .",40,0,23
dataset/preprocessed/training-data/sentence_compression/1,The presence or absence of these features is treated as signals which do not condition the output that the model can produce .,41,0,23
dataset/preprocessed/training-data/sentence_compression/1,Therefore the model is robust against noise present in the precomputed syntactic structures of the input sentences .,42,0,18
dataset/preprocessed/training-data/sentence_compression/1,The system was implemented based on the description by with two changes which were necessary due to the large size of the training data set used for model fitting .,43,0,30
dataset/preprocessed/training-data/sentence_compression/1,The first change was related to the learning procedure and the second one to the family of features used .,44,0,20
dataset/preprocessed/training-data/sentence_compression/1,"Regarding the learning procedure , the original model uses a large - margin learning framework , namely MIRA , but with some minor changes as presented by .",45,0,28
dataset/preprocessed/training-data/sentence_compression/1,"In this set - up , online learning is performed , and at each step an optimization procedure is made where K constraints are included , which correspond to the top - K solutions for a given training observation .",46,0,40
dataset/preprocessed/training-data/sentence_compression/1,"This optimization step is equivalent to a Quadratic Programming problem if K > 1 , which is time - costly to solve , and therefore not adequate for the large amount of data we used for training the model .",47,0,40
dataset/preprocessed/training-data/sentence_compression/1,"Furthermore , in his publication McDonald states clearly that different values of K did not actually have a major impact on the final performance of the model .",48,0,28
dataset/preprocessed/training-data/sentence_compression/1,"Consequently , and for the sake of being able to successfully train the model with largescale data , the learning procedure is implemented as a distributed structured perceptron with iterative parameter mixing , where each shard is processed with MIRA and K is set to 1 .",49,0,47
dataset/preprocessed/training-data/sentence_compression/0,Can Syntax Help ? Improving an LSTM - based Sentence Compression Model for New Domains,2,1,15
dataset/preprocessed/training-data/sentence_compression/0,"In this paper , we study how to improve the domain adaptability of a deletion - based Long Short - Term Memory ( LSTM ) neural network model for sentence compression .",4,0,32
dataset/preprocessed/training-data/sentence_compression/0,We hypothesize that syntactic information helps in making such models more robust across domains .,5,0,15
dataset/preprocessed/training-data/sentence_compression/0,We propose two major changes to the model : using explicit syntactic features and introducing syntactic constraints through Integer Linear Programming ( ILP ) .,6,0,25
dataset/preprocessed/training-data/sentence_compression/0,Our evaluation shows that the proposed model works better than the original model as well as a traditional non-neural - network - based model in a cross - domain setting .,7,0,31
dataset/preprocessed/training-data/sentence_compression/0,"Sentence compression is the task of compressing long , verbose sentences into short , concise ones .",9,0,17
dataset/preprocessed/training-data/sentence_compression/0,It can be used as a component of a text summarization system .,10,0,13
dataset/preprocessed/training-data/sentence_compression/0,shows two example input sentences and the compressed sentences written by human .,11,0,13
dataset/preprocessed/training-data/sentence_compression/0,The task has been studied for almost two decades .,12,0,10
dataset/preprocessed/training-data/sentence_compression/0,Early work on this task mostly relies on syntactic information such as constituency - based parse trees to help decide what to prune from a sentence or how to re-write a sentence .,13,0,33
dataset/preprocessed/training-data/sentence_compression/0,"Recently , there has been much interest in applying neural network models to solve the problem , where little or no linguistic analysis is performed except for tokenization .",14,0,29
dataset/preprocessed/training-data/sentence_compression/0,"Although neural network - based models have achieved good performance on this task recently , they tend to suffer from two problems :",15,0,23
dataset/preprocessed/training-data/sentence_compression/0,( 1 ) They require a large amount of data for training .,16,0,13
dataset/preprocessed/training-data/sentence_compression/0,"For example , used close to two : Examples of in - domain and out - ofdomain results by a standard abstractive sequenceto - sequence model trained on the Gigaword corpus .",17,0,32
dataset/preprocessed/training-data/sentence_compression/0,The first input sentence comes from the Gigaword corpus while the second input sentence comes from the written news corpus used by . million sentence pairs to train an LSTM - based sentence compression model .,18,0,36
dataset/preprocessed/training-data/sentence_compression/0,used about four million title - article pairs from the Gigaword corpus as training data .,19,0,16
dataset/preprocessed/training-data/sentence_compression/0,"Although it maybe easy to automatically obtain such training data in some domains ( e.g. , the news domain ) , for many other domains , it is not possible to obtain such a large amount of training data .",20,0,40
dataset/preprocessed/training-data/sentence_compression/0,( 2 ) These neural network models trained on data from one domain may notwork well on out - of - domain data .,21,0,24
dataset/preprocessed/training-data/sentence_compression/0,"For example , when we trained a standard neural sequence - to - sequence model 1 on 3.8 million title - article pairs from the Gigaword corpus and applied it to both in - domain data and out - of - domain data , we found that the performance on in - domain data was good but the performance on out - of - domain data could be very poor .",22,0,71
dataset/preprocessed/training-data/sentence_compression/0,Two example compressed sentences by this trained model are shown in to illustrate the comparison between in - domain and out - ofdomain performance .,23,0,25
dataset/preprocessed/training-data/sentence_compression/0,"The two limitations above imply that these neural network - based models may not be good at learning generalizable patterns , or in other words , they tend to overfit the training data .",24,0,34
dataset/preprocessed/training-data/sentence_compression/0,"This is not surprising because these models do not explicitly use much syntactic information , which is more general than lexical information .",25,0,23
dataset/preprocessed/training-data/sentence_compression/0,"In this paper , we aim to study how syntactic information can be incorporated into neural network models for sentence compression to improve their domain adaptability .",26,0,27
dataset/preprocessed/training-data/sentence_compression/0,We hope to train a model that performs well on both in - domain and out - ofdomain data .,27,0,20
dataset/preprocessed/training-data/sentence_compression/0,"To this end , we extend the deletionbased LSTM model for sentence compression by .",28,0,15
dataset/preprocessed/training-data/sentence_compression/0,"Although deletion - based sentence compression is not as flexible as abstractive sentence compression , we chose to work on deletion - based sentence compression for the following reason .",29,0,30
dataset/preprocessed/training-data/sentence_compression/0,"Abstractive sentence compression allows new words to be used in a compressed sentence , i.e. , words that do not occur in the input sentence .",30,0,26
dataset/preprocessed/training-data/sentence_compression/0,Oftentimes these new words serve as paraphrases of some words or phrases in the source sentence .,31,0,17
dataset/preprocessed/training-data/sentence_compression/0,"But to generate such paraphrases , the model needs to have seen them in the training data .",32,0,18
dataset/preprocessed/training-data/sentence_compression/0,"Because we are interested in a cross - domain setting , the paraphrases learned in one domain may notwork well in another domain if the two domains have very different vocabularies .",33,0,32
dataset/preprocessed/training-data/sentence_compression/0,"On the other hand , a deletion - based method does not face such a problem in a cross - domain setting .",34,0,23
dataset/preprocessed/training-data/sentence_compression/0,"Specifically , we propose two major changes to the model by : We explicitly introduce POS embeddings and dependency relation embeddings into the neural network model .",35,0,27
dataset/preprocessed/training-data/sentence_compression/0,"( 2 ) Inspired by a previous method , we formulate the final predictions as an Integer Linear Programming problem to incorporate constraints based on syntactic relations between words and expected lengths of the compressed sentences .",36,0,37
dataset/preprocessed/training-data/sentence_compression/0,"In addition to the two major changes above , we also use bi-directional LSTM to include contextual information from both directions into the model .",37,0,25
dataset/preprocessed/training-data/sentence_compression/0,"We evaluate our method using around 10,000 sentence pairs released by and two other data sets representing out - of - domain data .",38,0,24
dataset/preprocessed/training-data/sentence_compression/0,We test both in - domain and outof - domain performance .,39,0,12
dataset/preprocessed/training-data/sentence_compression/0,"The experimental results showed that our proposed method can achieve competitive performance compared with the original method in the single - domain setting but with much less training data ( around 8,000 sentence pairs for training instead of close to two million sentence pairs ) .",40,0,46
dataset/preprocessed/training-data/sentence_compression/0,"In the cross - domain setting , our proposed method can clearly outperform the original method .",41,0,17
dataset/preprocessed/training-data/sentence_compression/0,We also compare our method with a traditional ILP - based method using syntactic structures of sentences but not based on neural networks .,42,0,24
dataset/preprocessed/training-data/sentence_compression/0,We find that our method can outperform this baseline for both in - domain and out - of - domain data .,43,0,22
dataset/preprocessed/training-data/sentence_compression/0,"In this section , we present our sentence compression method that is aimed at working in a crossdomain setting .",45,0,20
dataset/preprocessed/training-data/sentence_compression/0,Recall that we focus on deletion - based sentence compression .,47,0,11
dataset/preprocessed/training-data/sentence_compression/0,Our problem setup is the same as that by .,48,0,10
dataset/preprocessed/training-data/sentence_compression/0,"Let us use s = ( w 1 , w 2 , . . . , w n ) to denote an input sentence , which consists of a sequence of words .",49,0,33
dataset/preprocessed/training-data/sentence_compression/2,Improving sentence compression by learning to predict gaze,2,1,8
dataset/preprocessed/training-data/sentence_compression/2,"We show how eye - tracking corpora can be used to improve sentence compression models , presenting a novel multi-task learning algorithm based on multi - layer LSTMs .",4,0,29
dataset/preprocessed/training-data/sentence_compression/2,We obtain performance competitive with or better than state - of - the - art approaches .,5,0,17
dataset/preprocessed/training-data/sentence_compression/2,"Readers fixate longer at rare words , words that are semantically ambiguous , and words that are mor -",6,0,19
dataset/preprocessed/training-data/sentence_compression/2,"Sentence compression is a basic operation in text simplification which has the potential to improve statistical machine translation and automatic summarization , as well as helping poor readers in need of assistive technologies .",8,0,34
dataset/preprocessed/training-data/sentence_compression/2,This work suggests using eye - tracking recordings for improving sentence compression for text simplification systems and is motivated by two observations : ( i ) Sentence compression is the task of automatically making sentences easier to process by shortening them .,9,0,42
dataset/preprocessed/training-data/sentence_compression/2,"( ii ) Eye -tracking measures such as first - pass reading time and time spent on regressions , i.e. , during second and later passes over the text , are known to correlate with perceived text difficulty .",10,0,39
dataset/preprocessed/training-data/sentence_compression/2,These two observations recently lead to suggest using eye - tracking measures as metrics in text simplification .,11,0,18
dataset/preprocessed/training-data/sentence_compression/2,We go beyond this by suggesting that eye - tracking recordings can be used to induce better models for sentence compression for text simplification .,12,0,25
dataset/preprocessed/training-data/sentence_compression/2,"Specifically , we show how to use existing eye - tracking recordings to improve the induction of Long Short - Term Memory models ( LSTMs ) for sentence compression .",13,0,30
dataset/preprocessed/training-data/sentence_compression/2,Our proposed model does not require that the gaze data and the compression data come from the same source .,14,0,20
dataset/preprocessed/training-data/sentence_compression/2,"Indeed , in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets .",15,0,24
dataset/preprocessed/training-data/sentence_compression/2,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users , based on their reading behavior .",16,0,30
dataset/preprocessed/training-data/sentence_compression/2,"Several approaches to sentence compression have been proposed , from noisy channel models over conditional random fields to tree - to - tree machine translation models ( Woodsend and Lapata , 2011 ) .",17,0,34
dataset/preprocessed/training-data/sentence_compression/2,"More recently , successfully used LSTMs for sentence compression on a large scale parallel dataset .",18,0,16
dataset/preprocessed/training-data/sentence_compression/2,"We do not review the literature here , and only compare to .",19,0,13
dataset/preprocessed/training-data/sentence_compression/2,We present a novel multi-task learning approach to sentence compression using labelled data for sentence compression and a disjoint eye - tracking corpus .,21,0,24
dataset/preprocessed/training-data/sentence_compression/2,Our method is fully competitive with state - of the - art across three corpora .,22,0,16
dataset/preprocessed/training-data/sentence_compression/2,Our code is made publicly available at https://bitbucket.org/soegaard/ gaze - mtl 16 .,23,0,13
dataset/preprocessed/training-data/sentence_compression/2,phologically complex .,24,0,3
dataset/preprocessed/training-data/sentence_compression/2,"These are also words that are likely to be replaced with simpler ones in sentence simplification , but it is not clear that they are words that would necessarily be removed in the context of sentence compression .",25,0,38
dataset/preprocessed/training-data/sentence_compression/2,show that syntactic complexity ( measured as dependency locality ) is also an important predictor of reading time .,26,0,19
dataset/preprocessed/training-data/sentence_compression/2,"Phrases that are often removed in sentence compressionlike fronted phrases , parentheticals , floating quantifiers , etc. - are often associated with non-local dependencies .",27,0,25
dataset/preprocessed/training-data/sentence_compression/2,"Also , there is evidence that people are more likely to fixate on the first word in a constituent than on its second word .",28,0,25
dataset/preprocessed/training-data/sentence_compression/2,"Being able to identify constituent borders is important for sentence compression , and reading fixation data may help our model learn a representation of our data that makes it easy to identify constituent boundaries .",29,0,35
dataset/preprocessed/training-data/sentence_compression/2,"In the experiments below , we learn models to predict the first pass duration of word fixations and the total duration of regressions to a word .",30,0,27
dataset/preprocessed/training-data/sentence_compression/2,These two measures constitute a perfect separation of the total reading time of each word split between the first pass and subsequent passes .,31,0,24
dataset/preprocessed/training-data/sentence_compression/2,Both measures are described below .,32,0,6
dataset/preprocessed/training-data/sentence_compression/2,They are both discretized into six bins as follows with only non-zero values contributing to the calculation of the standard deviation ( SD ) : 0 : measure = 0 or 1 : measure < 1 SD below reader 's average or 2 : measure < .5 SD below reader 's average or 3 : measure < .5 above reader 's average or 4 : measure > . 5 SD above reader 's average or 5 : measure >,33,0,79
dataset/preprocessed/training-data/sentence_compression/2,"1 SD above reader 's average First pass duration measures the total time spent reading a word first time it is fixated , including any immediately following re-fixations of the same word .",34,0,33
dataset/preprocessed/training-data/sentence_compression/2,"This measure correlates with word length , frequency and ambiguity because long words are likely to attract several fixations in a row unless they are particularly easily predicted or recognized .",35,0,31
dataset/preprocessed/training-data/sentence_compression/2,This effect arises because long words are less likely to fit inside the fovea of the eye .,36,0,18
dataset/preprocessed/training-data/sentence_compression/2,Note that for this measure the value 0 indicates that the word was not fixated by this reader .,37,0,19
dataset/preprocessed/training-data/sentence_compression/2,5 Regression duration measures the total time spent fixating a word after the gaze has already left it once .,39,0,20
dataset/preprocessed/training-data/sentence_compression/2,"This measure belongs to the group of late measures , i.e. , measures that are sensitive to the later cognitive processing stages including interpretation and integration of already decoded words .",40,0,31
dataset/preprocessed/training-data/sentence_compression/2,"Since the reader by definition has already had a chance to recognize the word , regressions are associated with semantic confusion and contradiction , incongruence and syntactic complexity , as famously experienced in garden path sentences .",41,0,37
dataset/preprocessed/training-data/sentence_compression/2,For this measure the value 0 indicates that the word was read at most once by this reader .,42,0,19
dataset/preprocessed/training-data/sentence_compression/2,See for an example of first pass duration and regression duration annotations for one reader and sentence .,43,0,18
dataset/preprocessed/training-data/sentence_compression/2,3 Sentence compression using multi - task deep bi - LSTMs,44,0,11
dataset/preprocessed/training-data/sentence_compression/2,Words FIRST PASS REGRESSIONS,45,0,4
dataset/preprocessed/training-data/sentence_compression/2,"Most recent approaches to sentence compression make use of syntactic analysis , either by operating directly on trees or by incorporating syntactic information in their model .",46,0,27
dataset/preprocessed/training-data/sentence_compression/2,"presented an approach to sentence compression using LSTMs with word embeddings , but without syntactic features .",47,0,17
dataset/preprocessed/training-data/sentence_compression/2,"We introduce a third way of using syntactic annotation by jointly learning a sequence model for predicting CCG supertags , in addition to our gaze and compression models .",48,0,29
dataset/preprocessed/training-data/sentence_compression/2,"Bi-directional recurrent neural networks ( bi - RNNs ) read in sequences in both regular and reversed order , enabling conditioning predictions on both left and right context .",49,0,29
dataset/preprocessed/training-data/relation_extraction/7,Neural Relation Extraction via Inner - Sentence Noise Reduction and Transfer Learning,2,1,12
dataset/preprocessed/training-data/relation_extraction/7,Extracting relations is critical for knowledge base completion and construction in which distant supervised methods are widely used to extract relational facts automatically with the existing knowledge bases .,4,0,29
dataset/preprocessed/training-data/relation_extraction/7,"However , the automatically constructed datasets comprise amounts of low - quality sentences containing noisy words , which is neglected by current distant supervised methods resulting in unacceptable precisions .",5,0,30
dataset/preprocessed/training-data/relation_extraction/7,"To mitigate this problem , we propose a novel word - level distant supervised approach for relation extraction .",6,0,19
dataset/preprocessed/training-data/relation_extraction/7,We first build Sub - Tree Parse ( STP ) to remove noisy words thatare irrelevant to relations .,7,0,19
dataset/preprocessed/training-data/relation_extraction/7,Then we construct a neural network inputting the subtree while applying the entity - wise attention to identify the important semantic features of relational words in each instance .,8,0,29
dataset/preprocessed/training-data/relation_extraction/7,"To make our model more robust against noisy words , we initialize our network with a priori knowledge learned from the relevant task of entity classification by transfer learning .",9,0,30
dataset/preprocessed/training-data/relation_extraction/7,We conduct extensive experiments using the corpora of New York Times ( NYT ) and Freebase .,10,0,17
dataset/preprocessed/training-data/relation_extraction/7,Experiments show that our approach is effective and improves the are a of Precision / Recall ( PR ) from 0.35 to 0.39 over the state - of - the - art work .,11,0,34
dataset/preprocessed/training-data/relation_extraction/7,Relation extraction aims to extract relations between pairs of marked entities in raw texts .,13,1,15
dataset/preprocessed/training-data/relation_extraction/7,Traditional supervised methods are time - consuming for the requirement of large - scale manually labeled data .,14,0,18
dataset/preprocessed/training-data/relation_extraction/7,"Thus , propose the distant supervised relation extraction , in which amounts of sentences are crawled from web pages of New York Times ( NYT ) and labeled with a known knowledge base automatically .",15,0,35
dataset/preprocessed/training-data/relation_extraction/7,"The method assumes that if two entities have a relation in a known knowledge base , all instances that mention these two entities will express the same relation .",16,0,29
dataset/preprocessed/training-data/relation_extraction/7,"Obviously , this assumption is too strong , since a sentence that mentions the two entities does not necessarily express the relation contained in a known knowledge base .",17,0,29
dataset/preprocessed/training-data/relation_extraction/7,"As described in , the assumption leads to the wrong labeling problem .",18,0,13
dataset/preprocessed/training-data/relation_extraction/7,"In order to tackle the wrong labeling problem , various multi-instance learning methods are adopted by mitigating noise between sentences .",19,0,21
dataset/preprocessed/training-data/relation_extraction/7,"Despite the wrong labeling problem , distant supervised methods may suffer from the low quality of sentences which derive from the large - scale automatically constructed dataset by crawling web pages .",20,0,32
dataset/preprocessed/training-data/relation_extraction/7,"To handle the problem of low - quality sentences , we have to face two major challenges :",21,0,18
dataset/preprocessed/training-data/relation_extraction/7,( 1 ) Reduce word - level noise within sentences ; ( 2 ) Improve the robustness of relation extraction against noise .,22,0,23
dataset/preprocessed/training-data/relation_extraction/7,"To explain the influence of word - level noise within sentences , we consider the following sentence as an example :",23,0,21
dataset/preprocessed/training-data/relation_extraction/7,"[ It is no accident that the main event will feature the junior welterweight champion miguel cotto , a puerto rican , against Paul Malignaggi , an Italian American from Brooklyn . ] , where Paul Malignaggi and Brooklyn are two corresponding entities .",24,0,44
dataset/preprocessed/training-data/relation_extraction/7,"The subsentence [ Paul Malignaggi , an Italian American from Brooklyn . ] keeps enough words to express the relation / people / person / place of birth , and the other words could be regarded as noise that may hamper the extractor 's performance .",25,0,46
dataset/preprocessed/training-data/relation_extraction/7,"Meanwhile , as shown in , half of the original sentences are longer than 40 words , which means that there are many irrelevant words inside sentences .",26,0,28
dataset/preprocessed/training-data/relation_extraction/7,"To be more detail , there are about 12 noisy words in each sentence on average , and 99.4 % of sentences in the NYT - 10 dataset have noise .",27,0,31
dataset/preprocessed/training-data/relation_extraction/7,"Although the Shortest Dependency Path ( SDP ) proposed by tries to get rid of irrelevant words for relation extraction , it is not suitable to handle such informal sentences .",28,0,31
dataset/preprocessed/training-data/relation_extraction/7,"Moreover , word - level attention has been leveraged to alleviate the impact of noisy words , but it weakens the importance of entity features for relation extraction .",29,0,29
dataset/preprocessed/training-data/relation_extraction/7,"As for the second challenge , a robust model could extract precise relation features even from low - quality sentences containing noisy words .",30,0,24
dataset/preprocessed/training-data/relation_extraction/7,"However , previous neural methods are always lacking in robustness because parameters are initialized randomly and hard to tune with noisy training data , resulting in the poor performance of extractors .",31,0,32
dataset/preprocessed/training-data/relation_extraction/7,"Inspired by , initializing neural networks with a priori knowledge learned from relevant tasks by transfer learning could improve the robustness of the target task .",32,0,26
dataset/preprocessed/training-data/relation_extraction/7,"For the relation extraction , entity type classification can be used as the relevant task since entity types provide abundant background knowledge .",33,0,23
dataset/preprocessed/training-data/relation_extraction/7,"For instance , the sentence [ Alfead Kahn , the Cornell - University economist who led the fight to deregulate airplanes . ] has a relation business / person / company , which is hard to decide without the information that Alfead Kahn is a person and Cornell - University is a company .",34,0,54
dataset/preprocessed/training-data/relation_extraction/7,"Therefore , type features learned from entity type classification are proper a priori knowledge to initialize the relation extractor .",35,0,20
dataset/preprocessed/training-data/relation_extraction/7,"In this paper , we propose a novel word - level approach for distant supervised relation extraction by reducing inner-sentence noise and improving robustness against noisy words .",36,1,28
dataset/preprocessed/training-data/relation_extraction/7,"To reduce innersentence noise , we utilize a novel Sub - Tree Parse ( STP ) method to remove irrelevant words by intercepting a subtree under the parent of entities ' lowest common ancestor .",37,0,35
dataset/preprocessed/training-data/relation_extraction/7,"As shown in , the average length of the parsed sentences is much shorter .",38,0,15
dataset/preprocessed/training-data/relation_extraction/7,"Furthermore , the entity - wise attention is adopted to alleviate the influence of noisy words in the subtree and emphasize the task - relevant features .",39,0,27
dataset/preprocessed/training-data/relation_extraction/7,"To tackle the second challenge , we initialize our model parameters with a priori knowledge learned from the entity type classification task by transfer learning .",40,0,26
dataset/preprocessed/training-data/relation_extraction/7,The experimental results show that our model can achieve satisfactory performance among the state - of - the - art works .,41,0,22
dataset/preprocessed/training-data/relation_extraction/7,Our contributions are summarized as follows :,42,0,7
dataset/preprocessed/training-data/relation_extraction/7,"To handle the problem of low - quality sentences , we propose the STP to remove noisy words of sentences and the entity - wise attention mechanism to enhance semantic features of relational words .",43,0,35
dataset/preprocessed/training-data/relation_extraction/7,"We first propose to initialize the neural relation extractor with a priori knowledge learned from entity type classification , which strengthens its robustness against low - quality corpus .",44,0,29
dataset/preprocessed/training-data/relation_extraction/7,"Our model achieves significant results for distant supervised relation extraction , which improves the Precision / Recall ( PR ) curve are a from 0.35 to 0.39 and increases top 100 predictions by 6.3 % over the state - of - the - art work .",45,0,46
dataset/preprocessed/training-data/relation_extraction/7,The distant supervised method plays an increasingly essential role in relation extraction due to its less requirement of human labor ) .,47,0,22
dataset/preprocessed/training-data/relation_extraction/7,"However , an evident drawback of the method is the wrong labeling problem .",48,0,14
dataset/preprocessed/training-data/relation_extraction/7,"Thus , multi-instance and multi-label learning methods are proposed to address this issue .",49,0,14
dataset/preprocessed/training-data/relation_extraction/3,Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers,2,1,11
dataset/preprocessed/training-data/relation_extraction/3,The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .,4,1,30
dataset/preprocessed/training-data/relation_extraction/3,"This paper proposes a new solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve a new state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark .",5,1,47
dataset/preprocessed/training-data/relation_extraction/3,Our solution is built on top of the pre-trained self - attentive models ( Transformer ) .,6,0,17
dataset/preprocessed/training-data/relation_extraction/3,"Since our method uses a single - pass to compute all relations at once , it scales to larger datasets easily ; which makes it more usable in real - world applications .",7,0,33
dataset/preprocessed/training-data/relation_extraction/3,Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .,10,1,22
dataset/preprocessed/training-data/relation_extraction/3,"A solution to this task is essential for many downstream NLP applications such as automatic knowledge - base completion , knowledge base question answering , and symbolic approaches for visual question answering , etc .",11,0,35
dataset/preprocessed/training-data/relation_extraction/3,One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .,12,1,29
dataset/preprocessed/training-data/relation_extraction/3,"Because in real - world applications , whose input paragraphs dominantly contain multiple pairs of entities , an efficient and effective solution for MRE has more important and more practical implications .",13,1,32
dataset/preprocessed/training-data/relation_extraction/3,"However , nearly all existing approaches for MRE tasks 2014 ; adopt some variations of the singlerelation extraction ( SRE ) approach , which treats each pair of entity mentions as an independent instance , and requires multiple passes of encoding for the multiple pairs of entities .",14,0,48
dataset/preprocessed/training-data/relation_extraction/3,"The drawback of this approach is obvious - it is computationally expensive and this issue becomes more severe when the input paragraph is large , making this solution impossible to implement when the encoding step involves deep models .",15,0,39
dataset/preprocessed/training-data/relation_extraction/3,"This work presents a solution that can resolve the inefficient multiple - passes issue of existing solutions for MRE by encoding the input only once , which significantly increases the efficiency and scalability .",16,0,34
dataset/preprocessed/training-data/relation_extraction/3,"Specifically , the proposed solution is built on top of the existing transformer - based , pretrained general - purposed language encoders .",17,0,23
dataset/preprocessed/training-data/relation_extraction/3,"In this paper we use Bidirectional Encoder Representations from Transformers ( BERT ) as the transformer - based encoder , but this solution is not limited to using BERT alone .",18,0,31
dataset/preprocessed/training-data/relation_extraction/3,The two novel modifications to the original BERT architecture are : ( 1 ) we introduce a structured prediction layer for predicting multiple relations for different entity pairs ; and ( 2 ) we make the selfattention layers aware of the positions of all en-tities in the input paragraph .,19,0,50
dataset/preprocessed/training-data/relation_extraction/3,"To the best of our knowledge , this work is the first promising solution that can solve MRE tasks with such high efficiency ( encoding the input in one - pass ) and effectiveness ( achieve a new state - of - the - art performance ) , as proved on the ACE 2005 benchmark .",20,0,56
dataset/preprocessed/training-data/relation_extraction/3,MRE is an important task as it is an essential prior step for many downstream tasks such as automatic knowledge - base completion and questionanswering .,22,0,26
dataset/preprocessed/training-data/relation_extraction/3,Popular MRE benchmarks include ACE and ERE .,23,0,8
dataset/preprocessed/training-data/relation_extraction/3,"In MRE , given as a text paragraph x = {x 1 , . . . , x N } and M mentions e = {e 1 , . . . , e M } as input , the goal is to predict the relation r ij for each mention pair ( e i , e j ) either belongs to one class of a list of pre-defined relations R or falls into a special class NA indicating no relation .",24,0,81
dataset/preprocessed/training-data/relation_extraction/3,"This paper uses "" entity mention "" , "" mention "" and "" entity "" interchangeably .",25,0,17
dataset/preprocessed/training-data/relation_extraction/3,"Existing MRE approaches are based on either feature and model architecture selection techniques , or domain adaptations approaches .",26,0,19
dataset/preprocessed/training-data/relation_extraction/3,"But these approaches require multiple passes of encoding over the paragraph , as they treat a MRE task as multiple passes of a SRE task .",27,0,26
dataset/preprocessed/training-data/relation_extraction/3,This section describes the proposed one - pass encoding MRE solution .,29,0,12
dataset/preprocessed/training-data/relation_extraction/3,"The solution is built upon BERT with a structured prediction layer to enable BERT to predict multiple relations with onepass encoding , and an entity - aware self - attention mechanism to infuse the relational information with regard to multiple entities at each layer of hidden states .",30,0,48
dataset/preprocessed/training-data/relation_extraction/3,The framework is illustrated in 1 .,31,0,7
dataset/preprocessed/training-data/relation_extraction/3,"It is worth mentioning that our solution can easily use other transformer - based encoders besides BERT , e.g..",32,0,19
dataset/preprocessed/training-data/relation_extraction/3,Structured Prediction with BERT for MRE,33,0,6
dataset/preprocessed/training-data/relation_extraction/3,The BERT model has been successfully applied to various NLP tasks .,34,0,12
dataset/preprocessed/training-data/relation_extraction/3,"However , the final prediction layers used in the original model is not applicable to MRE tasks .",35,0,18
dataset/preprocessed/training-data/relation_extraction/3,The MRE task essentially requires to perform edge predictions over a graph with entities as nodes .,36,0,17
dataset/preprocessed/training-data/relation_extraction/3,"Inspired by , we propose that we can first encode the input paragraph using BERT .",37,0,16
dataset/preprocessed/training-data/relation_extraction/3,"Thus , the representation for a pair of entity mentions ( e i , e j ) can be denoted as oi and o j respectively .",38,0,27
dataset/preprocessed/training-data/relation_extraction/3,"In the case of a mention e i consist of multiple hidden states ( due to the byte pair encoding ) , oi is aggregated via average - pooling over the hidden states of the corresponding tokens in the last BERT layer .",39,0,43
dataset/preprocessed/training-data/relation_extraction/3,"We then concatenate oi and o j denoted as [ o i : o j ] , and pass it to a linear classifier 2 to predict the relation",40,0,29
dataset/preprocessed/training-data/relation_extraction/3,where W L ?,41,0,4
dataset/preprocessed/training-data/relation_extraction/3,R 2 dzl .,42,0,4
dataset/preprocessed/training-data/relation_extraction/3,"dz is the dimension of BERT embedding at each token position , and l is the number of relation labels .",43,0,21
dataset/preprocessed/training-data/relation_extraction/3,Entity - Aware Self - Attention based on Relative Distance,44,0,10
dataset/preprocessed/training-data/relation_extraction/3,This section describes how we encode multiplerelations information into the model .,45,0,12
dataset/preprocessed/training-data/relation_extraction/3,The key concept is to use the relative distances between words and entities to encode the positional information for each entity .,46,0,22
dataset/preprocessed/training-data/relation_extraction/3,This information is propagated through different layers via attention computations .,47,0,11
dataset/preprocessed/training-data/relation_extraction/3,"Following , for each pair of word tokens ( x i , x j ) with the input representations from the previous layer ash i and h j , we extend the computation of self - attention z i as :",48,0,41
dataset/preprocessed/training-data/relation_extraction/3,"are the parameters of the model , and dz is the dimension of the output from the self - attention layer .",49,0,22
dataset/preprocessed/training-data/relation_extraction/10,Span - Level Model for Relation Extraction,2,1,7
dataset/preprocessed/training-data/relation_extraction/10,Relation Extraction is the task of identifying entity mention spans in raw text and then identifying relations between pairs of the entity mentions .,4,0,24
dataset/preprocessed/training-data/relation_extraction/10,Recent approaches for this spanlevel task have been token - level models which have inherent limitations .,5,0,17
dataset/preprocessed/training-data/relation_extraction/10,"They can not easily define and implement span - level features , can not model overlapping entity mentions and have cascading errors due to the use of sequential decoding .",6,0,30
dataset/preprocessed/training-data/relation_extraction/10,"To address these concerns , we present a model which directly models all possible spans and performs joint entity mention detection and relation extraction .",7,0,25
dataset/preprocessed/training-data/relation_extraction/10,We report a new state - of - the - art performance of 62.83 F 1 ( prev best was 60.49 ) on the ACE2005 dataset .,8,0,27
dataset/preprocessed/training-data/relation_extraction/10,Many NLP tasks follow the pattern of taking raw text as input and then : detecting relevant spans and classifying the relations between those spans .,10,0,26
dataset/preprocessed/training-data/relation_extraction/10,"Examples of this include Relation Extraction , Coreference Resolution and Semantic Role Labeling .",11,0,14
dataset/preprocessed/training-data/relation_extraction/10,This class of NLP problems are inherently span - level tasks .,12,0,12
dataset/preprocessed/training-data/relation_extraction/10,"This paper focuses on Relation Extraction ( RE ) , which is the task of entity mention detection and classifying the relations between each pair of those mentions .",13,1,29
dataset/preprocessed/training-data/relation_extraction/10,We report a new state - of - the - art performance of 62.83 F 1 ( prev best was 60.49 ) on the ACE2005 dataset .,14,0,27
dataset/preprocessed/training-data/relation_extraction/10,"Here is a simple example of Relation Extraction for the sentence , "" Washington , D.C. is the capital of the USA "" .",15,0,24
dataset/preprocessed/training-data/relation_extraction/10,"Step 1 , Entity Mention Detection will detect the spans "" Washington , D.C. "" and "" USA "" as LOCATIONS .",16,0,22
dataset/preprocessed/training-data/relation_extraction/10,"Step 2 , Relation Extraction will classify all directed pairs of detected entity mentions .",17,0,15
dataset/preprocessed/training-data/relation_extraction/10,"It will classify the directed pair ( "" Washington , D.C. "" , "" USA "" ) as having the relation IS CAPITAL OF .",18,0,25
dataset/preprocessed/training-data/relation_extraction/10,"But the directed pair ( "" USA "" , "" Washington , D.C. "" ) will be classified as having no relation .",19,0,23
dataset/preprocessed/training-data/relation_extraction/10,"In more complex cases , each entity could participate in multiple different relations .",20,0,14
dataset/preprocessed/training-data/relation_extraction/10,"Since , work on RE has revolved around end - to - end systems : single models which first perform entity mention detection and then relation extraction .",21,1,28
dataset/preprocessed/training-data/relation_extraction/10,These recent works have used sequential token - level methods for both the steps .,22,0,15
dataset/preprocessed/training-data/relation_extraction/10,Token - level models are primarily constrained by the fact that each token has a single fixed representation while each token is apart of many different spans .,23,0,28
dataset/preprocessed/training-data/relation_extraction/10,"To model and extract spans , these token - level models have to resort to approximate span - level features which are increasingly indirect and expensive :",24,0,27
dataset/preprocessed/training-data/relation_extraction/10,"Tree - LSTMs , CRFs , Beam Search and Pointer Networks .",25,0,12
dataset/preprocessed/training-data/relation_extraction/10,Their usage of the BILOU token - tagging scheme makes modelling overlapping entities impossible .,26,0,15
dataset/preprocessed/training-data/relation_extraction/10,"In general , these tokenlevel models are sequential in nature and hence have cascading errors .",27,0,16
dataset/preprocessed/training-data/relation_extraction/10,Another end - to - end approach for RE is to use a simple span - level model .,28,0,19
dataset/preprocessed/training-data/relation_extraction/10,"A model which creates explicit representations for all possible spans , uses them for the entity mention detection step and then explicitly compares ordered pairs of spans for the relation extraction step .",29,0,33
dataset/preprocessed/training-data/relation_extraction/10,Such a model is not constrained like the token - level models because it can define direct span - specific features for each span inexpensively .,30,0,26
dataset/preprocessed/training-data/relation_extraction/10,"Since each possible span is separately considered , selecting overlapping entity mentions is possible .",31,0,15
dataset/preprocessed/training-data/relation_extraction/10,Predicting one span as an entity no longer blocks another span from being predicted as an entity .,32,0,18
dataset/preprocessed/training-data/relation_extraction/10,This approach models each possible span independently and in parallel i.e. it is not sequential and does not suffer from cascad - ing errors .,33,0,25
dataset/preprocessed/training-data/relation_extraction/10,Such models have recently found success in similar NLP tasks like Coreference Resolution and Semantic Role Labeling .,34,0,18
dataset/preprocessed/training-data/relation_extraction/10,"In this paper , we present such a span - level model for Relation Extraction .",35,0,16
dataset/preprocessed/training-data/relation_extraction/10,We propose a simple bi - LSTM based model which generates span representations for each possible span .,36,0,18
dataset/preprocessed/training-data/relation_extraction/10,The span representations are used to perform entity mention detection on all spans in parallel .,37,0,16
dataset/preprocessed/training-data/relation_extraction/10,The same span representations are then used to perform relation extraction on all pairs of detected entity mentions .,38,0,19
dataset/preprocessed/training-data/relation_extraction/10,We evaluated the performance of our model on the ACE2005 dataset and report a new startof - the - art F 1 score of 62.83 for Relation Extraction .,39,0,29
dataset/preprocessed/training-data/relation_extraction/10,"Given text input , Relation Extraction involves two steps : span detection and classification of the relation between pairs of detected spans .",41,0,23
dataset/preprocessed/training-data/relation_extraction/10,"In the RE literature , these are more commonly called Entity Mention Detection and Relation Extraction respectively .",42,0,18
dataset/preprocessed/training-data/relation_extraction/10,"An earlier line of research has focused on only the second step , assuming that the arguments of the relations are given by some other system / oracle .",43,0,29
dataset/preprocessed/training-data/relation_extraction/10,The more interesting problem is joint Entity Mention Detection and Relation Extraction .,44,0,13
dataset/preprocessed/training-data/relation_extraction/10,"More interesting because it simultaneously addresses both steps , enriches embeddings from losses related to both sub - tasks and only requires using a single model during test .",45,0,29
dataset/preprocessed/training-data/relation_extraction/10,Past approaches include Integer Linear Programming and Probabilistic Graphical Models .,46,0,11
dataset/preprocessed/training-data/relation_extraction/10,modeled this joint task as a Structured Prediction problem and since then most work on RE has revolved around endto - end systems which do the joint task .,47,0,29
dataset/preprocessed/training-data/relation_extraction/10,A common theme in current end - to - end models is the use of token - level models .,48,0,20
dataset/preprocessed/training-data/relation_extraction/10,"For the entity mention detection step , recent works have used the BILOU tokentagging scheme .",49,0,16
dataset/preprocessed/training-data/relation_extraction/9,Relation Classification via Multi - Level Attention CNNs,2,1,8
dataset/preprocessed/training-data/relation_extraction/9,Relation classification is a crucial ingredient in numerous information extraction systems seeking to mine structured facts from text .,4,0,19
dataset/preprocessed/training-data/relation_extraction/9,"We propose a novel convolutional neural network architecture for this task , relying on two levels of attention in order to better discern patterns in heterogeneous contexts .",5,0,28
dataset/preprocessed/training-data/relation_extraction/9,"This architecture enables endto - end learning from task - specific labeled data , forgoing the need for external knowledge such as explicit dependency structures .",6,0,26
dataset/preprocessed/training-data/relation_extraction/9,"Experiments show that our model outperforms previous state - of - the - art methods , including those relying on much richer forms of prior knowledge .",7,0,27
dataset/preprocessed/training-data/relation_extraction/9,Relation classification is the task of identifying the semantic relation holding between two nominal entities in text .,9,0,18
dataset/preprocessed/training-data/relation_extraction/9,"It is a crucial component in natural language processing systems that need to mine explicit facts from text , e.g. for various information extraction applications as well as for question answering and knowledge base completion .",10,0,36
dataset/preprocessed/training-data/relation_extraction/9,"For instance , given the example input "" Fizzy and meat cause heart disease and . "" with annotated target entity mentions e 1 = "" drinks "" and e 2 = "" diabetes "" , the goal would be to automatically recognize that this sentence expresses a causeeffect relationship between e 1 and e 2 , for which we use the notation Cause - Effect ( e 1 ,e 2 ) .",11,0,73
dataset/preprocessed/training-data/relation_extraction/9,"Accurate relation classification facilitates precise sentence interpretations , discourse processing , and higherlevel NLP tasks .",12,0,16
dataset/preprocessed/training-data/relation_extraction/9,"Thus , * Equal contribution .",13,0,6
dataset/preprocessed/training-data/relation_extraction/9,Corresponding author .,14,0,3
dataset/preprocessed/training-data/relation_extraction/9,Email : liuzy@tsinghua.edu.cn relation classification has attracted considerable attention from researchers over the course of the past decades .,15,0,19
dataset/preprocessed/training-data/relation_extraction/9,"In the example given above , the verb corresponds quite closely to the desired target relation .",16,0,17
dataset/preprocessed/training-data/relation_extraction/9,"However , in the wild , we encounter a multitude of different ways of expressing the same kind of relationship .",17,0,21
dataset/preprocessed/training-data/relation_extraction/9,"This challenging variability can be lexical , syntactic , or even pragmatic in nature .",18,0,15
dataset/preprocessed/training-data/relation_extraction/9,"An effective solution needs to be able to account for useful semantic and syntactic features not only for the meanings of the target entities at the lexical level , but also for their immediate context and for the over all sentence structure .",19,0,43
dataset/preprocessed/training-data/relation_extraction/9,"Thus , it is not surprising that numerous featureand kernel - based approaches have been proposed , many of which rely on a full - fledged NLP stack , including POS tagging , morphological analysis , dependency parsing , and occasionally semantic analysis , as well as on knowledge resources to capture lexical and semantic features .",20,0,57
dataset/preprocessed/training-data/relation_extraction/9,"In recent years , we have seen a move towards deep architectures thatare capable of learning relevant representations and features without extensive manual feature engineering or use of external resources .",21,0,31
dataset/preprocessed/training-data/relation_extraction/9,"A number of convolutional neural network ( CNN ) , recurrent neural network ( RNN ) , and other neural architectures have been proposed for relation classification .",22,0,28
dataset/preprocessed/training-data/relation_extraction/9,"Still , these models often fail to identify critical cues , and many of them still require an external dependency parser .",23,0,22
dataset/preprocessed/training-data/relation_extraction/9,We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches .,24,0,16
dataset/preprocessed/training-data/relation_extraction/9,Our key contributions are as follows :,25,0,7
dataset/preprocessed/training-data/relation_extraction/9,"Our CNN architecture relies on a novel multi-level attention mechanism to capture both entity - specific attention ( primary attention at the input level , with respect to the target entities ) and relation - specific pooling attention ( secondary attention with respect to the target relations ) .",27,0,49
dataset/preprocessed/training-data/relation_extraction/9,"This allows it to detect more subtle cues despite the heterogeneous structure of the input sentences , enabling it to automatically learn which parts are relevant for a given classification .",28,0,31
dataset/preprocessed/training-data/relation_extraction/9,2 . We introduce a novel pair - wise margin - based objective function that proves superior to standard loss functions .,29,0,22
dataset/preprocessed/training-data/relation_extraction/9,"3 . We obtain the new state - of - the - art results for relation classification with an F1 score of 88.0 % on the SemEval 2010 Task 8 dataset , outperforming methods relying on significantly richer prior knowledge .",30,0,41
dataset/preprocessed/training-data/relation_extraction/9,"Apart from a few unsupervised clustering methods , the majority of work on relation classification has been supervised , typically cast as a standard multiclass or multi-label classification task .",32,0,30
dataset/preprocessed/training-data/relation_extraction/9,"Traditional feature - based methods rely on a set of features computed from the output of an explicit linguistic preprocessing step , while kernel - based methods make use of convolution tree kernels , subsequence kernels ( Mooney and Bunescu , 2005 ) , or dependency tree kernels .",33,0,49
dataset/preprocessed/training-data/relation_extraction/9,"These methods thus all depend either on carefully handcrafted features , often chosen on a trial - and - error basis , or on elaborately designed kernels , which in turn are often derived from other pre-trained NLP tools or lexical and semantic resources .",34,0,45
dataset/preprocessed/training-data/relation_extraction/9,"Although such approaches can benefit from the external NLP tools to discover the discrete structure of a sentence , syntactic parsing is error-prone and relying on its success may also impede performance .",35,0,33
dataset/preprocessed/training-data/relation_extraction/9,"Further downsides include their limited lexical generalization abilities for unseen words and their lack of robustness when applied to new domains , genres , or languages .",36,0,27
dataset/preprocessed/training-data/relation_extraction/9,"In recent years , deep neural networks have shown promising results .",37,0,12
dataset/preprocessed/training-data/relation_extraction/9,The Recursive Matrix - Vector Model ( MV - RNN ) by sought to capture the compositional aspects of the sentence semantics by exploiting syntactic trees .,38,0,27
dataset/preprocessed/training-data/relation_extraction/9,"proposed a deep convolutional neural network with softmax classification , extracting lexical and sentence level features .",39,0,17
dataset/preprocessed/training-data/relation_extraction/9,"However , these approaches still depend on additional features from lexical resources and NLP toolkits .",40,0,16
dataset/preprocessed/training-data/relation_extraction/9,"proposed the Factor - based Compositional Embedding Model , which uses syntactic dependency trees together with sentence - level embeddings .",41,0,21
dataset/preprocessed/training-data/relation_extraction/9,"In addition to dos , who proposed the Ranking CNN ( CR - CNN ) model with a class embedding matrix , similarly observed that LSTM - based RNNs are outperformed by models using CNNs , due to limited linguistic structure captured in the network architecture .",42,0,47
dataset/preprocessed/training-data/relation_extraction/9,"Some more elaborate variants have been proposed to address this , including bidirectional LSTMs , deep recurrent neural networks , and bidirectional treestructured LSTM - RNNs .",43,0,27
dataset/preprocessed/training-data/relation_extraction/9,"Several recent works also reintroduce a dependency tree - based design , e.g. , RNNs operating on syntactic trees , shortest dependency path - based CNNs , and the SDP - LSTM model .",44,0,34
dataset/preprocessed/training-data/relation_extraction/9,"Finally , Nguyen and Grishman ( 2015 ) train both CNNs and RNNs and variously aggregate their outputs using voting , stacking , or log - linear modeling .",45,0,29
dataset/preprocessed/training-data/relation_extraction/9,"Although these recent models achieve solid results , ideally , we would want a simple yet effective architecture that does not require dependency parsing or training multiple models .",46,0,29
dataset/preprocessed/training-data/relation_extraction/9,"Our experiments in Section 4 demonstrate that we can indeed achieve this , while also obtaining substantial improvements in terms of the obtained F1 scores .",47,0,26
dataset/preprocessed/training-data/relation_extraction/9,The Proposed Model,48,0,3
dataset/preprocessed/training-data/relation_extraction/9,"Given a sentence S with a labeled pair of entity mentions e 1 and e 2 ( as in our example from Section 1 ) , relation classification is the task of identifying the semantic relation holding between e 1 and e 2 among a set of candidate relation types .",49,0,51
dataset/preprocessed/training-data/relation_extraction/1,Simple BERT Models for Relation Extraction and Semantic Role Labeling,2,1,10
dataset/preprocessed/training-data/relation_extraction/1,We present simple BERT - based models for relation extraction and semantic role labeling .,4,0,15
dataset/preprocessed/training-data/relation_extraction/1,"In recent years , state - of - the - art performance has been achieved using neural models by incorporating lexical and syntactic features such as part - of - speech tags and dependency trees .",5,0,36
dataset/preprocessed/training-data/relation_extraction/1,"In this paper , extensive experiments on datasets for these two tasks show that without using any external features , a simple BERT - based model can achieve state - of - the - art performance .",6,0,37
dataset/preprocessed/training-data/relation_extraction/1,"To our knowledge , we are the first to successfully apply BERT in this manner .",7,0,16
dataset/preprocessed/training-data/relation_extraction/1,Our models provide strong baselines for future research .,8,0,9
dataset/preprocessed/training-data/relation_extraction/1,Relation extraction and semantic role labeling ( SRL ) are two fundamental tasks in natural language understanding .,10,1,18
dataset/preprocessed/training-data/relation_extraction/1,The task of relation extraction is to discern whether a relation exists between two entities in a sentence .,11,0,19
dataset/preprocessed/training-data/relation_extraction/1,"For example , in the sentence "" Obama was born in Honolulu "" , "" Obama "" is the subject entity and "" Honolulu "" is the object entity .",12,0,30
dataset/preprocessed/training-data/relation_extraction/1,"The task of a relation extraction model is to identify the relation between the entities , which is per:city of birth ( birth city for a person ) .",13,0,29
dataset/preprocessed/training-data/relation_extraction/1,"For SRL , the task is to extract the predicate - argument structure of a sentence , determining "" who did what to whom "" , "" when "" , "" where "" , etc .",14,1,36
dataset/preprocessed/training-data/relation_extraction/1,Both capabilities are useful in several downstream tasks such as question answering and open information extraction .,15,0,17
dataset/preprocessed/training-data/relation_extraction/1,"State - of - the - art neural models for both tasks typically rely on lexical and syntactic features , such as part - of - speech tags , syntactic trees , and global decoding constraints .",16,0,37
dataset/preprocessed/training-data/relation_extraction/1,"In particular , argue that syntactic features are necessary to achieve competitive performance in dependency - based SRL .",17,0,19
dataset/preprocessed/training-data/relation_extraction/1,also showed that dependency tree features can further improve relation extraction performance .,18,0,13
dataset/preprocessed/training-data/relation_extraction/1,"Although syntactic features are no doubt helpful , a known challenge is that parsers are not available for every language , and even when available , they may not be sufficiently robust , especially for out - of - domain text , which may even hurt performance .",19,0,48
dataset/preprocessed/training-data/relation_extraction/1,"Recently , the NLP community has seen excitement around neural models that make heavy use of pretraining based on language modeling .",20,0,22
dataset/preprocessed/training-data/relation_extraction/1,"The latest development is BERT , which has shown impressive gains in a wide variety of natural language tasks ranging from sentence classification to sequence labeling .",21,0,27
dataset/preprocessed/training-data/relation_extraction/1,"A natural question follows : can we leverage these pretrained models to further push the state of the art in relation extraction and semantic role labeling , without relying on lexical or syntactic features ?",22,0,35
dataset/preprocessed/training-data/relation_extraction/1,The answer is yes .,23,0,5
dataset/preprocessed/training-data/relation_extraction/1,We show that simple neural architectures built on top of BERT yields state - of - the - art performance on a variety of benchmark datasets for these two tasks .,24,0,31
dataset/preprocessed/training-data/relation_extraction/1,The remainder of this paper describes our models and experimental results for relation extraction and semantic role labeling in turn .,25,0,21
dataset/preprocessed/training-data/relation_extraction/1,BERT for Relation Extraction,26,0,4
dataset/preprocessed/training-data/relation_extraction/1,"The standard formulation of semantic role labeling decomposes into four subtasks : predicate detection , predicate sense dis ambiguation , argument identification , and argument classification .",28,0,27
dataset/preprocessed/training-data/relation_extraction/1,There are two representations for argument annotation : span - based and dependency - based .,29,0,16
dataset/preprocessed/training-data/relation_extraction/1,"Semantic banks such as PropBank usually represent arguments as syntactic constituents ( spans ) , whereas the CoNLL 2008 and 2009 shared tasks propose dependency - based SRL , where the goal is to identify the syntactic heads of arguments rather than the entire span .",30,0,46
dataset/preprocessed/training-data/relation_extraction/1,"Here , we follow to unify these two annotation schemes into one framework , without any declarative constraints for decoding .",31,0,21
dataset/preprocessed/training-data/relation_extraction/1,"For several SRL benchmarks , such as , and 2012 , the predicate is given during both training and testing .",32,0,21
dataset/preprocessed/training-data/relation_extraction/1,"Thus , in this paper , we only discuss predicate dis ambiguation and argument identification and classification .",33,0,18
dataset/preprocessed/training-data/relation_extraction/1,Predicate sense dis ambiguation .,34,0,5
dataset/preprocessed/training-data/relation_extraction/1,The predicate dis ambiguation task is to identify the correct meaning of a predicate in a given context .,35,0,19
dataset/preprocessed/training-data/relation_extraction/1,"As an example , for the sentence "" Barack Obama went to Paris "" , the predicate went has sense "" motion "" and has sense label 01 .",36,0,29
dataset/preprocessed/training-data/relation_extraction/1,We formulate this task as sequence labeling .,37,0,8
dataset/preprocessed/training-data/relation_extraction/1,"The input sentence is fed into the WordPiece tokenizer , which splits some words into sub-tokens .",38,0,17
dataset/preprocessed/training-data/relation_extraction/1,The predicate token is tagged with the sense label .,39,0,10
dataset/preprocessed/training-data/relation_extraction/1,"Following the original BERT paper , two labels are used for the remaining tokens : ' O' for the first ( sub - ) token of any word and ' X ' for any remaining fragments .",40,0,37
dataset/preprocessed/training-data/relation_extraction/1,"We feed the sequences into the BERT encoder to obtain the contextual representation H. A "" predicate indicator "" embedding is then concatenated to the contextual representation to distinguish the predicate tokens from nonpredicate ones .",41,0,36
dataset/preprocessed/training-data/relation_extraction/1,The final prediction is made using a one - hidden - layer MLP over the label set .,42,0,18
dataset/preprocessed/training-data/relation_extraction/1,Argument identification and classification .,43,0,5
dataset/preprocessed/training-data/relation_extraction/1,This task is to detect the argument spans or argument syntactic heads and assign them the correct semantic role labels .,44,0,21
dataset/preprocessed/training-data/relation_extraction/1,"In the above example , "" Barack Obama "" is the ARG1 of the predicate went , meaning the entity in motion .",45,0,23
dataset/preprocessed/training-data/relation_extraction/1,"Formally , our task is to predict a sequence z given a sentence - predicate pair ( X , v) as input , where the label set draws from the cross of the standard BIO tagging scheme and the arguments of the predicate ( e.g. , B - ARG1 ) .",46,0,51
dataset/preprocessed/training-data/relation_extraction/1,"The model architecture is illustrated in , at the point in the inference process where it is outputting a tag for the token "" Barack "" .",47,0,27
dataset/preprocessed/training-data/relation_extraction/1,"In order to encode the sentence in a predicate - aware manner , we design the input as [ [ CLS ] sentence [ SEP ] predicate ] , allowing the representation of the predicate to interact with the entire sentence via appropriate attention mechanisms .",48,0,46
dataset/preprocessed/training-data/relation_extraction/1,The input sequence as described above is fed into the BERT encoder .,49,0,13
dataset/preprocessed/training-data/relation_extraction/5,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,2,1,9
dataset/preprocessed/training-data/relation_extraction/5,Dependency trees help relation extraction models capture long - range relations between words .,4,0,14
dataset/preprocessed/training-data/relation_extraction/5,"However , existing dependency - based models either neglect crucial information ( e.g. , negation ) by pruning the dependency trees too aggressively , or are computationally inefficient because it is difficult to parallelize over different tree structures .",5,0,39
dataset/preprocessed/training-data/relation_extraction/5,"We propose an extension of graph convolutional networks that is tailored for relation extraction , which pools information over arbitrary dependency structures efficiently in parallel .",6,0,26
dataset/preprocessed/training-data/relation_extraction/5,"To incorporate relevant information while maximally removing irrelevant content , we further apply a novel pruning strategy to the input trees by keeping words immediately around the shortest path between the two entities among which a relation might hold .",7,0,40
dataset/preprocessed/training-data/relation_extraction/5,"The resulting model achieves state - of - the - art performance on the large - scale TACRED dataset , outperforming existing sequence and dependency - based neural models .",8,0,30
dataset/preprocessed/training-data/relation_extraction/5,"We also show through detailed analysis that this model has complementary strengths to sequence models , and combining them further improves the state of the art .",9,0,27
dataset/preprocessed/training-data/relation_extraction/5,* Equal contribution .,10,0,4
dataset/preprocessed/training-data/relation_extraction/5,The order of authorship was decided by a tossed coin .,11,0,11
dataset/preprocessed/training-data/relation_extraction/5,"Relation extraction involves discerning whether a relation exists between two entities in a sentence ( often termed subject and object , respectively ) .",13,0,24
dataset/preprocessed/training-data/relation_extraction/5,"Successful relation extraction is the cornerstone of applications requiring relational understanding of unstructured text on a large scale , such as question answering , knowledge base population , and biomedical knowledge discovery .",14,0,33
dataset/preprocessed/training-data/relation_extraction/5,"Models making use of dependency parses of the input sentences , or dependency - based models , : An example modified from the TAC KBP challenge corpus .",15,0,28
dataset/preprocessed/training-data/relation_extraction/5,"A subtree of the original UD dependency tree between the subject ( "" he "" ) and object ( "" Mike Cane "" ) is also shown , where the shortest dependency path between the entities is highlighted in bold .",16,0,41
dataset/preprocessed/training-data/relation_extraction/5,"Note that negation ( "" not "" ) is off the dependency path .",17,0,14
dataset/preprocessed/training-data/relation_extraction/5,"have proven to be very effective in relation extraction , because they capture long - range syntactic relations thatare obscure from the surface form alone ( e.g. , when long clauses or complex scoping are present ) .",18,0,38
dataset/preprocessed/training-data/relation_extraction/5,Traditional feature - based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees .,19,0,22
dataset/preprocessed/training-data/relation_extraction/5,"However , these models face the challenge of sparse feature spaces and are brittle to lexical variations .",20,0,18
dataset/preprocessed/training-data/relation_extraction/5,More recent neural models address this problem with distributed representations built from their computation graphs formed along parse trees .,21,0,20
dataset/preprocessed/training-data/relation_extraction/5,One common approach to leverage dependency information is to perform bottom - up or top - down computation along the parse tree or the subtree below the lowest common ancestor ( LCA ) of the entities .,22,0,37
dataset/preprocessed/training-data/relation_extraction/5,"Another popular approach , inspired by , is to reduce the parse tree to the shortest dependency path between the entities .",23,0,22
dataset/preprocessed/training-data/relation_extraction/5,"However , these models suffer from several drawbacks .",24,0,9
dataset/preprocessed/training-data/relation_extraction/5,"Neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient , because aligning trees for efficient batch training is usually nontrivial .",25,0,28
dataset/preprocessed/training-data/relation_extraction/5,"Models based on the shortest dependency path between the subject and object are computationally more efficient , but this simplifying assumption has major limitations as well .",26,0,27
dataset/preprocessed/training-data/relation_extraction/5,"shows a real - world example where crucial information ( i.e. , negation ) would be excluded when the model is restricted to only considering the dependency path .",27,0,29
dataset/preprocessed/training-data/relation_extraction/5,"In this work , we propose a novel extension of the graph convolutional network ) that is tailored for relation extraction .",28,0,22
dataset/preprocessed/training-data/relation_extraction/5,"Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .",29,0,28
dataset/preprocessed/training-data/relation_extraction/5,"We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .",30,0,35
dataset/preprocessed/training-data/relation_extraction/5,"We test our model on the popular SemEval 2010 Task 8 dataset and the more recent , larger TAC - RED dataset .",31,0,23
dataset/preprocessed/training-data/relation_extraction/5,"On both datasets , our model not only outperforms existing dependency - based neural models by a significant margin when combined with the new pruning technique , but also achieves a 10 - 100x speedup over existing tree - based models .",32,0,42
dataset/preprocessed/training-data/relation_extraction/5,"On TACRED , our model further achieves the state - of - the - art performance , surpassing a competitive neural sequence model baseline .",33,0,25
dataset/preprocessed/training-data/relation_extraction/5,"This model also exhibits complementary strengths to sequence models on TACRED , and combining these two model types through simple prediction interpolation further improves the state of the art .",34,0,30
dataset/preprocessed/training-data/relation_extraction/5,"To recap , our main contributions are : ( i ) we propose a neural model for relation extraction based on graph convolutional networks , which allows it to efficiently pool information over arbitrary dependency structures ; ( ii ) we present a new pathcentric pruning technique to help dependencybased models maximally remove irrelevant information without damaging crucial content to improve their robustness ; ( iii ) we present detailed analysis on the model and the pruning technique , and show that dependency - based models have complementary strengths with sequence models .",35,0,93
dataset/preprocessed/training-data/relation_extraction/5,"In this section , we first describe graph convolutional networks ( GCNs ) over dependency tree structures , and then we introduce an architecture that uses GCNs at its core for relation extraction .",37,0,34
dataset/preprocessed/training-data/relation_extraction/5,Graph Convolutional Networks over Dependency Trees,38,0,6
dataset/preprocessed/training-data/relation_extraction/5,The graph convolutional network is an adaptation of the convolutional neural network for encoding graphs .,39,0,16
dataset/preprocessed/training-data/relation_extraction/5,"Given a graph with n nodes , we can represent the graph structure with an n n adjacency matrix A where A ij = 1 if there is an edge going from node i to node j.",40,0,37
dataset/preprocessed/training-data/relation_extraction/5,"In an L-layer GCN , if we denote by h ( l?1 ) i the input vector and h ( l ) i the output vector of node i at the l - th layer , a graph convolution operation can be written as",41,0,44
dataset/preprocessed/training-data/relation_extraction/5,"where W ( l ) is a linear transformation , b ( l ) a bias term , and ?",42,0,20
dataset/preprocessed/training-data/relation_extraction/5,"a nonlinear function ( e.g. , ReLU ) .",43,0,9
dataset/preprocessed/training-data/relation_extraction/5,"Intuitively , during each graph convolution , each node gathers and summarizes information from its neighboring nodes in the graph .",44,0,21
dataset/preprocessed/training-data/relation_extraction/5,"We adapt the graph convolution operation to model dependency trees by converting each tree into its corresponding adjacency matrix A , where A ij = 1 if there is a dependency edge between tokens i and j .",45,0,38
dataset/preprocessed/training-data/relation_extraction/5,"However , naively applying the graph convolution operation in Equation ( 1 ) could lead to node representations with drastically different magnitudes , since the degree of a token varies a lot .",46,0,33
dataset/preprocessed/training-data/relation_extraction/5,This could bias our sentence representation towards favoring high - degree nodes regardless of the information carried in the node ( see details in Section 2.2 ) .,47,0,28
dataset/preprocessed/training-data/relation_extraction/5,"Furthermore , the information in h ( l?1 ) i is never carried over to hi , since nodes never connect to themselves in a dependency tree .",48,0,28
dataset/preprocessed/training-data/relation_extraction/5,"We resolve these issues by normalizing the activations in the graph convolution before feeding it through the nonlinearity , and adding self - loops to each node in the graph :",49,0,31
dataset/preprocessed/training-data/relation_extraction/8,Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks,2,1,10
dataset/preprocessed/training-data/relation_extraction/8,Two problems arise when using distant supervision for relation extraction .,4,0,11
dataset/preprocessed/training-data/relation_extraction/8,"First , in this method , an already existing knowledge base is heuristically aligned to texts , and the alignment results are treated as labeled data .",5,0,27
dataset/preprocessed/training-data/relation_extraction/8,"However , the heuristic alignment can fail , resulting in wrong label problem .",6,0,14
dataset/preprocessed/training-data/relation_extraction/8,"In addition , in previous approaches , statistical models have typically been applied toad hoc features .",7,0,17
dataset/preprocessed/training-data/relation_extraction/8,The noise that originates from the feature extraction process can cause poor performance .,8,0,14
dataset/preprocessed/training-data/relation_extraction/8,"In this paper , we propose a novel model dubbed the Piecewise Convolutional Neural Networks ( PCNNs ) with multi-instance learning to address these two problems .",9,0,27
dataset/preprocessed/training-data/relation_extraction/8,"To solve the first problem , distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account .",10,0,28
dataset/preprocessed/training-data/relation_extraction/8,"To address the latter problem , we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features .",11,0,25
dataset/preprocessed/training-data/relation_extraction/8,Experiments show that our method is effective and outperforms several competitive baseline methods .,12,0,14
dataset/preprocessed/training-data/relation_extraction/8,"In relation extraction , one challenge that is faced when building a machine learning system is the generation of training examples .",15,1,22
dataset/preprocessed/training-data/relation_extraction/8,"One common technique for coping with this difficulty is distant supervision ) which assumes that if two entities have a relationship in a known knowledge base , then all sentences that mention these two entities will express that relationship in someway .",16,0,42
dataset/preprocessed/training-data/relation_extraction/8,shows an example of the auto - matic labeling of data through distant supervision .,17,0,15
dataset/preprocessed/training-data/relation_extraction/8,"In this example , Apple and Steve Jobs are two related entities in Freebase 1 .",18,0,16
dataset/preprocessed/training-data/relation_extraction/8,All sentences that contain these two entities are selected as training instances .,19,0,13
dataset/preprocessed/training-data/relation_extraction/8,The distant supervision strategy is an effective method of automatically labeling training data .,20,0,14
dataset/preprocessed/training-data/relation_extraction/8,"However , it has two major shortcomings when used for relation extraction .",21,0,13
dataset/preprocessed/training-data/relation_extraction/8,"First , the distant supervision assumption is too strong and causes the wrong label problem .",22,0,16
dataset/preprocessed/training-data/relation_extraction/8,A sentence that mentions two entities does not necessarily express their relation in a knowledge base .,23,0,17
dataset/preprocessed/training-data/relation_extraction/8,It is possible that these two entities may simply share the same topic .,24,0,14
dataset/preprocessed/training-data/relation_extraction/8,"For instance , the upper sentence indeed expresses the "" company / founders "" relation in .",25,0,17
dataset/preprocessed/training-data/relation_extraction/8,"The lower sentence , however , does not express this relation but is still selected as a training instance .",26,0,20
dataset/preprocessed/training-data/relation_extraction/8,This will hinder the performance of a model trained on such noisy data .,27,0,14
dataset/preprocessed/training-data/relation_extraction/8,"Second , previous methods have typically applied supervised models to elaborately designed features when obtained the labeled data through distant supervision .",28,0,22
dataset/preprocessed/training-data/relation_extraction/8,These features are often derived from preexisting Natural Language Processing ( NLP ) tools .,29,0,15
dataset/preprocessed/training-data/relation_extraction/8,"Since errors inevitably exist in NLP tools , the use of traditional features leads to error propagation or accumulation .",30,0,20
dataset/preprocessed/training-data/relation_extraction/8,"Distant supervised relation extraction generally ad - dresses corpora from the Web , including many informal texts .",31,0,18
dataset/preprocessed/training-data/relation_extraction/8,shows the sentence length distribution of a benchmark distant supervision dataset that was developed by .,32,0,16
dataset/preprocessed/training-data/relation_extraction/8,Approximately half of the sentences are longer than 40 words .,33,0,11
dataset/preprocessed/training-data/relation_extraction/8,showed that the accuracy of syntactic parsing decreases significantly with increasing sentence length .,34,0,14
dataset/preprocessed/training-data/relation_extraction/8,"Therefore , when using traditional features , the problem of error propagation or accumulation will not only exist , it will grow more serious .",35,0,25
dataset/preprocessed/training-data/relation_extraction/8,"In this paper , we propose a novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs ) with multi-instance learning to address the two problems described above .",36,0,30
dataset/preprocessed/training-data/relation_extraction/8,"To address the first problem , distant supervised relation extraction is treated as a multi-instance problem similar to previous studies .",37,1,21
dataset/preprocessed/training-data/relation_extraction/8,"In multi-instance problem , the training set consists of many bags , and each contains many instances .",38,0,18
dataset/preprocessed/training-data/relation_extraction/8,"The labels of the bags are known ; however , the labels of the instances in the bags are unknown .",39,0,21
dataset/preprocessed/training-data/relation_extraction/8,We design an objective function at the bag level .,40,0,10
dataset/preprocessed/training-data/relation_extraction/8,"In the learning process , the uncertainty of instance labels can be taken into account ; this alleviates the wrong label problem .",41,0,23
dataset/preprocessed/training-data/relation_extraction/8,"To address the second problem , we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by .",42,0,22
dataset/preprocessed/training-data/relation_extraction/8,"Our proposal is an extension of , in which a single max pooling operation is utilized to determine the most significant features .",43,0,23
dataset/preprocessed/training-data/relation_extraction/8,"Although this operation has been shown to be effective for textual feature representation , it reduces the size of the hidden layers too rapidly and can not capture the structural information between two entities .",44,0,35
dataset/preprocessed/training-data/relation_extraction/8,"For example , to identify the relation between Steve Jobs and Apple in , we need to specify the entities and extract the structural features between them .",45,0,28
dataset/preprocessed/training-data/relation_extraction/8,Several approaches have employed manually crafted features that attempt to model such structural information .,46,0,15
dataset/preprocessed/training-data/relation_extraction/8,These approaches usually consider both internal and external contexts .,47,0,10
dataset/preprocessed/training-data/relation_extraction/8,A sentence is inherently divided into three segments according to the two given entities .,48,0,15
dataset/preprocessed/training-data/relation_extraction/8,"The internal context includes the characters inside the two entities , and the external context involves the characters around the two entities .",49,0,23
dataset/preprocessed/training-data/relation_extraction/6,Context - Aware Representations for Knowledge Base Relation Extraction,2,1,9
dataset/preprocessed/training-data/relation_extraction/6,We demonstrate that for sentence - level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation .,4,1,26
dataset/preprocessed/training-data/relation_extraction/6,Our architecture uses an LSTM - based encoder to jointly learn representations for all relations in a single sentence .,5,0,20
dataset/preprocessed/training-data/relation_extraction/6,We combine the context representations with an attention mechanism to make the final prediction .,6,0,15
dataset/preprocessed/training-data/relation_extraction/6,We use the Wikidata knowledge base to construct a dataset of multiple relations per sentence and to evaluate our approach .,7,0,21
dataset/preprocessed/training-data/relation_extraction/6,"Compared to a baseline system , our method results in an average error reduction of 24 % on a held - out set of relations .",8,0,26
dataset/preprocessed/training-data/relation_extraction/6,The code and the dataset to replicate the experiments are made available at https://github.com/ukplab.,9,0,14
dataset/preprocessed/training-data/relation_extraction/6,1 Unique IDs in Wikidata have a Q-prefix for entities and a P-prefix for relations .,10,0,16
dataset/preprocessed/training-data/relation_extraction/6,The main goal of relation extraction is to determine a type of relation between two target entities that appear together in a text .,12,1,24
dataset/preprocessed/training-data/relation_extraction/6,"In this paper , we consider the sentential relation extraction task : to each occurrence of the target entity pair e 1 , e 2 in some sentence s one has to assign a relation type r from a given set R. A triple e 1 , r , e 2 is called a relation instance and we refer to the relation of the target entity pair as target relation .",13,1,71
dataset/preprocessed/training-data/relation_extraction/6,Relation extraction is a fundamental task that enables a wide range of semantic applications from question answering to fact checking .,14,0,21
dataset/preprocessed/training-data/relation_extraction/6,"For relation extraction , it is crucial to be able to extract relevant features from the sentential context .",15,0,19
dataset/preprocessed/training-data/relation_extraction/6,Modern approaches focus just on the relation between the target entities and disregard other relations that might be present in the same sentence .,16,0,24
dataset/preprocessed/training-data/relation_extraction/6,"For example , in order to correctly identify the relation type between the movie e 1 and the director e 2 in ( 1 ) , it is important to separate out the INSTANCE OF relation between the movie and it s type e 3 :",17,0,46
dataset/preprocessed/training-data/relation_extraction/6,( 1 ),18,0,3
dataset/preprocessed/training-data/relation_extraction/6,[e 1 Star Wars VII ] is an American [e 3 space opera epic film ] directed by [e 2 J. J. Abrams ] .,19,0,25
dataset/preprocessed/training-data/relation_extraction/6,We present a novel architecture that considers other relations in the sentence as a context for predicting the label of the target relation .,20,0,24
dataset/preprocessed/training-data/relation_extraction/6,We use the term context relations to refer to them throughout the paper .,21,0,14
dataset/preprocessed/training-data/relation_extraction/6,Our architecture uses an LSTM - based encoder to jointly learn representations for all relations in a single sentence .,22,0,20
dataset/preprocessed/training-data/relation_extraction/6,The representation of the target relation and representations of the context relations are combined to make the final prediction .,23,0,20
dataset/preprocessed/training-data/relation_extraction/6,To facilitate the experiments we construct a dataset that contains multiple positive and negative relation instances per sentence .,24,0,19
dataset/preprocessed/training-data/relation_extraction/6,We employ a fast growing community managed knowledge base ( KB ) Wikidata to build the dataset .,25,0,18
dataset/preprocessed/training-data/relation_extraction/6,Our main contribution is the new neural network architecture for extracting relations between an entity pair that takes into account other relations in the sentence .,26,0,26
dataset/preprocessed/training-data/relation_extraction/6,We employ a neural network to automatically encode the target relation and the sentential context into a fixed - size feature vector .,28,0,23
dataset/preprocessed/training-data/relation_extraction/6,and have used manually engineered features based on part - of - speech tags and dependency parses to represent the target relations .,29,0,23
dataset/preprocessed/training-data/relation_extraction/6,"Recently , and have shown that one can successfully apply convo-lutional neural networks to extract sentence - level features automatically .",30,0,21
dataset/preprocessed/training-data/relation_extraction/6,Most of the methods focus on predicting a single relation type based on the combined evidence from all of the occurrences of an entity pair .,31,0,26
dataset/preprocessed/training-data/relation_extraction/6,"and assign multiple relation types to each entity pair , such that the predictions are tied to particular occurrences of the entity pair .",32,0,24
dataset/preprocessed/training-data/relation_extraction/6,We regard the relation extraction task similarly and predict relation types on the sentence level .,33,0,16
dataset/preprocessed/training-data/relation_extraction/6,We use a distant supervision approach to construct the dataset .,34,0,11
dataset/preprocessed/training-data/relation_extraction/6,and have applied it to create relation extraction datasets for a large - scale KB .,35,0,16
dataset/preprocessed/training-data/relation_extraction/6,"In contrast to our dataset , their data contains a single relation instance per sentence .",36,0,16
dataset/preprocessed/training-data/relation_extraction/6,That makes it incompatible with our method .,37,0,8
dataset/preprocessed/training-data/relation_extraction/6,All of the aforementioned approaches consider just the relation between the target entities and disregard other relations that might be present in the same sentence .,38,0,26
dataset/preprocessed/training-data/relation_extraction/6,Our method uses context relations to predict the target relation .,39,0,11
dataset/preprocessed/training-data/relation_extraction/6,One can also use other types of structured information from the nearby context to improve relation extraction .,40,0,18
dataset/preprocessed/training-data/relation_extraction/6,have combined named entity recognition and relation extraction in a structured prediction approach to improve both tasks .,41,0,18
dataset/preprocessed/training-data/relation_extraction/6,"Later , have implemented an end - to - end neural network to construct a context representation for joint entity and relation extraction .",42,0,24
dataset/preprocessed/training-data/relation_extraction/6,"Finally , have designed global features and constraints to extract multiple events and their arguments from the same sentence .",43,0,20
dataset/preprocessed/training-data/relation_extraction/6,"We do n't implement global constraints in our approach , since unlike events and arguments , there are no restrictions as to what relations can appear together .",44,0,28
dataset/preprocessed/training-data/relation_extraction/6,Instead we encode all relations in the same context into fixed - size vectors and use an attention mechanism to combine them .,45,0,23
dataset/preprocessed/training-data/relation_extraction/6,Data generation with Wikidata,46,0,4
dataset/preprocessed/training-data/relation_extraction/6,"Wikidata is a collaboratively constructed KB that encodes common world knowledge in a form of binary relation instances ( e.g. CAPITAL : P36 ( Hawaii : Q782 , Honolulu: Q18094 ) )",47,0,32
dataset/preprocessed/training-data/relation_extraction/6,1 . It contains more than 28 million entities and 160 million re - .,48,0,15
dataset/preprocessed/training-data/relation_extraction/6,We use the complete English Wikipedia corpus to generate training and evaluation data .,49,0,14
dataset/preprocessed/training-data/relation_extraction/11,RESIDE : Improving Distantly - Supervised Neural Relation Extraction using Side Information,2,1,12
dataset/preprocessed/training-data/relation_extraction/11,Distantly - supervised Relation Extraction ( RE ) methods train an extractor by automatically aligning relation instances in a Knowledge Base ( KB ) with unstructured text .,4,1,28
dataset/preprocessed/training-data/relation_extraction/11,"In addition to relation instances , KBs often contain other relevant side information , such as aliases of relations ( e.g. , founded and co-founded are aliases for the relation founder OfCompany ) .",5,0,34
dataset/preprocessed/training-data/relation_extraction/11,RE models usually ignore such readily available side information .,6,1,10
dataset/preprocessed/training-data/relation_extraction/11,"In this paper , we propose RESIDE , a distantly - supervised neural relation extraction method which utilizes additional side information from KBs for improved relation extraction .",7,0,28
dataset/preprocessed/training-data/relation_extraction/11,It uses entity type and relation alias information for imposing soft constraints while predicting relations .,8,0,16
dataset/preprocessed/training-data/relation_extraction/11,RE - SIDE employs Graph Convolution Networks ( GCN ) to encode syntactic information from text and improves performance even when limited side information is available .,9,0,27
dataset/preprocessed/training-data/relation_extraction/11,"Through extensive experiments on benchmark datasets , we demonstrate RESIDE 's effectiveness .",10,0,13
dataset/preprocessed/training-data/relation_extraction/11,We have made RESIDE 's source code available to encourage reproducible research .,11,0,13
dataset/preprocessed/training-data/relation_extraction/11,"The construction of large - scale Knowledge Bases ( KBs ) like Freebase and Wikidata has proven to be useful in many natural language processing ( NLP ) tasks like question - answering , web search , etc .",13,0,39
dataset/preprocessed/training-data/relation_extraction/11,"However , these KBs are not exhaustive .",14,0,8
dataset/preprocessed/training-data/relation_extraction/11,Relation Extraction ( RE ) attempts to fill this gap by extracting semantic relationships between entity pairs from plain text .,15,1,21
dataset/preprocessed/training-data/relation_extraction/11,This task can be modeled as a simple classification problem after the entity pairs are specified .,16,0,17
dataset/preprocessed/training-data/relation_extraction/11,"Formally , given an entity pair ( e 1 ,e 2 ) from the KB and an entity annotated sentence ( or instance ) , we aim to predict the relation r , from a predefined relation set , that exists between e 1 and e 2 .",17,0,48
dataset/preprocessed/training-data/relation_extraction/11,"If no relation exists , we simply label it NA .",18,0,11
dataset/preprocessed/training-data/relation_extraction/11,Most supervised relation extraction methods require large labeled training data which is expensive to construct .,19,0,16
dataset/preprocessed/training-data/relation_extraction/11,"Distant Supervision ( DS ) helps with the construction of this dataset automatically , under the assumption that if two entities have a relationship in a KB , then all sentences mentioning those entities express the same relation .",20,0,39
dataset/preprocessed/training-data/relation_extraction/11,"While this approach works well in generating large amounts of training instances , the DS assumption does not hold in all cases . ; propose multi -instance based learning to relax this assumption .",21,0,34
dataset/preprocessed/training-data/relation_extraction/11,"However , they use NLP tools to extract features , which can be noisy .",22,0,15
dataset/preprocessed/training-data/relation_extraction/11,"Recently , neural models have demonstrated promising performance on RE . employ Convolutional Neural Networks ( CNN ) to learn representations of instances .",23,0,24
dataset/preprocessed/training-data/relation_extraction/11,"For alleviating noise in distant supervised datasets , attention has been utilized by .",24,0,14
dataset/preprocessed/training-data/relation_extraction/11,Syntactic information from dependency parses has been used by for capturing long - range dependencies between tokens .,25,0,18
dataset/preprocessed/training-data/relation_extraction/11,Recently proposed Graph Convolution Networks ( GCN ) have been effectively employed for encoding this information .,26,0,17
dataset/preprocessed/training-data/relation_extraction/11,"However , all the above models rely only on the noisy instances from distant supervision for RE .",27,0,18
dataset/preprocessed/training-data/relation_extraction/11,Relevant side information can be effective for improving RE .,28,0,10
dataset/preprocessed/training-data/relation_extraction/11,"For instance , in the sentence , Microsoft was started by Bill Gates. , the type information of Bill Gates ( person ) and Microsoft ( organization ) can be helpful in predicting the correct relation founder OfCompany .",29,0,39
dataset/preprocessed/training-data/relation_extraction/11,This is because every relation constrains the type of its target en - h gcn h gru :,30,0,18
dataset/preprocessed/training-data/relation_extraction/11,Overview of RESIDE .,31,0,4
dataset/preprocessed/training-data/relation_extraction/11,"RESIDE first encodes each sentence in the bag by concatenating embeddings ( denoted by ? ) from Bi - GRU and Syntactic GCN for each token , followed byword attention .",32,0,31
dataset/preprocessed/training-data/relation_extraction/11,"Then , sentence embedding is concatenated with relation alias information , which comes from the Side Information Acquisition Section ) , before computing attention over sentences .",33,0,27
dataset/preprocessed/training-data/relation_extraction/11,"Finally , bag representation with entity type information is fed to a softmax classifier .",34,0,15
dataset/preprocessed/training-data/relation_extraction/11,Please see Section 5 for more details .,35,0,8
dataset/preprocessed/training-data/relation_extraction/11,"Similarly , relation phrase "" was started by "" extracted using Open Information Extraction ( Open IE ) methods can be useful , given that the aliases of relation founder OfCompany , e.g. , founded , co -founded , etc. , are available .",37,0,44
dataset/preprocessed/training-data/relation_extraction/11,KBs used for DS readily provide such information which has not been completely exploited by current models .,38,0,18
dataset/preprocessed/training-data/relation_extraction/11,"In this paper , we propose RESIDE , a novel distant supervised relation extraction method which utilizes additional supervision from KB through its neural network based architecture .",39,0,28
dataset/preprocessed/training-data/relation_extraction/11,"RESIDE makes principled use of entity type and relation alias information from KBs , to impose soft constraints while predicting the relation .",40,0,23
dataset/preprocessed/training-data/relation_extraction/11,"It uses encoded syntactic information obtained from Graph Convolution Networks ( GCN ) , along with embedded side information , to improve neural relation extraction .",41,0,26
dataset/preprocessed/training-data/relation_extraction/11,Our contributions can be summarized as follows :,42,0,8
dataset/preprocessed/training-data/relation_extraction/11,"We propose RESIDE , a novel neural method which utilizes additional supervision from KB in a principled manner for improving distant supervised RE . RESIDE uses Graph Convolution Networks ( GCN ) for modeling syntactic information and has been shown to perform competitively even with limited side information .",43,0,49
dataset/preprocessed/training-data/relation_extraction/11,"Through extensive experiments on benchmark datasets , we demonstrate RESIDE 's effectiveness over state - of - the - art baselines .",44,0,22
dataset/preprocessed/training-data/relation_extraction/11,RESIDE 's source code and datasets used in the paper are available at http://github.com / malllabiisc / RESIDE .,45,0,19
dataset/preprocessed/training-data/relation_extraction/11,Distant supervision :,47,0,3
dataset/preprocessed/training-data/relation_extraction/11,Relation extraction is the task of identifying the relationship between two entity mentions in a sentence .,48,0,17
dataset/preprocessed/training-data/relation_extraction/11,"In supervised paradigm , the task is considered as a multi-class classification problem but suffers from lack of large labeled training data .",49,0,23
dataset/preprocessed/training-data/relation_extraction/0,Going out on a limb : Joint Extraction of Entity Mentions and Relations without Dependency Trees,2,1,16
dataset/preprocessed/training-data/relation_extraction/0,We present a novel attention - based recurrent neural network for joint extraction of entity mentions and relations .,4,0,19
dataset/preprocessed/training-data/relation_extraction/0,We show that attention along with long short term memory ( LSTM ) network can extract semantic relations between entity mentions without having access to dependency trees .,5,0,28
dataset/preprocessed/training-data/relation_extraction/0,Experiments on Automatic Content Extraction ( ACE ) corpora show that our model significantly outperforms featurebased joint model by Li and Ji ( 2014 ) .,6,0,26
dataset/preprocessed/training-data/relation_extraction/0,We also compare our model with an end - toend tree - based LSTM model ( SPTree ) by Miwa and Bansal ( 2016 ) and show that our model performs within 1 % on entity mentions and 2 % on relations .,7,0,43
dataset/preprocessed/training-data/relation_extraction/0,"Our finegrained analysis also shows that our model performs significantly better on AGENT - ARTIFACT relations , while SPTree performs better on PHYSICAL and PART - WHOLE relations .",8,0,29
dataset/preprocessed/training-data/relation_extraction/0,Extraction of entities and their relations from text belongs to a very well - studied family of structured prediction tasks in NLP .,10,1,23
dataset/preprocessed/training-data/relation_extraction/0,"There are several NLP tasks such as fine - grained opinion mining , semantic role labeling , etc. , which have a similar structure ; thus making it an important and a challenging task .",11,0,35
dataset/preprocessed/training-data/relation_extraction/0,Several methods have been proposed for entity mention and relation extraction at the sentencelevel .,12,1,15
dataset/preprocessed/training-data/relation_extraction/0,"These can be broadly categorized into - 1 ) pipeline models that treat the identification of entity mentions and relation classification as two separate tasks ; and 2 ) joint models , also the more recent , which simultaneously identify the entity mention and relations .",13,0,46
dataset/preprocessed/training-data/relation_extraction/0,Joint models have been argued to perform better than the pipeline models as knowledge of the typed relation can increase the confidence of the model on entity extraction and vice versa .,14,0,32
dataset/preprocessed/training-data/relation_extraction/0,Recurrent networks ( RNNs ) ) have recently become very popular for sequence tagging tasks such as entity extraction that involves a set of contiguous tokens .,15,0,27
dataset/preprocessed/training-data/relation_extraction/0,"However , their ability to identify relations between non-adjacent tokens in a sequence , e.g. , the head nouns of two entities , is less explored .",16,0,27
dataset/preprocessed/training-data/relation_extraction/0,"For these tasks , RNNs that make use of tree structures have been deemed more suitable. , for example , propose an RNN comprised of a sequencebased long short term memory ( LSTM ) for entity identification and a separate tree - based dependency LSTM layer for relation classification using shared parameters between the two components .",17,0,57
dataset/preprocessed/training-data/relation_extraction/0,"As a result , their model depends critically on access to dependency trees , restricting it to sentencelevel extraction and to languages for which ( good ) dependency parsers exist .",18,0,31
dataset/preprocessed/training-data/relation_extraction/0,"Also , their model does not jointly extract entities and relations ; they first extract all entities and then perform relation classification on all pairs of entities in a sentence .",19,0,31
dataset/preprocessed/training-data/relation_extraction/0,"In our previous work , we address the same task in an opinion extraction context .",20,0,16
dataset/preprocessed/training-data/relation_extraction/0,Our LSTM - based formulation explicitly encodes distance between the head of entities into opinion relation labels .,21,0,18
dataset/preprocessed/training-data/relation_extraction/0,The output space of our model is quadratic in size of the entity and relation label set and we do not specifically identify the relation type .,22,0,27
dataset/preprocessed/training-data/relation_extraction/0,"Unfortunately , adding relation type makes the output label space very sparse , making it difficult for the model to learn .",23,0,22
dataset/preprocessed/training-data/relation_extraction/0,"In this paper , we propose a novel RNN - based model for the joint extraction of entity mentions and relations .",24,0,22
dataset/preprocessed/training-data/relation_extraction/0,"Unlike other models , our model does not depend on any dependency tree information .",25,0,15
dataset/preprocessed/training-data/relation_extraction/0,Our RNN - based model is a multi - layer bidirectional LSTM over a sequence .,26,0,16
dataset/preprocessed/training-data/relation_extraction/0,We encode the output sequence from left - to - right .,27,0,12
dataset/preprocessed/training-data/relation_extraction/0,"At each time step , we use an attention - like model on the previously decoded time steps , to identify the tokens in a specified relation with the current token .",28,0,32
dataset/preprocessed/training-data/relation_extraction/0,We also add an additional layer to our network to encode the output sequence from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,29,0,34
dataset/preprocessed/training-data/relation_extraction/0,"Our model significantly outperforms the feature - based structured perceptron model of , showing improvements on both entity and relation extraction on the ACE05 dataset .",30,0,26
dataset/preprocessed/training-data/relation_extraction/0,"In comparison to the dependency treebased LSTM model of , our model performs within 1 % on entities and 2 % on relations on ACE05 dataset .",31,0,27
dataset/preprocessed/training-data/relation_extraction/0,"We also find that our model performs significantly better than their tree - based model on the AGENT - ARTIFACT relation , while their tree - based model performs better on PHYSICAL and PART - WHOLE relations ; the two models perform comparably on all other relation types .",32,0,49
dataset/preprocessed/training-data/relation_extraction/0,The very competitive performance of our non-tree - based model bodes well for relation extraction of non-adjacent entities in low - resource languages that lack good parsers .,33,0,28
dataset/preprocessed/training-data/relation_extraction/0,"In the sections that follow , we describe related work ( Section 2 ) ; our bi-directional LSTM model with attention ( Section 3 ) ; the training ( Section 4 ) ; the experiments on ACE dataset ( Section 5 ) ; results ( Section 6 ) ; error analysis ( Section 7 ) and conclusion ( Section 8 ) .",34,0,62
dataset/preprocessed/training-data/relation_extraction/0,"RNNs have been recently applied to many sequential modeling and prediction tasks , such as machine translation , named entity recognition ( NER ) , opinion mining .",36,0,28
dataset/preprocessed/training-data/relation_extraction/0,Variants such as adding CRF - like objective on top of LSTMs have been found to produce state - of - the - art results on several sequence prediction NLP tasks .,37,0,32
dataset/preprocessed/training-data/relation_extraction/0,"These models assume conditional independence at the output layer whereas the model we propose in this paper does not assume any conditional indepen - dence at the output layer , allowing it to model an arbitrary distribution over output sequences .",38,0,41
dataset/preprocessed/training-data/relation_extraction/0,"Relation classification has been widely studied as a stand - alone task , assuming that the arguments of the relations are known in advance .",39,0,25
dataset/preprocessed/training-data/relation_extraction/0,There have been several models proposed including featurebased models and neural network based models .,40,0,15
dataset/preprocessed/training-data/relation_extraction/0,"For joint - extraction of entities and relations , feature - based structured prediction models , joint inference integer linear programming models , card - pyramid parsing ( Kate and Mooney , 2010 ) and probabilistic graphical models have been proposed .",41,0,42
dataset/preprocessed/training-data/relation_extraction/0,"In contrast , we propose a neural network model which does not depend on the availability of any features such as part of speech ( POS ) tags , dependency trees , etc .",42,0,34
dataset/preprocessed/training-data/relation_extraction/0,"Recently , proposed an end - to - end LSTM based sequence and treestructured model .",43,0,16
dataset/preprocessed/training-data/relation_extraction/0,They extract entities via a sequence layer and relations between the entities via the shortest path dependency tree network .,44,0,20
dataset/preprocessed/training-data/relation_extraction/0,"In this paper , we try to investigate recurrent neural networks with attention for extracting semantic relations between entity mentions without using any dependency parse tree features .",45,0,28
dataset/preprocessed/training-data/relation_extraction/0,We also present the first neural network based joint model that can extract entity mentions and relations along with the relation type .,46,0,23
dataset/preprocessed/training-data/relation_extraction/0,"In our previous work , as explained earlier , we proposed a LSTM - based model for joint extraction of opinion entities and relations , but no relation types .",47,0,30
dataset/preprocessed/training-data/relation_extraction/0,This model can not be directly extended to include relation types as the output space becomes sparse making it difficult for the model to learn .,48,0,26
dataset/preprocessed/training-data/relation_extraction/0,Recent advances in recurrent neural network has seen the application of attention on recurrent neural networks to obtain a representation weighted by the importance of tokens in the sequence model .,49,0,31
dataset/preprocessed/training-data/relation_extraction/12,Attention Guided Graph Convolutional Networks for Relation Extraction,2,1,8
dataset/preprocessed/training-data/relation_extraction/12,Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text .,4,0,18
dataset/preprocessed/training-data/relation_extraction/12,"However , how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question .",5,0,24
dataset/preprocessed/training-data/relation_extraction/12,Existing approaches employing rule based hard - pruning strategies for selecting relevant partial dependency structures may not always yield optimal results .,6,0,22
dataset/preprocessed/training-data/relation_extraction/12,"In this work , we propose Attention Guided Graph Convolutional Networks ( AGGCNs ) , a novel model which directly takes full dependency trees as inputs .",7,0,27
dataset/preprocessed/training-data/relation_extraction/12,Our model can be understood as a soft - pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task .,8,0,29
dataset/preprocessed/training-data/relation_extraction/12,"Extensive results on various tasks including cross - sentence n-ary relation extraction and large - scale sentence - level relation extraction show that our model is able to better leverage the structural information of the full dependency trees , giving significantly better results than previous approaches .",9,0,47
dataset/preprocessed/training-data/relation_extraction/12,Case study and attention visualization of this example are provided in the supplementary material .,11,0,15
dataset/preprocessed/training-data/relation_extraction/12,"The deletion mutation on exon - 19 of EGFR gene was present in 16 patients , while the L858E point mutation on exon - 21 was noted .",12,0,28
dataset/preprocessed/training-data/relation_extraction/12,All patients were treated response .,13,0,6
dataset/preprocessed/training-data/relation_extraction/12,with gefitinib and showed a partial ROOT DET NN PREP _,14,0,11
dataset/preprocessed/training-data/relation_extraction/12,ON PREP_OF NN NSUBJ COP NUM PREP_IN NN NN DET MARK PREP_ ON AUXPASS NSUBJPASS ADVCL DET NSUBJPASS PREP_WITH CONJ_AND DOBJ DET AMOD NEXT ROOT,15,0,25
dataset/preprocessed/training-data/relation_extraction/12,Relation extraction aims to detect relations among entities in the text .,17,0,12
dataset/preprocessed/training-data/relation_extraction/12,"It plays a significant role in a variety of natural language processing applications including biomedical knowledge discovery , knowledge base population and question answering .",18,0,25
dataset/preprocessed/training-data/relation_extraction/12,"shows an example about expressing a relation sensitivity among three entities L858E , EGFR and gefitinib in two sentences .",19,0,20
dataset/preprocessed/training-data/relation_extraction/12,Most existing relation extraction models can be categorized into two classes : sequence - based and dependency - based .,20,0,20
dataset/preprocessed/training-data/relation_extraction/12,"Sequence - based models operate only on the word sequences 2014 ; , whereas dependencybased models incorporate dependency trees into the models .",21,0,23
dataset/preprocessed/training-data/relation_extraction/12,"Compared to sequence - based models , dependency - based models are able to capture non-local syntactic relations thatare obscure from the surface form alone .",22,0,26
dataset/preprocessed/training-data/relation_extraction/12,Various pruning strategies are also proposed to distill the dependency information in order to further improve the performance .,23,0,19
dataset/preprocessed/training-data/relation_extraction/12,apply neural networks only on the shortest dependency path between the entities in the full tree .,24,0,17
dataset/preprocessed/training-data/relation_extraction/12,reduce the full tree to the subtree below the lowest common ancestor ( LCA ) of the entities .,25,0,19
dataset/preprocessed/training-data/relation_extraction/12,apply graph convolutional networks ( GCNs ) model over a pruned tree .,26,0,13
dataset/preprocessed/training-data/relation_extraction/12,This tree includes tokens thatare up to distance K away from the dependency path in the LCA subtree .,27,0,19
dataset/preprocessed/training-data/relation_extraction/12,"However , rule - based pruning strategies might eliminate some important information in the full tree .",28,0,17
dataset/preprocessed/training-data/relation_extraction/12,shows an example in cross - sentence n- ary relation extraction that the key tokens partial response would be excluded if the model only takes the pruned tree into consideration .,29,0,31
dataset/preprocessed/training-data/relation_extraction/12,"Ideally , the model should be able to learn how to maintain a balance between including and excluding information in the full tree .",31,0,24
dataset/preprocessed/training-data/relation_extraction/12,"In this paper , we propose the novel Attention Guided Graph Convolutional Networks ( AGGCNs ) , which operate directly on the full tree .",32,0,25
dataset/preprocessed/training-data/relation_extraction/12,"Intuitively , we develop a "" soft pruning "" strategy that transforms the original dependency tree into a fully connected edgeweighted graph .",33,0,23
dataset/preprocessed/training-data/relation_extraction/12,"These weights can be viewed as the strength of relatedness between nodes , which can be learned in an end - to - end fashion by using self - attention mechanism .",34,0,32
dataset/preprocessed/training-data/relation_extraction/12,"In order to encode a large fully connected graph , :",35,0,11
dataset/preprocessed/training-data/relation_extraction/12,An example dependency tree for two sentences expressing a relation ( sensitivity ) among three entities .,36,0,17
dataset/preprocessed/training-data/relation_extraction/12,The shortest dependency path between these entities is highlighted in bold ( edges and tokens ) .,37,0,17
dataset/preprocessed/training-data/relation_extraction/12,The root node of the LCA subtree of entities is present .,38,0,12
dataset/preprocessed/training-data/relation_extraction/12,The dotted edges indicate tokens K = 1 away from the subtree .,39,0,13
dataset/preprocessed/training-data/relation_extraction/12,"Note that tokens partial response off these paths ( shortest dependency path , LCA subtree , pruned tree when K = 1 ) .",40,0,24
dataset/preprocessed/training-data/relation_extraction/12,we next introduce dense connections ) to the GCN model following .,41,0,12
dataset/preprocessed/training-data/relation_extraction/12,"For GCNs , L layers will be needed in order to capture neighborhood information that is L hops away .",42,0,20
dataset/preprocessed/training-data/relation_extraction/12,A shallow GCN model may not be able to capture non-local interactions of large graphs .,43,0,16
dataset/preprocessed/training-data/relation_extraction/12,"Interestingly , while deeper GCNs can capture richer neighborhood information of a graph , empirically it has been observed that the best performance is achieved with a 2 - layer model .",44,0,32
dataset/preprocessed/training-data/relation_extraction/12,"With the help of dense connections , we are able to train the AGGCN model with a large depth , allowing rich local and non-local dependency information to be captured .",45,0,31
dataset/preprocessed/training-data/relation_extraction/12,Experiments show that our model is able to achieve better performance for various tasks .,46,0,15
dataset/preprocessed/training-data/relation_extraction/12,"For the cross - sentence relation extraction task , our model surpasses the current state - of - the - art models on multi-class ternary and binary relation extraction by 8 % and 6 % in terms of accuracy respectively .",47,0,41
dataset/preprocessed/training-data/relation_extraction/12,"For the large - scale sentence - level extraction task ( TACRED dataset ) , our model is also consistently better than others , showing the effectiveness of the model on a large training set .",48,0,36
dataset/preprocessed/training-data/relation_extraction/12,Our code is available at https://github.com/Cartus / AGGCN_TACRED,49,0,8
dataset/preprocessed/training-data/relation_extraction/2,Enriching Pre-trained Language Model with Entity Information for Relation Classification,2,1,10
dataset/preprocessed/training-data/relation_extraction/2,Relation classification is an important NLP task to extract relations between entities .,4,0,13
dataset/preprocessed/training-data/relation_extraction/2,The state - of - the - art methods for relation classification are primarily based on Convolutional or Recurrent Neural Networks .,5,0,22
dataset/preprocessed/training-data/relation_extraction/2,"Recently , the pre-trained BERT model achieves very successful results in many NLP classification / sequence labeling tasks .",6,0,19
dataset/preprocessed/training-data/relation_extraction/2,Relation classification differs from those tasks in that it relies on information of both the sentence and the two target entities .,7,0,22
dataset/preprocessed/training-data/relation_extraction/2,"In this paper , we propose a model that both leverages the pretrained BERT language model and incorporates information from the target entities to tackle the relation classification task .",8,0,30
dataset/preprocessed/training-data/relation_extraction/2,We locate the target entities and transfer the information through the pre-trained architecture and incorporate the corresponding encoding of the two entities .,9,0,23
dataset/preprocessed/training-data/relation_extraction/2,We achieve significant improvement over the state - of - the - art method on the SemEval - 2010 task 8 relational dataset .,10,0,24
dataset/preprocessed/training-data/relation_extraction/2,The task of relation classification is to predict semantic relations between pairs of nominals .,12,0,15
dataset/preprocessed/training-data/relation_extraction/2,"Given a sequence of text ( usually a sentence ) sand a pair of nominals e 1 and e 2 , the objective is to identify the relation between e 1 and e 2 .",13,0,35
dataset/preprocessed/training-data/relation_extraction/2,It is an important NLP task which is normally used as an intermediate step in variety of NLP applications .,14,0,20
dataset/preprocessed/training-data/relation_extraction/2,"The following example shows the Component - Whole relation between the nominals "" kitchen "" and "" house "" :",15,0,20
dataset/preprocessed/training-data/relation_extraction/2,""" The [ kitchen ] e 1 is the last renovated part of the [ house ] e 1 . """,16,0,21
dataset/preprocessed/training-data/relation_extraction/2,"Recently , deep neural networks have applied to relation classification .",17,0,11
dataset/preprocessed/training-data/relation_extraction/2,These methods usually use some features derived from lexical resources such as Word - Net or NLP tools such as dependency parsers and named entity recognizers ( NER ) .,18,0,30
dataset/preprocessed/training-data/relation_extraction/2,Language model pre-training has been shown to be effective for improving many natural language processing tasks .,19,0,17
dataset/preprocessed/training-data/relation_extraction/2,The pretrained model BERT proposed by has especially significant impact .,20,0,11
dataset/preprocessed/training-data/relation_extraction/2,It has been applied to multiple NLP tasks and obtains new state - of - theart results on eleven tasks .,21,0,21
dataset/preprocessed/training-data/relation_extraction/2,The tasks that BERT has been applied to are typically modeled as classification problems and sequence labeling problems .,22,0,19
dataset/preprocessed/training-data/relation_extraction/2,"It has also been applied to the SQuAD question answering problem , in which the objective is to find the starting point and ending point of an answer span .",23,0,30
dataset/preprocessed/training-data/relation_extraction/2,"As far as we know , the pretrained BERT model has not been applied to relation classification , which relies not only on the information of the whole sentence but also on the information of the specific target entities .",24,0,40
dataset/preprocessed/training-data/relation_extraction/2,"In this paper , we apply the pretrained BERT model for relation classification .",25,0,14
dataset/preprocessed/training-data/relation_extraction/2,"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .",26,0,41
dataset/preprocessed/training-data/relation_extraction/2,We then locate the positions of the two target entities in the output embedding from BERT model .,27,0,18
dataset/preprocessed/training-data/relation_extraction/2,We use their embeddings as well as the sentence encoding ( embedding of the special first token in the setting of BERT ) as the input to a multi - layer neural network for classification .,28,0,36
dataset/preprocessed/training-data/relation_extraction/2,"By this way , it captures both the semantics of the sentence and the two target entities to better fit the relation classification task .",29,0,25
dataset/preprocessed/training-data/relation_extraction/2,Our contributions are as follows :,30,0,6
dataset/preprocessed/training-data/relation_extraction/2,( 1 ) We put forward an innovative approach to incorporate entity - level information into the pretrained language model for relation classification .,31,0,24
dataset/preprocessed/training-data/relation_extraction/2,( 2 ) We achieve the new state - of - the - art for the relation classification task .,32,0,20
dataset/preprocessed/training-data/relation_extraction/2,"There has been some work with deep learning methods for relation classification , such as MVRNN model ) applies a recursive neural network ( RNN ) to relation classification .",34,0,30
dataset/preprocessed/training-data/relation_extraction/2,They assign a matrix - vector representation to every node in a parse tree and compute the representation for the complete sentence from bottom up according to the syntactic structure of the parse tree .,35,0,35
dataset/preprocessed/training-data/relation_extraction/2,propose a CNN model by incorporating both word embeddings and position features as input .,36,0,15
dataset/preprocessed/training-data/relation_extraction/2,Then they concatenate lexical features and the output from CNN into a single vector and feed them into a softmax layer for prediction .,37,0,24
dataset/preprocessed/training-data/relation_extraction/2,"propose a Factor - based Compositional Embedding Model ( FCM ) by constructing sentencelevel and substructure embeddings from word embeddings , through dependency trees and named entities .",38,0,28
dataset/preprocessed/training-data/relation_extraction/2,"( Santos et al. , 2015 ) tackle the relation classification task by ranking with a convolutional neural network named CR - CNN .",39,0,24
dataset/preprocessed/training-data/relation_extraction/2,Their loss function is based on pairwise ranking .,40,0,9
dataset/preprocessed/training-data/relation_extraction/2,"In our work , we take advantage of a pre-trained language model for the relation classification task , without relying on CNN or RNN architecutures . ) utilize a CNN encoder in conjunction with a sentence representation that weights the words by attention between the target entities and the words in the sentence to perform relation classification .",41,0,58
dataset/preprocessed/training-data/relation_extraction/2,propose a convolutional neural network architecture with two levels of attention in order to catch the patterns in heterogeneous contexts to classify relations .,42,0,24
dataset/preprocessed/training-data/relation_extraction/2,develop an end - to - end recurrent neural model which incorporates an entity - aware attention mechanism with a latent entity typing for relation classification .,43,0,27
dataset/preprocessed/training-data/relation_extraction/2,"There are some related work on the relation extraction based on distant supervision , for example , .",44,0,18
dataset/preprocessed/training-data/relation_extraction/2,The difference between relation classification on regular data and on distantly supervised data is that the latter may contain a large number of noisy labels .,45,0,26
dataset/preprocessed/training-data/relation_extraction/2,"In this paper , we focus on the regular relation classification problem , without noisy labels .",46,0,17
dataset/preprocessed/training-data/relation_extraction/2,Pre-trained Model BERT,48,0,3
dataset/preprocessed/training-data/relation_extraction/2,The pre-trained BERT model is a multi - layer bidirectional Transformer encoder .,49,0,13
dataset/preprocessed/training-data/relation_extraction/4,Position - aware Attention and Supervised Data Improve Slot Filling,2,0,10
dataset/preprocessed/training-data/relation_extraction/4,"Organized relational knowledge in the form of "" knowledge graphs "" is important for many applications .",4,0,17
dataset/preprocessed/training-data/relation_extraction/4,"However , the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly .",5,1,19
dataset/preprocessed/training-data/relation_extraction/4,This paper simultaneously addresses two issues that have held back prior work .,6,0,13
dataset/preprocessed/training-data/relation_extraction/4,"We first propose an effective new model , which combines an LSTM sequence model with a form of entity position - aware attention that is better suited to relation extraction .",7,0,31
dataset/preprocessed/training-data/relation_extraction/4,"Then we build TACRED , a large ( 119,474 examples ) supervised relation extraction dataset , obtained via crowdsourcing and targeted towards TAC KBP relations .",8,0,26
dataset/preprocessed/training-data/relation_extraction/4,The combination of better supervised data and a more appropriate high - capacity model enables much better relation extraction performance .,9,0,21
dataset/preprocessed/training-data/relation_extraction/4,"When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system , its F 1 score increases markedly from 22.2 % to 26.7 % .",10,0,37
dataset/preprocessed/training-data/relation_extraction/4,A basic but highly important challenge in natural language understanding is being able to populate a knowledge base with relational facts contained in a piece of text .,12,1,28
dataset/preprocessed/training-data/relation_extraction/4,"For the text shown in , the system should extract triples , or equivalently , knowledge graph edges , such as hPenner , per : spouse , Lis a Dillmani .",13,0,31
dataset/preprocessed/training-data/relation_extraction/4,"Combining such extractions , a system can produce a knowledge graph of relational facts between persons , organizations , and locations in the text .",14,0,25
dataset/preprocessed/training-data/relation_extraction/4,"This task involves entity recognition , mention coreference and / or entity linking , and relation extraction ; we focus on the Penner is survived by his brother , John , a copy editor at the Times , and his former wife , Times sportswriter Lis a Dillman .",15,0,49
dataset/preprocessed/training-data/relation_extraction/4,"most challenging "" slot filling "" task of filling in the relations between entities in the text .",16,0,18
dataset/preprocessed/training-data/relation_extraction/4,"Organized relational knowledge in the form of "" knowledge graphs "" has become an important knowledge resource .",17,0,18
dataset/preprocessed/training-data/relation_extraction/4,"These graphs are now extensively used by search engine companies , both to provide information to end-users and internally to the system , as away to understand relationships .",18,0,29
dataset/preprocessed/training-data/relation_extraction/4,"However , up until now , automatic knowledge extraction has proven sufficiently difficult that most of the facts in these knowledge graphs have been built up by hand .",19,0,29
dataset/preprocessed/training-data/relation_extraction/4,It is therefore a key challenge to show that NLP technology can effectively contribute to this important problem .,20,0,19
dataset/preprocessed/training-data/relation_extraction/4,"Existing work on relation extraction ( e.g. , has been unable to achieve sufficient recall or precision for the results to be usable versus hand - constructed knowledge bases .",21,1,30
dataset/preprocessed/training-data/relation_extraction/4,"Supervised training data has been scarce and , while techniques like distant supervision appear to be a promising way to extend knowledge bases at low cost , in practice the training data has often been too noisy for reliable training of relation extraction systems .",22,0,45
dataset/preprocessed/training-data/relation_extraction/4,"As a result most systems fail to make correct extractions even in apparently straightforward cases like ,",23,0,17
dataset/preprocessed/training-data/relation_extraction/4,Example Entity Types & Label,24,0,5
dataset/preprocessed/training-data/relation_extraction/4,"Carey will succeed Cathleen P. Black , who held the position for 15 years and will take on a new role as chairwoman of Hearst Magazines , the company said .",25,0,31
dataset/preprocessed/training-data/relation_extraction/4,where the best system at the NIST TAC Knowledge Base Population ( TAC KBP ) 2015 evaluation failed to recognize the relation between Penner and Dillman .,26,0,27
dataset/preprocessed/training-data/relation_extraction/4,1 Consequently most automatic systems continue to make heavy use of hand - written rules or patterns because it has been hard for machine learning systems to achieve adequate precision or to generalize as well across text types .,27,0,39
dataset/preprocessed/training-data/relation_extraction/4,"We believe machine learning approaches have suffered from two key problems : ( 1 ) the models used have been insufficiently tailored to relation extraction , and ( 2 ) there has been insufficient annotated data available to satisfy the training of data - hungry models , such as deep learning models .",28,0,53
dataset/preprocessed/training-data/relation_extraction/4,This work addresses both of these problems .,29,0,8
dataset/preprocessed/training-data/relation_extraction/4,"We propose a new , effective neural network sequence model for relation classification .",30,0,14
dataset/preprocessed/training-data/relation_extraction/4,Its architecture is better customized for the slot filling task : the word representations are augmented by extra distributed representations of word position relative to the subject and object of the putative relation .,31,0,34
dataset/preprocessed/training-data/relation_extraction/4,This means that the neural attention model can effectively exploit the combination of semantic similarity - based attention and positionbased attention .,32,0,22
dataset/preprocessed/training-data/relation_extraction/4,"Secondly , we markedly improve the availability of supervised training data by using Mechanical Turk crowd annotation to produce a large supervised training dataset , suitable for the common relations between people , organizations and locations which are used in the TAC KBP evaluations .",33,0,45
dataset/preprocessed/training-data/relation_extraction/4,"We name this dataset the TAC Relation Extraction Dataset ( TACRED ) , and will make it available through the Linguistic Data Consortium ( LDC ) in order to respect copyrights on the underlying text .",34,0,36
dataset/preprocessed/training-data/relation_extraction/4,Combining these two gives a system with markedly better slot filling performance .,35,0,13
dataset/preprocessed/training-data/relation_extraction/4,This is 1 Note : former spouses count as spouses in the ontology .,36,0,14
dataset/preprocessed/training-data/relation_extraction/4,shown not only for a relation classification task on the crowd - annotated data but also for the incorporation of the resulting classifiers into a complete cold start knowledge base population system .,37,0,33
dataset/preprocessed/training-data/relation_extraction/4,"On TACRED , our system achieves a relation classification F 1 score that is 7.9 % higher than that of a strong feature - based classifier , and 3.5 % higher than that of the best previous neural architecture that we re-implemented .",38,0,43
dataset/preprocessed/training-data/relation_extraction/4,"When this model is used in concert with a pattern - based system on the TAC KBP 2015 Cold Start Slot Filling evaluation data , the system achieves an F 1 score of 26.7 % , which exceeds the previous state - of - the - art by 4.5 % absolute .",39,0,52
dataset/preprocessed/training-data/relation_extraction/4,While this performance certainly does not solve the knowledge base population problemachieving sufficient recall remains a formidable challenge - this is nevertheless notable progress .,40,0,25
dataset/preprocessed/training-data/relation_extraction/4,A Position - aware Neural Sequence Model Suitable for Relation Extraction,41,0,11
dataset/preprocessed/training-data/relation_extraction/4,"Existing work on neural relation extraction ( e.g. , has focused on convolutional neural networks ( CNNs ) , recurrent neural networks ( RNNs ) , or their combination .",42,0,30
dataset/preprocessed/training-data/relation_extraction/4,"While these models generally work well on the datasets they are tested on , as we will show , they often fail to generalize to the longer sentences that are common in real - world text ( such as in TAC KBP ) .",43,0,44
dataset/preprocessed/training-data/relation_extraction/4,We believe that existing model architectures suffer from two problems :,44,0,11
dataset/preprocessed/training-data/relation_extraction/4,"( 1 ) Although modern sequence models such as Long Short - Term Memory ( LSTM ) networks have gating mechanisms to control the relative influence of each individual word to the final sentence representation ( Hochreiter and Schmidhuber , 1997 ) , these controls are not explicitly conditioned on the entire sentence being classified ; ( 2 ) Most existing work either does not explicitly model the positions of entities ( i.e. , subject and object ) in the sequence , or models the positions only within a local region .",45,0,92
dataset/preprocessed/training-data/relation_extraction/4,"Here , we propose a new neural sequence model with a position - aware attention mechanism over an LSTM network to tackle these challenges .",46,0,25
dataset/preprocessed/training-data/relation_extraction/4,"This model can ( 1 ) evaluate the relative contribution of each word after seeing the entire sequence , and ( 2 ) base this evaluation not only on the semantic information of the sequence , but also on the global positions of the entities within the sequence .",47,0,49
dataset/preprocessed/training-data/relation_extraction/4,We formalize the relation extraction task as follows :,48,0,9
dataset/preprocessed/training-data/relation_extraction/4,"Let X = [ x 1 , ... , x n ] denote a sentence , where xi is the i - th token .",49,0,25
dataset/preprocessed/training-data/relation_extraction/13,Matching the Blanks : Distributional Similarity for Relation Learning,2,1,9
dataset/preprocessed/training-data/relation_extraction/13,"General purpose relation extractors , which can model arbitrary relations , are a core aspiration in information extraction .",4,0,19
dataset/preprocessed/training-data/relation_extraction/13,"Efforts have been made to build general purpose extractors that represent relations with their surface forms , or which jointly embed surface forms with relations from an existing knowledge graph .",5,0,31
dataset/preprocessed/training-data/relation_extraction/13,"However , both of these approaches are limited in their ability to generalize .",6,0,14
dataset/preprocessed/training-data/relation_extraction/13,"In this paper , we build on extensions of Harris ' distributional hypothesis to relations , as well as recent advances in learning text representations ( specifically , BERT ) , to build task agnostic relation representations solely from entity - linked text .",7,0,44
dataset/preprocessed/training-data/relation_extraction/13,We show that these representations significantly outperform previous work on exemplar based relation extraction ( FewRel ) even without using any of that task 's training data .,8,0,28
dataset/preprocessed/training-data/relation_extraction/13,"We also show that models initialized with our task agnostic representations , and then tuned on supervised relation extraction datasets , significantly outperform the previous methods on Se - m Eval 2010 Task 8 , KBP37 , and TACRED .",9,0,40
dataset/preprocessed/training-data/relation_extraction/13,Reading text to identify and extract relations between entities has been along standing goal in natural language processing .,11,1,19
dataset/preprocessed/training-data/relation_extraction/13,Typically efforts in relation extraction fall into one of three groups .,12,1,12
dataset/preprocessed/training-data/relation_extraction/13,"In a first group , supervised , or distantly supervised relation extractors ) learn a mapping from text to relations in a limited schema .",13,0,25
dataset/preprocessed/training-data/relation_extraction/13,"Forming a second group , open information extraction removes the limitations of a predefined schema by instead representing relations using their surface forms , which increases scope but also leads * Work done as part of the Google AI residency .",14,0,41
dataset/preprocessed/training-data/relation_extraction/13,to an associated lack of generality since many surface forms can express the same relation .,15,0,16
dataset/preprocessed/training-data/relation_extraction/13,"Finally , the universal schema embraces both the diversity of text , and the concise nature of schematic relations , to build a joint representation that has been extended to arbitrary textual input , and arbitrary entity pairs .",16,0,39
dataset/preprocessed/training-data/relation_extraction/13,"However , like distantly supervised relation extractors , universal schema rely on large knowledge graphs ( typically Freebase ) that can be aligned to text .",17,0,26
dataset/preprocessed/training-data/relation_extraction/13,"Building on 's extension of Harris ' distributional hypothesis to relations , as well as recent advances in learning word representations from observations of their contexts , we propose a new method of learning relation representations directly from text .",18,0,40
dataset/preprocessed/training-data/relation_extraction/13,"First , we study the ability of the Transformer neural network architecture to encode relations between entity pairs , and we identify a method of representation that outperforms previous work in supervised relation extraction .",19,0,35
dataset/preprocessed/training-data/relation_extraction/13,"Then , we present a method of training this relation representation without any supervision from a knowledge graph or human annotators by matching the blanks .",20,0,26
dataset/preprocessed/training-data/relation_extraction/13,"[ BLANK ] , inspired by Cale 's earlier cover , recorded one of the most acclaimed versions of "" [ BLANK ] """,21,0,24
dataset/preprocessed/training-data/relation_extraction/13,"[ BLANK ] 's rendition of "" [ BLANK ] "" has been called "" one of the great songs "" by Time , and is included on Rolling Stone 's list of "" The 500 Greatest Songs of All Time "" .",22,0,43
dataset/preprocessed/training-data/relation_extraction/13,"Following , we assume access to a corpus of text in which entities have been linked to unique identifiers and we define a rela-tion statement to be a block of text containing two marked entities .",23,0,36
dataset/preprocessed/training-data/relation_extraction/13,"From this , we create training data that contains relation statements in which the entities have been replaced with a special symbol , as illustrated in .",24,0,27
dataset/preprocessed/training-data/relation_extraction/13,"Our training procedure takes in pairs of blank - containing relation statements , and has an objective that encourages relation representations to be similar if they range over the same pairs of entities .",25,0,34
dataset/preprocessed/training-data/relation_extraction/13,"After training , we employ learned relation representations to the recently released FewRel task in which specific relations , such as ' original language of work ' are represented with a few exemplars , such as The Crowd ( Italian : La Folla ) is a 1951 Italian film .",26,0,50
dataset/preprocessed/training-data/relation_extraction/13,"presented FewRel as a supervised dataset , intended to evaluate models ' ability to adapt to relations from new domains at test time .",27,0,24
dataset/preprocessed/training-data/relation_extraction/13,"We show that through training by matching the blanks , we can outperform 's top performance on FewRel , without having seen any of the FewRel training data .",28,0,29
dataset/preprocessed/training-data/relation_extraction/13,We also show that a model pre-trained by matching the blanks and tuned on FewRel outperforms humans on the FewRel evaluation .,29,0,22
dataset/preprocessed/training-data/relation_extraction/13,"Similarly , by training by matching the blanks and then tuning on labeled data , we significantly improve performance on the SemEval 2010 Task 8 , and TACRED relation extraction benchmarks .",30,0,32
dataset/preprocessed/training-data/relation_extraction/13,"In this paper , we focus on learning mappings from relation statements to relation representations .",33,0,16
dataset/preprocessed/training-data/relation_extraction/13,"Formally , let x = [ x 0 . . . x n ] be a sequence of tokens , where x 0 = [ CLS ] and x n = [ SEP ] are special start and end markers .",34,0,41
dataset/preprocessed/training-data/relation_extraction/13,"Let s 1 = ( i , j ) and s 2 = ( k , l ) be pairs of integers such that 0 < i < j ? 1 , j < k , k ? l ? 1 , and l ? n.",35,0,46
dataset/preprocessed/training-data/relation_extraction/13,"A relation statement is a triple r = ( x , s 1 , s 2 ) , where the indices in s 1 and s 2 delimit entity mentions in x : the sequence [ x i . . . x j?1 ] mentions an entity , and so does the sequence [ x k . . . x l?1 ] .",36,0,63
dataset/preprocessed/training-data/relation_extraction/13,Our goal is to learn a function hr = f ? ( r ) that maps the relation statement to a fixed - length vector hr ?,37,0,27
dataset/preprocessed/training-data/relation_extraction/13,Rd that represents the relation expressed in x between the entities marked by s 1 and s 2 .,38,0,19
dataset/preprocessed/training-data/relation_extraction/13,This paper contains two main contributions .,40,0,7
dataset/preprocessed/training-data/relation_extraction/13,"First , in Section 3.1 we investigate different architectures for the relation encoder f ? , all built on top of the widely used Transformer se-quence model .",41,0,28
dataset/preprocessed/training-data/relation_extraction/13,We evaluate each of these architectures by applying them to a suite of relation extraction benchmarks with supervised training .,42,0,20
dataset/preprocessed/training-data/relation_extraction/13,"Our second , more significant , contributionpresented in Section 4 - is to show that f ?",43,0,17
dataset/preprocessed/training-data/relation_extraction/13,can be learned from widely available distant supervision in the form of entity linked text .,44,0,16
dataset/preprocessed/training-data/relation_extraction/13,Architectures for Relation Learning,45,0,4
dataset/preprocessed/training-data/relation_extraction/13,The primary goal of this work is to develop models that produce relation representations directly from text .,46,0,18
dataset/preprocessed/training-data/relation_extraction/13,"Given the strong performance of recent deep transformers trained on variants of language modeling , we adopt 's BERT model as the basis for our work .",47,0,27
dataset/preprocessed/training-data/relation_extraction/13,"In this section , we explore different methods of representing relations with the Transformer model .",48,0,16
dataset/preprocessed/training-data/relation_extraction/13,Relation Classification and Extraction Tasks,49,0,5
dataset/preprocessed/training-data/sentence_classification/1,Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts,2,1,11
dataset/preprocessed/training-data/sentence_classification/1,Prevalent models based on artificial neural network ( ANN ) for sentence classification often classify sentences in isolation without considering the context in which sentences appear .,4,0,27
dataset/preprocessed/training-data/sentence_classification/1,"This hampers the traditional sentence classification approaches to the problem of sequential sentence classification , where structured prediction is needed for better over all classification performance .",5,1,27
dataset/preprocessed/training-data/sentence_classification/1,"In this work , we present a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence .",6,0,28
dataset/preprocessed/training-data/sentence_classification/1,Our model outperforms the state - of - the - art results by 2 % - 3 % on two benchmarking datasets for sequential sentence classification in medical scientific abstracts .,7,0,31
dataset/preprocessed/training-data/sentence_classification/1,"Since 1665 , over 50 million scholarly research articles have been published , with approximately 2.5 million new scientific papers coming out each year .",9,0,25
dataset/preprocessed/training-data/sentence_classification/1,"While this enormous corpus provides us with the ability to conclusively accept or reject hypotheses and yields insight into promising research directions , it is getting harder and harder to extract useful information from the literature in an efficient and timely manner due to its sheer amount .",10,0,48
dataset/preprocessed/training-data/sentence_classification/1,"Therefore , an automatic and intelligent tool to help users locate the information of interest quickly and comprehensively is highly desired .",11,0,22
dataset/preprocessed/training-data/sentence_classification/1,"When searching for relevant literature for a certain field , investigators first check the abstracts of scientific papers to see whether they match the criterion of interest .",12,0,28
dataset/preprocessed/training-data/sentence_classification/1,"This process can be expedited if the abstracts are structured ; that is , if the rhetorical structural elements of scientific abstracts such as purpose , methods , results , and conclusions ) are explicitly stated .",13,0,37
dataset/preprocessed/training-data/sentence_classification/1,"However , even today , a significant portion of scientific abstracts is still unstructured , which causes great difficulty in information retrieval .",14,0,23
dataset/preprocessed/training-data/sentence_classification/1,"In this paper , we develop a machine - learning based approach to automatically categorize sentences in scientific abstracts into rhetorical sections so that the desired information can be efficiently retrieved .",15,0,32
dataset/preprocessed/training-data/sentence_classification/1,"In a scientific abstract , each sentence can be assigned to a rhetorical structural element sequentially .",16,0,17
dataset/preprocessed/training-data/sentence_classification/1,"This rhetorical structure profiling process can be formulated as a sequential sentence classification task , as the element assignment of any single sentence is greatly associated with the assignments of the surrounding sentences .",17,0,34
dataset/preprocessed/training-data/sentence_classification/1,"This is in contrast to the general sentence classification problem , where each sentence is classified individually and no contextual information can be used .",18,0,25
dataset/preprocessed/training-data/sentence_classification/1,"Previous state - of - the - art methods relied on Conditional Random Fields ( CRFs ) to take into account the inter-dependence between subsequent labels , which improved joint sentence classification performance by considering the label sequence information .",19,0,40
dataset/preprocessed/training-data/sentence_classification/1,"In this work , we add a bi-directional long short - term memory ( bi - LSTM ) layer over the representations of individual sentences so that it can encode the contextual content and semantics from preceding and succeeding sentences for better categorical inference of the current one .",20,0,49
dataset/preprocessed/training-data/sentence_classification/1,"In this work , we present a hierarchical neural network model for the sequential sentence classification task , which we call a hierarchical sequential labeling network ( HSLN ) .",21,0,30
dataset/preprocessed/training-data/sentence_classification/1,"Our model first uses a RNN or CNN layer to individually encode the sentence representation from the sequence of word embeddings , then uses another bi - LSTM layer to take as input the individual sentence representation and output the contextualized sentence representation , subsequently uses a single - hidden - layer feed - forward network to transform the sentence representation to the probability vector , and finally optimizes the predicted label sequence jointly via a CRF layer .",22,0,79
dataset/preprocessed/training-data/sentence_classification/1,"We evaluate our model on two benchmarking datasets , PubMed RCT ( Dernoncourt and and NICTA - PIBOSO , which were both generated from the PubMed data base 1 .",23,0,30
dataset/preprocessed/training-data/sentence_classification/1,Our key contributions are summarized as follows :,24,0,8
dataset/preprocessed/training-data/sentence_classification/1,"Based on the previous best performing architecture for sequential sentence classification , we add one more layer to extract contextual information from surrounding sentences for more accurate prediction of the current one .",26,0,33
dataset/preprocessed/training-data/sentence_classification/1,"Together with the CRF algorithm , this allows us to make use of not only the preceding labels ' information but also the content and semantics of adjacent sentences to infer the label of the target sentence .",27,0,38
dataset/preprocessed/training-data/sentence_classification/1,We remove the need for a character - based word embedding component without sacrificing performance .,29,0,16
dataset/preprocessed/training-data/sentence_classification/1,"For individual sentence encoding , we propose the use of a CNN module as an alternative to RNN for small datasets , suffering less from over-fitting as evidenced by our experiments .",30,0,32
dataset/preprocessed/training-data/sentence_classification/1,"Moreover , we incorporate attention - based pooling in both RNN and CNN models to further improve the performance .",31,0,20
dataset/preprocessed/training-data/sentence_classification/1,We adopt dropout with expectation - linear regularization instead of the standard one to reduce the performance gap between training and test phases .,33,0,24
dataset/preprocessed/training-data/sentence_classification/1,"We obtain state - of - the - art results on two datasets for sequential sentence classification in medical abstracts , outperforming the previous best models by at least 2 % in terms of F1 scores .",35,0,37
dataset/preprocessed/training-data/sentence_classification/1,Previous systems for sequential sentence classification concentrate on the rhetorical structure analysis of biomedical abstracts .,37,0,16
dataset/preprocessed/training-data/sentence_classification/1,"They are mainly based on naive Bayes , support vector machine ( SVM ) , Hidden Markov Model ( HMM ) , and CRF ) .",38,0,26
dataset/preprocessed/training-data/sentence_classification/1,"All these methods heavily rely on numerous carefully hand - engineered features such as lexical ( bag - of - words ( BOW ) ) , semantic ( hypernyms , synonyms ) , structural ( part of speech ( POS ) tags , lemmas , orthographic shapes , headings ) , statistical ( statistical distributions of token types ) and sequential ( sentence position , surrounding features , predicted labels ) features .",39,0,73
dataset/preprocessed/training-data/sentence_classification/1,"In contrast , current emerging artificial neural network ( ANN ) based models have removed the need for manually selected features ; instead , features are self - learned from the token and / or character embeddings .",40,0,38
dataset/preprocessed/training-data/sentence_classification/1,"These deep learning models have revolutionized the natural language processing ( NLP ) field with state - of - the - art results achieved in various tasks , including the most relevant text classification task .",41,0,36
dataset/preprocessed/training-data/sentence_classification/1,"Most of these models are built upon deep CNNs or RNN s as well as combinations of them , where CNN is good at extracting local n-gram features while RNN is suitable for sequence modeling .",42,0,36
dataset/preprocessed/training-data/sentence_classification/1,"The above - mentioned works for short - text classification do not consider any context of sentence semantics in the models , making them underperform in the sequential sentence classification scenario , where surrounding sentences can play a big role in inferring the label of the current sentence .",43,0,49
dataset/preprocessed/training-data/sentence_classification/1,"Recent works that apply deep neural networks to the sequential sentence classification problem include the system proposed by , where the preceding utterances were used to help classify the current utterance in a dialog into the corresponding dialogue act .",44,0,40
dataset/preprocessed/training-data/sentence_classification/1,Most recent work from Dernoncourt et al .,45,0,8
dataset/preprocessed/training-data/sentence_classification/1,"used a CRF layer to optimize the predicted label sequence , where the preceding labels have influence on determining the current label .",46,0,23
dataset/preprocessed/training-data/sentence_classification/1,This model outperformed the state - of - the - art results on two datasets PubMed RCT and NICTA - PIBOSO for sentence classification in medical abstracts .,47,0,28
dataset/preprocessed/training-data/sentence_classification/0,Structural Scaffolds for Citation Intent Classification in Scientific Publications,2,1,9
dataset/preprocessed/training-data/sentence_classification/0,"Identifying the intent of a citation in scientific papers ( e.g. , background information , use of methods , comparing results ) is critical for machine reading of individual publications and automated analysis of the scientific literature .",4,1,38
dataset/preprocessed/training-data/sentence_classification/0,"We propose structural scaffolds , a multitask model to incorporate structural information of scientific papers into citations for effective classification of citation intents .",5,0,24
dataset/preprocessed/training-data/sentence_classification/0,"Our model achieves a new state - of the - art on an existing ACL anthology dataset ( ACL - ARC ) with a 13.3 % absolute increase in F1 score , without relying on external linguistic resources or hand - engineered features as done in existing methods .",6,0,49
dataset/preprocessed/training-data/sentence_classification/0,"In addition , we introduce a new dataset of citation intents ( Sci - Cite ) which is more than five times larger and covers multiple scientific domains compared with existing datasets .",7,0,33
dataset/preprocessed/training-data/sentence_classification/0,Our code and data are available at : https://github.com/ allenai/scicite .,8,0,11
dataset/preprocessed/training-data/sentence_classification/0,Citations play a unique role in scientific discourse and are crucial for understanding and analyzing scientific work .,10,0,18
dataset/preprocessed/training-data/sentence_classification/0,"They are also typically used as the main measure for assessing impact of scientific publications , venues , and researchers .",11,0,21
dataset/preprocessed/training-data/sentence_classification/0,The nature of citations can be different .,12,0,8
dataset/preprocessed/training-data/sentence_classification/0,Some citations indicate direct use of a method while some others merely serve as acknowledging a prior work .,13,0,19
dataset/preprocessed/training-data/sentence_classification/0,"Therefore , identifying the intent of citations ( is critical in improving automated analysis of academic literature and scientific impact measurement .",14,0,22
dataset/preprocessed/training-data/sentence_classification/0,"Other applications of citation intent classification are enhanced research experience , information retrieval , summarization ( Co - .",15,0,19
dataset/preprocessed/training-data/sentence_classification/0,"A previously described computerized force sensitive system was used to quantify gait cycle timing , specifically the swing time and the stride - to - stride variability of swing time .",16,0,31
dataset/preprocessed/training-data/sentence_classification/0,"Title : Gait asymmetry in patients with Parkinson 's disease and elderly fallers ... han and , and studying evolution of scientific fields .",18,0,24
dataset/preprocessed/training-data/sentence_classification/0,"In this work , we approach the problem of citation intent classification by modeling the language expressed in the citation context .",19,1,22
dataset/preprocessed/training-data/sentence_classification/0,A citation context includes text spans in a citing paper describing a referenced work and has been shown to be the primary signal in intent classification .,20,0,27
dataset/preprocessed/training-data/sentence_classification/0,"Existing models for this problem are feature - based , modeling the citation context with respect to a set of predefined handengineered features ( such as linguistic patterns or cue phrases ) and ignoring other signals that could improve prediction .",21,0,41
dataset/preprocessed/training-data/sentence_classification/0,"In this paper we argue that better representations can be obtained directly from data , sidestepping problems associated with external features .",22,0,22
dataset/preprocessed/training-data/sentence_classification/0,"To this end , we propose a neural multitask learning framework to incorporate knowledge into citations from the structure of scientific papers .",23,0,23
dataset/preprocessed/training-data/sentence_classification/0,"In particular , we propose two auxiliary tasks as structural scaffolds to improve citation intent prediction : 1 ( 1 ) predicting the section title in which the citation occurs and ( 2 ) predicting whether a sentence needs a citation .",24,0,42
dataset/preprocessed/training-data/sentence_classification/0,"Unlike the primary task of citation intent prediction , it is easy to collect large amounts of training data for scaffold tasks since the labels naturally occur in the process of writing a paper and thus , there is no need for manual annotation .",25,0,45
dataset/preprocessed/training-data/sentence_classification/0,"On two datasets , we show that the proposed neural scaffold model outperforms existing methods by large margins .",26,0,19
dataset/preprocessed/training-data/sentence_classification/0,"Our contributions are : ( i ) we propose a neural scaffold framework for citation intent classification to incorporate into citations knowledge from structure of scientific papers ; ( ii ) we achieve a new state - of - the - art of 67.9 % F1 on the ACL - ARC citations benchmark , an absolute 13.3 % increase over the previous state - of - the - art ; and ( iii ) we introduce SciCite , a new dataset of citation intents which is at least five times as large as existing datasets and covers a variety of scientific domains .",27,0,103
dataset/preprocessed/training-data/sentence_classification/0,We propose a neural multitask learning framework for classification of citation intents .,29,0,13
dataset/preprocessed/training-data/sentence_classification/0,"In particular , we introduce and use two structural scaffolds , auxiliary tasks related to the structure of scientific papers .",30,0,21
dataset/preprocessed/training-data/sentence_classification/0,The auxiliary tasks may not be of interest by themselves but are used to inform the main task .,31,0,19
dataset/preprocessed/training-data/sentence_classification/0,Our model uses a large auxiliary dataset to incorporate this structural information available in scientific documents into the citation intents .,32,0,21
dataset/preprocessed/training-data/sentence_classification/0,The overview of our model is illustrated in .,33,0,9
dataset/preprocessed/training-data/sentence_classification/0,Let C denote the citation and x denote the ci-tation context relevant to C .,34,0,15
dataset/preprocessed/training-data/sentence_classification/0,"We encode the tokens in the citation context of size n as x = {x 1 , ... , x n } , where xi ?",35,0,26
dataset/preprocessed/training-data/sentence_classification/0,"Rd 1 is a word vector of size d 1 which concatenates non-contextualized word representations ( Glo Ve , and contextualized embeddings , i.e. :",36,0,25
dataset/preprocessed/training-data/sentence_classification/0,We then use a bidirectional long short - term memory ) ( BiL - STM ) network with hidden size of d 2 to obtain a contextual representation of each token vector with respect to the entire sequence :,37,0,39
dataset/preprocessed/training-data/sentence_classification/0,"where h ? R ( n , 2d 2 ) and ? ??? ?",39,0,14
dataset/preprocessed/training-data/sentence_classification/0,"LSTM ( x , i ) processes x from left to write and returns the LSTM hidden state at position i ( and vice versa for the backward direction ? ??? ?",40,0,32
dataset/preprocessed/training-data/sentence_classification/0,LSTM ) .,41,0,3
dataset/preprocessed/training-data/sentence_classification/0,We then use an attention mechanism to get a single vector representing the whole input sequence :,42,0,17
dataset/preprocessed/training-data/sentence_classification/0,where w is a parameter served as the query vector for dot - product attention .,43,0,16
dataset/preprocessed/training-data/sentence_classification/0,So far we have obtained the citation representation as a vector z .,45,0,13
dataset/preprocessed/training-data/sentence_classification/0,"Next , we describe our two proposed structural scaffolds for citation intent prediction .",46,0,14
dataset/preprocessed/training-data/sentence_classification/0,In scientific writing there is a connection between the structure of scientific papers and the intent of citations .,48,0,19
dataset/preprocessed/training-data/sentence_classification/0,"To leverage this connection for more effective classification of citation intents , we propose a multitask framework with two structural scaffolds ( auxiliary tasks ) related to the structure of scientific documents .",49,0,33
dataset/preprocessed/training-data/sentence_classification/2,Translations as Additional Contexts for Sentence Classification,2,1,7
dataset/preprocessed/training-data/sentence_classification/2,"In sentence classification tasks , additional contexts , such as the neighboring sentences , may improve the accuracy of the classifier .",4,0,22
dataset/preprocessed/training-data/sentence_classification/2,"However , such contexts are domain - dependent and thus can not be used for another classification task with an inappropriate domain .",5,0,23
dataset/preprocessed/training-data/sentence_classification/2,"In contrast , we propose the use of translated sentences as domain - free context that is always available regardless of the domain .",6,0,24
dataset/preprocessed/training-data/sentence_classification/2,"We find that naive feature expansion of translations gains only marginal improvements and may decrease the performance of the classifier , due to possible inaccurate translations thus producing noisy sentence vectors .",7,0,32
dataset/preprocessed/training-data/sentence_classification/2,"To this end , we present multiple context fixing attachment ( MCFA ) , a series of modules attached to multiple sentence vectors to fix the noise in the vectors using the other sentence vectors as context .",8,0,38
dataset/preprocessed/training-data/sentence_classification/2,"We show that our method performs competitively compared to previous models , achieving best classification performance on multiple data sets .",9,0,21
dataset/preprocessed/training-data/sentence_classification/2,We are the first to use translations as domainfree contexts for sentence classification .,10,0,14
dataset/preprocessed/training-data/sentence_classification/2,"One of the primary tasks in natural language processing ( NLP ) is sentence classification , where given a sentence ( e.g. a sentence of a review ) as input , we are tasked to classify it into one of multiple classes ( e.g. into positive or negative ) .",12,0,50
dataset/preprocessed/training-data/sentence_classification/2,"This task is important as it is widely used in almost all subare as of NLP such as sentiment classification for sentiment analysis and question type classification for question answering , to name a few .",13,0,36
dataset/preprocessed/training-data/sentence_classification/2,"While past methods require feature engineering , recent methods enjoy neural - based methods to automatically encode the sentences into low - dimensional dense vectors .",14,0,26
dataset/preprocessed/training-data/sentence_classification/2,"Despite the success of these methods , the major challenge in this task is that extracting features from a single sentence limits the performance .",15,0,25
dataset/preprocessed/training-data/sentence_classification/2,"To overcome this limitation , recent works attempted to augment different kinds of features to the sentence , such as the neighboring sentences and the topics of the sentences .",16,0,30
dataset/preprocessed/training-data/sentence_classification/2,"However , these methods used domain - dependent contexts thatare only effective when the domain of the task is appropriate .",17,0,21
dataset/preprocessed/training-data/sentence_classification/2,"For one thing , neighboring sentences may not be available in some tasks such as question type classification .",18,0,19
dataset/preprocessed/training-data/sentence_classification/2,"Moreover , topics inferred using topic models may produce less useful topics when the data set is domain - specific such as movie review sentiment classification .",19,0,27
dataset/preprocessed/training-data/sentence_classification/2,"In this paper , we propose the usage of translations as compelling and effective domain - free contexts , or contexts that are always available no matter what the task domain is .",20,0,33
dataset/preprocessed/training-data/sentence_classification/2,We observe two opportunities when using translations .,21,0,8
dataset/preprocessed/training-data/sentence_classification/2,"First , each language has its own linguistic and cultural characteristics that may contain different signals to effectively classify a specific class .",22,0,23
dataset/preprocessed/training-data/sentence_classification/2,contrasts the sentence vectors of the original English sentences and their Arabictranslated sentences in the question type classification task .,23,0,20
dataset/preprocessed/training-data/sentence_classification/2,A yellow circle signifies a clear separation of a class .,24,0,11
dataset/preprocessed/training-data/sentence_classification/2,"For example , the green class , or the numeric question type , is circled in the Arabic space as it is clearly separated from other classes , while such separation can not be observed in English .",25,0,38
dataset/preprocessed/training-data/sentence_classification/2,"Meanwhile , location type questions ( in orange ) are better classified in English .",26,0,15
dataset/preprocessed/training-data/sentence_classification/2,"Second , the original sentences may include languagespecific ambiguity , which maybe resolved when presented with its translations .",27,0,19
dataset/preprocessed/training-data/sentence_classification/2,"Consider the example English sentence "" The movie is terribly amazing "" for the sentiment classification task .",28,0,18
dataset/preprocessed/training-data/sentence_classification/2,"In this case , terribly can be used in both positive and negative sense , thus introduces ambiguity in the sentence .",29,0,22
dataset/preprocessed/training-data/sentence_classification/2,"When translated to Korean , it becomes "" ? ? ?? ?? ? ? ? ?? ? ?? ? ? ? ? ? ? ?? ? ?? ?? ? "" which means "" The movie is greatly magnificent "" , removing the ambiguity .",30,0,44
dataset/preprocessed/training-data/sentence_classification/2,The above two observations hold only when translations are supported for ( nearly ) arbitrary language pairs with sufficiently high quality .,31,0,22
dataset/preprocessed/training-data/sentence_classification/2,"Thankfully , translation services ( e.g. Google Translate )",32,0,9
dataset/preprocessed/training-data/sentence_classification/2,"Moreover , recent research on neural machine translation ( NMT ) improved the efficiency and even enabled zero - shot translation of models for languages with no parallel data .",33,0,30
dataset/preprocessed/training-data/sentence_classification/2,"This provides an opportunity to leverage on as many languages as possible to any domain , providing a much wider context compared to the limited contexts provided by past studies .",34,0,31
dataset/preprocessed/training-data/sentence_classification/2,"However , despite the maturity of translation , naively concatenating their vectors to the original sentence vector may introduce more noise than signals .",35,0,24
dataset/preprocessed/training-data/sentence_classification/2,The unaltered translation space on the left of shows an example where translation noises make the two classes indistinguishable .,36,0,20
dataset/preprocessed/training-data/sentence_classification/2,"In this paper , we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations .",37,0,25
dataset/preprocessed/training-data/sentence_classification/2,Suppose there are two translated sentences a and b with slight errors .,38,0,13
dataset/preprocessed/training-data/sentence_classification/2,"We posit that a can be used to fix b when a is used as a context of b , and vice versa",39,0,23
dataset/preprocessed/training-data/sentence_classification/2,"Revisiting the example above , to fix the vector of the English sentence "" The movie is terribly amazing "" , we use the Korean translation to move the vector towards the location where the vector "" The movie is greatly magnificent "" is .",41,0,45
dataset/preprocessed/training-data/sentence_classification/2,"Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .",42,0,18
dataset/preprocessed/training-data/sentence_classification/2,"MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .",43,0,34
dataset/preprocessed/training-data/sentence_classification/2,"Fixing the vectors is done by selectively moving the vectors to a location in the same vector space that better separates the class , as shown in .",44,0,28
dataset/preprocessed/training-data/sentence_classification/2,Noises from translation may cause adverse effects to the vector itself ( e.g. when a noisy vector is directly used for the task ) and relatively to other vectors ( e.g. when a noisy vector is used to fix another noisy vector ) .,45,0,44
dataset/preprocessed/training-data/sentence_classification/2,MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .,46,0,36
dataset/preprocessed/training-data/sentence_classification/2,"( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.",47,0,23
dataset/preprocessed/training-data/sentence_classification/2,Listed below are the three main strengths of the MCFA attachment .,48,0,12
dataset/preprocessed/training-data/sentence_classification/2,"( 1 ) MCFA is attached after encoding the sentence , which makes it widely adaptable to other models .",49,0,20
dataset/preprocessed/training-data/question_similarity/0,Tha3aroon at NSURL - 2019 Task 8 : Semantic Question Similarity in Arabic,2,1,13
dataset/preprocessed/training-data/question_similarity/0,"In this paper , we describe our team 's effort on the semantic text question similarity task of NSURL 2019 .",4,1,21
dataset/preprocessed/training-data/question_similarity/0,Our top performing system utilizes several innovative data augmentation techniques to enlarge the training data .,5,0,16
dataset/preprocessed/training-data/question_similarity/0,"Then , it takes ELMo pre-trained contextual embeddings of the data and feeds them into an ON - LSTM network with self - attention .",6,0,25
dataset/preprocessed/training-data/question_similarity/0,This results in sequence representation vectors that are used to predict the relation between the question pairs .,7,0,18
dataset/preprocessed/training-data/question_similarity/0,"The model is ranked in the 1st place with 96.499 F1score ( same as the second place F1-score ) and the 2nd place with 94.848 F1- score ( differs by 1.076 F1-score from the first place ) on the public and private leaderboards , respectively .",8,0,46
dataset/preprocessed/training-data/question_similarity/0,Semantic Text Similarity ( STS ) problems are both real - life and challenging .,10,1,15
dataset/preprocessed/training-data/question_similarity/0,"For example , in the paraphrase identification task , STS is used to predict if one sentence is a paraphrase of the other or not .",11,1,26
dataset/preprocessed/training-data/question_similarity/0,"Also , in answer sentence selection task , it is utilized to determine the relevance between question - answer pairs and rank the answers sentences from the most relevant to the least .",12,0,33
dataset/preprocessed/training-data/question_similarity/0,This idea can also be applied to search engines in order to find documents relevant to a query .,13,0,19
dataset/preprocessed/training-data/question_similarity/0,A new task has been proposed by Mawdoo3 1 company with a new dataset provided by their data annotation team for Semantic Question Similarity ( SQS ) for the Arabic language .,14,1,32
dataset/preprocessed/training-data/question_similarity/0,"SQS is a variant of STS , which aims to compare a pair of questions and determine whether they have the same meaning or not .",15,1,26
dataset/preprocessed/training-data/question_similarity/0,The SQS in Arabic task is one of the shared tasks of the Workshop on NLP Solutions for Under Resourced Languages and it consists of 12K questions pairs .,16,0,29
dataset/preprocessed/training-data/question_similarity/0,"In this paper , we describe our team 's efforts to tackle this task .",17,0,15
dataset/preprocessed/training-data/question_similarity/0,"After preprocessing the data , we use four data augmentation steps to enlarge the training data to about four times the size of the original training data .",18,0,28
dataset/preprocessed/training-data/question_similarity/0,We then build a neural network model with four components .,19,0,11
dataset/preprocessed/training-data/question_similarity/0,The model uses ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings as an input and builds sequence representation vectors that are used to predict the relation between the question pairs .,20,0,36
dataset/preprocessed/training-data/question_similarity/0,"The task is hosted on Kaggle 2 platform and our model is ranked in the first place with 96.499 F1- score ( same as the second place F1-score ) and in the second place with 94.848 F1 - score ( differs by 1.076 F1-score from the first place ) on the public and private leaderboards , respectively .",21,0,58
dataset/preprocessed/training-data/question_similarity/0,The rest of this paper is organized as follows .,22,0,10
dataset/preprocessed/training-data/question_similarity/0,"In Section 2 , we describe our methodology , including data preprocessing , data augmentation , and model structure , while in Section 3 , we present our experimental results and discuss some insights from our model .",23,0,38
dataset/preprocessed/training-data/question_similarity/0,"Finally , the paper is concluded in Section 4 .",24,0,10
dataset/preprocessed/training-data/question_similarity/0,"In this section , we present a detailed description of our model .",26,0,13
dataset/preprocessed/training-data/question_similarity/0,"We start by discussing the preprecessing steps we take before going into the details of the first novel aspect of our work , which is the data augmentation techniques .",27,0,30
dataset/preprocessed/training-data/question_similarity/0,We then discuss the neural network model starting from the input all the way to the decision step .,28,0,19
dataset/preprocessed/training-data/question_similarity/0,The implementation is available on a public repository .,29,0,9
dataset/preprocessed/training-data/question_similarity/0,"In this work , we only consider one preprocessing step , which is to separate the punctuation marks shown in from the letters .",32,0,24
dataset/preprocessed/training-data/question_similarity/0,"For example , if the question was : "" "" , then it will be processed as follows : "" "" .",33,0,22
dataset/preprocessed/training-data/question_similarity/0,This is done to preserve as much information as possible in the questions while keeping the words clear of punctuations .,34,0,21
dataset/preprocessed/training-data/question_similarity/0,"The training data contains 11,997 question pairs : 5,397 labeled as 1 ( i.e. , similar ) and 6,600 labeled as 0 ( i.e. , not similar ) .",36,0,29
dataset/preprocessed/training-data/question_similarity/0,"To obtain a larger dataset , we augment the data using the following rules .",37,0,15
dataset/preprocessed/training-data/question_similarity/0,"Suppose we have questions A , B and C",38,0,9
dataset/preprocessed/training-data/question_similarity/0,Positive Transitive :,39,0,3
dataset/preprocessed/training-data/question_similarity/0,"If A is similar to B , and B is similar to C , then A is similar to C.",40,0,20
dataset/preprocessed/training-data/question_similarity/0,Negative Transitive :,41,0,3
dataset/preprocessed/training-data/question_similarity/0,"If A is similar to B , and B is NOT similar to C , then A is NOT similar to C.",42,0,22
dataset/preprocessed/training-data/question_similarity/0,"Note : The previous two rules generates 5,490 extra examples ( bringing the total up to 17,487 ) .",43,0,19
dataset/preprocessed/training-data/question_similarity/0,"If A is similar to B then B is similar to A , and if A is not similar to B then B is not similar to A.",45,0,28
dataset/preprocessed/training-data/question_similarity/0,"This rule doubles the number of examples to 34,974 in total .",47,0,12
dataset/preprocessed/training-data/question_similarity/0,shows the growth of the training dataset after each data augmentation step .,48,0,13
dataset/preprocessed/training-data/question_generation/1,Multimodal Differential Network for Visual Question Generation,2,1,7
dataset/preprocessed/training-data/question_generation/1,Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations .,4,1,22
dataset/preprocessed/training-data/question_generation/1,"Images can have multiple visual and language contexts thatare relevant for generating questions namely places , captions , and tags .",5,0,21
dataset/preprocessed/training-data/question_generation/1,"In this paper , we propose the use of exemplars for obtaining the relevant context .",6,0,16
dataset/preprocessed/training-data/question_generation/1,We obtain this by using a Multimodal Differential Network to produce natural and engaging questions .,7,0,16
dataset/preprocessed/training-data/question_generation/1,The generated questions show a remarkable similarity to the natural questions as validated by a human study .,8,0,18
dataset/preprocessed/training-data/question_generation/1,"Further , we observe that the proposed approach substantially improves over state - of - the - art benchmarks on the quantitative metrics ( BLEU , METEOR , ROUGE , and CIDEr ) .",9,0,34
dataset/preprocessed/training-data/question_generation/1,"To understand the progress towards multimedia vision and language understanding , a visual Turing test was proposed by that was aimed at visual question answering .",11,0,26
dataset/preprocessed/training-data/question_generation/1,Visual ) is a natural extension for VQA .,12,0,9
dataset/preprocessed/training-data/question_generation/1,"Current dialog systems as evaluated in show that when trained between bots , AI - AI dialog systems show improvement , but that does not translate to actual improvement for Human - AI dialog .",13,0,35
dataset/preprocessed/training-data/question_generation/1,"This is because , the questions generated by bots are not natural ( human - like ) and therefore does not translate to improved human dialog .",14,0,27
dataset/preprocessed/training-data/question_generation/1,Therefore it is imperative that improvement in the quality of questions will enable dialog agents to perform well in human interactions .,15,0,22
dataset/preprocessed/training-data/question_generation/1,"Further , show that unanswered questions can be used for improving VQA , Image captioning and Object Classification .",16,0,19
dataset/preprocessed/training-data/question_generation/1,An interesting line of work in this respect is the work of .,17,0,13
dataset/preprocessed/training-data/question_generation/1,Here the au-thors have proposed the challenging task of generating natural questions for an image .,18,1,16
dataset/preprocessed/training-data/question_generation/1,One aspect that is central to a question is the context that is relevant to generate it .,19,0,18
dataset/preprocessed/training-data/question_generation/1,"However , this context changes for every image .",20,0,9
dataset/preprocessed/training-data/question_generation/1,"As can be seen in , an image with a person on a skateboard would result in questions related to the event .",21,0,23
dataset/preprocessed/training-data/question_generation/1,"Whereas for a little girl , the questions could be related to age rather than the action .",22,0,18
dataset/preprocessed/training-data/question_generation/1,How can one have widely varying context provided for generating questions ?,23,0,12
dataset/preprocessed/training-data/question_generation/1,"To solve this problem , we use the context obtained by considering exemplars , specifically we use the difference between relevant and irrelevant exemplars .",24,0,25
dataset/preprocessed/training-data/question_generation/1,"We consider different contexts in the form of Location , Caption , and Part of Speech tags .",25,0,18
dataset/preprocessed/training-data/question_generation/1,Our method implicitly uses a differential context obtained through supporting and contrasting exemplars to obtain a differentiable embedding .,26,0,19
dataset/preprocessed/training-data/question_generation/1,This embedding is used by a question decoder to decode the appropriate question .,27,0,14
dataset/preprocessed/training-data/question_generation/1,"As discussed further , we observe this implicit differential context to perform better than an explicit keyword based context .",28,0,20
dataset/preprocessed/training-data/question_generation/1,The difference between the two approaches is illustrated in .,29,0,10
dataset/preprocessed/training-data/question_generation/1,This also allows for better optimization as we can backpropagate through the whole network .,30,0,15
dataset/preprocessed/training-data/question_generation/1,We provide detailed empirical evidence to support our hypothesis .,31,0,10
dataset/preprocessed/training-data/question_generation/1,As seen in our method generates natural questions and improves over the state - of the - art techniques for this problem .,32,0,23
dataset/preprocessed/training-data/question_generation/1,2 : Here we provide intuition for using implicit embeddings instead of explicit ones .,34,0,15
dataset/preprocessed/training-data/question_generation/1,"As explained in section 1 , the question obtained by the implicit embeddings are natural and holistic than the explicit ones .",35,0,22
dataset/preprocessed/training-data/question_generation/1,"To summarize , we propose a multimodal differential network to solve the task of visual question generation .",36,0,18
dataset/preprocessed/training-data/question_generation/1,Our contributions are : ( 1 ) A method to incorporate exemplars to learn differential embeddings that captures the subtle differences between supporting and contrasting examples and aid in generating natural questions .,37,0,33
dataset/preprocessed/training-data/question_generation/1,"( 2 ) We provide Multimodal differential embeddings , as image or text alone does not capture the whole context and we show that these embeddings outperform the ablations which incorporate cues such as only image , or tags or place information .",38,0,43
dataset/preprocessed/training-data/question_generation/1,( 3 ) We provide a thorough comparison of the proposed network against state - of - the - art benchmarks along with a user study and statistical significance test .,39,0,31
dataset/preprocessed/training-data/question_generation/1,Generating a natural and engaging question is an interesting and challenging task for a smart robot ( like chat - bot ) .,41,0,23
dataset/preprocessed/training-data/question_generation/1,It is a step towards having a natural visual dialog instead of the widely prevalent visual question answering bots .,42,0,20
dataset/preprocessed/training-data/question_generation/1,"Further , having the ability to ask natural questions based on different contexts is also useful for artificial agents that can interact with visually impaired people .",43,0,27
dataset/preprocessed/training-data/question_generation/1,"While the task of generating question automatically is well studied in NLP community , it has been relatively less studied for image - related natural questions .",44,0,27
dataset/preprocessed/training-data/question_generation/1,This is still a difficult task that has gained recent interest in the community .,45,0,15
dataset/preprocessed/training-data/question_generation/1,Recently there have been many deep learning based approaches as well for solving the textbased question generation task such as .,46,0,21
dataset/preprocessed/training-data/question_generation/1,"Further , have proposed a method to generate a factoid based question based on triplet set { subject , relation and ob - ject } to capture the structural representation of text and the corresponding generated question .",47,0,38
dataset/preprocessed/training-data/question_generation/1,"These methods , however , were limited to textbased question generation .",48,0,12
dataset/preprocessed/training-data/question_generation/1,"There has been extensive work done in the Vision and Language domain for solving image captioning , paragraph generation , Visual Question Answering ( VQA ) and Visual Dialog .",49,0,30
dataset/preprocessed/training-data/question_generation/0,Neural Question Generation from Text : A Preliminary Study,2,1,9
dataset/preprocessed/training-data/question_generation/0,Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub- spans of the given passage .,4,1,27
dataset/preprocessed/training-data/question_generation/0,Traditional methods mainly use rigid heuristic rules to transform a sentence into related questions .,5,0,15
dataset/preprocessed/training-data/question_generation/0,"In this work , we propose to apply the neural encoderdecoder model to generate meaningful and diverse questions from natural language sentences .",6,0,23
dataset/preprocessed/training-data/question_generation/0,"The encoder reads the input text and the answer position , to produce an answer - aware input representation , which is fed to the decoder to generate an answer focused question .",7,0,33
dataset/preprocessed/training-data/question_generation/0,"We conduct a preliminary study on neural question generation from text with the SQuAD dataset , and the experiment results show that our method can produce fluent and diverse questions .",8,0,31
dataset/preprocessed/training-data/question_generation/0,"Automatic question generation from natural language text aims to generate questions taking text as input , which has the potential value of education purpose ) .",10,1,26
dataset/preprocessed/training-data/question_generation/0,"As the reverse task of question answering , question generation also has the potential for providing a large scale corpus of question - answer pairs .",11,1,26
dataset/preprocessed/training-data/question_generation/0,Previous works for question generation mainly use rigid heuristic rules to transform a sentence into related questions .,12,0,18
dataset/preprocessed/training-data/question_generation/0,"However , these methods heavily rely on human - designed transformation and generation rules , which can not be easily adopted to other domains .",13,0,25
dataset/preprocessed/training-data/question_generation/0,"Instead of generating questions from texts , proposed a neu - * Contribution during internship at Microsoft Research .",14,0,19
dataset/preprocessed/training-data/question_generation/0,ral network method to generate factoid questions from structured data .,15,0,11
dataset/preprocessed/training-data/question_generation/0,"In this work we conduct a preliminary study on question generation from text with neural networks , which is denoted as the Neural Question Generation ( NQG ) framework , to generate natural language questions from text without pre-defined rules .",16,0,41
dataset/preprocessed/training-data/question_generation/0,The Neural Question Generation framework extends the sequence - to - sequence models by enriching the encoder with answer and lexical features to generate answer focused questions .,17,0,28
dataset/preprocessed/training-data/question_generation/0,"Concretely , the encoder reads not only the input sentence , but also the answer position indicator and lexical features .",18,0,21
dataset/preprocessed/training-data/question_generation/0,"The answer position feature denotes the answer span in the input sentence , which is essential to generate answer relevant questions .",19,0,22
dataset/preprocessed/training-data/question_generation/0,The lexical features include part - of - speech ( POS ) and named entity ( NER ) tags to help produce better sentence encoding .,20,0,26
dataset/preprocessed/training-data/question_generation/0,"Lastly , the decoder with attention mechanism generates an answer specific question of the sentence .",21,0,16
dataset/preprocessed/training-data/question_generation/0,Large - scale manually annotated passage and question pairs play a crucial role in developing question generation systems .,22,0,19
dataset/preprocessed/training-data/question_generation/0,We propose to adapt the recently released Stanford Question Answering Dataset ( SQuAD ) as the training and development datasets for the question generation task .,23,0,26
dataset/preprocessed/training-data/question_generation/0,"In SQuAD , the answers are labeled as subsequences in the given sentences by crowed sourcing , and it contains more than 100K questions which makes it feasible to train our neural network models .",24,0,35
dataset/preprocessed/training-data/question_generation/0,"We conduct the experiments on SQuAD , and the experiment results show the neural network models can produce fluent and diverse questions from text .",25,0,25
dataset/preprocessed/training-data/question_generation/0,"In this section , we introduce the NQG framework , which consists of a feature - rich encoder and an arXiv : 1704.01792v3 [ cs. CL ] 18 Apr 2017 attention - based decoder .",27,0,35
dataset/preprocessed/training-data/question_generation/0,provides an overview of our NQG framework .,28,0,8
dataset/preprocessed/training-data/question_generation/0,Feature - Rich Encoder,29,0,4
dataset/preprocessed/training-data/question_generation/0,"In the NQG framework , we use Gated Recurrent Unit ( GRU ) to build the encoder .",30,0,18
dataset/preprocessed/training-data/question_generation/0,"To capture more context information , we use bidirectional GRU ( BiGRU ) to read the inputs in both forward and backward orders .",31,0,24
dataset/preprocessed/training-data/question_generation/0,"Inspired by ; , the BiGRU encoder not only reads the sentence words , but also handcrafted features , to produce a sequence of word - and - feature vectors .",32,0,31
dataset/preprocessed/training-data/question_generation/0,"We concatenate the word vector , lexical feature embedding vectors and answer position indicator embedding vector as the input of BiGRU encoder .",33,0,23
dataset/preprocessed/training-data/question_generation/0,"Concretely , the BiGRU encoder reads the concatenated sentence word vector , lexical features , and answer position feature , x = ( x 1 , x 2 , . . . , x n ) , to produce two sequences of hidden vectors , i.e. , the forward sequence ( h 1 , h 2 , . . . , h n ) and the backward sequence ( h 1 , h 2 , . . . , h n ) .",34,0,83
dataset/preprocessed/training-data/question_generation/0,"Lastly , the output sequence of the encoder is the concatenation of the two sequences , i.e. ,",35,0,18
dataset/preprocessed/training-data/question_generation/0,Answer Position Feature,36,0,3
dataset/preprocessed/training-data/question_generation/0,"To generate a question with respect to a specific answer in a sentence , we propose using answer position feature to locate the target answer .",37,0,26
dataset/preprocessed/training-data/question_generation/0,"In this work , the BIO tagging scheme is used to label the position of a target answer .",38,0,19
dataset/preprocessed/training-data/question_generation/0,"In this scheme , tag B denotes the start of an answer , tag I continues the answer and tag O marks words that do not form part of an answer .",39,0,32
dataset/preprocessed/training-data/question_generation/0,The BIO tags of answer position are embedded to real - valued vectors throu and fed to the featurerich encoder .,40,0,21
dataset/preprocessed/training-data/question_generation/0,"With the BIO tagging feature , the answer position is encoded to the hidden vectors and used to generate answer focused questions .",41,0,23
dataset/preprocessed/training-data/question_generation/0,"Besides the sentence words , we also feed other lexical features to the encoder .",43,0,15
dataset/preprocessed/training-data/question_generation/0,"To encode more linguistic information , we select word case , POS and NER tags as the lexical features .",44,0,20
dataset/preprocessed/training-data/question_generation/0,"As an intermediate layer of full parsing , POS tag feature is important in many NLP tasks , such as information extraction and dependency parsing .",45,0,26
dataset/preprocessed/training-data/question_generation/0,"Considering that SQuAD is constructed using Wikipedia articles , which contain lots of named entities , we add NER feature to help detecting them .",46,0,25
dataset/preprocessed/training-data/question_generation/0,Attention - Based Decoder,47,0,4
dataset/preprocessed/training-data/question_generation/0,We employ an attention - based GRU decoder to decode the sentence and answer information to generate questions .,48,0,19
dataset/preprocessed/training-data/question_generation/0,"At decoding time step t , the GRU decoder reads the previous word embedding w t?1 and context vector c t?1 to compute the new hidden state st .",49,0,29
dataset/preprocessed/training-data/sarcasm_detection/1,CASCADE : Contextual Sarcasm Detection in Online Discussion Forums,2,1,9
dataset/preprocessed/training-data/sarcasm_detection/1,"The literature in automated sarcasm detection has mainly focused on lexical , syntactic and semantic - level analysis of text .",4,1,21
dataset/preprocessed/training-data/sarcasm_detection/1,"However , a sarcastic sentence can be expressed with contextual presumptions , background and commonsense knowledge .",5,0,17
dataset/preprocessed/training-data/sarcasm_detection/1,"In this paper , we propose CASCADE ( a ContextuAl SarCasm DEtector ) that adopts a hybrid approach of both content and context - driven modeling for sarcasm detection in online social media discussions .",6,1,35
dataset/preprocessed/training-data/sarcasm_detection/1,"For the latter , CASCADE aims at extracting contextual information from the discourse of a discussion thread .",7,0,18
dataset/preprocessed/training-data/sarcasm_detection/1,"Also , since the sarcastic nature and form of expression can vary from person to person , CASCADE utilizes user embeddings that encode stylometric and personality features of the users .",8,0,31
dataset/preprocessed/training-data/sarcasm_detection/1,"When used along with content - based feature extractors such as Convolutional Neural Networks ( CNNs ) , we see a significant boost in the classification performance on a large Reddit corpus .",9,0,33
dataset/preprocessed/training-data/sarcasm_detection/1,Sarcasm is a linguistic tool that uses irony to express contempt .,11,0,12
dataset/preprocessed/training-data/sarcasm_detection/1,Its figurative nature poses a great challenge for affective systems performing sentiment analysis .,12,0,14
dataset/preprocessed/training-data/sarcasm_detection/1,"Previous research in automated sarcasm detection has primarily focused on lexical , pragmatic cues found in sentences .",13,0,18
dataset/preprocessed/training-data/sarcasm_detection/1,"Interjections , punctuations , sentimental shifts , etc. , have been considered as major indicators of sarcasm .",14,0,18
dataset/preprocessed/training-data/sarcasm_detection/1,"When such lexical cues are present in sentences , sarcasm detection can achieve high accuracy .",15,0,16
dataset/preprocessed/training-data/sarcasm_detection/1,"However , sarcasm is also expressed implicitly , i.e. , without the use of any explicit lexical cues .",16,0,19
dataset/preprocessed/training-data/sarcasm_detection/1,Such use of sarcasm also relies on the context which involves the presumption of commonsense and background knowledge of an event .,17,0,22
dataset/preprocessed/training-data/sarcasm_detection/1,"When it comes to detecting sarcasm in a discussion forum , it may not only require understanding the context of the previous comments but also need necessary external background knowledge about the topic of discussion .",18,0,36
dataset/preprocessed/training-data/sarcasm_detection/1,The usage of slangs and informal language also diminishes the reliance on lexical cues .,19,0,15
dataset/preprocessed/training-data/sarcasm_detection/1,This particular type of sarcasm is tough to detect .,20,0,10
dataset/preprocessed/training-data/sarcasm_detection/1,Contextual dependencies for sarcasm can take many forms .,21,0,9
dataset/preprocessed/training-data/sarcasm_detection/1,"As an example , a sarcastic post from Reddit 1 , "" I 'm sure Hillary would 've done that , lmao. "" requires background knowledge about the event , i.e. , Hillary Clinton 's action at the time the post was made .",22,0,44
dataset/preprocessed/training-data/sarcasm_detection/1,"Similarly , sarcastic posts like "" But atheism , yeah * that 's * a religion ! "" requires the knowledge that topics like atheism often contain argumentative discussions and are more prone towards sarcasm .",23,0,36
dataset/preprocessed/training-data/sarcasm_detection/1,"In this work , we attempt the task of sarcasm detection in online discussion forums .",24,0,16
dataset/preprocessed/training-data/sarcasm_detection/1,"Particularly , we propose a hybrid network , named CASCADE , that utilizes both content and contextual - information required for sarcasm detection .",25,0,24
dataset/preprocessed/training-data/sarcasm_detection/1,It starts by processing contextual information in two ways .,26,0,10
dataset/preprocessed/training-data/sarcasm_detection/1,"First , it performs user profiling to create user embeddings that capture indicative behavioral traits for sarcasm .",27,0,18
dataset/preprocessed/training-data/sarcasm_detection/1,"Recent findings suggest that such modeling of the user and their preferences , is highly effective for the given task .",28,0,21
dataset/preprocessed/training-data/sarcasm_detection/1,"It makes use of users ' historical posts to model their writing style ( stylometry ) and personality indicators , which are then fused into comprehensive user embeddings using a multi-view fusion approach , Canonical Correlation Analysis ( CCA ) .",29,0,41
dataset/preprocessed/training-data/sarcasm_detection/1,"Second , it extracts contextual information from the discourse of comments in the discussion forums .",30,0,16
dataset/preprocessed/training-data/sarcasm_detection/1,This is done by document modeling of these consolidated comments belonging to the same forum .,31,0,16
dataset/preprocessed/training-data/sarcasm_detection/1,"We hypothesize that these discourse features would give the important contextual information , background cues along with topical information required for detecting sarcasm .",32,0,24
dataset/preprocessed/training-data/sarcasm_detection/1,"After the contextual modeling phase , CASCADE is provided with a comment for sarcasm detection .",33,0,16
dataset/preprocessed/training-data/sarcasm_detection/1,It performs content - modeling using a Convolutional Neural Network ( CNN ) to extract its syntactic features .,34,0,19
dataset/preprocessed/training-data/sarcasm_detection/1,This CNN representation is then concatenated with the relevant user embedding and discourse features to get the final representation which is used for classification .,35,0,25
dataset/preprocessed/training-data/sarcasm_detection/1,The over all contribution of this work can be summarized as :,36,0,12
dataset/preprocessed/training-data/sarcasm_detection/1,"We propose a novel hybrid sarcasm detector , CASCADE that models content and contextual information .",37,0,16
dataset/preprocessed/training-data/sarcasm_detection/1,We model stylometric and personality details of users along with discourse features of discussion forums to learn informative contextual representations .,38,0,21
dataset/preprocessed/training-data/sarcasm_detection/1,"Experiments on a large Reddit corpus , SARC , demonstrate significant performance improvement over state - of - the - art automated sarcasm detectors .",39,0,25
dataset/preprocessed/training-data/sarcasm_detection/1,"In the remaining paper , Section 2 compares our model to related works ; Section 3 provides the task description and proposed approach ; here , Section 3.3 explains the process of learning contextual features comprising user embeddings and discourse features ; Section 3.6 presents the hybrid prediction model followed by experimentation details and result analysis in Section 4 ; finally , Section 5 draws conclusion .",40,0,67
dataset/preprocessed/training-data/sarcasm_detection/1,Automated sarcasm detection is a relatively recent field of research .,42,0,11
dataset/preprocessed/training-data/sarcasm_detection/1,"The previous works in the literature can be largely classified into two categories , content and context - based sarcasm detection models .",43,0,23
dataset/preprocessed/training-data/sarcasm_detection/1,Content - based :,44,0,4
dataset/preprocessed/training-data/sarcasm_detection/1,These networks model the problem of sarcasm detection as a standard classification task and try to find lexical and pragmatic indicators to identify sarcasm .,45,0,25
dataset/preprocessed/training-data/sarcasm_detection/1,Numerous works have taken this path and presented innovative ways to unearth interesting cues for sarcasm .,46,0,17
dataset/preprocessed/training-data/sarcasm_detection/1,investigate sarcasm detection in spoken dialogue systems using prosodic and spectral cues .,47,0,13
dataset/preprocessed/training-data/sarcasm_detection/1,"use linguistic features like positive predicates , interjections and gestural clues such as emoticons , quotation marks , etc. , use syntactic patterns to construct classifiers .",48,0,27
dataset/preprocessed/training-data/sarcasm_detection/1,"also study the use of emoticons , mainly amongst tweets .",49,0,11
dataset/preprocessed/training-data/sarcasm_detection/0,A Large Self - Annotated Corpus for Sarcasm,2,0,8
dataset/preprocessed/training-data/sarcasm_detection/0,"We introduce the Self - Annotated Reddit Corpus ( SARC ) , a large corpus for sarcasm research and for training and evaluating systems for sarcasm detection .",4,1,28
dataset/preprocessed/training-data/sarcasm_detection/0,"The corpus has 1.3 million sarcastic statements - 10 times more than any previous dataset - and many times more instances of non-sarcastic statements , allowing for learning in both balanced and unbalanced label regimes .",5,0,36
dataset/preprocessed/training-data/sarcasm_detection/0,"Each statement is furthermore self - annotated - sarcasm is labeled by the author , not an independent annotator - and provided with user , topic , and conversation context .",6,0,31
dataset/preprocessed/training-data/sarcasm_detection/0,"We evaluate the corpus for accuracy , construct benchmarks for sarcasm detection , and evaluate baseline methods .",7,0,18
dataset/preprocessed/training-data/sarcasm_detection/0,"Sarcasm detection is an important component in many natural language processing ( NLP ) systems , directly relevant to natural language understanding , dialogue systems , and text mining .",9,0,30
dataset/preprocessed/training-data/sarcasm_detection/0,"However , detecting sarcasm is difficult because it occurs infrequently and is difficult for even humans to discern .",10,0,19
dataset/preprocessed/training-data/sarcasm_detection/0,"Despite these properties , existing datasets either have balanced labels - data with roughly the same number of examples of each label - or use humans to annotate sarcastic statements .",11,0,31
dataset/preprocessed/training-data/sarcasm_detection/0,"In this work , we make available the first corpus 1 for sarcasm detection that has both unbalanced and self - annotated labels and does not consist of short text snippets from Twitter 2 .",12,0,35
dataset/preprocessed/training-data/sarcasm_detection/0,"With more than a million examples of sarcastic statements , each provided with author , topic , and contex information , the dataset exceeds all previous sarcasm corpora by an order of magnitude in size .",13,0,36
dataset/preprocessed/training-data/sarcasm_detection/0,This is possible due to the comment structure of the social media site Reddit 3 as well as its frequently - used and standardized annotation for sarcasm .,14,0,28
dataset/preprocessed/training-data/sarcasm_detection/0,"Following a discussion of corpus construction and relevant statistics in Section 3 , we discuss the quality of this dataset compared to alternative sources in Section 4 , manually evaluating our corpus for noise .",15,0,35
dataset/preprocessed/training-data/sarcasm_detection/0,Then in Section 5 we use our dataset to construct suitable benchmarks for sarcasm detection systems and examine the performance of simple baseline methods and human evaluators on these subsets .,16,0,31
dataset/preprocessed/training-data/sarcasm_detection/0,"Since our main contribution is a corpus and not a method for sarcasm detection , we point the reader to a recent survey by that discusses many interesting efforts in this are a .",18,0,34
dataset/preprocessed/training-data/sarcasm_detection/0,"Note that many of the works the authors mention will be discussed by us in this section , with many papers using their own datasets and illustrating the need for common evaluation baselines .",19,0,34
dataset/preprocessed/training-data/sarcasm_detection/0,"Sarcasm datasets can largely be distinguished by the sources used to get sarcastic and non-sarcastic statements , the amount of human annotation , and whether the dataset is balanced or unbalanced .",20,0,32
dataset/preprocessed/training-data/sarcasm_detection/0,"Reddit has been used before , notably by ; while the authors allow unabalanced labeling , they do not exploit the possibility of using self - annotation and generate around 10,000 human - labeled sentences .",21,0,36
dataset/preprocessed/training-data/sarcasm_detection/0,"Twitter is a frequent source due to the self - annotation provided by hashtags such as #sarcasm , #notsarcasm , and #irony .",22,0,23
dataset/preprocessed/training-data/sarcasm_detection/0,"As discussed in Section 4.2 , its abbreviated language and other properties make Twitter a less attractive source for annotated comments .",23,0,22
dataset/preprocessed/training-data/sarcasm_detection/0,"However , it is by far the largest raw source of data for this purpose and has led to some large unbalanced corpora in previous efforts .",24,0,27
dataset/preprocessed/training-data/sarcasm_detection/0,"A further source of comments is the Internet Argument Corpus ( IAC ) , a scraped corpus of Internet discussions that can be further annotated for sarcasm by humans or by machine learning ; this is done by and , in both cases resulting in around 10,000 labeled statements .",25,0,50
dataset/preprocessed/training-data/sarcasm_detection/0,Reddit Structure and Annotation,27,0,4
dataset/preprocessed/training-data/sarcasm_detection/0,"Reddit is asocial media site in which users communicate by commenting on submissions , which are titled posts consisting of embedded media , external links , and / or text , thatare posted on topic - specific forums known as subreddits ; examples of subreddits include funny , pics , and science .",28,0,53
dataset/preprocessed/training-data/sarcasm_detection/0,"Users comment on submissions and on other comments , resulting in tree - like conversation structure such that each comment has a parent comment .",29,0,25
dataset/preprocessed/training-data/sarcasm_detection/0,"We refer to elements as any nodes in the tree of a Reddit link ( i.e. , comments or submissions ) .",30,0,22
dataset/preprocessed/training-data/sarcasm_detection/0,"Reddit users have adopted a common method for sarcasm annotation consisting of adding the marker "" / s "" to the end of sarcastic statements ; this originates from the HTML text delineation < sarcasm >... < / sarcasm >.",31,0,40
dataset/preprocessed/training-data/sarcasm_detection/0,"As with Twitter hashtags , using these markers as indicators of sarcasm is noisy , especially since many users do not use the marker , do not know about it , or only use it where sarcastic intent is not otherwise obvious .",32,0,43
dataset/preprocessed/training-data/sarcasm_detection/0,We discuss the extent of this noise in Section 4.1 .,33,0,11
dataset/preprocessed/training-data/sarcasm_detection/0,"Reddit comments from December 2005 have been made available due to web - scraping 4 ; we construct our dataset as a subset of comments from January 2009 - April 2017 , comprising the vast majority of comments and excluding noisy data from earlier years .",35,0,46
dataset/preprocessed/training-data/sarcasm_detection/0,"For each comment we provide a sarcasm label , author , the subreddit it appeared in , the comment score as voted on by users , the date of the comment , and identifiers linking back to the original dataset of all comments .",36,0,44
dataset/preprocessed/training-data/sarcasm_detection/0,"To reduce noise , we use several filters to remove noisy and uninformative comments .",37,0,15
dataset/preprocessed/training-data/sarcasm_detection/0,Many of these are standard preprocessing steps such as excluding URLs and limiting characters to be ASCII .,38,0,18
dataset/preprocessed/training-data/sarcasm_detection/0,"To handle Reddit data , we also exclude comments thatare descendants of sarcastic comments in the conversation tree , as annotation in such cases is extremely noisy , with authors agreeing or dis agreeing with the previously expressed sarcasm with their own sarcasm but often with no marking .",39,0,49
dataset/preprocessed/training-data/sarcasm_detection/0,Our raw corpus consists of three files :,40,0,8
dataset/preprocessed/training-data/sarcasm_detection/0,"An array in CSV format containing 533 million comments , of which around 1.3 million are sarcastic .",42,0,18
dataset/preprocessed/training-data/sarcasm_detection/0,This file only contains those comments whose authors know about the standard sarcasm annotation ; this is determined by whether they have used the annotation in the same month as the comment was made or earlier .,43,0,37
dataset/preprocessed/training-data/sarcasm_detection/0,This limitation is added in order to reduce false negatives due to authors not annotating their sarcasm .,44,0,18
dataset/preprocessed/training-data/sarcasm_detection/0,Each row also contains the parent comment .,45,0,8
dataset/preprocessed/training-data/sarcasm_detection/0,A hashtable in JSON format containing all comments,46,0,8
dataset/preprocessed/training-data/sarcasm_detection/0,and posts in the conversation thread of a sarcastic comment as well as all siblings of sarcastic comments .,47,0,19
dataset/preprocessed/training-data/sarcasm_detection/0,"An array in CSV format , with each row containing a sequence of comments leading up to a sarcastic comment , the ( sarcastic and non-sarcastic ) responses to the last element in that sequence , and the labels of those responses .",49,0,43
dataset/preprocessed/training-data/semantic_parsing/1,TRANX : A Transition - based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation,2,1,16
dataset/preprocessed/training-data/semantic_parsing/1,"We present TRANX , a transition - based neural semantic parser that maps natural language ( NL ) utterances into formal meaning representations ( MRs ) .",4,0,27
dataset/preprocessed/training-data/semantic_parsing/1,"TRANX uses a transition system based on the abstract syntax description language for the target MR , which gives it two major advantages : ( 1 ) it is highly accurate , using information from the syntax of the target MR to constrain the output space and model the information flow , and ( 2 ) it is highly generalizable , and can easily be applied to new types of MR by just writing a new abstract syntax description corresponding to the allowable structures in the MR .",5,0,88
dataset/preprocessed/training-data/semantic_parsing/1,"Experiments on four different semantic parsing and code generation tasks show that our system is generalizable , extensible , and effective , registering strong results compared to existing neural semantic parsers .",6,0,32
dataset/preprocessed/training-data/semantic_parsing/1,Semantic parsing is the task of transducing natural language ( NL ) utterances into formal meaning representations ( MRs ) .,9,0,21
dataset/preprocessed/training-data/semantic_parsing/1,The target MRs can be defined according to a wide variety of formalisms .,10,0,14
dataset/preprocessed/training-data/semantic_parsing/1,This include linguistically - motivated semantic representations thatare designed to capture the meaning of any sentence such as ? calculus or the abstract meaning representations .,11,0,26
dataset/preprocessed/training-data/semantic_parsing/1,"Alternatively , for more task - driven approaches to semantic parsing , it is common for meaning representations to represent executable programs such as SQL queries , robotic commands , smartphone instructions , and even general - purpose programming languages like Python and Java .",12,0,45
dataset/preprocessed/training-data/semantic_parsing/1,"Because of these varying formalisms for MRs , the design of semantic parsers , particularly neural network - based ones has generally focused on a small subset of tasks - in order to ensure the syntactic well - formedness of generated MRs , a parser is usually specifically designed to reflect the domain - dependent grammar of MRs in the structure of the model .",13,0,65
dataset/preprocessed/training-data/semantic_parsing/1,"To alleviate this issue , there have been recent efforts in neural semantic parsing with general - purpose grammar models .",14,0,21
dataset/preprocessed/training-data/semantic_parsing/1,"put forward a neural sequence - to - sequence model that generates tree - structured MRs using a series of tree - construction actions , guided by the task - specific context free grammar provided to the model a priori .",15,0,41
dataset/preprocessed/training-data/semantic_parsing/1,"propose the abstract syntax networks ( ASNs ) , where domain - specific MRs are represented by abstract syntax trees ( ASTs , Left ) specified under the abstract syntax description language ( ASDL ) framework .",16,0,37
dataset/preprocessed/training-data/semantic_parsing/1,"An ASN employs a modular architecture , generating an AST using specifically designed neural networks for each construct in the ASDL grammar .",17,0,23
dataset/preprocessed/training-data/semantic_parsing/1,"Inspired by this existing research , we have developed TRANX , a TRANsition - based abstract syntaX parser for semantic parsing and code generation .",18,0,25
dataset/preprocessed/training-data/semantic_parsing/1,TRANX is designed with the following principles in mind :,19,0,10
dataset/preprocessed/training-data/semantic_parsing/1,"Generalization ability TRANX employs ASTs as a general - purpose intermediate meaning representation , and the task - dependent grammar is provided to the system as external knowledge to guide the parsing process , therefore decoupling the semantic parsing procedure with specificities of grammars .",20,0,45
dataset/preprocessed/training-data/semantic_parsing/1,Extensibility TRANX uses a simple transition system to parse NL utterances into tree -,21,0,14
dataset/preprocessed/training-data/semantic_parsing/1,Figure 1 : Workflow of TRANX structured ASTs .,22,0,9
dataset/preprocessed/training-data/semantic_parsing/1,"The transition system is designed to be easy to extend , requiring minimal engineering to adapt to tasks that need to handle extra domain - specific information .",23,0,28
dataset/preprocessed/training-data/semantic_parsing/1,"We test TRANX on four semantic parsing ( ATIS , GEO ) and code generation ( DJANGO , WIKISQL ) tasks , and demonstrate that TRANX is capable of generalizing to different domains while registering strong performance , out - performing existing neural networkbased approaches on three of the four datasets ( GEO , ATIS , DJANGO ) .",25,0,59
dataset/preprocessed/training-data/semantic_parsing/1,"Given an NL utterance , TRANX parses the utterance into a formal meaning representation , typically represented as ?- calculus logical forms , domain - specific , or general - purpose programming languages ( e.g. , Python ) .",27,0,39
dataset/preprocessed/training-data/semantic_parsing/1,"In the following description we use Python code generation as a running example , where a programmer 's natural language intents are mapped to Python source code .",28,0,28
dataset/preprocessed/training-data/semantic_parsing/1,depicts the workflow of TRANX .,29,0,6
dataset/preprocessed/training-data/semantic_parsing/1,We will present more use cases of TRANX in 3 .,30,0,11
dataset/preprocessed/training-data/semantic_parsing/1,The core of TRANX is a transition system .,31,0,9
dataset/preprocessed/training-data/semantic_parsing/1,"Given an input NL utterance x , TRANX employs the transition system to map the utterance x into an AST z using a series of treeconstruction actions ( 2.2 ) .",32,0,31
dataset/preprocessed/training-data/semantic_parsing/1,TRANX employs ASTs as the intermediate meaning representation to abstract over domain - specific structure of MRs .,33,0,18
dataset/preprocessed/training-data/semantic_parsing/1,"This parsing process is guided by the userdefined , domain - specific grammar specified under the ASDL formalism ( 2.1 ) .",34,0,22
dataset/preprocessed/training-data/semantic_parsing/1,"Given the generated AST z , the parser calls the user - defined function , AST to MR ( ) , to convert the intermediate AST into a domain - specific meaning representation y , completing the parsing process .",35,0,40
dataset/preprocessed/training-data/semantic_parsing/1,"TRANX uses a probabilistic model p ( z|x ) , parameterized by a neural network , to score each hypothesis AST ( 2.3 ) .",36,0,25
dataset/preprocessed/training-data/semantic_parsing/1,Modeling ASTs using ASDL Grammar,37,0,5
dataset/preprocessed/training-data/semantic_parsing/1,"TRANX uses ASTs as the general - purpose , intermediate semantic representation for MRs .",38,0,15
dataset/preprocessed/training-data/semantic_parsing/1,"ASTs are commonly used to represent programming languages , and can also be used to represent other tree - structured MRs ( e.g. , ?- calculus ) .",39,0,28
dataset/preprocessed/training-data/semantic_parsing/1,The ASDL framework is a grammatical formalism to define ASTs .,40,0,11
dataset/preprocessed/training-data/semantic_parsing/1,See for an excerpt of the Python ASDL grammar .,41,0,10
dataset/preprocessed/training-data/semantic_parsing/1,TRANX provides APIs to read such a grammar from human - readable text files .,42,0,15
dataset/preprocessed/training-data/semantic_parsing/1,An ASDL grammar has two basic constructs : types and constructors .,43,0,12
dataset/preprocessed/training-data/semantic_parsing/1,A composite type is defined by the set of constructors under that type .,44,0,14
dataset/preprocessed/training-data/semantic_parsing/1,"For example , the stmt and expr composite types in refer to Python statements and expressions , repectively , each defined by a series of constructors .",45,0,27
dataset/preprocessed/training-data/semantic_parsing/1,A constructor specifies a language construct of a particular type using its fields .,46,0,14
dataset/preprocessed/training-data/semantic_parsing/1,"For instance , the Call constructor under the composite type expr denotes function call expressions , and has three fields : func , args and keywords .",47,0,27
dataset/preprocessed/training-data/semantic_parsing/1,"Each field in a constructor is also strongly typed , which specifies the type of value the field can hold .",48,0,21
dataset/preprocessed/training-data/semantic_parsing/1,A field with a composite type can be instantiated by constructors of the same type .,49,0,16
dataset/preprocessed/training-data/semantic_parsing/0,Spider : A Large - Scale Human - Labeled Dataset for Complex and Cross - Domain Semantic Parsing and Text - to - SQL Task,2,1,25
dataset/preprocessed/training-data/semantic_parsing/0,"We present Spider , a large - scale , complex and cross-domain semantic parsing and textto - SQL dataset annotated by 11 college students .",4,0,25
dataset/preprocessed/training-data/semantic_parsing/0,"It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 data bases with multiple tables , covering 138 different domains .",5,0,24
dataset/preprocessed/training-data/semantic_parsing/0,We define a new complex and cross-domain semantic parsing and textto - SQL task where different complex SQL queries and data bases appear in train and test sets .,6,0,29
dataset/preprocessed/training-data/semantic_parsing/0,"In this way , the task requires the model to generalize well to both new SQL queries and new data base schemas .",7,0,23
dataset/preprocessed/training-data/semantic_parsing/0,Spider is distinct from most of the previous semantic parsing tasks because they all use a single data base and the exact same programs in the train set and the test set .,8,0,33
dataset/preprocessed/training-data/semantic_parsing/0,We experiment with various state - of - the - art models and the best model achieves only 12.4 % exact matching accuracy on a data base split setting .,9,0,30
dataset/preprocessed/training-data/semantic_parsing/0,This shows that Spider presents a strong challenge for future research .,10,0,12
dataset/preprocessed/training-data/semantic_parsing/0,Our dataset and task are publicly available at https://yale-lily. github.io/ spider .,11,0,12
dataset/preprocessed/training-data/semantic_parsing/0,Semantic parsing ( SP ) is one of the most important tasks in natural language processing ( NLP ) .,13,1,20
dataset/preprocessed/training-data/semantic_parsing/0,"It requires both understanding the meaning of natural language sentences and mapping them to meaningful executable queries such as logical forms , SQL queries , and Python code .",14,0,29
dataset/preprocessed/training-data/semantic_parsing/0,"Recently , some state - of - the - art methods with Seq2Seq architectures are able to achieve over 80 % exact matching accuracy even on some complex benchmarks such as ATIS and GeoQuery .",15,0,35
dataset/preprocessed/training-data/semantic_parsing/0,These models seem to have already solved most problems in this field .,16,0,13
dataset/preprocessed/training-data/semantic_parsing/0,"However , previous tasks in this field have a simple but problematic task definition because most of these results are predicted by semantic "" matching "" department Annotators check data base schema ( e.g. , data base : college )",17,0,40
dataset/preprocessed/training-data/semantic_parsing/0,...... rather than semantic parsing .,18,0,6
dataset/preprocessed/training-data/semantic_parsing/0,Existing datasets for SP have two shortcomings .,19,1,8
dataset/preprocessed/training-data/semantic_parsing/0,"First , those that have complex programs are too small in terms of the number of programs for training modern data - intensive models and have only a single dataset , meaning that the same data base is used for both training and testing the model .",20,0,47
dataset/preprocessed/training-data/semantic_parsing/0,"More importantly , the number of logic forms or SQL labels is small and each program has about 4 - 10 paraphrases of natural language problem to expand the size of the dataset .",21,0,34
dataset/preprocessed/training-data/semantic_parsing/0,"Therefore , the exact same target programs appear in both the train and test sets .",22,0,16
dataset/preprocessed/training-data/semantic_parsing/0,The models can achieve decent performances even on very complex programs by memorizing the patterns of question and program pairs during training and decoding the programs exactly the same way as it saw in the training set during testing .,23,0,40
dataset/preprocessed/training-data/semantic_parsing/0,split the dataset by programs so that no two identical programs would be in both the train and test sets .,24,0,21
dataset/preprocessed/training-data/semantic_parsing/0,They show that the models built on this question - splitting data setting fail to generalize to unseen programs .,25,0,20
dataset/preprocessed/training-data/semantic_parsing/0,"Second , existing datasets thatare large in terms of the number of programs and data bases such as WikiSQL contain only simple SQL queries and single tables .",26,0,28
dataset/preprocessed/training-data/semantic_parsing/0,"In order to test a model 's real semantic parsing performance on unseen complex programs and its ability to generalize to new domains , an SP dataset that includes a large amount of complex programs and data bases with multiple tables is a must .",27,0,45
dataset/preprocessed/training-data/semantic_parsing/0,"However , compared to other large , realistic datasets such as ImageNet for object recognition and SQuAD for reading comprehension , creating such SP dataset is even more time - consuming and challenging in some aspects due to the following reasons .",28,0,42
dataset/preprocessed/training-data/semantic_parsing/0,"First , it is hard to find many data bases with multiple tables online .",29,0,15
dataset/preprocessed/training-data/semantic_parsing/0,"Second , given a data base , annotators have to understand the complex data base schema to create a set of questions such that their corresponding SQL queries cover all SQL patterns .",30,0,33
dataset/preprocessed/training-data/semantic_parsing/0,"Moreover , it is even more challenging to write different complex SQL queries .",31,0,14
dataset/preprocessed/training-data/semantic_parsing/0,"Additionally , reviewing and quality - checking of question and SQL pairs takes a significant amount of time .",32,0,19
dataset/preprocessed/training-data/semantic_parsing/0,All of these processes require very specific knowledge in data bases .,33,0,12
dataset/preprocessed/training-data/semantic_parsing/0,"To address the need for a large and high - quality dataset for a new complex and cross-domain semantic parsing task , we introduce Spider , which consists of 200 databases with multiple tables , 10,181 questions , and 5,693 corresponding complex SQL queries , all written by 11 college students spending a total of 1,000 man-hours .",34,0,58
dataset/preprocessed/training-data/semantic_parsing/0,"As illustrates , given a database with multiple tables including foreign keys , our corpus creates and annotates complex questions and SQL queries including different SQL clauses such as joining and nested query .",35,0,34
dataset/preprocessed/training-data/semantic_parsing/0,"In order to generate the SQL query given the input question , models need to understand both the natural language question and relationships between tables and columns in the data base schema .",36,0,33
dataset/preprocessed/training-data/semantic_parsing/0,"In addition , we also propose a new task for the text - to - SQL problem .",37,0,18
dataset/preprocessed/training-data/semantic_parsing/0,"Since Spider contains 200 data bases with foreign keys , we can split the dataset with complex SQL queries in a way that no data base overlaps in train and test , which overcomes the two shortcomings of prior datasets , and defines a new semantic parsing task in which the model needs to generalize not only to new programs but also to new data bases .",38,0,67
dataset/preprocessed/training-data/semantic_parsing/0,Models have to take questions and data base schemas as inputs and predict unseen queries on new data bases .,39,0,20
dataset/preprocessed/training-data/semantic_parsing/0,"To assess the task difficulty , we experiment with several state - of - the - art semantic parsing models .",40,0,21
dataset/preprocessed/training-data/semantic_parsing/0,All of them struggle with this task .,41,0,8
dataset/preprocessed/training-data/semantic_parsing/0,The best model achieves only 12.4 % exact matching accuracy in the data base split setting .,42,0,17
dataset/preprocessed/training-data/semantic_parsing/0,This suggests that there is a large room for improvement .,43,0,11
dataset/preprocessed/training-data/semantic_parsing/0,Related Work and Existing Datasets,44,0,5
dataset/preprocessed/training-data/semantic_parsing/0,Several semantic parsing datasets with different queries have been created .,45,0,11
dataset/preprocessed/training-data/semantic_parsing/0,"The output can be in many formats , e.g. , logic forms .",46,0,13
dataset/preprocessed/training-data/semantic_parsing/0,"These datasets include ATIS , , and JOBS ( Tang and Mooney , 2001 a ) .",47,0,17
dataset/preprocessed/training-data/semantic_parsing/0,They have been studied extensively .,48,0,6
dataset/preprocessed/training-data/semantic_parsing/0,"However , they are domain specific and there is no standard label guidance for multiple SQL queries .",49,0,18
dataset/preprocessed/training-data/semantic_parsing/2,Coarse - to - Fine Decoding for Neural Semantic Parsing,2,1,10
dataset/preprocessed/training-data/semantic_parsing/2,Semantic parsing aims at mapping natural language utterances into structured meaning representations .,4,1,13
dataset/preprocessed/training-data/semantic_parsing/2,"In this work , we propose a structure - aware neural architecture which decomposes the semantic parsing process into two stages .",5,0,22
dataset/preprocessed/training-data/semantic_parsing/2,"Given an input utterance , we first generate a rough sketch of its meaning , where low - level information ( such as variable names and arguments ) is glossed over .",6,0,32
dataset/preprocessed/training-data/semantic_parsing/2,"Then , we fill in missing details by taking into account the natural language input and the sketch itself .",7,0,20
dataset/preprocessed/training-data/semantic_parsing/2,"Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance , achieving competitive results despite the use of relatively simple decoders .",8,0,31
dataset/preprocessed/training-data/semantic_parsing/2,"Semantic parsing maps natural language utterances onto machine interpretable meaning representations ( e.g. , executable queries or logical forms ) .",10,0,21
dataset/preprocessed/training-data/semantic_parsing/2,The successful application of recurrent neural networks to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence - to - sequence problem .,11,0,30
dataset/preprocessed/training-data/semantic_parsing/2,The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure .,12,0,23
dataset/preprocessed/training-data/semantic_parsing/2,"Examples include tree decoders , decoders constrained by a grammar model , or modular decoders which use syntax to dynamically compose various submodels .",13,0,24
dataset/preprocessed/training-data/semantic_parsing/2,"In this work , we propose to decompose the decoding process into two stages .",14,0,15
dataset/preprocessed/training-data/semantic_parsing/2,"The first decoder focuses on predicting a rough sketch of the meaning representation , which omits low - level details , such as arguments and variable names .",15,0,28
dataset/preprocessed/training-data/semantic_parsing/2,Example sketches for various meaning representations are shown in .,16,0,10
dataset/preprocessed/training-data/semantic_parsing/2,"Then , a second decoder fills in missing details by conditioning on the natural language input and the sketch itself .",17,0,21
dataset/preprocessed/training-data/semantic_parsing/2,"Specifically , the sketch constrains the generation process and is encoded into vectors to guide decoding .",18,0,17
dataset/preprocessed/training-data/semantic_parsing/2,We argue that there are at least three advantages to the proposed approach .,19,0,14
dataset/preprocessed/training-data/semantic_parsing/2,"Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the decoders to model meaning at different levels of granularity .",20,0,28
dataset/preprocessed/training-data/semantic_parsing/2,"As shown in , sketches are more compact and as a result easier to generate compared to decoding the entire meaning structure in one go .",21,0,26
dataset/preprocessed/training-data/semantic_parsing/2,"Secondly , the model can explicitly share knowledge of coarse structures for the examples that have the same sketch ( i.e. , basic meaning ) , even though their actual meaning representations are different ( e.g. , due to different details ) .",22,0,43
dataset/preprocessed/training-data/semantic_parsing/2,"Thirdly , after generating the sketch , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the prediction of the final details .",23,0,38
dataset/preprocessed/training-data/semantic_parsing/2,Our framework is flexible and not restricted to specific tasks or any particular model .,24,0,15
dataset/preprocessed/training-data/semantic_parsing/2,"We conduct experiments on four datasets representative of various semantic parsing tasks ranging from logical form parsing , to code generation , and SQL query generation .",25,0,27
dataset/preprocessed/training-data/semantic_parsing/2,We adapt our architecture to these tasks and present several ways to obtain sketches from their respective meaning representations .,26,0,20
dataset/preprocessed/training-data/semantic_parsing/2,Experimental results show that our framework achieves competitive performance compared ar Xiv : 1805.04793v1 [ cs. CL ] 12 May 2018 Dataset Length Example GEO 7.6 13.7 6.9,27,0,28
dataset/preprocessed/training-data/semantic_parsing/2,which state has the most rivers running through it ?,29,0,10
dataset/preprocessed/training-data/semantic_parsing/2,y : ( argmax $ 0 ( state : t,30,0,10
dataset/preprocessed/training-data/semantic_parsing/2,$ 0 ) ( count $ 1 ( and ( river : t,31,0,13
dataset/preprocessed/training-data/semantic_parsing/2,$ 1 ) ( loc : t $ 1 $ 0 ) ) ) ) a : ( argmax # 1 state : t@1 ( count# 1 ( and river : t@1 loc : t@2 ) ) ) ATIS 11.1 21.1 9.2,32,0,42
dataset/preprocessed/training-data/semantic_parsing/2,all flights from dallas before 10 am y : ( lambda $ 0 e ( and ( flight $ 0 ) ( from $ 0 dallas:ci ) (< ( departure time $ 0 ) 1000 :ti ) ) ) a : ( lambda#2 ( and flight @ 1 from @ 2 ( < departure time @ 1 ? ) ) ) DJANGO 14.4 8.7 8.0,34,0,65
dataset/preprocessed/training-data/semantic_parsing/2,"if length of bits is lesser than integer 3 or second element of bits is not equal to string ' as ' , y : if len ( bits ) < 3 or bits [ 1 ] != ' a s ' : a : if len ( NAME ) < NUMBER or NAME [ NUMBER ] !=",36,0,58
dataset/preprocessed/training-data/semantic_parsing/2,STRING : WIKISQL 17.9 13.3 13.0 2.7,37,0,7
dataset/preprocessed/training-data/semantic_parsing/2,Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations .,39,0,22
dataset/preprocessed/training-data/semantic_parsing/2,These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input .,40,0,20
dataset/preprocessed/training-data/semantic_parsing/2,"More recently , neural sequence - to - sequence models have been applied to semantic parsing with promising results , eschewing the need for extensive feature engineering .",41,0,28
dataset/preprocessed/training-data/semantic_parsing/2,"Several ideas have been explored to enhance the performance of these models such as data augmentation , transfer learning , sharing parameters for multiple languages or meaning representations , and utilizing user feedback signals .",42,0,35
dataset/preprocessed/training-data/semantic_parsing/2,There are also efforts to develop structured decoders that make use of the syntax of meaning representations .,43,0,18
dataset/preprocessed/training-data/semantic_parsing/2,and Alvarez - Melis and Jaakkola ( 2017 ) develop models which generate tree structures in a topdown fashion .,44,0,20
dataset/preprocessed/training-data/semantic_parsing/2,and employ the grammar to constrain the decoding process .,45,0,10
dataset/preprocessed/training-data/semantic_parsing/2,use a transition system to generate variable - free queries .,46,0,11
dataset/preprocessed/training-data/semantic_parsing/2,"design a grammar model for the generation of abstract syntax trees in depth - first , left - to - right order .",47,0,23
dataset/preprocessed/training-data/semantic_parsing/2,propose a modular decoder whose submodels are dynamically composed according to the generated tree structure .,48,0,16
dataset/preprocessed/training-data/semantic_parsing/2,Our own work also aims to model the structure of meaning representations more faithfully .,49,0,15
dataset/preprocessed/training-data/negation_scope_resolution/0,NegBERT : A Transfer Learning Approach for Negation Detection and Scope Resolution,2,1,12
dataset/preprocessed/training-data/negation_scope_resolution/0,"Negation is an important characteristic of language , and a major component of information extraction from text .",4,0,18
dataset/preprocessed/training-data/negation_scope_resolution/0,This subtask is of considerable importance to the biomedical domain .,5,0,11
dataset/preprocessed/training-data/negation_scope_resolution/0,"Over the years , multiple approaches have been explored to address this problem : simple rule - based systems , Machine Learning classifiers , Conditional Random Field Models , CNNs and more recently BiLSTMs .",6,0,35
dataset/preprocessed/training-data/negation_scope_resolution/0,"In this paper , we look at applying Transfer Learning to this problem .",7,0,14
dataset/preprocessed/training-data/negation_scope_resolution/0,"First , we extensively review previous literature addressing Negation Detection and Scope Resolution across the 3 datasets that have gained popularity over the years : BioScope Corpus , the Sherlock dataset , and the SFU Review Corpus .",8,0,38
dataset/preprocessed/training-data/negation_scope_resolution/0,"We then explore the decision choices involved with using BERT , a popular transfer learning model , for this task , and report a new state - of - the - art for scope resolution across all 3 datasets .",9,0,40
dataset/preprocessed/training-data/negation_scope_resolution/0,"Our model , referred to as NegBERT , achieves a token level F 1 score on scope resolution of 92.36 on the Sherlock dataset , 95.68 on the BioScope Abstracts , 91.24 on the BioScope Full Papers , 90.95 on the SFU dataset , outperforming the previous state - of - the - art by a significant margin .",10,0,59
dataset/preprocessed/training-data/negation_scope_resolution/0,We also analyze the model 's generalizability to datasets on which it is not trained .,11,0,16
dataset/preprocessed/training-data/negation_scope_resolution/0,"Negation Detection and Scope Resolution is an important subtask for tasks ranging from Sentiment Analysis , where the sentiment of a given sentence is dependent on negation , to query response systems like Chatbots , where negation entirely changes the meaning and hence the relevance of a certain body of text .",13,0,52
dataset/preprocessed/training-data/negation_scope_resolution/0,"A substantial portion of the research till date on this topic focused solely on data from the biomedical domain where use of negation cues is abundant , as in medical reports .",14,0,32
dataset/preprocessed/training-data/negation_scope_resolution/0,"While negation is intuitive for humans to spot , finding the exact words that indicate such negation and delineating the scope of such negation cues has proven to be a tricky problem for computer - based systems .",15,0,38
dataset/preprocessed/training-data/negation_scope_resolution/0,"One could imagine that finding negation cues and their scopes could be easily solved via rules and carefully designed heuristics , and this was the exact approach used by the initial few systems attempting this task .",16,0,37
dataset/preprocessed/training-data/negation_scope_resolution/0,"But given the complexities of human language , these approaches were n't accurate enough .",17,0,15
dataset/preprocessed/training-data/negation_scope_resolution/0,"Thus , other methods were explored , and Deep Learning - based approaches have shown to be particularly promising .",18,0,20
dataset/preprocessed/training-data/negation_scope_resolution/0,A simple example of negation is as follows :,19,0,9
dataset/preprocessed/training-data/negation_scope_resolution/0,This is not [ a negation ] .,20,0,8
dataset/preprocessed/training-data/negation_scope_resolution/0,"We can observe that ' not ' is the negation word ( known as the negation cue ) and the words whose meaning is altered by ' not ' are ' a ' and ' negation ' , which belong to what is known as the cue 's scope .",21,0,50
dataset/preprocessed/training-data/negation_scope_resolution/0,"Negation detection involves finding these negation cues , and scope resolution for each cue necessitates finding the words affected negatively by that cue ( finding its scope ) .",22,0,29
dataset/preprocessed/training-data/negation_scope_resolution/0,Cues can come in a variety of ways :,23,0,9
dataset/preprocessed/training-data/negation_scope_resolution/0,"1 . An affix : ( im ) perfect , ( a ) typical , ca ( n't )",24,0,19
dataset/preprocessed/training-data/negation_scope_resolution/0,"2 . A single word : not , no , failed , lacks",25,0,13
dataset/preprocessed/training-data/negation_scope_resolution/0,A set of consecutive words or discontinuous words : neithernor,27,0,10
dataset/preprocessed/training-data/negation_scope_resolution/0,The scope of a cue is also not constrained to be continuous .,28,0,13
dataset/preprocessed/training-data/negation_scope_resolution/0,"These facts , coupled with the relatively small dataset sizes compared to other NLP datasets , make this task particularly challenging to solve .",29,0,24
dataset/preprocessed/training-data/negation_scope_resolution/0,"Transfer Learning , a method in which we train deep learning systems on huge corpora and then ' transfer ' or tune these pretrained architectures on downstream tasks which have a dearth of data , has taken the NLP community by storm , achieving state - of - the - art results on almost every NLP task these models have been applied to .",30,0,64
dataset/preprocessed/training-data/negation_scope_resolution/0,"This method was originally used in Computer Vision , by training models on the ImageNet dataset which allowed them to capture important features in a picture , and then apply to other datasets by changing the output layer and training on the downstream task .",31,0,45
dataset/preprocessed/training-data/negation_scope_resolution/0,"Recently , a number of architectures including BERT have applied this to NLP , contributing massively to the advancement of research in the field .",32,0,25
dataset/preprocessed/training-data/negation_scope_resolution/0,"Almost every NLP task benefitted from transfer learning , as training on massive corpora allowed these models to learn an understanding of language .",33,0,24
dataset/preprocessed/training-data/negation_scope_resolution/0,"Motivated by the success of transfer learning , we apply BERT to negation detection and scope resolution .",34,0,18
dataset/preprocessed/training-data/negation_scope_resolution/0,"We explore the set of design choices involved , and experiment on all 3 public datasets available : the BioScope Corpus ( Abstracts and Full Papers ) , the Sherlock Dataset and the SFU Review Corpus .",35,0,37
dataset/preprocessed/training-data/negation_scope_resolution/0,"We train NegBERT on one dataset and report the scores on testing all datasets , thus showing the generalizability of NegBERT .",36,0,22
dataset/preprocessed/training-data/negation_scope_resolution/0,"Since the BioScope dataset is primarily from the biomedical domain , while the Sherlock dataset is taken from stories by Sir Author Conan Doyle ( literary work ) , and the SFU Review Corpus is a collection of product reviews ( free text by human users ) , the 3 datasets belong to different domains .",37,0,56
dataset/preprocessed/training-data/negation_scope_resolution/0,"This paper is organized as follows : In Section 2 , we extensively review available literature on the subject .",38,0,20
dataset/preprocessed/training-data/negation_scope_resolution/0,"Section 3 contains the details of the methodology used for Neg - BERT , while 4 includes experimental details for our experimentation .",39,0,23
dataset/preprocessed/training-data/negation_scope_resolution/0,"In Section 5 , we report the results and analyze them .",40,0,12
dataset/preprocessed/training-data/negation_scope_resolution/0,Our conclusions and our perspective on the future scope for this problem is presented in Section 6 .,41,0,18
dataset/preprocessed/training-data/negation_scope_resolution/0,"In 2012 , the * sem Shared Task for the year was negation cue detection and scope resolution .",43,0,19
dataset/preprocessed/training-data/negation_scope_resolution/0,The dataset used for this conference was the Sherlock dataset .,44,0,11
dataset/preprocessed/training-data/negation_scope_resolution/0,"The other 2 publicly available datasets are the Bioscope Corpus , and the SFU Review Corpus .",45,0,17
dataset/preprocessed/training-data/negation_scope_resolution/0,"In this section , we look at the previous literature addressing this task and summarize the results of those approaches at the end in a tabular format .",46,0,28
dataset/preprocessed/training-data/negation_scope_resolution/0,Rule - Based Approaches,47,0,4
dataset/preprocessed/training-data/negation_scope_resolution/0,The first approach that was explored in literature was a simple rule - based system ..,48,0,16
dataset/preprocessed/training-data/negation_scope_resolution/0,They tested the hypothesis that a lexical scanner that uses regular expressions to generate a finite state machine can detect negation cues in natural language .,49,0,26
dataset/preprocessed/training-data/question_answering/3,Multi - Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,2,1,12
dataset/preprocessed/training-data/question_answering/3,Sequence encoders are crucial components in many neural architectures for learning to read and comprehend .,4,0,16
dataset/preprocessed/training-data/question_answering/3,This paper presents a new compositional encoder for reading comprehension ( RC ) .,5,1,14
dataset/preprocessed/training-data/question_answering/3,Our proposed encoder is not only aimed at being fast but also expressive .,6,0,14
dataset/preprocessed/training-data/question_answering/3,"Specifically , the key novelty behind our encoder is that it explicitly models across multiple granularities using a new dilated composition mechanism .",7,0,23
dataset/preprocessed/training-data/question_answering/3,"In our approach , gating functions are learned by modeling relationships and reasoning over multi-granular sequence information , enabling compositional learning that is aware of both long and short term information .",8,0,32
dataset/preprocessed/training-data/question_answering/3,"We conduct experiments on three RC datasets , showing that our proposed encoder demonstrates very promising results both as a standalone encoder as well as a complementary building block .",9,1,30
dataset/preprocessed/training-data/question_answering/3,Empirical results show that simple Bi-Attentive architectures augmented with our proposed encoder not only achieves state - of - the - art / highly competitive results but is also considerably faster than other published works .,10,0,36
dataset/preprocessed/training-data/question_answering/3,"Teaching machines to read , comprehend and reason lives at the heart of reading comprehension ( RC ) tasks .",12,0,20
dataset/preprocessed/training-data/question_answering/3,"In these tasks , the goal is to answer questions based on a given passage , effectively testing the learner 's capability to understand natural language .",13,0,27
dataset/preprocessed/training-data/question_answering/3,"This has been an extremely productive are a of research in the recent years , giving rise to many highly advanced neural network architectures .",14,0,25
dataset/preprocessed/training-data/question_answering/3,"A common denominator in many of these models is the compositional encoder , i.e. , usually a bidirectional recurrent - based ( LSTM or GRU ) encoder that sequentially parses the text sequence wordby - word .",15,0,37
dataset/preprocessed/training-data/question_answering/3,"This helps to model compositionality of words , capturing rich and complex linguistic and syntactic structure in language .",16,0,19
dataset/preprocessed/training-data/question_answering/3,"While the usage of recurrent encoder is often regarded as indispensable in highly complex RC tasks , there are still several challenges and problems pertaining to its usage in modern RC tasks .",17,0,33
dataset/preprocessed/training-data/question_answering/3,"Firstly , documents can be extremely long to the point where running a BiRNN model across along document is computationally prohibitive 1 .",18,0,23
dataset/preprocessed/training-data/question_answering/3,This is aggravated since RC tasks can be easily extended to reasoning over multiple long documents .,19,0,17
dataset/preprocessed/training-data/question_answering/3,"Secondly , recurrent encoders have limited access to long term context since each word is sequentially parsed .",20,0,18
dataset/preprocessed/training-data/question_answering/3,This restricts any form of multisentence and intra-document reasoning from happening within compositional encoder layer .,21,0,16
dataset/preprocessed/training-data/question_answering/3,"To this end , we propose a new compositional encoder that can either be used in place of standard RNN encoders or serve as a new module that is complementary to existing neural architectures .",22,0,35
dataset/preprocessed/training-data/question_answering/3,Our proposed encoder leverages dilated compositions to model relationships across multiple granularities .,23,0,13
dataset/preprocessed/training-data/question_answering/3,"That is , for a given word in the target sequence , our encoder exploits both long - term ( far ) and short - term ( near ) information to decide how much information to retain for it .",24,0,40
dataset/preprocessed/training-data/question_answering/3,"Intuitively , this can be interpreted as learning to compose based on modeling relationships between word - level , phrase - level , sentence - level , paragraph - level and soon .",25,0,33
dataset/preprocessed/training-data/question_answering/3,"The output of the dilated composition mechanism acts as gating functions , which are then used to learn compositional representations of the input sequence .",26,0,25
dataset/preprocessed/training-data/question_answering/3,A brief high - level overview to our proposed encoder is given as follows :,27,0,15
dataset/preprocessed/training-data/question_answering/3,"Firstly , sequences are chunked into blocks based on user - defined ( hyperparameter ) block sizes .",28,0,18
dataset/preprocessed/training-data/question_answering/3,"Block sizes are often dilated in nature , i.e. , 1 , 2 , 4 , 10 , 25 , etc. , in order to capture more long - term information .",29,0,32
dataset/preprocessed/training-data/question_answering/3,Our encoder takes the neural bag - of - words representation of each block size and compresses / folds all words ( that reside in each block ) into a single summed embedding .,30,0,34
dataset/preprocessed/training-data/question_answering/3,All blocks are then passed into fully - connected layers and re-expanded / unfolded to their original sequence lengths .,31,0,20
dataset/preprocessed/training-data/question_answering/3,"For each word , the gating vectors are then constructed by modeling the relationships between all blocks that this word resides in .",32,0,23
dataset/preprocessed/training-data/question_answering/3,"As such , this can be interpreted as a divide - and - conquer sequence encoding method .",33,0,18
dataset/preprocessed/training-data/question_answering/3,This has several advantages .,34,0,5
dataset/preprocessed/training-data/question_answering/3,"Firstly , we enable a major speedup by avoiding either costly step - bystep gate construction while still maintaining interactions between neighboring words .",35,0,24
dataset/preprocessed/training-data/question_answering/3,"As such , our model belongs to a class of architectures which is inspired by QRNNs and SRUs .",36,0,19
dataset/preprocessed/training-data/question_answering/3,"The key difference is that our gates are not constructed by convolution layers but explicit block - based matching across multiple ranges , both long and short .",37,0,28
dataset/preprocessed/training-data/question_answering/3,"Secondly , modeling at along range ( e.g. , 25 or 50 ) enables our model to look further ahead as opposed to only one step forward .",38,0,28
dataset/preprocessed/training-data/question_answering/3,"As such , the learned gates not only possess information about nearby words but also a larger overview of the context .",39,0,22
dataset/preprocessed/training-data/question_answering/3,"This is in similar 2 spirit to self - attention , albeit occuring within the encoder .",40,0,17
dataset/preprocessed/training-data/question_answering/3,"Thirdly , the final gates are formed by modeling relationships between multi-range projections ( n - gram blocks ) , allowing for fine - grained intra-document relationships to be captured .",41,0,31
dataset/preprocessed/training-data/question_answering/3,The over all contributions of our work are as follows :,42,0,11
dataset/preprocessed/training-data/question_answering/3,"We propose DCU ( Dilated Compositional Units 3 ) , a new compositional encoder for both fast and expressive sequence encoding .",43,0,22
dataset/preprocessed/training-data/question_answering/3,We propose an over all architecture that utilizes DCU within a Bi- Attentive framework for both multiple choice and span prediction RC tasks .,44,0,24
dataset/preprocessed/training-data/question_answering/3,"DCU can be used as a standalone ( without RNNs ) for fast reading and / or to - gether with RNN models ( i.e. , DCU - LSTM ) for more expressive reading .",45,0,35
dataset/preprocessed/training-data/question_answering/3,"We conduct extensive experiments on three large - scale and challenging RC datasets - RACE , Search QA and Narrative QA .",46,0,22
dataset/preprocessed/training-data/question_answering/3,"Our model is lightweight , fast and efficient , achieving state - of - the - art or highly competitive performance on three datasets .",47,0,25
dataset/preprocessed/training-data/question_answering/3,Dilated Compositional Units ( DCU ),48,0,6
dataset/preprocessed/training-data/question_answering/3,"In this section , we describe our proposed DCU encoder .",49,0,11
dataset/preprocessed/training-data/question_answering/1,BI - DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION,2,1,8
dataset/preprocessed/training-data/question_answering/1,"Machine comprehension ( MC ) , answering a query about a given context paragraph , requires modeling complex interactions between the context and the query .",4,1,26
dataset/preprocessed/training-data/question_answering/1,"Recently , attention mechanisms have been successfully extended to MC .",5,1,11
dataset/preprocessed/training-data/question_answering/1,"Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed - size vector , couple attentions temporally , and / or often form a uni-directional attention .",6,0,37
dataset/preprocessed/training-data/question_answering/1,"In this paper we introduce the Bi- Directional Attention Flow ( BIDAF ) network , a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query - aware context representation without early summarization .",7,0,46
dataset/preprocessed/training-data/question_answering/1,Our experimental evaluations show that our model achieves the state - of - the - art results in Stanford Question Answering Dataset ( SQuAD ) and CNN / DailyMail cloze test .,8,0,32
dataset/preprocessed/training-data/question_answering/1,The tasks of machine comprehension ( MC ) and question answering ( QA ) have gained significant popularity over the past few years within the natural language processing and computer vision communities .,10,1,33
dataset/preprocessed/training-data/question_answering/1,Systems trained end - to - end now achieve promising results on a variety of tasks in the text and image domains .,11,0,23
dataset/preprocessed/training-data/question_answering/1,"One of the key factors to the advancement has been the use of neural attention mechanism , which enables the system to focus on a targeted are a within a context paragraph ( for MC ) or within an image ( for Visual QA ) , that is most relevant to answer the question .",12,0,55
dataset/preprocessed/training-data/question_answering/1,Attention mechanisms in previous works typically have one or more of the following characteristics .,13,0,15
dataset/preprocessed/training-data/question_answering/1,"First , the computed attention weights are often used to extract the most relevant information from the context for answering the question by summarizing the context into a fixed - size vector .",14,0,33
dataset/preprocessed/training-data/question_answering/1,"Second , in the text domain , they are often temporally dynamic , whereby the attention weights at the current time step are a function of the attended vector at the previous time step .",15,0,35
dataset/preprocessed/training-data/question_answering/1,"Third , they are usually uni-directional , wherein the query attends on the context paragraph or the image .",16,0,19
dataset/preprocessed/training-data/question_answering/1,"In this paper , we introduce the Bi- Directional Attention Flow ( BIDAF ) network , a hierarchical multi-stage architecture for modeling the representations of the context paragraph at different levels of granularity ) .",17,0,35
dataset/preprocessed/training-data/question_answering/1,"BIDAF includes character - level , word - level , and contextual embeddings , and uses bi-directional attention flow to obtain a query - aware context representation .",18,0,28
dataset/preprocessed/training-data/question_answering/1,Our attention mechanism offers following improvements to the previously popular attention paradigms .,19,0,13
dataset/preprocessed/training-data/question_answering/1,"First , our attention layer is not used to summarize the context paragraph into a fixed - size vector .",20,0,20
dataset/preprocessed/training-data/question_answering/1,"Instead , the attention is computed for every time step , and the attended vector at each time step , along with the representations from previous layers , is allowed to flow through to the subsequent modeling layer .",21,0,39
dataset/preprocessed/training-data/question_answering/1,This reduces the information loss caused by early summarization .,22,0,10
dataset/preprocessed/training-data/question_answering/1,"Second , we use a memory - less attention mechanism .",23,0,11
dataset/preprocessed/training-data/question_answering/1,"That is , while we iteratively compute attention through time as in , the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step .",24,0,49
dataset/preprocessed/training-data/question_answering/1,We hypothesize that this simplification leads to the division of labor between the attention layer and the modeling layer .,25,0,20
dataset/preprocessed/training-data/question_answering/1,"It forces the attention layer to focus on learning the attention between the query and the context , and enables the modeling layer to focus on learning the interaction within the query - aware context representation ( the output of the attention layer ) .",26,0,45
dataset/preprocessed/training-data/question_answering/1,It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps .,27,0,20
dataset/preprocessed/training-data/question_answering/1,Our experiments show that memory - less attention gives a clear advantage over dynamic attention .,28,0,16
dataset/preprocessed/training-data/question_answering/1,"Third , we use attention mechanisms in both directions , query - to - context and context - to - query , which provide complimentary information to each other .",29,0,30
dataset/preprocessed/training-data/question_answering/1,Our BIDAF model 1 outperforms all previous approaches on the highly - competitive Stanford Question Answering Dataset ( SQuAD ) test set leaderboard at the time of submission .,30,0,29
dataset/preprocessed/training-data/question_answering/1,"With a modification to only the output layer , BIDAF achieves the state - of - the - art results on the CNN / DailyMail cloze test .",31,0,28
dataset/preprocessed/training-data/question_answering/1,"We also provide an in - depth ablation study of our model on the SQuAD development set , visualize the intermediate feature spaces in our model , and analyse its performance as compared to a more traditional language model for machine comprehension .",32,0,43
dataset/preprocessed/training-data/question_answering/1,Our machine comprehension model is a hierarchical multi-stage process and consists of six layers ( ) :,34,0,17
dataset/preprocessed/training-data/question_answering/1,Character Embedding Layer maps each word to a vector space using character - level CNNs .,36,0,16
dataset/preprocessed/training-data/question_answering/1,2 . Word Embedding,37,0,4
dataset/preprocessed/training-data/question_answering/1,Layer maps each word to a vector space using a pre-trained word embedding model .,38,0,15
dataset/preprocessed/training-data/question_answering/1,Contextual Embedding Layer utilizes contextual cues from surrounding words to refine the embedding of the words .,40,0,17
dataset/preprocessed/training-data/question_answering/1,These first three layers are applied to both the query and context .,41,0,13
dataset/preprocessed/training-data/question_answering/1,Attention flow layer is responsible for linking and fusing information from the context and the query words .,44,0,18
dataset/preprocessed/training-data/question_answering/1,"Unlike previously popular attention mechanisms , the attention flow layer is not used to summarize the query and context into single feature vectors .",45,0,24
dataset/preprocessed/training-data/question_answering/1,"Instead , the attention vector at each time step , along with the embeddings from previous layers , are allowed to flow through to the subsequent modeling layer .",46,0,29
dataset/preprocessed/training-data/question_answering/1,This reduces the information loss caused by early summarization .,47,0,10
dataset/preprocessed/training-data/question_answering/1,The inputs to the layer are contextual vector representations of the context H and the query U .,48,0,18
dataset/preprocessed/training-data/question_answering/1,"The outputs of the layer are the query - aware vector representations of the context words , G , along with the contextual embeddings from the previous layer .",49,0,29
dataset/preprocessed/training-data/question_answering/5,EVIDENCE AGGREGATION FOR ANSWER RE - RANKING IN OPEN - DOMAIN QUESTION ANSWERING,2,1,13
dataset/preprocessed/training-data/question_answering/5,A popular recent approach to answering open - domain questions is to first search for question - related passages and then apply reading comprehension models to extract answers .,4,0,29
dataset/preprocessed/training-data/question_answering/5,Existing methods usually extract answers from single passages independently .,5,0,10
dataset/preprocessed/training-data/question_answering/5,But some questions require a combination of evidence from across different sources to answer correctly .,6,0,16
dataset/preprocessed/training-data/question_answering/5,"In this paper , we propose two models which make use of multiple passages to generate their answers .",7,0,19
dataset/preprocessed/training-data/question_answering/5,Both use an answerreranking approach which reorders the answer candidates generated by an existing state - of - the - art QA model .,8,0,24
dataset/preprocessed/training-data/question_answering/5,"We propose two methods , namely , strengthbased re-ranking and coverage - based re-ranking , to make use of the aggregated evidence from different passages to better determine the answer .",9,0,31
dataset/preprocessed/training-data/question_answering/5,Our models have achieved state - of - the - art results on three public open - domain QA datasets :,10,0,21
dataset/preprocessed/training-data/question_answering/5,"Quasar - T , Search QA and the open - domain version of TriviaQA , with about 8 percentage points of improvement over the former two datasets .",11,0,28
dataset/preprocessed/training-data/question_answering/5,* Equal contribution .,12,0,4
dataset/preprocessed/training-data/question_answering/5,Published as a conference paper at ICLR 2018 Question1 : What is the more popular name for the londonderry air ?,13,0,21
dataset/preprocessed/training-data/question_answering/5,A1 : tune from county P1 : the best known title for this melody is londonderry airlrb - sometimes also called the tune from county derry - rrb -.,14,0,29
dataset/preprocessed/training-data/question_answering/5,Open-domain question answering ( QA ) aims to answer questions from a broad range of domains by effectively marshalling evidence from large open - domain knowledge sources .,16,1,28
dataset/preprocessed/training-data/question_answering/5,"Such resources can be Wikipedia , the whole web , structured knowledge bases or combinations of the above .",17,0,19
dataset/preprocessed/training-data/question_answering/5,Recent work on open - domain QA has focused on using unstructured text retrieved from the web to build machine comprehension models .,18,1,23
dataset/preprocessed/training-data/question_answering/5,"These studies adopt a two - step process : an information retrieval ( IR ) model to coarsely select passages relevant to a question , followed by a reading comprehension ( RC ) model to infer an answer from the passages .",19,0,42
dataset/preprocessed/training-data/question_answering/5,"These studies have made progress in bringing together evidence from large data sources , but they predict an answer to the question with only a single retrieved passage at a time .",20,0,32
dataset/preprocessed/training-data/question_answering/5,"However , answer accuracy can often be improved by using multiple passages .",21,0,13
dataset/preprocessed/training-data/question_answering/5,"In some cases , the answer can only be determined by combining multiple passages .",22,0,15
dataset/preprocessed/training-data/question_answering/5,"In this paper , we propose a method to improve open - domain QA by explicitly aggregating evidence from across multiple passages .",23,0,23
dataset/preprocessed/training-data/question_answering/5,Our method is inspired by two notable observations from previous open - domain QA results analysis :,24,0,17
dataset/preprocessed/training-data/question_answering/5,"First , compared with incorrect answers , the correct answer is often suggested by more passages repeatedly .",25,0,18
dataset/preprocessed/training-data/question_answering/5,"For example , in , the correct answer "" danny boy "" has more passages providing evidence relevant to the question compared to the incorrect one .",26,0,27
dataset/preprocessed/training-data/question_answering/5,This observation can be seen as multiple passages collaboratively enhancing the evidence for the correct answer .,27,0,17
dataset/preprocessed/training-data/question_answering/5,"Second , sometimes the question covers multiple answer aspects , which spreads over multiple passages .",28,0,16
dataset/preprocessed/training-data/question_answering/5,"In order to infer the correct answer , one has to find ways to aggregate those multiple passages in an effective yet sensible way to try to cover all aspects .",29,0,31
dataset/preprocessed/training-data/question_answering/5,"In the correct answer "" Galileo Galilei "" at the bottom has passages P1 , "" Galileo was a physicist ... "" and P2 , "" Galileo discovered the first 4 moons of Jupiter "" , mentioning two pieces of evidence to match the question .",30,0,46
dataset/preprocessed/training-data/question_answering/5,"In this case , the aggregation of these two pieces of evidence can help entail the ground - truth answer "" Galileo Galilei "" .",31,0,25
dataset/preprocessed/training-data/question_answering/5,"In comparison , the incorrect answer "" Isaac Newton "" has passages providing partial evidence on only "" physicist , mathematician and astronomer "" .",32,0,25
dataset/preprocessed/training-data/question_answering/5,This observation illustrates the way in which multiple passages may provide complementary evidence to better infer the correct answer to a question .,33,0,23
dataset/preprocessed/training-data/question_answering/5,"To provide more accurate answers for open - domain QA , we hope to make better use of multiple passages for the same question by aggregating both the strengthened and the complementary evidence from all the passages .",34,0,38
dataset/preprocessed/training-data/question_answering/5,We formulate the above evidence aggregation as an answer re-ranking problem .,35,0,12
dataset/preprocessed/training-data/question_answering/5,"Re-ranking has been commonly used in NLP problems , such as in parsing and translation , in order to make use of high - order or global features thatare too expensive for decoding algorithms .",36,0,35
dataset/preprocessed/training-data/question_answering/5,"Here we apply the idea of re-ranking ; for each answer candidate , we efficiently incorporate global information from multiple pieces of textual evidence without significantly increasing the complexity of the prediction of the RC model .",37,0,37
dataset/preprocessed/training-data/question_answering/5,"Specifically , we first collect the top - K candidate answers based on their probabilities computed by a standard RC / QA system , and then we use two proposed re-rankers to re-score the answer candidates by aggregating each candidate 's evidence in different ways .",38,0,46
dataset/preprocessed/training-data/question_answering/5,The re-rankers are :,39,0,4
dataset/preprocessed/training-data/question_answering/5,"A strength - based re-ranker , which ranks the answer candidates according to how often their evidence occurs in different passages .",40,0,22
dataset/preprocessed/training-data/question_answering/5,"The re-ranker is based on the first observation if an answer candidate has multiple pieces of evidence , and each passage containing some evidence tends to predict the answer with a relatively high score ( although it may not be the top score ) , then the candidate is more likely to be correct .",41,0,55
dataset/preprocessed/training-data/question_answering/5,"The passage count of each candidate , and the aggregated probabilities for the candidate , reflect how strong its evidence is , and thus in turn suggest how likely the candidate is the corrected answer .",42,0,36
dataset/preprocessed/training-data/question_answering/5,"A coverage - based re-ranker , which aims to rank an answer candidate higher if the union of all its contexts in different passages could cover more aspects included in the question .",43,0,33
dataset/preprocessed/training-data/question_answering/5,"To achieve this , for each answer we concatenate all the passages that contain the answer together .",44,0,18
dataset/preprocessed/training-data/question_answering/5,The result is a new context that aggregates all the evidence necessary to entail the answer for the question .,45,0,20
dataset/preprocessed/training-data/question_answering/5,"We then treat the new context as one sequence to represent the answer , and build an attention - based match - LSTM model between the sequence and the question to measure how well the new aggregated context could entail the question .",46,0,43
dataset/preprocessed/training-data/question_answering/5,"Overall , our contributions are as follows :",47,0,8
dataset/preprocessed/training-data/question_answering/5,"1 ) We propose a re-ranking - based framework to make use of the evidence from multiple passages in open - domain QA , and two re-rankers , namely , a strengthbased re-ranker and a coverage - based re-ranker , to perform evidence aggregation in existing opendomain QA datasets .",48,0,50
dataset/preprocessed/training-data/question_answering/5,We find the second re-ranker performs better than the first one on two of the three public datasets .,49,0,19
dataset/preprocessed/training-data/question_answering/0,Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering,2,1,12
dataset/preprocessed/training-data/question_answering/0,The most approaches to Knowledge Base Question Answering are based on semantic parsing .,4,0,14
dataset/preprocessed/training-data/question_answering/0,"In this paper , we address the problem of learning vector representations for complex semantic parses that consist of multiple entities and relations .",5,0,24
dataset/preprocessed/training-data/question_answering/0,Previous work largely focused on selecting the correct semantic relations for a question and disregarded the structure of the semantic parse : the connections between entities and the directions of the relations .,6,0,33
dataset/preprocessed/training-data/question_answering/0,We propose to use Gated Graph Neural Networks to encode the graph structure of the semantic parse .,7,0,18
dataset/preprocessed/training-data/question_answering/0,We show on two data sets that the graph networks outperform all baseline models that do not explicitly model the structure .,8,0,22
dataset/preprocessed/training-data/question_answering/0,The error analysis confirms that our approach can successfully process complex semantic parses .,9,0,14
dataset/preprocessed/training-data/question_answering/0,Knowledge base question answering ( QA ) is an important natural language processing problem .,11,1,15
dataset/preprocessed/training-data/question_answering/0,"Given a natural language question , the task is to find a set of entities in a knowledge base ( KB ) that constitutes the answer .",12,0,27
dataset/preprocessed/training-data/question_answering/0,"For example , for a question "" What is Princess Leia 's home planet ? "" the answer , "" Alderaan "" , could be retrieved from a general - purpose KB , such as Wikidata 1 .",13,0,38
dataset/preprocessed/training-data/question_answering/0,A successful KB QA system would ultimately provide a universally accessible natural language interface to factual knowledge .,14,0,18
dataset/preprocessed/training-data/question_answering/0,QA requires precise modeling of the question semantics through the entities and relations available in the KB in order to retrieve the correct answer .,15,1,25
dataset/preprocessed/training-data/question_answering/0,shows how the above example question could be modeled using Wikidata .,16,0,12
dataset/preprocessed/training-data/question_answering/0,The depicted graph structure consists of entities and relations from the KB and the special q-node .,17,0,17
dataset/preprocessed/training-data/question_answering/0,Any entity in the KB that can take the place of the q - node will be apart of the answer .,18,0,22
dataset/preprocessed/training-data/question_answering/0,"In this paper , we describe a semantic parsing approach to the problem of KB QA .",19,1,17
dataset/preprocessed/training-data/question_answering/0,"That is , for each input question , we construct an explicit structural semantic parse ( semantic graph ) , as in .",20,0,23
dataset/preprocessed/training-data/question_answering/0,Semantic parses can be deterministically converted to a query to extract the answers from the KB .,21,0,17
dataset/preprocessed/training-data/question_answering/0,Similar graphical representations were used in previous work .,22,0,9
dataset/preprocessed/training-data/question_answering/0,"However , the modern semantic parsing approaches usually focus either on the syntactic analysis of the input question or on detecting individual KB relations , whereas the structure of the semantic parse is ignored or only approximately modeled .",23,0,39
dataset/preprocessed/training-data/question_answering/0,use the syntactic structure of the question to build all possible semantic parses and then apply a linear model with manually defined features to choose the correct parse .,24,0,29
dataset/preprocessed/training-data/question_answering/0,A subset of their features encodes basic information about the graph structure of the semantic parse ( e.g. number of nodes ) .,25,0,23
dataset/preprocessed/training-data/question_answering/0,The state - of - the - art approach of and restricts all semantic parses to a single core relation and a small set of constraints that can be added to it .,26,0,33
dataset/preprocessed/training-data/question_answering/0,Their system uses manual features for the constraints and a similarity score between the core relation and the question to model the semantic parses .,27,0,25
dataset/preprocessed/training-data/question_answering/0,The abovementioned systems were evaluated on the WebQuestions data set .,28,0,11
dataset/preprocessed/training-data/question_answering/0,plots results for the state - of - the - art systems by the number of relations that needs to be identified to get the correct answer to a question .,29,0,31
dataset/preprocessed/training-data/question_answering/0,"2 For example , the question in to find the answer .",30,0,12
dataset/preprocessed/training-data/question_answering/0,It can be clearly seen in that the lack of structure modeling in the modern approaches results in a worse performance on more complex questions that require more than one relation .,31,0,32
dataset/preprocessed/training-data/question_answering/0,We claim that one needs to explicitly model the semantic structure to be able to find the correct semantic parse for complex questions .,32,0,24
dataset/preprocessed/training-data/question_answering/0,"In this paper , we address this gap and investigate ways to encode the structure of a semantic parse and to improve the performance for more complex questions .",33,0,29
dataset/preprocessed/training-data/question_answering/0,"In particular , we adapt Gated Graph Neural Networks ( GGNNs ) , described in , to process and score semantic parses .",34,0,23
dataset/preprocessed/training-data/question_answering/0,"To verify that GGNNs indeed offer an improvement , we construct a set of baselines based on the previous work that we train and evaluate in the same controlled QA environment .",35,0,32
dataset/preprocessed/training-data/question_answering/0,"Throughout the experiments , we use the Wikidata open - domain KB to construct semantic parses and retrieve the answers .",36,0,21
dataset/preprocessed/training-data/question_answering/0,"To summarize , the main contributions of our work are :",39,0,11
dataset/preprocessed/training-data/question_answering/0,( i ) Our analysis shows that the current solutions for KB QA do not perform well on complex questions ;,40,0,21
dataset/preprocessed/training-data/question_answering/0,( ii ) We apply Gated Graph Neural Networks on directed graphs with labeled edges and adapt them to handle a large set of possible entities and relations types from the KB .,41,0,33
dataset/preprocessed/training-data/question_answering/0,"To the best of our knowledge , we are the first to use GGNNs for semantic parsing and KB QA ;",42,0,21
dataset/preprocessed/training-data/question_answering/0,( iii ) Our Gated Graph Neural Network implementation for semantic parsing improves performance on complex questions in comparison to strong baselines .,43,0,23
dataset/preprocessed/training-data/question_answering/0,The results show a 27.4 % improvement of the F - score against the best non -graph model .,44,0,19
dataset/preprocessed/training-data/question_answering/0,Code and data sets,45,0,4
dataset/preprocessed/training-data/question_answering/0,Our system can be used with a pre-trained model to answer factual questions with Wikidata or trained a new on any data set that has question - answer pairs .,46,0,30
dataset/preprocessed/training-data/question_answering/0,"The complete code , the scripts that produce the evaluation data and the installation instructions can be found here :",47,0,20
dataset/preprocessed/training-data/question_answering/0,2 Semantic parsing,49,0,3
dataset/preprocessed/training-data/question_answering/2,Focal Visual - Text Attention for Visual Question Answering,2,1,9
dataset/preprocessed/training-data/question_answering/2,Recent insights on language and vision with neural networks have been successfully applied to simple singleimage visual question answering .,4,0,20
dataset/preprocessed/training-data/question_answering/2,"However , to tackle reallife question answering problems on multimedia collections such as personal photos , we have to look at whole collections with sequences of photos or videos .",5,0,30
dataset/preprocessed/training-data/question_answering/2,"When answering questions from a large collection , a natural problem is to identify snippets to support the answer .",6,0,20
dataset/preprocessed/training-data/question_answering/2,"In this paper , we describe a novel neural network called Focal Visual - Text Attention network ( FVTA ) for collective reasoning in visual question answering , where both visual and text sequence information such as images and text metadata are presented .",7,0,44
dataset/preprocessed/training-data/question_answering/2,FVTA introduces an end - to - end approach that makes use of a hierarchical process to dynamically determine what media and what time to focus on in the sequential data to answer the question .,8,0,36
dataset/preprocessed/training-data/question_answering/2,FVTA can not only answer the questions well but also provides the justifications which the system results are based upon to get the answers .,9,0,25
dataset/preprocessed/training-data/question_answering/2,FVTA achieves state - of - the - art performance on the MemexQA dataset and competitive results on the MovieQA dataset .,10,0,22
dataset/preprocessed/training-data/question_answering/2,Language and vision have emerged as a popular research are a in computer vision .,12,0,15
dataset/preprocessed/training-data/question_answering/2,"Visual question answering ( VQA ) is a successful direction utilizing both computer vision and natural language processing techniques to solve an interesting problem : given a pair of image and a question ( in natural language ) , the goal is to learn an inference model that can the answer questions according to cues discovered from the image .",13,1,60
dataset/preprocessed/training-data/question_answering/2,"A variety of methods have been proposed to address the challenges from different aspects , with remarkable progress on answering about a single image .",14,0,25
dataset/preprocessed/training-data/question_answering/2,"Extending from VQA on a single image , this paper considers the following problem :",15,1,15
dataset/preprocessed/training-data/question_answering/2,Suppose a user 's photos and videos are organized in a sequence ordered by their creation time .,16,0,18
dataset/preprocessed/training-data/question_answering/2,"Some photos or videos maybe associated with meta labels or annotations such as time , GPS , captions , comments , and meaningful title .",17,0,25
dataset/preprocessed/training-data/question_answering/2,We are interested in training,18,0,5
dataset/preprocessed/training-data/question_answering/2,There are two challenges to solve the above problem .,19,0,10
dataset/preprocessed/training-data/question_answering/2,"First , the input is provided in an unstructured form .",20,0,11
dataset/preprocessed/training-data/question_answering/2,"The question is associated with multiple sequences , in the form of videos or images .",21,0,16
dataset/preprocessed/training-data/question_answering/2,"Such sequences are temporally ordered , and each sequence contains multiple time steps .",22,0,14
dataset/preprocessed/training-data/question_answering/2,"At each time there are visual data , text annotations and other metadata .",23,0,14
dataset/preprocessed/training-data/question_answering/2,"In this paper , we call the format visual - text sequence data .",24,0,14
dataset/preprocessed/training-data/question_answering/2,"Note that not all the photos and videos are annotated , which requires a robust method to leverage inconsistently available multimodal data .",25,0,23
dataset/preprocessed/training-data/question_answering/2,The second challenge requires interpretable justifications in addition to direct answer based on sequence data .,26,0,16
dataset/preprocessed/training-data/question_answering/2,"To help users with a lot of photos and videos , a natural requirement is to identify the supporting evidence for the answer .",27,0,24
dataset/preprocessed/training-data/question_answering/2,"An example question as shown in , is "" when was the last time I went to a bar ? """,28,0,21
dataset/preprocessed/training-data/question_answering/2,"From the users ' viewpoint , a good QA system should not only give a definite answer ( e.g. , January 20 , 2016 ) , but also ground evidential images or text snippets in the input sequence to justify the reasoning process .",29,0,44
dataset/preprocessed/training-data/question_answering/2,"Given imperfect VQA models , humans often want to verify the answer .",30,0,13
dataset/preprocessed/training-data/question_answering/2,The inspection process maybe trivial for a single image but can take a significant amount of time to examine every image and the complete text words .,31,0,27
dataset/preprocessed/training-data/question_answering/2,"To address these two challenges , we propose a focal visual - text attention ( FVTA ) model for sequential data",32,0,21
dataset/preprocessed/training-data/question_answering/2,Our model is motivated by the reasoning process of humans .,34,0,11
dataset/preprocessed/training-data/question_answering/2,"In order to answer a question , a human would first quickly skim the input and then focus on a few , small temporal regions in the visual - text sequences to derive an answer .",35,0,36
dataset/preprocessed/training-data/question_answering/2,"In fact , statistics suggest that , on average , humans only need 1.5 images to answer a question after the skimming .",36,0,23
dataset/preprocessed/training-data/question_answering/2,"Inspired by this process , FVTA first learns to localize relevant information within a few , small , temporally consecutive regions over the input sequences , and learns to infer an answer based on the cross-modal statistics pooled from these regions .",37,0,42
dataset/preprocessed/training-data/question_answering/2,FVTA proposes a novel kernel to compute the attention tensor that jointly models the latent information in three sources :,38,0,20
dataset/preprocessed/training-data/question_answering/2,"1 ) answer - signaling words in the question , 2 ) temporal correlation within a sequence , and 3 ) cross-modal interaction between the text and image .",39,0,29
dataset/preprocessed/training-data/question_answering/2,"FVTA attention allows for collective reasoning by the attention kernel learned over a few , small , consecutive sub-sequences of text and image .",40,0,24
dataset/preprocessed/training-data/question_answering/2,It can also produce a list of evidential images / texts to justify the reasoning .,41,0,16
dataset/preprocessed/training-data/question_answering/2,"As shown in , the highlighted cubes are regions of high activations in the proposed FVTA .",42,0,17
dataset/preprocessed/training-data/question_answering/2,"To summarize , the contribution of this paper is threefold :",43,0,11
dataset/preprocessed/training-data/question_answering/2,We propose a novel attention kernel for VQA on visual - text data .,44,0,14
dataset/preprocessed/training-data/question_answering/2,Experiments show that it outperforms existing attention methods .,45,0,9
dataset/preprocessed/training-data/question_answering/2,The proposed attention tensor can be used to localize evidential image and text snippets to explain the reasoning process .,46,0,20
dataset/preprocessed/training-data/question_answering/2,We quantitatively verify that the evidence produced by our method are more correlated to that of human annotators .,47,0,19
dataset/preprocessed/training-data/question_answering/2,Our method achieves the state - of - the - art results on two VQA benchmarks .,48,0,17
dataset/preprocessed/training-data/question_answering/4,Densely Connected Attention Propagation for Reading Comprehension,2,1,7
dataset/preprocessed/training-data/question_answering/4,"We propose DECAPROP ( Densely Connected Attention Propagation ) , a new densely connected neural architecture for reading comprehension ( RC ) .",4,1,23
dataset/preprocessed/training-data/question_answering/4,There are two distinct characteristics of our model .,5,0,9
dataset/preprocessed/training-data/question_answering/4,"Firstly , our model densely connects all pairwise layers of the network , modeling relationships between passage and query across all hierarchical levels .",6,0,24
dataset/preprocessed/training-data/question_answering/4,"Secondly , the dense connectors in our network are learned via attention instead of standard residual skip - connectors .",7,0,20
dataset/preprocessed/training-data/question_answering/4,"To this end , we propose novel Bidirectional Attention Connectors ( BAC ) for efficiently forging connections throughout the network .",8,0,21
dataset/preprocessed/training-data/question_answering/4,We conduct extensive experiments on four challenging RC benchmarks .,9,1,10
dataset/preprocessed/training-data/question_answering/4,"Our proposed approach achieves state - of - the - art results on all four , outperforming existing baselines by up to 2.6 % ?",10,0,25
dataset/preprocessed/training-data/question_answering/4,14.2 % in absolute F1 score .,11,0,7
dataset/preprocessed/training-data/question_answering/4,The dominant neural architectures for reading comprehension ( RC ) typically follow a standard ' encode - interact - point ' design .,13,0,23
dataset/preprocessed/training-data/question_answering/4,"Following the embedding layer , a compositional encoder typically encodes Q ( query ) and P ( passage ) individually .",14,0,21
dataset/preprocessed/training-data/question_answering/4,"Subsequently , an ( bidirectional ) attention layer is then used to model interactions between P/Q .",15,0,17
dataset/preprocessed/training-data/question_answering/4,"Finally , these attended representations are then reasoned over to find ( point to ) the best answer span .",16,0,20
dataset/preprocessed/training-data/question_answering/4,"While , there might be slight variants of this architecture , this over all architectural design remains consistent across many RC models .",17,0,23
dataset/preprocessed/training-data/question_answering/4,"Intuitively , the design of RC models often possess some depth , i.e. , every stage of the network easily comprises several layers .",18,0,24
dataset/preprocessed/training-data/question_answering/4,"For example , the R - NET architecture adopts three BiRNN layers as the encoder and two additional BiRNN layers at the interaction layer .",19,0,25
dataset/preprocessed/training-data/question_answering/4,"BiDAF uses two BiLSTM layers at the pointer layer , etc .",20,0,12
dataset/preprocessed/training-data/question_answering/4,"As such , RC models are often relatively deep , at the very least within the context of NLP .",21,0,20
dataset/preprocessed/training-data/question_answering/4,"Unfortunately , the depth of a model is not without implications .",22,0,12
dataset/preprocessed/training-data/question_answering/4,"It is well - established fact that increasing the depth may impair gradient flow and feature propagation , making networks harder to train .",23,0,24
dataset/preprocessed/training-data/question_answering/4,"This problem is prevalent in computer vision , where mitigation strategies that rely on shortcut connections such as Residual networks , GoogLeNet and DenseNets were incepted .",24,0,27
dataset/preprocessed/training-data/question_answering/4,"Naturally , many of the existing RC models already have some built - in designs to workaround this issue by shortening the signal path in the network .",25,0,28
dataset/preprocessed/training-data/question_answering/4,"Examples include attention flow , residual connections or simply the usage of highway encoders .",26,0,15
dataset/preprocessed/training-data/question_answering/4,"As such , we hypothesize that explicitly improving information flow can lead to further and considerable improvements in RC models .",27,0,21
dataset/preprocessed/training-data/question_answering/4,"A second observation is that the flow of P/Q representations across the network are often well - aligned and ' synchronous ' , i.e. , P is often only matched with Q at the same hierarchical stage ( e.g. , only after they have passed through a fixed number of encoder layers ) .",28,0,54
dataset/preprocessed/training-data/question_answering/4,"To this end , we hypothesize that increasing the number of interaction interfaces , i.e. , matching in an asynchronous , cross-hierarchical fashion , can also lead to an improvement in performance .",29,0,33
dataset/preprocessed/training-data/question_answering/4,"Based on the above mentioned intuitions , this paper proposes a new architecture with two distinct characteristics .",30,0,18
dataset/preprocessed/training-data/question_answering/4,"Firstly , our network is densely connected , connecting every layer of P with every layer of Q .",31,0,19
dataset/preprocessed/training-data/question_answering/4,This not only facilitates information flow but also increases the number of interaction interfaces between P/Q .,32,0,17
dataset/preprocessed/training-data/question_answering/4,"Secondly , our network is densely connected by attention , making it vastly different from any residual mitigation strategy in the literature .",33,0,23
dataset/preprocessed/training-data/question_answering/4,"To the best of our knowledge , this is the first work that explicitly considers attention as a form of skip - connector .",34,0,24
dataset/preprocessed/training-data/question_answering/4,"Notably , models such as BiDAF incorporates a form of attention propagation ( flow ) .",35,0,16
dataset/preprocessed/training-data/question_answering/4,"However , this is inherently unsuitable for forging dense connections throughout the network since this would incur a massive increase in the representation size in subsequent layers .",36,0,28
dataset/preprocessed/training-data/question_answering/4,"To this end , we propose efficient Bidirectional Attention Connectors ( BAC ) as a base building block to connect two sequences at arbitrary layers .",37,0,26
dataset/preprocessed/training-data/question_answering/4,"The key idea is to compress the attention outputs so that they can be small enough to propagate , yet enabling a connection between two sequences .",38,0,27
dataset/preprocessed/training-data/question_answering/4,"The propagated features are collectively passed into prediction layers , which effectively connect shallow layers to deeper layers .",39,0,19
dataset/preprocessed/training-data/question_answering/4,"Therefore , this enables multiple bidirectional attention calls to be executed without much concern , allowing us to efficiently connect multiple layers together .",40,0,24
dataset/preprocessed/training-data/question_answering/4,"Overall , we propose DECAPROP ( Densely Connected Attention Propagation ) , a novel architecture for reading comprehension .",41,0,19
dataset/preprocessed/training-data/question_answering/4,DECAPROP achieves a significant gain of 2.6 % ?,42,0,9
dataset/preprocessed/training-data/question_answering/4,"14.2 % absolute improvement in F1 score over the existing state - of - the - art on four challenging RC datasets , namely News QA , Quasar - T , Search QA and Narrative QA .",43,0,37
dataset/preprocessed/training-data/question_answering/4,Bidirectional Attention Connectors ( BAC ),44,0,6
dataset/preprocessed/training-data/question_answering/4,This section introduces the Bidirectional Attention Connectors ( BAC ) module which is central to our over all architecture .,45,0,20
dataset/preprocessed/training-data/question_answering/4,The BAC module can bethought of as a connector component that connects two sequences / layers .,46,0,17
dataset/preprocessed/training-data/question_answering/4,"The key goals of this module are to ( 1 ) connect any two layers of P/Q in the network , returning a residual feature that can be propagated 1 to deeper layers , ( 2 ) model cross-hierarchical interactions between P/Q and ( 3 ) minimize any costs incurred to other network components such that this component maybe executed multiple times across all layers .",47,0,66
dataset/preprocessed/training-data/question_answering/4,Let P ?,48,0,3
dataset/preprocessed/training-data/question_answering/4,R p d and Q ?,49,0,6
dataset/preprocessed/training-data/semantic_role_labeling/3,Deep Semantic Role Labeling with Self - Attention,2,1,8
dataset/preprocessed/training-data/semantic_role_labeling/3,Semantic Role Labeling ( SRL ) is believed to be a crucial step towards natural language understanding and has been widely studied .,4,1,23
dataset/preprocessed/training-data/semantic_role_labeling/3,"Recent years , end - to - end SRL with recurrent neural networks ( RNN ) has gained increasing attention .",5,0,21
dataset/preprocessed/training-data/semantic_role_labeling/3,"However , it remains a major challenge for RNNs to handle structural information and long range dependencies .",6,0,18
dataset/preprocessed/training-data/semantic_role_labeling/3,"In this paper , we present a simple and effective architecture for SRL which aims to address these problems .",7,1,20
dataset/preprocessed/training-data/semantic_role_labeling/3,Our model is based on self - attention which can directly capture the relationships between two tokens regardless of their distance .,8,0,22
dataset/preprocessed/training-data/semantic_role_labeling/3,"Our single model achieves F1 = 83.4 on the CoNLL - 2005 shared task dataset and F1 = 82.7 on the CoNLL - 2012 shared task dataset , which outperforms the previous state - of - the - art results by 1.8 and 1.0 F1 score respectively .",9,0,48
dataset/preprocessed/training-data/semantic_role_labeling/3,"Besides , our model is computationally efficient , and the parsing speed is 50 K tokens per second on a single Titan X GPU .",10,0,25
dataset/preprocessed/training-data/semantic_role_labeling/3,"Semantic Role Labeling is a shallow semantic parsing task , whose goal is to determine essentially "" who did what to whom "" , "" when "" and "" where "" .",12,1,32
dataset/preprocessed/training-data/semantic_role_labeling/3,"Semantic roles indicate the basic event properties and relations among relevant entities in the sentence and provide an intermediate level of semantic representation thus benefiting many NLP applications , such as Information Extraction , Question Answering , Machine Translation ) and Multi-document Abstractive Summarization .",13,0,45
dataset/preprocessed/training-data/semantic_role_labeling/3,Semantic roles are closely related to syntax .,14,0,8
dataset/preprocessed/training-data/semantic_role_labeling/3,"Therefore , traditional SRL approaches rely heavily on the syntactic structure of a sentence , which brings intrinsic complexity and restrains these systems to be domain specific .",15,0,28
dataset/preprocessed/training-data/semantic_role_labeling/3,"Recently , end - to - end models for SRL without syntactic inputs achieved promising results on this task .",16,0,20
dataset/preprocessed/training-data/semantic_role_labeling/3,"As the pioneering work , introduced a stacked long short - term memory network ( LSTM ) and achieved the state - of - the - art results .",17,0,29
dataset/preprocessed/training-data/semantic_role_labeling/3,reported further improvements by using deep highway bidirectional LSTMs with constrained decoding .,18,0,13
dataset/preprocessed/training-data/semantic_role_labeling/3,These successes involving end - to - end models reveal the potential ability of LSTMs for handling the underlying syntactic structure of the sentences .,19,0,25
dataset/preprocessed/training-data/semantic_role_labeling/3,"Despite recent successes , these RNN - based models have limitations .",20,0,12
dataset/preprocessed/training-data/semantic_role_labeling/3,RNNs treat each sentence as a sequence of words and recursively compose each word with its previous hidden state .,21,0,20
dataset/preprocessed/training-data/semantic_role_labeling/3,"The recurrent connections make RNNs applicable for sequential prediction tasks with arbitrary length , however , there still remain several challenges in practice .",22,0,24
dataset/preprocessed/training-data/semantic_role_labeling/3,The first one is related to memory compression problem .,23,0,10
dataset/preprocessed/training-data/semantic_role_labeling/3,"As the entire history is encoded into a single fixed - size vector , the model requires larger memory capacity to store information for longer sentences .",24,0,27
dataset/preprocessed/training-data/semantic_role_labeling/3,The unbalanced way of dealing with sequential information leads the network performing poorly on long sentences while wasting memory on shorter ones .,25,0,23
dataset/preprocessed/training-data/semantic_role_labeling/3,The second one is concerned with the inherent structure of sentences .,26,0,12
dataset/preprocessed/training-data/semantic_role_labeling/3,RNNs lack away to tackle the tree - structure of the inputs .,27,0,13
dataset/preprocessed/training-data/semantic_role_labeling/3,"The sequential way to process the inputs remains the network depth - in - time , and the number of nonlinearities depends on the time - steps .",28,0,28
dataset/preprocessed/training-data/semantic_role_labeling/3,"To address these problems above , we present a deep attentional neural network ( DEEPATT ) for the task of SRL 1 .",29,0,23
dataset/preprocessed/training-data/semantic_role_labeling/3,Our models rely on the self - attention mechanism which directly draws the global dependencies of the inputs .,30,0,19
dataset/preprocessed/training-data/semantic_role_labeling/3,"In contrast to RNNs , a major advantage of self - attention is that it conducts direct connections between two arbitrary tokens in a sentence .",31,0,26
dataset/preprocessed/training-data/semantic_role_labeling/3,"Therefore , distant elements can interact with each other by shorter paths ( O ( 1 ) v.s. O ( n ) ) , which allows unimpeded information flow through the network .",32,0,33
dataset/preprocessed/training-data/semantic_role_labeling/3,"Self - attention also provides a more flexible way to select , represent and synthesize the information of the inputs and is complementary to RNN based models .",33,0,28
dataset/preprocessed/training-data/semantic_role_labeling/3,"Along with self - attention , DEEP - ATT comes with three variants which uses recurrent ( RNN ) , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the representations .",34,0,39
dataset/preprocessed/training-data/semantic_role_labeling/3,"Although DEEPATT is fairly simple , it gives remarkable empirical results .",35,0,12
dataset/preprocessed/training-data/semantic_role_labeling/3,Our single model outperforms the previ-ous state - of - the - art systems on the CoNLL - 2005 shared task dataset and the CoNLL - 2012 shared task dataset by 1.8 and 1.0 F 1 score respectively .,36,0,39
dataset/preprocessed/training-data/semantic_role_labeling/3,"It is also worth mentioning that on the out - of - domain dataset , we achieve an improvement upon the previous end - to - end approach ) by 2.0 F 1 score .",37,0,35
dataset/preprocessed/training-data/semantic_role_labeling/3,"The feed - forward variant of DEEPATT allows significantly more parallelization , and the parsing speed is 50 K tokens per second on a single Titan X GPU .",38,0,29
dataset/preprocessed/training-data/semantic_role_labeling/3,Semantic Role Labeling,39,0,3
dataset/preprocessed/training-data/semantic_role_labeling/3,"Given a sentence , the goal of SRL is to identify and classify the arguments of each target verb into semantic roles .",40,0,23
dataset/preprocessed/training-data/semantic_role_labeling/3,"For example , for the sentence "" Marry borrowed a book from John last week . "" and the target verb borrowed , SRL yields the following outputs :",41,0,29
dataset/preprocessed/training-data/semantic_role_labeling/3,"Here ARG0 represents the borrower , ARG1 represents the thing borrowed , ARG2 represents the entity borrowed from , AM - TMP is an adjunct indicating the timing of the action and V represents the verb .",42,0,37
dataset/preprocessed/training-data/semantic_role_labeling/3,"Generally , semantic role labeling consists of two steps : identifying and classifying arguments .",43,0,15
dataset/preprocessed/training-data/semantic_role_labeling/3,"The former step involves assigning either a semantic argument or nonargument for a given predicate , while the latter includes labeling a specific semantic role for the identified argument .",44,0,30
dataset/preprocessed/training-data/semantic_role_labeling/3,It is also common to prune obvious non-candidates before the first step and to apply post-processing procedure to fix inconsistent predictions after the second step .,45,0,26
dataset/preprocessed/training-data/semantic_role_labeling/3,"Finally , a dynamic programming algorithm is often applied to find the global optimum solution for this typical sequence labeling problem at the inference stage .",46,0,26
dataset/preprocessed/training-data/semantic_role_labeling/3,"In this paper , we treat SRL as a BIO tagging problem .",47,0,13
dataset/preprocessed/training-data/semantic_role_labeling/3,Our approach is extremely simple .,48,0,6
dataset/preprocessed/training-data/semantic_role_labeling/3,"As illustrated in , the original utterances and the corresponding predicate masks are first projected into real - value vectors , namely embeddings , which are fed to the next layer .",49,0,32
dataset/preprocessed/training-data/semantic_role_labeling/1,Linguistically - Informed Self - Attention for Semantic Role Labeling,2,1,10
dataset/preprocessed/training-data/semantic_role_labeling/1,Current state - of - the - art semantic role labeling ( SRL ) uses a deep neural network with no explicit linguistic features .,4,1,25
dataset/preprocessed/training-data/semantic_role_labeling/1,"However , prior work has shown that gold syntax trees can dramatically improve SRL decoding , suggesting the possibility of increased accuracy from explicit modeling of syntax .",5,1,28
dataset/preprocessed/training-data/semantic_role_labeling/1,"In this work , we present linguistically - informed self - attention ( LISA ) : a neural network model that combines multi-head self - attention with multi-task learning across dependency parsing , part - ofspeech tagging , predicate detection and SRL .",6,0,43
dataset/preprocessed/training-data/semantic_role_labeling/1,"Unlike previous models which require significant pre-processing to prepare linguistic features , LISA can incorporate syntax using merely raw tokens as input , encoding the sequence only once to simultaneously perform parsing , predicate detection and role labeling for all predicates .",7,0,42
dataset/preprocessed/training-data/semantic_role_labeling/1,Syntax is incorporated by training one attention head to attend to syntactic parents for each token .,8,0,17
dataset/preprocessed/training-data/semantic_role_labeling/1,"Moreover , if a high - quality syntactic parse is already available , it can be beneficially injected at test time without re-training our SRL model .",9,0,27
dataset/preprocessed/training-data/semantic_role_labeling/1,"In experiments on CoNLL - 2005 SRL , LISA achieves new state - of - the - art performance for a model using predicted predicates and standard word embeddings , attaining 2.5 F1 absolute higher than the previous state - of - the - art on newswire and more than 3.5 F1 on outof - domain data , nearly 10 % reduction in error .",10,0,65
dataset/preprocessed/training-data/semantic_role_labeling/1,On ConLL- 2012 English SRL we also show an improvement of more than 2.5 F1 .,11,0,16
dataset/preprocessed/training-data/semantic_role_labeling/1,"LISA also out - performs the state - of - the - art with contextually - encoded ( ELMo ) word representations , by nearly 1.0 F1 on news and more than 2.0 F1 on out - of - domain text .",12,0,42
dataset/preprocessed/training-data/semantic_role_labeling/1,"Semantic role labeling ( SRL ) extracts a high - level representation of meaning from a sentence , labeling e.g. who did what to whom .",14,0,26
dataset/preprocessed/training-data/semantic_role_labeling/1,"Explicit representations of such semantic information have been shown to improve results in challenging downstream tasks such as dialog systems , machine reading and translation .",15,0,26
dataset/preprocessed/training-data/semantic_role_labeling/1,"Though syntax was long considered an obvious prerequisite for SRL systems , recently deep neural network architectures have surpassed syntacticallyinformed models , achieving state - of - the art SRL performance with no explicit modeling of syntax .",16,0,38
dataset/preprocessed/training-data/semantic_role_labeling/1,"An additional benefit of these end - to - end models is that they require just raw tokens and ( usually ) detected predicates as input , whereas richer linguistic features typically require extraction by an auxiliary pipeline of models .",17,0,41
dataset/preprocessed/training-data/semantic_role_labeling/1,"Still , recent work indicates that neural network models could see even higher accuracy gains by leveraging syntactic information rather than ignoring it .",18,0,24
dataset/preprocessed/training-data/semantic_role_labeling/1,"indicate that many of the errors made by a syntaxfree neural network on SRL are tied to certain syntactic confusions such as prepositional phrase attachment , and show that while constrained inference using a relatively low - accuracy predicted parse can provide small improvements in SRL accuracy , providing a gold - quality parse leads to substantial gains . incorporate syntax from a high - quality parser ( Kiperwasser and Goldberg , 2016 ) using graph convolutional neural networks , but like they attain only small increases over a model with no syntactic parse , and even perform worse than a syntax - free model on out - of - domain data .",19,0,113
dataset/preprocessed/training-data/semantic_role_labeling/1,"These works suggest that though syntax has the potential to improve neural network SRL models , we have not yet designed an architecture which maximizes the benefits of auxiliary syntactic information .",20,0,32
dataset/preprocessed/training-data/semantic_role_labeling/1,"In response , we propose linguistically - informed self - attention ( LISA ) : a model that combines multi-task learning with stacked layers of multi-head self - attention ; the model is trained to : ( 1 ) jointly predict parts of speech and predicates ; ( 2 ) perform parsing ; and ( 3 ) attend to syntactic parse parents , while ( 4 ) assigning semantic role labels .",21,0,72
dataset/preprocessed/training-data/semantic_role_labeling/1,"Whereas prior work typically requires separate models to provide linguistic analysis , including most syntaxfree neural models which still rely on external predicate detection , our model is truly end - to - end : earlier layers are trained to predict prerequisite parts - of - speech and predicates , the latter of which are supplied to later layers for scoring .",22,0,62
dataset/preprocessed/training-data/semantic_role_labeling/1,"Though prior work re-encodes each sentence to predict each desired task and again with respect to each predicate to perform SRL , we more efficiently encode each sentence only once , predict its predicates , part - of - speech tags and labeled syntactic parse , then predict the semantic roles for all predicates in the sentence in parallel .",23,0,60
dataset/preprocessed/training-data/semantic_role_labeling/1,"The model is trained such that , as syntactic parsing models improve , providing high - quality parses at test time will improve its performance , allowing the model to leverage updated parsing models without requiring re-training .",24,0,38
dataset/preprocessed/training-data/semantic_role_labeling/1,In experiments on the CoNLL - 2005 and CoNLL - 2012 datasets we show that our linguistically - informed models out - perform the syntax - free state - of - the - art .,25,0,35
dataset/preprocessed/training-data/semantic_role_labeling/1,"On CoNLL - 2005 with predicted predicates and standard word embeddings , our single model out - performs the previous state - of - the - art model on the WSJ test set by 2.5 F1 points absolute .",26,0,39
dataset/preprocessed/training-data/semantic_role_labeling/1,"On the challenging out - of - domain Brown test set , our model improves substantially over the previous state - of - the - art by more than 3.5 F1 , a nearly 10 % reduction in error .",27,0,40
dataset/preprocessed/training-data/semantic_role_labeling/1,"On CoNLL - 2012 , our model gains more than 2.5 F1 absolute over the previous state - of - the - art .",28,0,24
dataset/preprocessed/training-data/semantic_role_labeling/1,"Our models also show improvements when using contextually - encoded word representations , obtaining nearly 1.0 F1 higher than the state - of - the - art on CoNLL - 2005 news and more than 2.0 F1 improvement on out - of - domain text .",29,0,46
dataset/preprocessed/training-data/semantic_role_labeling/1,1 Our implementation in Tensor Flow is available at : http://github.com/strubell/ LISA ) .,31,0,14
dataset/preprocessed/training-data/semantic_role_labeling/1,Layer r is input for a joint predicate / POS classifier .,32,0,12
dataset/preprocessed/training-data/semantic_role_labeling/1,Representations from layer r corresponding to predicted predicates are passed to a bilinear operation scoring distinct predicate and role representations to produce per-token SRL predictions with respect to each predicted predicate .,33,0,32
dataset/preprocessed/training-data/semantic_role_labeling/1,Our goal is to design an efficient neural network model which makes use of linguistic information as effectively as possible in order to perform endto - end SRL .,35,0,29
dataset/preprocessed/training-data/semantic_role_labeling/1,LISA achieves this by combining :,36,0,6
dataset/preprocessed/training-data/semantic_role_labeling/1,( 1 ) A new technique of supervising neural attention to predict syntactic dependencies with ( 2 ) multi-task learning across four related tasks .,37,0,25
dataset/preprocessed/training-data/semantic_role_labeling/1,depicts the over all architecture of our model .,38,0,9
dataset/preprocessed/training-data/semantic_role_labeling/1,The basis for our model is the Transformer encoder introduced by : we transform word embeddings into contextually - encoded token representations using stacked multi-head self - attention and feedforward layers ( 2.1 ) .,39,0,35
dataset/preprocessed/training-data/semantic_role_labeling/1,"To incorporate syntax , one self - attention head is trained to attend to each token 's syntactic parent , allowing the model to use this attention head as an oracle for syntactic dependencies .",40,0,35
dataset/preprocessed/training-data/semantic_role_labeling/1,We introduce this syntactically - informed self - attention ( ) in more detail in 2.2 .,41,0,17
dataset/preprocessed/training-data/semantic_role_labeling/1,Our model is designed for the more realistic setting in which gold predicates are not provided at test - time .,42,0,21
dataset/preprocessed/training-data/semantic_role_labeling/1,Our model predicts predicates and integrates part - of - speech ( POS ) information into earlier layers by re-purposing representations closer to the input to predict predicate and POS tags us - MatMul :,43,0,35
dataset/preprocessed/training-data/semantic_role_labeling/1,2 : Syntactically - informed self - attention for the query word sloth .,45,0,14
dataset/preprocessed/training-data/semantic_role_labeling/1,"A parse heavily weight the token 's syntactic governor , saw , in a weighted average over the token values V parse .",47,0,23
dataset/preprocessed/training-data/semantic_role_labeling/1,"The other attention heads act as usual , and the attended representations from all heads are concatenated and projected through a feed - forward layer to produce the syntacticallyinformed representation for sloth .",48,0,33
dataset/preprocessed/training-data/semantic_role_labeling/1,ing hard parameter sharing ( 2.3 ) .,49,0,8
dataset/preprocessed/training-data/semantic_role_labeling/0,Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,2,1,10
dataset/preprocessed/training-data/semantic_role_labeling/0,"Recent BIO - tagging - based neural semantic role labeling models are very high performing , but assume gold predicates as part of the input and can not incorporate span - level features .",4,0,34
dataset/preprocessed/training-data/semantic_role_labeling/0,"We propose an endto - end approach for jointly predicting all predicates , arguments spans , and the relations between them .",5,0,22
dataset/preprocessed/training-data/semantic_role_labeling/0,"The model makes independent decisions about what relationship , if any , holds between every possible word - span pair , and learns contextualized span representations that provide rich , shared input features for each decision .",6,0,37
dataset/preprocessed/training-data/semantic_role_labeling/0,Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates .,7,0,19
dataset/preprocessed/training-data/semantic_role_labeling/0,"Semantic role labeling ( SRL ) captures predicateargument relations , such as "" who did what to whom . """,10,1,20
dataset/preprocessed/training-data/semantic_role_labeling/0,"Recent high - performing SRL models are BIO - taggers , labeling argument spans for a single predicate at a time ( as shown in .",11,1,26
dataset/preprocessed/training-data/semantic_role_labeling/0,"They are typically only evaluated with gold predicates , and must be pipelined with error - prone predicate identification models for deployment .",12,0,23
dataset/preprocessed/training-data/semantic_role_labeling/0,We propose an end - to - end approach for predicting all the predicates and their argument spans in one forward pass .,13,0,23
dataset/preprocessed/training-data/semantic_role_labeling/0,"Our model builds on a recent coreference resolution model , by making central use of learned , contextualized span representations .",14,0,21
dataset/preprocessed/training-data/semantic_role_labeling/0,We use these representations to predict SRL graphs directly over text spans .,15,0,13
dataset/preprocessed/training-data/semantic_role_labeling/0,"Each edge is identified by independently predicting which role , if any , holds between every possible pair of text spans , while using aggressive beam 1 Code and models : https://github.com/luheng/lsgn pruning for efficiency .",16,0,36
dataset/preprocessed/training-data/semantic_role_labeling/0,The final graph is simply the union of predicted SRL roles ( edges ) and their associated text spans ( nodes ) .,17,0,23
dataset/preprocessed/training-data/semantic_role_labeling/0,Our span - graph formulation overcomes a key limitation of semi-markov and BIO - based models : it can model overlapping spans across different predicates in the same output structure ( see ) .,18,0,34
dataset/preprocessed/training-data/semantic_role_labeling/0,"The span representations also generalize the token - level representations in BIObased models , letting the model dynamically decide which spans and roles to include , without using previously standard syntactic features .",19,0,33
dataset/preprocessed/training-data/semantic_role_labeling/0,"To the best of our knowledge , this is the first span - based SRL model that does not assume that predicates are given .",20,0,25
dataset/preprocessed/training-data/semantic_role_labeling/0,"In this more realistic setting , where the predicate must be predicted , our model achieves state - of - the - art performance on PropBank .",21,0,27
dataset/preprocessed/training-data/semantic_role_labeling/0,"It also reinforces the strong performance of similar span embedding methods for coreference , suggesting that this style of models could be used for other span - span relation tasks , such as syntactic parsing , relation extraction , and QA - SRL .",22,0,44
dataset/preprocessed/training-data/semantic_role_labeling/0,"We consider the space of possible predicates to be all the tokens in the input sentence , and the space of arguments to be all continuous spans .",24,0,28
dataset/preprocessed/training-data/semantic_role_labeling/0,Our model decides what relation exists between each predicate - argument pair ( including no relation ) .,25,0,18
dataset/preprocessed/training-data/semantic_role_labeling/0,"Formally , given a sequence X = w 1 , . . . , w n , we wish to predict a set of labeled predicateargument relations Y ? P A L , where P = {w 1 , . . . , w n } is the set of all tokens ( predicates ) , A = { ( w i , . . . , w j ) | 1 ? i ? j ? n}",26,0,78
dataset/preprocessed/training-data/semantic_role_labeling/0,"contains all the spans ( arguments ) , and L is the space of semantic role labels , including a null label indicating no relation .",27,0,26
dataset/preprocessed/training-data/semantic_role_labeling/0,"The final SRL output would be all the non-empty relations { ( p , a , l ) ? Y | l = }.",28,0,24
dataset/preprocessed/training-data/semantic_role_labeling/0,"We then define a set of random variables , where each random variable y p , a corresponds to a predicate p ?",29,0,23
dataset/preprocessed/training-data/semantic_role_labeling/0,P and an argument a ?,30,0,6
dataset/preprocessed/training-data/semantic_role_labeling/0,"A , taking value from the discrete label space L.",31,0,10
dataset/preprocessed/training-data/semantic_role_labeling/0,"The random variables y p , a are conditionally independent of each other given the input X :",32,0,18
dataset/preprocessed/training-data/semantic_role_labeling/0,"Where ? ( p , a , l ) is a scoring function for a possible ( predicate , argument , label ) combination . ?",33,0,26
dataset/preprocessed/training-data/semantic_role_labeling/0,"is decomposed into two unary scores on the predicate and the argument ( defined in Section 3 ) , as well as a label - specific score for the relation :",34,0,31
dataset/preprocessed/training-data/semantic_role_labeling/0,"The score for the null label is set to a constant : ? ( p , a , ) = 0 , similar to logistic regression .",35,0,27
dataset/preprocessed/training-data/semantic_role_labeling/0,"Learning For each input X , we minimize the negative log likelihood of the gold structure Y * :",36,0,19
dataset/preprocessed/training-data/semantic_role_labeling/0,"As our model deals with O ( n 2 ) possible argument spans and O ( n ) possible predicates , it needs to consider O ( n 3 | L | ) possible relations , which is computationally impractical .",38,0,41
dataset/preprocessed/training-data/semantic_role_labeling/0,"To overcome this issue , we define two beams Ba and B p for storing the candidate arguments and predicates , respectively .",39,0,23
dataset/preprocessed/training-data/semantic_role_labeling/0,The candidates in each beam are ranked by their unary score (? a or ? p ) .,40,0,18
dataset/preprocessed/training-data/semantic_role_labeling/0,The sizes of the beams are limited by ? an and ? p n.,41,0,14
dataset/preprocessed/training-data/semantic_role_labeling/0,Elements that fallout of the beam do not participate in computing the edge factors ?,42,0,15
dataset/preprocessed/training-data/semantic_role_labeling/0,"( l ) rel , reducing the over all number of relational factors evaluated by the model to O ( n 2 | L| ) .",43,0,26
dataset/preprocessed/training-data/semantic_role_labeling/0,"We also limit the maximum width of spans to a fixed number W ( e.g. W = 30 ) , further reducing the number of computed unary factors to O ( n ) .",44,0,34
dataset/preprocessed/training-data/semantic_role_labeling/0,"Our model builds contextualized representations for argument spans a and predicate words p based on BiLSTM outputs ( ) and uses feedforward networks to compute the factor scores in ? ( p , a , l ) described in Section 2 ( ) .",46,0,44
dataset/preprocessed/training-data/semantic_role_labeling/0,Word - level contexts,47,0,4
dataset/preprocessed/training-data/semantic_role_labeling/0,"The bottom layer consists of pre-trained word embeddings concatenated with character - based representations , i.e. for each token w i , we have x i = [ WORDEMB ( w i ) ; CHARCNN ( w i ) ] .",48,0,41
dataset/preprocessed/training-data/semantic_role_labeling/0,"We then contextualize each x i using an m-layered bidirectional LSTM with highway connections , which we denote asx i .",49,0,21
dataset/preprocessed/training-data/semantic_role_labeling/2,Deep Semantic Role Labeling : What Works and What 's Next,2,1,11
dataset/preprocessed/training-data/semantic_role_labeling/2,"We introduce a new deep learning model for semantic role labeling ( SRL ) that significantly improves the state of the art , along with detailed analyses to reveal its strengths and limitations .",4,1,34
dataset/preprocessed/training-data/semantic_role_labeling/2,"We use a deep highway BiLSTM architecture with constrained decoding , while observing a number of recent best practices for initialization and regularization .",5,0,24
dataset/preprocessed/training-data/semantic_role_labeling/2,"Our 8 - layer ensemble model achieves 83.2 F1 on the CoNLL 2005 test set and 83.4 F1 on CoNLL 2012 , roughly a 10 % relative error reduction over the previous state of the art .",6,0,37
dataset/preprocessed/training-data/semantic_role_labeling/2,"Extensive empirical analysis of these gains show that ( 1 ) deep models excel at recovering long - distance dependencies but can still make surprisingly obvious errors , and ( 2 ) that there is still room for syntactic parsers to improve these results .",7,0,45
dataset/preprocessed/training-data/semantic_role_labeling/2,"Semantic role labeling ( SRL ) systems aim to recover the predicate - argument structure of a sentence , to determine essentially "" who did what to whom "" , "" when "" , and "" where . """,9,0,39
dataset/preprocessed/training-data/semantic_role_labeling/2,Recently breakthroughs involving end - to - end deep models for SRL without syntactic input seem to overturn the long - held belief that syntactic parsing is a prerequisite for this task .,10,1,33
dataset/preprocessed/training-data/semantic_role_labeling/2,"In this paper , we show that this result can be pushed further using deep highway bidirectional LSTMs with constrained decoding , again significantly moving the state of the art ( another 2 points on CoNLL 2005 ) .",11,0,39
dataset/preprocessed/training-data/semantic_role_labeling/2,We also present a careful empirical analysis to determine what works well and what might be done to progress even further .,12,0,22
dataset/preprocessed/training-data/semantic_role_labeling/2,Our model combines a number of best practices in the recent deep learning literature .,13,0,15
dataset/preprocessed/training-data/semantic_role_labeling/2,"Fol - lowing , we treat SRL as a BIO tagging problem and use deep bidirectional LSTMs .",14,0,18
dataset/preprocessed/training-data/semantic_role_labeling/2,"However , we differ by ( 1 ) simplifying the input and output layers , ( 2 ) introducing highway connections , ( 3 ) using recurrent dropout , ( 4 ) decoding with BIOconstraints , and ( 5 ) ensembling with a product of experts .",15,0,47
dataset/preprocessed/training-data/semantic_role_labeling/2,Our model gives a 10 % relative error reduction over previous state of the art on the test sets of CoNLL 2005 and 2012 .,16,0,25
dataset/preprocessed/training-data/semantic_role_labeling/2,We also report performance with predicted predicates to encourage future exploration of end - to - end SRL systems .,17,0,20
dataset/preprocessed/training-data/semantic_role_labeling/2,"We present detailed error analyses to better understand the performance gains , including ( 1 ) design choices on architecture , initialization , and regularization that have a surprisingly large impact on model performance ; ( 2 ) different types of prediction errors showing , e.g. , that deep models excel at predicting long - distance dependencies but still struggle with known challenges such as PPattachment errors and adjunct - argument distinctions ; ( 3 ) the role of syntax , showing that there is significant room for improvement given oracle syntax but errors from existing automatic parsers prevent effective use in SRL .",18,0,104
dataset/preprocessed/training-data/semantic_role_labeling/2,"In summary , our main contributions incluede :",19,0,8
dataset/preprocessed/training-data/semantic_role_labeling/2,"A new state - of - the - art deep network for endto - end SRL , supported by publicly available code and models .",20,0,25
dataset/preprocessed/training-data/semantic_role_labeling/2,"An in - depth error analysis indicating where the model works well and where it still struggles , including discussion of structural consistency and long - distance dependencies .",22,0,29
dataset/preprocessed/training-data/semantic_role_labeling/2,"Experiments that point toward directions for future improvements , including a detailed discussion of how and when syntactic parsers could be used to improve these results .",23,0,27
dataset/preprocessed/training-data/semantic_role_labeling/2,"Two major factors contribute to the success of our deep SRL model : ( 1 ) applying recent advances in training deep recurrent neural networks such as highway connections and , 2 and ( 2 ) using an A * decoding algorithm to enforce structural consistency at prediction time without adding more complexity to the training process .",25,0,58
dataset/preprocessed/training-data/semantic_role_labeling/2,"Formally , our task is to predict a sequence y given a sentence - predicate pair ( w , v ) as input .",26,0,24
dataset/preprocessed/training-data/semantic_role_labeling/2,Each y i ?,27,0,4
dataset/preprocessed/training-data/semantic_role_labeling/2,y belongs to a discrete set of BIO tags T .,28,0,11
dataset/preprocessed/training-data/semantic_role_labeling/2,"Words outside argument spans have the tag O , and words at the beginning and inside of argument spans with role r have the tags Br and Ir respectively .",29,0,30
dataset/preprocessed/training-data/semantic_role_labeling/2,Let n = | w| = | y | be the length of the sequence .,30,0,16
dataset/preprocessed/training-data/semantic_role_labeling/2,Predicting an SRL structure under our model involves finding the highest - scoring tag sequence over the space of all possibilities Y:,31,0,22
dataset/preprocessed/training-data/semantic_role_labeling/2,We use a deep bidirectional LSTM ( BiLSTM ) to learn a locally decomposed scoring function conditioned on the input : n t =1 log p ( y t | w ) .,32,0,33
dataset/preprocessed/training-data/semantic_role_labeling/2,"To incorporate additional information ( e.g. , structural consistency , syntactic input ) , we augment the scoring function with penalization terms :",33,0,23
dataset/preprocessed/training-data/semantic_role_labeling/2,Each constraint function c applies a non-negative penalty given the input wand a length -t prefix y 1:t .,34,0,19
dataset/preprocessed/training-data/semantic_role_labeling/2,These constraints can be hard or soft depending on whether the penalties are finite .,35,0,15
dataset/preprocessed/training-data/semantic_role_labeling/2,"Our model computes the distribution over tags using stacked BiLSTMs , which we define as follows :",38,0,17
dataset/preprocessed/training-data/semantic_role_labeling/2,We thank Mingxuan Wang for suggesting highway connections with simplified inputs and outputs .,39,0,14
dataset/preprocessed/training-data/semantic_role_labeling/2,Part of our model is extended from his unpublished implementation .,40,0,11
dataset/preprocessed/training-data/semantic_role_labeling/2,"where x l ,t is the input to the LSTM at layer land timestep t. ? l is either 1 or ? 1 , indicating the directionality of the LSTM at layer l .",41,0,34
dataset/preprocessed/training-data/semantic_role_labeling/2,"To stack the LSTMs in an interleaving pattern , as proposed by , the layerspecific inputs x l,t and directionality ?",42,0,21
dataset/preprocessed/training-data/semantic_role_labeling/2,l are arranged in the following manner :,43,0,8
dataset/preprocessed/training-data/semantic_role_labeling/2,"The input vector x 1 , t is the concatenation of token wt 's word embedding and an embedding of the binary feature ( t = v ) indicating whether wt word is the given predicate .",44,0,37
dataset/preprocessed/training-data/semantic_role_labeling/2,"Finally , the locally normalized distribution over output tags is computed via a softmax layer :",45,0,16
dataset/preprocessed/training-data/semantic_role_labeling/2,"To alleviate the vanishing gradient problem when training deep BiL - STMs , we use gated highway connections .",47,0,19
dataset/preprocessed/training-data/semantic_role_labeling/2,We include transform gates rt to control the weight of linear and non-linear transformations between layers ( See ) .,48,0,20
dataset/preprocessed/training-data/semantic_role_labeling/2,"The output h l,t is changed to :",49,0,8
dataset/preprocessed/training-data/semantic_role_labeling/4,A Span Selection Model for Semantic Role Labeling,2,1,8
dataset/preprocessed/training-data/semantic_role_labeling/4,We present a simple and accurate span - based model for semantic role labeling ( SRL ) .,4,1,18
dataset/preprocessed/training-data/semantic_role_labeling/4,Our model directly takes into account all possible argument spans and scores them for each label .,5,0,17
dataset/preprocessed/training-data/semantic_role_labeling/4,"At decoding time , we greedily select higher scoring labeled spans .",6,0,12
dataset/preprocessed/training-data/semantic_role_labeling/4,"One advantage of our model is to allow us to design and use spanlevel features , thatare difficult to use in tokenbased BIO tagging approaches .",7,0,26
dataset/preprocessed/training-data/semantic_role_labeling/4,"Experimental results demonstrate that our ensemble model achieves the state - of - the - art results , 87.4 F1 and 87.0 F1 on the CoNLL - 2005 and 2012 datasets , respectively .",8,0,34
dataset/preprocessed/training-data/semantic_role_labeling/4,Semantic Role Labeling ( SRL ) is a shallow semantic parsing task whose goal is to recognize the predicate - argument structure of each predicate .,10,0,26
dataset/preprocessed/training-data/semantic_role_labeling/4,"Given a sentence and a target predicate , SRL systems have to predict semantic arguments of the predicate .",11,1,19
dataset/preprocessed/training-data/semantic_role_labeling/4,"Each argument is a span , a unit that consists of one or more words .",12,0,16
dataset/preprocessed/training-data/semantic_role_labeling/4,A key to the argument span prediction is how to represent and model spans .,13,0,15
dataset/preprocessed/training-data/semantic_role_labeling/4,One popular approach to it is based on BIO tagging schemes .,14,0,12
dataset/preprocessed/training-data/semantic_role_labeling/4,State - of - the - art neural SRL models adopt this approach .,15,0,14
dataset/preprocessed/training-data/semantic_role_labeling/4,"Using features induced by neural networks , they predict a BIO tag for each word .",16,0,16
dataset/preprocessed/training-data/semantic_role_labeling/4,"Words at the beginning and inside of argument spans have the "" B "" and "" I "" tags , and words outside argument spans have the tag "" O . """,17,0,32
dataset/preprocessed/training-data/semantic_role_labeling/4,"While yielding high accuracies , this approach reconstructs argument spans from the predicted BIO tags instead of directly predicting the spans .",18,0,22
dataset/preprocessed/training-data/semantic_role_labeling/4,Another approach is based on labeled span prediction .,19,0,9
dataset/preprocessed/training-data/semantic_role_labeling/4,This approach scores each span with its label .,20,0,9
dataset/preprocessed/training-data/semantic_role_labeling/4,"One advantage of this approach is to allow us to design and use span - level features , thatare difficult to use in BIO tagging approaches .",21,0,27
dataset/preprocessed/training-data/semantic_role_labeling/4,"However , the performance has lagged behind that of the state - of - the - art BIO - based neural models .",22,0,23
dataset/preprocessed/training-data/semantic_role_labeling/4,"To fill this gap , this paper presents a simple and accurate span - based model .",23,0,17
dataset/preprocessed/training-data/semantic_role_labeling/4,"Inspired by recent span - based models in syntactic parsing and coreference resolution , our model directly scores all possible labeled spans based on span representations induced from neural networks .",24,0,31
dataset/preprocessed/training-data/semantic_role_labeling/4,"At decoding time , we greedily select higher scoring labeled spans .",25,0,12
dataset/preprocessed/training-data/semantic_role_labeling/4,The model parameters are learned by optimizing loglikelihood of correct labeled spans .,26,0,13
dataset/preprocessed/training-data/semantic_role_labeling/4,We evaluate the performance of our spanbased model on the datasets .,27,0,12
dataset/preprocessed/training-data/semantic_role_labeling/4,Experimental results show that the span - based model outperforms the BiLSTM - CRF model .,28,0,16
dataset/preprocessed/training-data/semantic_role_labeling/4,"In addition , by using contextualized word representations , ELMo , our ensemble model achieves the state - of - the - art results , 87.4 F1 and 87.0 F1 on the CoNLL - 2005 and 2012 datasets , respectively .",29,0,41
dataset/preprocessed/training-data/semantic_role_labeling/4,Empirical analysis on these results shows that the label prediction ability of our span - based model is better than that of the CRF - based model .,30,0,28
dataset/preprocessed/training-data/semantic_role_labeling/4,Another finding is that ELMo improves the model performance for span boundary identification .,31,0,14
dataset/preprocessed/training-data/semantic_role_labeling/4,"In summary , our main contributions include :",32,0,8
dataset/preprocessed/training-data/semantic_role_labeling/4,A simple span - based model that achieves the state - of - the - art results .,33,0,18
dataset/preprocessed/training-data/semantic_role_labeling/4,"We treat SRL as span selection , in which we select appropriate spans from a set of possible spans for each label .",34,0,23
dataset/preprocessed/training-data/semantic_role_labeling/4,This section formalizes the problem and provides our span selection model .,35,0,12
dataset/preprocessed/training-data/semantic_role_labeling/4,Span Selection Problem,36,0,3
dataset/preprocessed/training-data/semantic_role_labeling/4,"Given a sentence that consists of T words w 1:T = w 1 , , w T and the target predicate position index p , the goal is to predict a set of labeled spans",38,0,35
dataset/preprocessed/training-data/semantic_role_labeling/4,"Input : X = {w 1:T , p},",39,0,8
dataset/preprocessed/training-data/semantic_role_labeling/4,"Each labeled span i , j , r consists of word indices i and j in the sentence ( 1 ? i ? j ? T ) and a semantic role label r ?",40,0,34
dataset/preprocessed/training-data/semantic_role_labeling/4,"One simple method to predict Y is to select the highest scoring span ( i , j ) from all possible spans S for each label r ,",42,0,28
dataset/preprocessed/training-data/semantic_role_labeling/4,"( 1 ) Function SCORE r ( i , j ) returns are al value for each span ( i , j ) ?",43,0,24
dataset/preprocessed/training-data/semantic_role_labeling/4,S ( described in Section 2.2 in more detail ) .,44,0,11
dataset/preprocessed/training-data/semantic_role_labeling/4,The number of possible spans S in the input sentence w 1:T is T ( T +1 ),45,0,18
dataset/preprocessed/training-data/semantic_role_labeling/4,", and S is defined as follows ,",47,0,8
dataset/preprocessed/training-data/semantic_role_labeling/4,Note that some semantic roles may not appear in the sentence .,48,0,12
dataset/preprocessed/training-data/semantic_role_labeling/4,"To deal with the absence of some labels , we define the predicate position span ( p , p ) as a NULL span and train a model to select the NULL span when there is no span for the label .",49,0,42
dataset/preprocessed/training-data/paraphrase_generation/1,A Deep Generative Framework for Paraphrase Generation,2,1,7
dataset/preprocessed/training-data/paraphrase_generation/1,"Paraphrase generation is an important problem in NLP , especially in question answering , information retrieval , information extraction , conversation systems , to name a few .",4,0,28
dataset/preprocessed/training-data/paraphrase_generation/1,"In this paper , we address the problem of generating paraphrases automatically .",5,1,13
dataset/preprocessed/training-data/paraphrase_generation/1,"Our proposed method is based on a combination of deep generative models ( VAE ) with sequence - to - sequence models ( LSTM ) to generate paraphrases , given an input sentence .",6,0,34
dataset/preprocessed/training-data/paraphrase_generation/1,Traditional VAEs when combined with recurrent neural networks can generate free text but they are not suitable for paraphrase generation for a given sentence .,7,0,25
dataset/preprocessed/training-data/paraphrase_generation/1,"We address this problem by conditioning the both , encoder and decoder sides of VAE , on the original sentence , so that it can generate the given sentence 's paraphrases .",8,0,32
dataset/preprocessed/training-data/paraphrase_generation/1,"Unlike most existing models , our model is simple , modular and can generate multiple paraphrases , for a given sentence .",9,0,22
dataset/preprocessed/training-data/paraphrase_generation/1,"Quantitative evaluation of the proposed method on a benchmark paraphrase dataset demonstrates its efficacy , and its performance improvement over the state - of - the - art methods by a significant margin , whereas qualitative human evaluation indicate that the generated paraphrases are well - formed , grammatically correct , and are relevant to the input sentence .",10,0,59
dataset/preprocessed/training-data/paraphrase_generation/1,"Furthermore , we evaluate our method on a newly released question paraphrase dataset , and establish a new baseline for future research .",11,0,23
dataset/preprocessed/training-data/paraphrase_generation/1,"Paraphrase generation is an important problem in many NLP applications such as question answering , information retrieval , information extraction , and summarization .",13,0,24
dataset/preprocessed/training-data/paraphrase_generation/1,"QA systems are often susceptible to the way questions are asked ; in fact , for knowledge - based ( KB ) QA systems , question paraphrasing is crucial for bridging the gap between questions asked by users and knowledge based assertions .",14,0,43
dataset/preprocessed/training-data/paraphrase_generation/1,"In an open QA system pipeline , question analysis and paraphrasing is a critical first step , in which a given question is reformulated by expanding it with its various paraphrases with the intention of improvement in recall , an important metric in the early stage of the pipeline .",15,0,50
dataset/preprocessed/training-data/paraphrase_generation/1,"Similarly paraphrasing finds applications in information retrieval by generating query variants , and in machine translation or summarization by generating variants for automatic evaluation .",16,0,25
dataset/preprocessed/training-data/paraphrase_generation/1,"Copyright c 2018 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",17,0,15
dataset/preprocessed/training-data/paraphrase_generation/1,All rights reserved .,18,0,4
dataset/preprocessed/training-data/paraphrase_generation/1,"In addition to being directly useful in QA systems , paraphrase generation is also important for generating training data for various learning tasks , such as question type classification , paraphrase detection , etc. , thatare useful in other applications .",19,0,41
dataset/preprocessed/training-data/paraphrase_generation/1,"Question type classification has application in conversation systems , while paraphrase detection is an important problem for translation , summarization , social QA ( finding closest question to FAQs / already asked question ) .",20,0,35
dataset/preprocessed/training-data/paraphrase_generation/1,"Due to the nature and complexity of the task , all of these problems suffer from lack of training data , a problem that can readily benefit from the paraphrase generation task .",21,0,33
dataset/preprocessed/training-data/paraphrase_generation/1,"Despite the importance of the paraphrase generation problem , there has been relatively little prior work in the literature , though much larger amount of work exists on paraphrase detection problem .",22,0,32
dataset/preprocessed/training-data/paraphrase_generation/1,"Traditionally , paraphrase generation has been addressed using rule - based approaches ) , primarily due to the inherent difficulty of the underlying natural language generation problem .",23,0,28
dataset/preprocessed/training-data/paraphrase_generation/1,"However , recent advances in deep learning , in particular generative models , have led to powerful , data - driven approaches to text generation .",24,0,26
dataset/preprocessed/training-data/paraphrase_generation/1,"In this paper , we present a deep generative framework for automatically generating paraphrases , given a sentence .",25,0,19
dataset/preprocessed/training-data/paraphrase_generation/1,"Our framework combines the power of sequenceto - sequence models , specifically the long short - term memory ( LSTM ) , and deep generative models , specifically the variational autoencoder ( VAE ) , to develop a novel , end - to - end deep learning architecture for the task of paraphrase generation .",26,0,55
dataset/preprocessed/training-data/paraphrase_generation/1,"In contrast to the recent usage of VAE for sentence generation , a key differentiating aspect of our proposed VAE based architecture is that it needs to generate paraphrases , given an original sentence as input .",27,0,37
dataset/preprocessed/training-data/paraphrase_generation/1,"That is , the generated paraphrased version of the sentence should capture the essence of the original sentence .",28,0,19
dataset/preprocessed/training-data/paraphrase_generation/1,"Therefore , unconditional sentence generation models , such as , are not suited for this task .",29,0,17
dataset/preprocessed/training-data/paraphrase_generation/1,"To address this limitation , we present a mechanism to condition our VAE model on the original sentence for which we wish to generate the paraphrases .",30,0,27
dataset/preprocessed/training-data/paraphrase_generation/1,"In the past , conditional generative models ) have been applied in computer vision to generate images conditioned on the given class label .",31,0,24
dataset/preprocessed/training-data/paraphrase_generation/1,"Unlike these methods where number of classes are finite , and do not require any intermediate representation , our method conditions both the sides ( i.e. encoder and decoder ) of VAE on the intermediate representation of the input question obtained through LSTM .",32,0,44
dataset/preprocessed/training-data/paraphrase_generation/1,"One potential approach to solve the paraphrase generation problem could be to use existing sequence - to - sequence models , in fact , one variation of sequence - to - sequence model using stacked residual LSTM ) is the current state of the art for this task .",33,0,49
dataset/preprocessed/training-data/paraphrase_generation/1,"However , most of the existing models for this task including stacked residual LSTM , despite having sophisticated model architectures , lack a principled generative framework .",34,0,27
dataset/preprocessed/training-data/paraphrase_generation/1,"In contrast , our deep generative model enjoys a simple , modular architecture , and can generate not just a single but multiple , semantically sensible , paraphrases for any given sentence .",35,0,33
dataset/preprocessed/training-data/paraphrase_generation/1,"It is worth noting that existing models such as sequenceto - sequence models , when applied using beam search , are notable to produce multiple paraphrases in a principled way .",36,0,31
dataset/preprocessed/training-data/paraphrase_generation/1,"Although one can choose top k variations from the ranked results returned by beam - search , k th variation will be qualitatively worse ( by the nature of beam - search ) than the first variation .",37,0,38
dataset/preprocessed/training-data/paraphrase_generation/1,"This is in contrast to the proposed method where all variations will be of relatively better quality since they are the top beam - search result , generated based on different z sampled from a latent space .",38,0,38
dataset/preprocessed/training-data/paraphrase_generation/1,"We compare our framework with various sophisticated sequence - to - sequence models including the state - of - the - art stacked residual model for paraphrase generation , and show its efficacy on benchmark datasets , on which it outperforms the stateof - the - art by significant margins .",39,0,51
dataset/preprocessed/training-data/paraphrase_generation/1,"Due to the importance of the paraphrase generation task in QA system , we perform a comprehensive evaluation of our proposed model on the recently released Quora questions dataset 1 , and demonstrates its effectiveness for the task of question paraphrase generation through both quantitative metrics , as well as qualitative analysis .",40,0,53
dataset/preprocessed/training-data/paraphrase_generation/1,"Human evaluation indicate that the paraphrases generated by our system are well - formed , and grammatically correct for the most part , and are able to capture new concepts related to the input sentence .",41,0,36
dataset/preprocessed/training-data/paraphrase_generation/1,Our framework uses a variational autoencoder ( VAE ) as a generative model for paraphrase generation .,43,0,17
dataset/preprocessed/training-data/paraphrase_generation/1,"In contrast to the standard VAE , however , we additionally condition the encoder and decoder modules of the VAE on the original sentence .",44,0,25
dataset/preprocessed/training-data/paraphrase_generation/1,This enables us to generate paraphrase ( s ) specific to an input sentence at test time .,45,0,18
dataset/preprocessed/training-data/paraphrase_generation/1,"In this section , we first provide a brief overview of VAE , and then describe our framework .",46,0,19
dataset/preprocessed/training-data/paraphrase_generation/1,Variational Autoencoder ( VAE ),47,0,5
dataset/preprocessed/training-data/paraphrase_generation/1,"The VAE ( Kingma and Welling 2014 ; Rezende , Mohamed , and Wierstra 2014 ) is a deep generative latent variable model 1 https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs :",48,0,26
dataset/preprocessed/training-data/paraphrase_generation/1,"A macro-view of our model : the paraphrase generation model is also conditioned on the original sentence that allows learning rich , nonlinear representations for highdimensional inputs .",49,0,28
dataset/preprocessed/training-data/paraphrase_generation/0,Learning Semantic Sentence Embeddings using Pair- wise Discriminator,2,1,8
dataset/preprocessed/training-data/paraphrase_generation/0,"In this paper , we propose a method for obtaining sentence - level embeddings .",4,1,15
dataset/preprocessed/training-data/paraphrase_generation/0,"While the problem of securing word - level embeddings is very well studied , we propose a novel method for obtaining sentence - level embeddings .",5,0,26
dataset/preprocessed/training-data/paraphrase_generation/0,This is obtained by a simple method in the context of solving the paraphrase generation task .,6,0,17
dataset/preprocessed/training-data/paraphrase_generation/0,"If we use a sequential encoder - decoder model for generating paraphrase , we would like the generated paraphrase to be semantically close to the original sentence .",7,0,28
dataset/preprocessed/training-data/paraphrase_generation/0,One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far .,8,0,26
dataset/preprocessed/training-data/paraphrase_generation/0,This is ensured by using a sequential pair - wise discriminator that shares weights with the encoder that is trained with a suitable loss function .,9,0,26
dataset/preprocessed/training-data/paraphrase_generation/0,Our loss function penalizes paraphrase sentence embedding distances from being too large .,10,0,13
dataset/preprocessed/training-data/paraphrase_generation/0,This loss is used in combination with a sequential encoder - decoder network .,11,0,14
dataset/preprocessed/training-data/paraphrase_generation/0,We also validated our method by evaluating the obtained embeddings for a sentiment analysis task .,12,0,16
dataset/preprocessed/training-data/paraphrase_generation/0,The proposed method results in semantic embeddings and outperforms the state - of - the - art on the paraphrase generation and sentiment analysis task on standard datasets .,13,0,29
dataset/preprocessed/training-data/paraphrase_generation/0,These results are also shown to be statistically significant .,14,0,10
dataset/preprocessed/training-data/paraphrase_generation/0,The problem of obtaining a semantic embedding for a sentence that ensures that the related sentences are closer and unrelated sentences are farther lies at the core of understanding languages .,16,0,31
dataset/preprocessed/training-data/paraphrase_generation/0,This would be relevant for a wide variety of machine reading comprehension and related tasks such as sentiment analysis .,17,0,20
dataset/preprocessed/training-data/paraphrase_generation/0,"Towards this problem , we propose a supervised method that uses a sequential encoder - decoder framework for paraphrase generation .",18,0,21
dataset/preprocessed/training-data/paraphrase_generation/0,The task of generating paraphrases is closely related to the task of obtaining semantic sentence embeddings .,19,0,17
dataset/preprocessed/training-data/paraphrase_generation/0,"In our approach , we aim to ensure that the generated paraphrase embedding should be close to the true corresponding sentence and far from unrelated sentences .",20,0,27
dataset/preprocessed/training-data/paraphrase_generation/0,The embeddings so obtained help us to obtain state - of - the - art results for paraphrase generation task .,21,0,21
dataset/preprocessed/training-data/paraphrase_generation/0,Our model consists of a sequential encoder - decoder that is further trained using a pairwise discriminator .,22,0,18
dataset/preprocessed/training-data/paraphrase_generation/0,The encoder - decoder architecture has been widely used for machine translation and machine comprehension tasks .,23,0,17
dataset/preprocessed/training-data/paraphrase_generation/0,"In general , the model ensures a ' local ' loss that is incurred for each recurrent unit cell .",24,0,20
dataset/preprocessed/training-data/paraphrase_generation/0,It only ensures that a particular word token is present at an appropriate place .,25,0,15
dataset/preprocessed/training-data/paraphrase_generation/0,"This , however , does not imply that the whole sentence is correctly generated .",26,0,15
dataset/preprocessed/training-data/paraphrase_generation/0,"To ensure that the whole sentence is correctly encoded , we make further use of a pair - wise discriminator that encodes the whole sentence and obtains an embedding for it .",27,0,32
dataset/preprocessed/training-data/paraphrase_generation/0,We further ensure that this is close to the desired ground - truth embeddings while being far from other ( sentences in the corpus ) embeddings .,28,0,27
dataset/preprocessed/training-data/paraphrase_generation/0,This model thus provides a ' global ' loss that ensures the sentence embedding as a whole is close to other semantically related sentence embeddings .,29,0,26
dataset/preprocessed/training-data/paraphrase_generation/0,This is illustrated in .,30,0,5
dataset/preprocessed/training-data/paraphrase_generation/0,We further evaluate the validity of the sentence embeddings by using them for the task of sentiment analysis .,31,0,19
dataset/preprocessed/training-data/paraphrase_generation/0,We observe that the proposed sentence embeddings result in state - of - the - art performance for both these tasks .,32,0,22
dataset/preprocessed/training-data/paraphrase_generation/0,Our contributions are : a),33,0,5
dataset/preprocessed/training-data/paraphrase_generation/0,We propose a model for obtaining sentence embeddings for solving the paraphrase generation task using a pair - wise discriminator loss added to an encoder - decoder network .,34,0,29
dataset/preprocessed/training-data/paraphrase_generation/0,We show that these embeddings can also be used for the sentiment analysis task .,36,0,15
dataset/preprocessed/training-data/paraphrase_generation/0,We validate the model using standard datasets with a detailed comparison with state - of - the - art methods and also ensure that the results are statistically significant . :,38,0,31
dataset/preprocessed/training-data/paraphrase_generation/0,Pairwise Discriminator based Encoder - Decoder for Paraphrase Generation :,39,0,10
dataset/preprocessed/training-data/paraphrase_generation/0,"This is the basic outline of our model which consists of an LSTM encoder , decoder and discriminator .",40,0,19
dataset/preprocessed/training-data/paraphrase_generation/0,Here the encoders share the weights .,41,0,7
dataset/preprocessed/training-data/paraphrase_generation/0,The discriminator generates discriminative embeddings for the Ground Truth - Generated paraphrase pair with the help of ' global ' loss .,42,0,22
dataset/preprocessed/training-data/paraphrase_generation/0,Our model is jointly trained with the help of a ' local ' and ' global ' loss which we describe in section 3 .,43,0,25
dataset/preprocessed/training-data/paraphrase_generation/0,"Given the flexibility and diversity of natural language , it has always been a challenging task to represent text efficiently .",45,0,21
dataset/preprocessed/training-data/paraphrase_generation/0,There have been several hypotheses proposed for representing the same .,46,0,11
dataset/preprocessed/training-data/paraphrase_generation/0,"proposed a distribution hypothesis to represent words , i.e. , words which occur in the same context have similar meanings .",47,0,21
dataset/preprocessed/training-data/paraphrase_generation/0,"One popular hypothesis is the bag - of - words ( BOW ) or Vector Space Model , in which a text ( such as a sentence or a document ) is represented as the bag ( multiset ) of its words .",48,0,43
dataset/preprocessed/training-data/paraphrase_generation/0,"proposed an extended distributional hypothesis and proposed a latent relation hypothesis , in which a pair of words that co-occur in similar patterns tend to have similar semantic relation .",49,0,30
dataset/preprocessed/training-data/query_wellformedness/0,Identifying Well - formed Natural Language Questions,2,1,7
dataset/preprocessed/training-data/query_wellformedness/0,"Understanding search queries is a hard problem as it involves dealing with "" word salad "" text ubiquitously issued by users .",4,0,22
dataset/preprocessed/training-data/query_wellformedness/0,"However , if a query resembles a well - formed question , a natural language processing pipeline is able to perform more accurate interpretation , thus reducing downstream compounding errors .",5,0,31
dataset/preprocessed/training-data/query_wellformedness/0,"Hence , identifying whether or not a query is well formed can enhance query understanding .",6,0,16
dataset/preprocessed/training-data/query_wellformedness/0,"Here , we introduce a new task of identifying a well - formed natural language question .",7,0,17
dataset/preprocessed/training-data/query_wellformedness/0,"We construct and release a dataset of 25,100 publicly available questions classified into well - formed and non-wellformed categories and report an accuracy of 70.7 % on the test set .",8,0,31
dataset/preprocessed/training-data/query_wellformedness/0,We also show that our classifier can be used to improve the performance of neural sequence - to - sequence models for generating questions for reading comprehension .,9,0,28
dataset/preprocessed/training-data/query_wellformedness/0,"User issued search queries often do not follow formal grammatical structure , and require specialized language processing .",11,0,18
dataset/preprocessed/training-data/query_wellformedness/0,Traditional natural language processing ( NLP ) tools trained on formal text ( e.g. treebanks ) often have difficulty analyzing search queries ; the lack of regularity in the structure of queries makes it difficult to train models that can optimally process the query to extract information that can help understand the user intent behind the query .,12,0,58
dataset/preprocessed/training-data/query_wellformedness/0,One clear direction to improve query processing is to annotate a large number of queries with the desired annotation scheme .,13,0,21
dataset/preprocessed/training-data/query_wellformedness/0,"However , such an annotation can be prohibitively expensive and models trained on such queries might suffer from freshness issues , as the domain and nature of queries evolve frequently .",14,0,31
dataset/preprocessed/training-data/query_wellformedness/0,"Another direction is to obtain a paraphrase of the given query that is a grammatical natural language question , and then analyze that paraphrase to extract the required information .",15,0,30
dataset/preprocessed/training-data/query_wellformedness/0,"There are available tools and datasets , such as Quora question paraphrases and the Paralex dataset ) - for identifying query paraphrases , but these datasets do not contain information about whether a query is a natural language question or not .",16,0,42
dataset/preprocessed/training-data/query_wellformedness/0,Identifying well - formed natural language questions can also facilitate a more natural interaction between a user and a machine in personal assistants or chatbots or while recommending related queries in search - engines .,17,0,35
dataset/preprocessed/training-data/query_wellformedness/0,"Identifying a well - formed question should be easy by parsing with a grammar , such as the English resource grammar , but such grammars are highly precise and fail to parse more than half of web queries .",18,0,39
dataset/preprocessed/training-data/query_wellformedness/0,"Thus , in this paper we present a model to predict whether a given query is a well - formed natural language question .",19,0,24
dataset/preprocessed/training-data/query_wellformedness/0,"We construct and publicly release a dataset of 25,100 queries annotated with the probability of being a well - formed natural language question ( 2.1 ) .",20,0,27
dataset/preprocessed/training-data/query_wellformedness/0,We then train a feed - forward neural network classifier that uses the lexical and syntactic features extracted from the query on this data ( 2.2 ) .,21,0,28
dataset/preprocessed/training-data/query_wellformedness/0,"On a test set of 3,850 queries , we report an accuracy of 70.1 % on the binary classification task .",22,0,21
dataset/preprocessed/training-data/query_wellformedness/0,We also demonstrate that such a query well - formedness classifier can be used to improve the quality of a sequence - to - sequence question generation model by showing an improvement of 0.2 BLEU score in its performance ( 3 ) .,23,0,43
dataset/preprocessed/training-data/query_wellformedness/0,Our dataset ise available for download at http://goo.gl/language/ query-wellformedness .,24,0,10
dataset/preprocessed/training-data/query_wellformedness/0,2 Well - formed Natural Language Question Classifier,25,0,8
dataset/preprocessed/training-data/query_wellformedness/0,"In this section we describe the data annotation , and the models used for question well - formedness classification .",26,0,20
dataset/preprocessed/training-data/query_wellformedness/0,We use the Paralex corpus that contains pairs of noisy paraphrase questions .,28,0,13
dataset/preprocessed/training-data/query_wellformedness/0,"These questions were issued by users in WikiAnswers ( a Question - Answer forum ) and consist of both web - search query like constructs ( "" 5 parts of chloroplast ? "" ) and well - formed questions ( "" What is the punishment for grand theft ? "" ) , and thus is a good resource for constructing the question well - formedness dataset .",29,0,67
dataset/preprocessed/training-data/query_wellformedness/0,"We select 25,100 queries from the unique list of queries extracted from the corpus such that no two queries in the selected set are paraphrases .",30,0,26
dataset/preprocessed/training-data/query_wellformedness/0,The queries are then annotated into well - formed or non-wellformed questions .,31,0,13
dataset/preprocessed/training-data/query_wellformedness/0,We define a query to be a well - formed natural language question if it satisfies the following :,32,0,19
dataset/preprocessed/training-data/query_wellformedness/0,Query is grammatical .,34,0,4
dataset/preprocessed/training-data/query_wellformedness/0,2 . Query is an explicit question .,35,0,8
dataset/preprocessed/training-data/query_wellformedness/0,3 . Query does not contain spelling errors .,36,0,9
dataset/preprocessed/training-data/query_wellformedness/0,shows some examples that were shown to the annotators to illustrate each of the above conditions .,37,0,17
dataset/preprocessed/training-data/query_wellformedness/0,Every query was labeled by five different crowdworkers with a binary label indicating whether a query is well - formed or not .,38,0,23
dataset/preprocessed/training-data/query_wellformedness/0,We average the ratings of the five annotators to get the probability of a query being well - formed .. 1 shows some queries with obtained human annotation .,39,0,29
dataset/preprocessed/training-data/query_wellformedness/0,"Humans are pretty good at identifying an implicit query ( "" Population of owls ... "" ) or a simple well - formed question ( "" What is released ... "" ) , but may miss out on subtle spelling mistakes like "" disscovered "" or dis agree on whether the determiner "" the "" is needed before the word "" genocide "" ( "" What countries have genocide happened in ? "" ) .",40,0,75
dataset/preprocessed/training-data/query_wellformedness/0,Similar to other NLP tasks like entailment ( Dagan Query ( q ) p wf ( q ) population of owls just in north america ?,41,0,26
dataset/preprocessed/training-data/query_wellformedness/0,what is released when anion is formed ?,42,0,8
dataset/preprocessed/training-data/query_wellformedness/0,"1.0 , paraphrasing etc. we rely on the wisdom of the crowd to get such annotations in order to make the data collection scalable and languageindependent .",43,0,27
dataset/preprocessed/training-data/query_wellformedness/0,is the histogram of query wellformedness probability across the dataset .,44,0,11
dataset/preprocessed/training-data/query_wellformedness/0,"Interestingly , the number of queries where at least 4 or more annotators agree 1 on well - formedness is large : | {q | 0.8 ? p wf ( q ) ? 0.2 } | = 19206 queries .",45,0,40
dataset/preprocessed/training-data/query_wellformedness/0,These constitute 76.5 % of all queries in the dataset .,46,0,11
dataset/preprocessed/training-data/query_wellformedness/0,The Fleiss ' kappa for measuring agreement among multiple annotators is computed to be ? =,47,0,16
dataset/preprocessed/training-data/query_wellformedness/0,0.52 which shows moderate agreement .,48,0,6
dataset/preprocessed/training-data/query_wellformedness/0,"We then randomly divided the dataset in approx . 70 % , 15 % , 15 % ratio into training , development and test sets containing 17500 , 3750 , and 3850 queries respectively .",49,0,35
dataset/preprocessed/training-data/sentiment_analysis/28,Attentional Encoder Network for Targeted Sentiment Classification,2,1,7
dataset/preprocessed/training-data/sentiment_analysis/28,Targeted sentiment classification aims at determining the sentimental tendency towards specific targets .,4,0,13
dataset/preprocessed/training-data/sentiment_analysis/28,Most of the previous approaches model context and target words with RNN and attention .,5,0,15
dataset/preprocessed/training-data/sentiment_analysis/28,"However , RNNs are difficult to parallelize and truncated backpropagation through time brings difficulty in remembering long - term patterns .",6,0,21
dataset/preprocessed/training-data/sentiment_analysis/28,"To address this issue , this paper proposes an Attentional Encoder Network ( AEN ) which eschews recurrence and employs attention based encoders for the modeling between context and target .",7,0,31
dataset/preprocessed/training-data/sentiment_analysis/28,We raise the label unreliability issue and introduce label smoothing regularization .,8,0,12
dataset/preprocessed/training-data/sentiment_analysis/28,We also apply pretrained BERT to this task and obtain new stateof - the - art results .,9,0,18
dataset/preprocessed/training-data/sentiment_analysis/28,Experiments and analysis demonstrate the effectiveness and lightweight of our model .,10,0,12
dataset/preprocessed/training-data/sentiment_analysis/28,"Targeted sentiment classification is a fine - grained sentiment analysis task , which aims at determining the sentiment polarities ( e.g. , negative , neutral , or positive ) of a sentence over "" opinion targets "" that explicitly appear in the sentence .",12,1,44
dataset/preprocessed/training-data/sentiment_analysis/28,"For example , given a sentence "" I hated their service , but their food was great "" , the sentiment polarities for the target "" service "" and "" food "" are negative and positive respectively .",13,0,38
dataset/preprocessed/training-data/sentiment_analysis/28,A target is usually an entity or an entity aspect .,14,0,11
dataset/preprocessed/training-data/sentiment_analysis/28,"In recent years , neural network models are designed to automatically learn useful lowdimensional representations from targets and contexts and obtain promising results .",15,0,24
dataset/preprocessed/training-data/sentiment_analysis/28,"However , these neural network models are still in infancy to deal with the fine - grained targeted sentiment classification task .",16,1,22
dataset/preprocessed/training-data/sentiment_analysis/28,"Attention mechanism , which has been successfully used in machine translation , is incorporated to enforce the model to pay more attention to context words with closer semantic relations with the target .",17,0,33
dataset/preprocessed/training-data/sentiment_analysis/28,There are already some studies use attention to generate targetspecific sentence representations or to transform sentence representations according to target words .,18,0,22
dataset/preprocessed/training-data/sentiment_analysis/28,"However , these studies depend on complex recurrent neural networks ( RNNs ) as sequence encoder to compute hidden semantics of texts .",19,0,23
dataset/preprocessed/training-data/sentiment_analysis/28,The first problem with previous works is that the modeling of text relies on RNNs .,20,0,16
dataset/preprocessed/training-data/sentiment_analysis/28,"RNNs , such as LSTM , are very expressive , but they are hard to parallelize and backpropagation through time ( BPTT ) requires large amounts of memory and computation .",21,0,31
dataset/preprocessed/training-data/sentiment_analysis/28,"Moreover , essentially every training algorithm of RNN is the truncated BPTT , which affects the model 's ability to capture dependencies over longer time scales .",22,0,27
dataset/preprocessed/training-data/sentiment_analysis/28,"Although LSTM can alleviate the vanishing gradient problem to a certain extent and thus maintain long distance information , this usually requires a large amount of training data .",23,0,29
dataset/preprocessed/training-data/sentiment_analysis/28,"Another problem that previous studies ignore is the label unreliability issue , since neutral sentiment is a fuzzy sentimental state and brings difficulty for model learning .",24,0,27
dataset/preprocessed/training-data/sentiment_analysis/28,"As far as we know , we are the first to raise the label unreliability issue in the targeted sentiment classification task .",25,0,23
dataset/preprocessed/training-data/sentiment_analysis/28,This paper propose an attention based model to solve the problems above .,26,0,13
dataset/preprocessed/training-data/sentiment_analysis/28,"Specifically , our model eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words .",27,0,26
dataset/preprocessed/training-data/sentiment_analysis/28,"To deal with the label unreliability issue , we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels .",28,0,26
dataset/preprocessed/training-data/sentiment_analysis/28,We also apply pre-trained BERT to this task and show our model enhances the performance of basic BERT model .,29,0,20
dataset/preprocessed/training-data/sentiment_analysis/28,Experimental results on three benchmark datasets show that the proposed model achieves competitive performance and is a lightweight alternative of the best RNN based models .,30,0,26
dataset/preprocessed/training-data/sentiment_analysis/28,The main contributions of this work are presented as follows :,31,0,11
dataset/preprocessed/training-data/sentiment_analysis/28,We design an attentional encoder network to draw the hidden states and semantic interactions between target and context words .,33,0,20
dataset/preprocessed/training-data/sentiment_analysis/28,We raise the label unreliability issue and add an effective label smoothing regularization term to the loss function for encouraging the model to be less confident with the training labels .,35,0,31
dataset/preprocessed/training-data/sentiment_analysis/28,"We apply pre-trained BERT to this task , our model enhances the performance of basic BERT model and obtains new state - of - the - art results .",37,0,29
dataset/preprocessed/training-data/sentiment_analysis/28,We evaluate the model sizes of the compared models and show the lightweight of the proposed model .,39,0,18
dataset/preprocessed/training-data/sentiment_analysis/28,The research approach of the targeted sentiment classification task including traditional machine learning methods and neural networks methods .,41,0,19
dataset/preprocessed/training-data/sentiment_analysis/28,"Traditional machine learning methods , including rule - based methods and statistic - based methods , mainly focus on extracting a set of features like sentiment lexicons features and bag - of - words features to train a sentiment classifier ) .",42,0,42
dataset/preprocessed/training-data/sentiment_analysis/28,"The performance of these methods highly depends on the effectiveness of the feature engineering works , which are labor intensive .",43,0,21
dataset/preprocessed/training-data/sentiment_analysis/28,"In recent years , neural network methods are getting more and more attention as they do not need handcrafted features and can encode sentences with low - dimensional word vectors where rich semantic information stained .",44,0,36
dataset/preprocessed/training-data/sentiment_analysis/28,"In order to incorporate target words into a model , propose TD - LSTM to extend LSTM by using two single - directional LSTM to model the left context and right context of the target word respectively .",45,0,38
dataset/preprocessed/training-data/sentiment_analysis/28,design MemNet which consists of a multi-hop attention mechanism with an external memory to capture the importance of each context word concerning the given target .,46,0,26
dataset/preprocessed/training-data/sentiment_analysis/28,Multiple attention is paid to the memory represented byword embeddings to build higher semantic information .,47,0,16
dataset/preprocessed/training-data/sentiment_analysis/28,propose ATAE - LSTM which concatenates target embeddings with word representations and let targets participate in computing attention weights .,48,0,20
dataset/preprocessed/training-data/sentiment_analysis/28,propose RAM which adopts multiple - attention mechanism on the memory built with bidirectional LSTM and nonlinearly combines the attention results with gated recurrent units ( GRU s ) .,49,0,30
dataset/preprocessed/training-data/sentiment_analysis/7,MULTI - MODAL EMOTION RECOGNITION ON IEMOCAP WITH NEURAL NETWORKS,2,1,10
dataset/preprocessed/training-data/sentiment_analysis/7,Emotion recognition has become an important field of research in human computer interactions and there is a growing need for automatic emotion recognition systems .,4,1,25
dataset/preprocessed/training-data/sentiment_analysis/7,One of the directions the research is heading is the use of neural networks which are adept at estimating complex functions that depend on a large number and diverse source of input data .,5,0,34
dataset/preprocessed/training-data/sentiment_analysis/7,"In this paper we attempt to exploit this effectiveness of neural networks to enable us to perform multimodal emotion recognition on IEMOCAP dataset using data from speech , text , and motions captured from face expressions , rotation and hand movements .",6,0,42
dataset/preprocessed/training-data/sentiment_analysis/7,Our approach first identifies best individual architectures for classification on each modality and performs fusion only at the final layer which allows for a more robust and accurate emotion detection .,7,0,31
dataset/preprocessed/training-data/sentiment_analysis/7,"Emotion is a psycho-physiological process that can be triggered by conscious and / or unconscious perception of objects and situations , associated with multitude of factors such as mood , temperament , personality , disposition , and motivation .",9,0,39
dataset/preprocessed/training-data/sentiment_analysis/7,"Emotions are very important in human decision handling , interaction and cognitive process .",10,0,14
dataset/preprocessed/training-data/sentiment_analysis/7,"With the advancement of technology and as our understanding of emotions is advancing , there is a growing need for automatic emotion recognition systems .",11,0,25
dataset/preprocessed/training-data/sentiment_analysis/7,"Emotion recognition has been studied widely using speech [ 4 ] , text , facial cues , and EEG based brain waves individually .",12,0,24
dataset/preprocessed/training-data/sentiment_analysis/7,"One of the biggest opensourced multimodal resources available in emotion detection is IEMOCAP dataset which consists of approximately 12 hours of audio - visual data , including facial recordings , speech and text transcriptions .",13,0,35
dataset/preprocessed/training-data/sentiment_analysis/7,In this paper we combine these modes to make a stronger and more robust detector for emotions .,14,0,18
dataset/preprocessed/training-data/sentiment_analysis/7,We explore various deep learning based architectures to first get the best individual detection accuracy from each of the different modes .,15,0,22
dataset/preprocessed/training-data/sentiment_analysis/7,We then combine them in an ensemble based architecture to allow for training across the different modalities using the variations of the better individual models .,16,0,26
dataset/preprocessed/training-data/sentiment_analysis/7,"Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .",17,0,47
dataset/preprocessed/training-data/sentiment_analysis/7,This allows us to individually target each modality and only perform feature fusion at the final stage .,18,0,18
dataset/preprocessed/training-data/sentiment_analysis/7,The advantages of our study are two - fold .,19,0,10
dataset/preprocessed/training-data/sentiment_analysis/7,"First , since we target each modality individually , lack of availability of any modality does not cripple our algorithm and would not require retraining of other modalities but only the prefinal layer .",20,0,34
dataset/preprocessed/training-data/sentiment_analysis/7,This also allows our approach to be modular .,21,0,9
dataset/preprocessed/training-data/sentiment_analysis/7,"Second , we use Motion - capture data instead of Video recording , hence we do not use 3D - Convolutions but 2D - Convolutions which are faster have less memory requirements .",22,0,33
dataset/preprocessed/training-data/sentiment_analysis/7,We also use advanced hyperparameter optimization tools to achieve the best possible model configuration depending on our resource constraints .,23,0,20
dataset/preprocessed/training-data/sentiment_analysis/7,Our code is open sourced for other researchers to repeat and enhance our study .,24,0,15
dataset/preprocessed/training-data/sentiment_analysis/7,Most of the early research on IEMOCAP has concentrated specifically on emotion detection using speech data .,26,0,17
dataset/preprocessed/training-data/sentiment_analysis/7,"One of the early important papers on this dataset is where they used segment level feature extraction , to feed those features to a MLP based architecture , where the input is 750 dimensional feature vector , followed by 3 hidden layer of 256 neurons each with rectilinear units as non-linearity .",27,0,52
dataset/preprocessed/training-data/sentiment_analysis/7,follows and they train long short - term memory ( LSTM ) based recurrent neural network .,28,0,17
dataset/preprocessed/training-data/sentiment_analysis/7,"First they divide each utterance into small segments with voiced region , then assume that the label sequences of each segment follows a Markov chain .",29,0,26
dataset/preprocessed/training-data/sentiment_analysis/7,"They extract 32 features for every frame with 12 - dimensional Mel- frequency cepstral coefficients ( MFCC ) with log energy , and their first time derivatives among others .",30,0,30
dataset/preprocessed/training-data/sentiment_analysis/7,The network contains 2 hidden layers with 128 BLSTM cells ( 64 forward nodes and 64 backward nodes ) .,31,0,20
dataset/preprocessed/training-data/sentiment_analysis/7,"Another research we closely follow is , where they use CTC loss function to improve upon RNN based Emotion prediction .",32,0,21
dataset/preprocessed/training-data/sentiment_analysis/7,"They use 34 features including 12 MFCC , chromagram - based and spectrum properties like flux and rolloff .",33,0,19
dataset/preprocessed/training-data/sentiment_analysis/7,For all speech intervals they calculate features in 0.2 second window and moving it with 0.1 second step .,34,0,19
dataset/preprocessed/training-data/sentiment_analysis/7,"The use of CTC loss helps , as often , almost the whole utterance has no emotion , but emotionality is contained only in a few words or phonemes in an utterance which the CTC loss handles well .",35,0,39
dataset/preprocessed/training-data/sentiment_analysis/7,"Unlike which uses only the improv data , Chernykh et .",36,0,11
dataset/preprocessed/training-data/sentiment_analysis/7,al. use all the session data for the emotion classification .,37,0,11
dataset/preprocessed/training-data/sentiment_analysis/7,Multi-modal emotion classification has recently gathered more traction and IEMOCAP remains the significant dataset for this research direction .,38,0,19
dataset/preprocessed/training-data/sentiment_analysis/7,The current state - of - art classification on IEMOCAP is provided by which builds on the prior work .,39,0,20
dataset/preprocessed/training-data/sentiment_analysis/7,"They use 3D - CNN for visual feature extraction , text - CNN for textual features extraction and open SMILE for audio feature extraction .",40,0,25
dataset/preprocessed/training-data/sentiment_analysis/7,They use Contextual LSTM Architecture on top of these unimodal input features .,41,0,13
dataset/preprocessed/training-data/sentiment_analysis/7,They are then merged with multi-modal contextual LSTM layers which performs feature fusion .,42,0,14
dataset/preprocessed/training-data/sentiment_analysis/7,This layer finally feeds to the classification module .,43,0,9
dataset/preprocessed/training-data/sentiment_analysis/7,In our paper we adopt a different approach to this study and achieve similar performance with certain advantages .,44,0,19
dataset/preprocessed/training-data/sentiment_analysis/7,IEMOCAP has 12 hours of audio- visual data from 10 actors where the recordings follow dialogues between a male and a female actor in both scripted or improvised topics .,46,0,30
dataset/preprocessed/training-data/sentiment_analysis/7,After the audio - visual data has been collected it is divided into small utterances of length between 3 to 15 seconds which are then labelled by evaluators .,47,0,29
dataset/preprocessed/training-data/sentiment_analysis/7,Each utterance is evaluated by 3 - 4 assessors .,48,0,10
dataset/preprocessed/training-data/sentiment_analysis/7,"The evaluation form contained 10 options ( neutral , happiness , sadness , anger , surprise , fear , disgust frustration , excited , other ) .",49,0,27
dataset/preprocessed/training-data/sentiment_analysis/49,Contextual Inter-modal Attention for Multi-modal Sentiment Analysis,2,1,7
dataset/preprocessed/training-data/sentiment_analysis/49,"Multi-modal sentiment analysis offers various challenges , one being the effective combination of different input modalities , namely text , visual and acoustic .",4,0,24
dataset/preprocessed/training-data/sentiment_analysis/49,"In this paper , we propose a recurrent neural network based multi-modal attention framework that leverages the contextual information for utterance - level sentiment prediction .",5,0,26
dataset/preprocessed/training-data/sentiment_analysis/49,The proposed approach applies attention on multi-modal multi-utterance representations and tries to learn the contributing features amongst them .,6,0,19
dataset/preprocessed/training-data/sentiment_analysis/49,"We evaluate our proposed approach on two multi-modal sentiment analysis benchmark datasets , viz.",7,0,14
dataset/preprocessed/training-data/sentiment_analysis/49,CMU Multi-modal Opinion - level Sentiment Intensity ( CMU - MOSI ) corpus and the recently released CMU Multi-modal Opinion Sentiment and Emotion Intensity ( CMU - MOSEI ) corpus .,8,0,31
dataset/preprocessed/training-data/sentiment_analysis/49,"Evaluation results show the effectiveness of our proposed approach with the accuracies of 82.31 % and 79. 80 % for the MOSI and MO - SEI datasets , respectively .",9,0,30
dataset/preprocessed/training-data/sentiment_analysis/49,These are approximately 2 and 1 points performance improvement over the state - of - the - art models for the datasets .,10,0,23
dataset/preprocessed/training-data/sentiment_analysis/49,"Traditionally , sentiment analysis has been applied to a wide variety of texts .",12,1,14
dataset/preprocessed/training-data/sentiment_analysis/49,"In contrast , multi-modal sentiment analysis has recently gained attention due to the tremendous growth of many social media platforms such as YouTube , Instagram , Twitter , Facebook etc .",13,0,31
dataset/preprocessed/training-data/sentiment_analysis/49,"It depends on the information that can be obtained from more than one modality ( e.g. text , visual and acoustic ) for the analysis .",14,0,26
dataset/preprocessed/training-data/sentiment_analysis/49,The motivation is to leverage the varieties of ( often distinct ) information from multiple sources for building an efficient system .,15,0,22
dataset/preprocessed/training-data/sentiment_analysis/49,"For ex-ample , it is a non-trivial task to detect the sentiment of a sarcastic sentence "" My neighbours are home ! !",16,0,23
dataset/preprocessed/training-data/sentiment_analysis/49,"it is good to wake up at 3 am in the morning . "" as negative considering only the textual information .",17,0,22
dataset/preprocessed/training-data/sentiment_analysis/49,"However , if the system has access to some other sources of information , e.g. visual , it can easily detect the unpleasant gestures of the speaker and would classify it with the negative sentiment polarity .",18,0,37
dataset/preprocessed/training-data/sentiment_analysis/49,"Similarly , for some instances acoustic features such as intensity , pitch , pause etc. have important roles to play in the correctness of the system .",19,0,27
dataset/preprocessed/training-data/sentiment_analysis/49,"However , combining these information in an effective manner is a non-trivial task that researchers often have to face .",20,0,20
dataset/preprocessed/training-data/sentiment_analysis/49,A video provides a good source for extracting multi-modal information .,21,0,11
dataset/preprocessed/training-data/sentiment_analysis/49,"In addition to the visual frames , it also provides information such as acoustic and textual representation of spoken language .",22,0,21
dataset/preprocessed/training-data/sentiment_analysis/49,"Additionally , a speaker can utter multiple utterances in a single video and these utterances can have different sentiments .",23,0,20
dataset/preprocessed/training-data/sentiment_analysis/49,The sentiment information of an utterance often has inter-dependence on other contextual utterances .,24,0,14
dataset/preprocessed/training-data/sentiment_analysis/49,Classifying such an utterance in an independent manner poses many challenges to the underlying algorithm .,25,0,16
dataset/preprocessed/training-data/sentiment_analysis/49,"In this paper , we propose a novel method that employs a recurrent neural network based multimodal multi-utterance attention framework for sentiment prediction .",26,0,24
dataset/preprocessed/training-data/sentiment_analysis/49,We hypothesize that applying attention to contributing neighboring utterances and / or multi-modal representations may assist the network to learn in a better way .,27,0,25
dataset/preprocessed/training-data/sentiment_analysis/49,The main challenge in multi-modal sentiment analysis lies in the proper utilization of the information extracted from multiple modalities .,28,0,20
dataset/preprocessed/training-data/sentiment_analysis/49,"Although it is often argued that incorporation of all the available modalities is always beneficial for enhanced performance , it must be noted that not all the modalities play equal role .",29,0,32
dataset/preprocessed/training-data/sentiment_analysis/49,Another concern in multi-modal framework is that the presence of noise in one modality can affect the over all performance .,30,0,21
dataset/preprocessed/training-data/sentiment_analysis/49,To better address these concerns we propose a novel fusion method by focusing on inter-modality relations computed between the target utterance and its context .,31,0,25
dataset/preprocessed/training-data/sentiment_analysis/49,"We argue that in multi-modal sentiment classification , not only the relation among two modalities of the same utterance is important , but also relatedness with the modalities across its context are important .",32,0,34
dataset/preprocessed/training-data/sentiment_analysis/49,Think of an utterance,33,0,4
dataset/preprocessed/training-data/sentiment_analysis/49,"Ut that constitutes of three modalities , say At ( i.e. audio ) , Vt ( i.e. visual ) and T t ( i.e. text ) .",34,0,27
dataset/preprocessed/training-data/sentiment_analysis/49,"Let us also assume U k being a member of the contextual utterances consisting of the modalities - A k , V k and T k .",35,0,27
dataset/preprocessed/training-data/sentiment_analysis/49,"In this case , our model computes the relatedness among the modalities ( for e.g. , Vt and T k ) of Ut and U kin order to produce a richer multi-modal representation for final classification .",36,0,37
dataset/preprocessed/training-data/sentiment_analysis/49,The attention mechanism is then used to attend to the important contextual utterances having higher relatedness or similarity ( computed using inter-modality correlations ) with the target utterance .,37,0,29
dataset/preprocessed/training-data/sentiment_analysis/49,"Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the contextual utterances by computing correlations among the modalities of the target utterance and the context utterances .",38,0,35
dataset/preprocessed/training-data/sentiment_analysis/49,This explicitly helps us to distinguish which modalities of the relevant contextual utterances are more important for sentiment prediction of the target utterance .,39,0,24
dataset/preprocessed/training-data/sentiment_analysis/49,The model facilitates this modality selection by attending over the contextual utterances and thus generates better multimodal feature representation when these modalities from the context are combined with the modalities of the target utterance .,40,0,35
dataset/preprocessed/training-data/sentiment_analysis/49,"We evaluate our proposed approach on two recent benchmark datasets , i.e. CMU - MOSI and CMU - MOSEI , with one being the largest ( CMU - MOSEI ) available dataset for multimodal sentiment analysis ( c.f. Section 4.1 ) .",41,0,42
dataset/preprocessed/training-data/sentiment_analysis/49,"Evaluation shows that the proposed attention framework attains better performance than the state - of - the - art systems for various combinations of input modalities ( i.e. text , visual & acoustic ) .",42,0,35
dataset/preprocessed/training-data/sentiment_analysis/49,The main contributions of our proposed work are three - fold : a ) we propose a novel technique for multi-modal sentiment analysis ; b ) we propose an effective attention framework that leverages contributing features across multiple modalities and neighboring utterances for sentiment analysis ; and c) we present the state - of - the - art systems for sentiment analysis in two different benchmark datasets .,43,0,68
dataset/preprocessed/training-data/sentiment_analysis/49,A survey of the literature suggests that multimodal sentiment prediction is relatively a new are a as compared to textual based sentiment prediction .,45,0,24
dataset/preprocessed/training-data/sentiment_analysis/49,A good review covering the literature from uni-modal analysis to multi-modal analysis is presented in .,46,0,16
dataset/preprocessed/training-data/sentiment_analysis/49,"An application of multi-kernel learning based fusion technique was proposed in , where they employed deep convolutional neural networks for extracting the textual features and fused it with other ( visual & acoustic ) modalities for prediction .",47,0,38
dataset/preprocessed/training-data/sentiment_analysis/49,introduced the multi-modal dictionary to better understand the interaction between facial gestures and spoken words when expressing the sentiment .,48,0,20
dataset/preprocessed/training-data/sentiment_analysis/49,"Authors introduced the MOSI dataset , the first of its kind to enable the studies of multi-modal sentiment intensity analysis .",49,0,21
dataset/preprocessed/training-data/sentiment_analysis/3,Knowledge - Enriched Transformer for Emotion Detection in Textual Conversations,2,1,10
dataset/preprocessed/training-data/sentiment_analysis/3,Messages in human conversations inherently convey emotions .,4,0,8
dataset/preprocessed/training-data/sentiment_analysis/3,The task of detecting emotions in textual conversations leads to a wide range of applications such as opinion mining in social networks .,5,1,23
dataset/preprocessed/training-data/sentiment_analysis/3,"However , enabling machines to analyze emotions in conversations is challenging , partly because humans often rely on the context and commonsense knowledge to express emotions .",6,0,27
dataset/preprocessed/training-data/sentiment_analysis/3,"In this paper , we address these challenges by proposing a Knowledge - Enriched Transformer ( KET ) , where contextual utterances are interpreted using hierarchical self - attention and external commonsense knowledge is dynamically leveraged using a context - aware affective graph attention mechanism .",7,0,46
dataset/preprocessed/training-data/sentiment_analysis/3,Experiments on multiple textual conversation datasets demonstrate that both context and commonsense knowledge are consistently beneficial to the emotion detection performance .,8,0,22
dataset/preprocessed/training-data/sentiment_analysis/3,"In addition , the experimental results show that our KET model outperforms the state - of - the - art models on most of the tested datasets in F1 score .",9,0,31
dataset/preprocessed/training-data/sentiment_analysis/3,"Emotions are "" generated states in humans that reflect evaluative judgments of the environment , the self and other social agents "" .",11,0,23
dataset/preprocessed/training-data/sentiment_analysis/3,Messages in human communications inherently convey emotions .,12,0,8
dataset/preprocessed/training-data/sentiment_analysis/3,"With the prevalence of social media platforms such as Facebook Messenger , as well as conversational agents such as Amazon Alexa , there is an emerging need for machines to understand human emotions in natural conversations .",13,0,37
dataset/preprocessed/training-data/sentiment_analysis/3,"This work addresses the task of detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations , where the emotion of an utterance is detected in the conversational context .",14,1,36
dataset/preprocessed/training-data/sentiment_analysis/3,Being able to effectively detect emotions in conversations leads to a wide range of applications ranging from opinion mining in social media platforms :,15,0,24
dataset/preprocessed/training-data/sentiment_analysis/3,An example conversation with annotated labels from the DailyDialog dataset .,16,0,11
dataset/preprocessed/training-data/sentiment_analysis/3,"By referring to the context , "" it "" in the third utterance is linked to "" birthday "" in the first utterance .",17,0,24
dataset/preprocessed/training-data/sentiment_analysis/3,"By leveraging an external knowledge base , the meaning of "" friends "" in the forth utterance is enriched by associated knowledge entities , namely "" socialize "" , "" party "" , and "" movie "" .",18,0,38
dataset/preprocessed/training-data/sentiment_analysis/3,"Thus , the implicit "" happiness "" emotion in the fourth utterance can be inferred more easily via its enriched meaning .",19,0,22
dataset/preprocessed/training-data/sentiment_analysis/3,to building emotion - aware conversational agents .,20,0,8
dataset/preprocessed/training-data/sentiment_analysis/3,"However , enabling machines to analyze emotions in human conversations is challenging , partly because humans often rely on the context and commonsense knowledge to express emotions , which is difficult to be captured by machines .",21,0,37
dataset/preprocessed/training-data/sentiment_analysis/3,shows an example conversation demonstrating the importance of context and commonsense knowledge in understanding conversations and detecting implicit emotions .,22,0,20
dataset/preprocessed/training-data/sentiment_analysis/3,There are several recent studies that model contextual information to detect emotions in conversations .,23,0,15
dataset/preprocessed/training-data/sentiment_analysis/3,"and leveraged recurrent neural networks ( RNN ) to model the contextual utterances in sequence , where each utterance is represented by a feature vector extracted by convolutional neural networks ( CNN ) at an earlier stage .",24,0,38
dataset/preprocessed/training-data/sentiment_analysis/3,"Similarly , proposed to use extracted CNN features in memory networks to model contextual utterances .",25,0,16
dataset/preprocessed/training-data/sentiment_analysis/3,"However , these methods require separate feature extraction and tuning , which may not be ideal for real - time applications .",26,0,22
dataset/preprocessed/training-data/sentiment_analysis/3,"In addition , to the best of our knowledge , no attempts have been made in the literature to incorporate commonsense knowledge from external knowledge bases to detect emotions in textual conversations .",27,0,33
dataset/preprocessed/training-data/sentiment_analysis/3,Commonsense knowledge is fundamental to understanding conversations and generating appropriate responses .,28,0,12
dataset/preprocessed/training-data/sentiment_analysis/3,"To this end , we propose a Knowledge - Enriched Transformer ( KET ) to effectively incorporate contextual information and external knowledge bases to address the aforementioned challenges .",29,0,29
dataset/preprocessed/training-data/sentiment_analysis/3,The Transformer has been shown to be a powerful representation learning model in many NLP tasks such as machine translation and language understanding .,30,0,24
dataset/preprocessed/training-data/sentiment_analysis/3,"The self - attention and cross-attention modules in the Transformer capture the intra-sentence and inter-sentence correlations , respectively .",31,0,19
dataset/preprocessed/training-data/sentiment_analysis/3,The shorter path of information flow in these two modules compared to gated RNNs and CNNs allows KET to model contextual information more efficiently .,32,0,25
dataset/preprocessed/training-data/sentiment_analysis/3,"In addition , we propose a hierarchical self - attention mechanism allowing KET to model the hierarchical structure of conversations .",33,0,21
dataset/preprocessed/training-data/sentiment_analysis/3,"Our model separates context and response into the encoder and decoder , respectively , which is different from other Transformer - based models , e.g. , BERT , which directly concatenate context and response , and then train language models using only the encoder part .",34,0,46
dataset/preprocessed/training-data/sentiment_analysis/3,"Moreover , to exploit commonsense knowledge , we leverage external knowledge bases to facilitate the understanding of each word in the utterances by referring to related knowledge entities .",35,0,29
dataset/preprocessed/training-data/sentiment_analysis/3,The referring process is dynamic and balances between relatedness and affectiveness of the retrieved knowledge entities using a context - aware affective graph attention mechanism .,36,0,26
dataset/preprocessed/training-data/sentiment_analysis/3,"In summary , our contributions are as follows :",37,0,9
dataset/preprocessed/training-data/sentiment_analysis/3,"For the first time , we apply the Transformer to analyze conversations and detect emotions .",38,0,16
dataset/preprocessed/training-data/sentiment_analysis/3,Our hierarchical self - attention and crossattention modules allow our model to exploit contextual information more efficiently than existing gated RNNs and CNNs .,39,0,24
dataset/preprocessed/training-data/sentiment_analysis/3,"We derive dynamic , context - aware , and emotion - related commonsense knowledge from external knowledge bases and emotion lexicons to facilitate the emotion detection in conversations .",40,0,29
dataset/preprocessed/training-data/sentiment_analysis/3,We conduct extensive experiments demonstrating that both contextual information and commonsense knowledge are beneficial to the emotion detection performance .,41,0,20
dataset/preprocessed/training-data/sentiment_analysis/3,"In addition , our proposed KET model outperforms the state - of - the - art models on most of the tested datasets across different domains .",42,0,27
dataset/preprocessed/training-data/sentiment_analysis/3,Emotion Detection in Conversations :,44,0,5
dataset/preprocessed/training-data/sentiment_analysis/3,Early studies on emotion detection in conversations focus on call center dialogs using lexicon - based methods and audio features .,45,0,21
dataset/preprocessed/training-data/sentiment_analysis/3,annotated and detected emotions in call center dialogs using unigram topic modelling .,46,0,13
dataset/preprocessed/training-data/sentiment_analysis/3,"In recent years , there is an emerging research trend on emotion detection in conversational videos and multi-turn Tweet s using deep learning methods .",47,0,25
dataset/preprocessed/training-data/sentiment_analysis/3,proposed along short - term memory network ( LSTM ) based model to capture contextual information for sentiment analysis in user - generated videos .,48,0,25
dataset/preprocessed/training-data/sentiment_analysis/3,"proposed the DialogueRNN model that uses three gated recurrent units ( GRU ) to model the speaker , the context from the preceding utterances , and the emotions of the preceding utterances , respectively .",49,0,35
dataset/preprocessed/training-data/sentiment_analysis/25,Aspect Based Sentiment Analysis with Gated Convolutional Networks,2,1,8
dataset/preprocessed/training-data/sentiment_analysis/25,"Aspect based sentiment analysis ( ABSA ) can provide more detailed information than general sentiment analysis , because it aims to predict the sentiment polarities of the given aspects or entities in text .",4,1,34
dataset/preprocessed/training-data/sentiment_analysis/25,We summarize previous approaches into two subtasks : aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .,5,1,26
dataset/preprocessed/training-data/sentiment_analysis/25,"Most previous approaches employ long short - term memory and attention mechanisms to predict the sentiment polarity of the concerned targets , which are often complicated and need more training time .",6,0,32
dataset/preprocessed/training-data/sentiment_analysis/25,"We propose a model based on convolutional neural networks and gating mechanisms , which is more accurate and efficient .",7,0,20
dataset/preprocessed/training-data/sentiment_analysis/25,"First , the novel Gated Tanh - ReLU Units can selectively output the sentiment features according to the given aspect or entity .",8,0,23
dataset/preprocessed/training-data/sentiment_analysis/25,The architecture is much simpler than attention layer used in the existing models .,9,0,14
dataset/preprocessed/training-data/sentiment_analysis/25,"Second , the computations of our model could be easily parallelized during training , because convolutional layers do not have time dependency as in LSTM layers , and gating units also work independently .",10,0,34
dataset/preprocessed/training-data/sentiment_analysis/25,The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models .,11,0,14
dataset/preprocessed/training-data/sentiment_analysis/25,Opinion mining and sentiment analysis on user - generated reviews can provide valuable information for providers and consumers .,14,0,19
dataset/preprocessed/training-data/sentiment_analysis/25,"Instead of predicting the over all sen-timent polarity , fine - grained aspect based sentiment analysis ( ABSA ) ) is proposed to better understand reviews than traditional sentiment analysis .",15,0,31
dataset/preprocessed/training-data/sentiment_analysis/25,"Specifically , we are interested in the sentiment polarity of aspect categories or target entities in the text .",16,0,19
dataset/preprocessed/training-data/sentiment_analysis/25,"Sometimes , it is coupled with aspect term extractions .",17,0,10
dataset/preprocessed/training-data/sentiment_analysis/25,"A number of models have been developed for ABSA , but there are two different subtasks , namely aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .",18,1,36
dataset/preprocessed/training-data/sentiment_analysis/25,"The goal of ACSA is to predict the sentiment polarity with regard to the given aspect , which is one of a few predefined categories .",19,1,26
dataset/preprocessed/training-data/sentiment_analysis/25,"On the other hand , the goal of ATSA is to identify the sentiment polarity concerning the target entities that appear in the text instead , which could be a multi-word phrase or a single word .",20,1,37
dataset/preprocessed/training-data/sentiment_analysis/25,The number of distinct words contributing to aspect terms could be more than a thousand .,21,0,16
dataset/preprocessed/training-data/sentiment_analysis/25,"For example , in the sentence "" Average to good Thai food , but terrible delivery . "" , ATSA would ask the sentiment polarity towards the entity Thai food ; while ACSA would ask the sentiment polarity toward the aspect service , even though the word service does not appear in the sentence .",22,0,55
dataset/preprocessed/training-data/sentiment_analysis/25,"Many existing models use LSTM layers to distill sentiment information from embedding vectors , and apply attention mechanisms to enforce models to focus on the text spans related to the given aspect / entity .",23,0,35
dataset/preprocessed/training-data/sentiment_analysis/25,"Such models include Attention - based LSTM with Aspect Embedding ( ATAE - LSTM ) for ACSA ; Target - Dependent Sentiment Classification ( TD - LSTM ) , Gated Neural Networks and Recurrent Attention Memory Network ( RAM ) for ATSA .",24,0,43
dataset/preprocessed/training-data/sentiment_analysis/25,Attention mechanisms has been successfully used in many NLP tasks .,25,0,11
dataset/preprocessed/training-data/sentiment_analysis/25,It first computes the alignment scores between context vectors and target vector ; then carry out a weighted sum with the scores and the context vectors .,26,0,27
dataset/preprocessed/training-data/sentiment_analysis/25,"However , the context vectors have to encode both the aspect and sentiment information , and the alignment scores are applied across all feature dimensions regardless of the differences between these two types of information .",27,0,36
dataset/preprocessed/training-data/sentiment_analysis/25,Both LSTM and attention layer are very timeconsuming during training .,28,0,11
dataset/preprocessed/training-data/sentiment_analysis/25,LSTM processes one token in a step .,29,0,8
dataset/preprocessed/training-data/sentiment_analysis/25,Attention layer involves exponential operation and normalization of all alignment scores of all the words in the sentence .,30,0,19
dataset/preprocessed/training-data/sentiment_analysis/25,"Moreover , some models needs the positional information between words and targets to produce weighted LSTM , which can be unreliable in noisy review text .",31,0,26
dataset/preprocessed/training-data/sentiment_analysis/25,"Certainly , it is possible to achieve higher accuracy by building more and more complicated LSTM cells and sophisticated attention mechanisms ; but one has to hold more parameters in memory , get more hyper - parameters to tune and spend more time in training .",32,0,46
dataset/preprocessed/training-data/sentiment_analysis/25,"In this paper , we propose a fast and effective neural network for ACSA and ATSA based on convolutions and gating mechanisms , which has much less training time than LSTM based networks , but with better accuracy .",33,0,39
dataset/preprocessed/training-data/sentiment_analysis/25,"For ACSA task , our model has two separate convolutional layers on the top of the embedding layer , whose outputs are combined by novel gating units .",34,0,28
dataset/preprocessed/training-data/sentiment_analysis/25,Convolutional layers with multiple filters can efficiently extract n-gram features at many granularities on each receptive field .,35,0,18
dataset/preprocessed/training-data/sentiment_analysis/25,"The proposed gating units have two nonlinear gates , each of which is connected to one convolutional layer .",36,0,19
dataset/preprocessed/training-data/sentiment_analysis/25,"With the given aspect information , they can selectively extract aspect - specific sentiment information for sentiment prediction .",37,0,19
dataset/preprocessed/training-data/sentiment_analysis/25,"For example , in the sentence "" Average to good Thai food , but terrible delivery . "" , when the aspect food is provided , the gating units automatically ignore the negative sentiment of aspect delivery from the second clause , and only output the positive sentiment from the first clause .",38,0,53
dataset/preprocessed/training-data/sentiment_analysis/25,"Since each component of the proposed model could be easily parallelized , it has much less training time than the models based on LSTM and attention mechanisms .",39,0,28
dataset/preprocessed/training-data/sentiment_analysis/25,"For ATSA task , where the aspect terms consist of multiple words , we extend our model to include another convolutional layer for the target expressions .",40,0,27
dataset/preprocessed/training-data/sentiment_analysis/25,"We evaluate our models on the SemEval datasets , which contains restaurants and laptops reviews with labels on aspect level .",41,0,21
dataset/preprocessed/training-data/sentiment_analysis/25,"To the best of our knowledge , no CNNbased model has been proposed for aspect based sentiment analysis so far .",42,0,21
dataset/preprocessed/training-data/sentiment_analysis/25,We present the relevant studies into following two categories .,44,0,10
dataset/preprocessed/training-data/sentiment_analysis/25,"Recently , neural networks have gained much popularity on sentiment analysis or sentence classification task .",46,0,16
dataset/preprocessed/training-data/sentiment_analysis/25,"Tree - based recursive neural networks such as Recursive Neural Tensor Network and Tree - LSTM , make use of syntactic interpretation of the sentence structure , but these methods suffer from time inefficiency and parsing errors on review text .",47,0,41
dataset/preprocessed/training-data/sentiment_analysis/25,Recurrent Neural Networks ( RNNs ) such as LSTM and GRU have been used for sentiment analysis on data instances having variable length .,48,0,24
dataset/preprocessed/training-data/sentiment_analysis/25,"There are also many models that use convolutional neural networks ( CNNs ) in NLP , which also prove that convolution operations can capture compositional structure of texts with rich semantic information without laborious feature engineering .",49,0,37
dataset/preprocessed/training-data/sentiment_analysis/40,Recurrent Attention Network on Memory for Aspect Sentiment Analysis,2,1,9
dataset/preprocessed/training-data/sentiment_analysis/40,We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment / review .,4,0,22
dataset/preprocessed/training-data/sentiment_analysis/40,"Our framework adopts multiple - attention mechanism to capture sentiment features separated by along distance , so that it is more robust against irrelevant information .",5,0,26
dataset/preprocessed/training-data/sentiment_analysis/40,"The results of multiple attentions are non-linearly combined with a recurrent neural network , which strengthens the expressive power of our model for handling more complications .",6,0,27
dataset/preprocessed/training-data/sentiment_analysis/40,"The weightedmemory mechanism not only helps us avoid the labor - intensive feature engineering work , but also provides a tailor - made memory for different opinion targets of a sentence .",7,0,32
dataset/preprocessed/training-data/sentiment_analysis/40,"We examine the merit of our model on four datasets : two are from Se-m Eval2014 , i.e. reviews of restaurants and laptops ; atwitter dataset , for testing its performance on social media data ; and a Chinese news comment dataset , for testing its language sensitivity .",8,0,49
dataset/preprocessed/training-data/sentiment_analysis/40,The experimental results show that our model consistently outperforms the state - of - the - art methods on different types of data .,9,0,24
dataset/preprocessed/training-data/sentiment_analysis/40,"The goal of aspect sentiment analysis is to identify the sentiment polarity ( i.e. , negative , neutral , or positive ) of a specific opinion target expressed in a comment / review by a reviewer .",11,0,37
dataset/preprocessed/training-data/sentiment_analysis/40,"For example , in "" I bought a mobile phone , it s camera is wonderful but the battery life is short "" , there are three opinion targets , "" camera "" , "" battery life "" , and "" mobile phone "" .",12,0,45
dataset/preprocessed/training-data/sentiment_analysis/40,"The reviewer has a positive sentiment on the "" camera "" , a negative sentiment on the * Corresponding author .",13,0,21
dataset/preprocessed/training-data/sentiment_analysis/40,""" battery life "" , and a mixed sentiment on the "" mobile phone "" .",14,0,16
dataset/preprocessed/training-data/sentiment_analysis/40,Sentence - oriented sentiment analysis methods are not capable to capture such fine - grained sentiments on opinion targets .,15,0,20
dataset/preprocessed/training-data/sentiment_analysis/40,"In order to identify the sentiment of an individual opinion target , one critical task is to model appropriate context features for the target in its original sentence .",16,0,29
dataset/preprocessed/training-data/sentiment_analysis/40,"In simple cases , the sentiment of a target is identifiable with a syntactically nearby opinion word , e.g. "" wonderful "" for "" camera "" .",17,0,27
dataset/preprocessed/training-data/sentiment_analysis/40,"However , there are many cases in which opinion words are enclosed in more complicated contexts .",18,0,17
dataset/preprocessed/training-data/sentiment_analysis/40,""" It s camera is not wonderful enough "" might express a neutral sentiment on "" camera "" , but not negative .",20,0,23
dataset/preprocessed/training-data/sentiment_analysis/40,Such complications usually hinder conventional approaches to aspect sentiment analysis .,21,0,11
dataset/preprocessed/training-data/sentiment_analysis/40,"To model the sentiment of the above phraselike word sequence ( i.e. "" not wonderful enough "" ) , LSTM - based methods are proposed , such as target dependent LSTM ( TD - LSTM ) .",22,0,37
dataset/preprocessed/training-data/sentiment_analysis/40,"TD - LSTM might suffer from the problem that after it captures a sentiment feature far from the target , it needs to propagate the feature word byword to the target , in which case it 's likely to lose this feature , such as the feature "" cost - effective "" for "" the phone "" in "" My over all feeling is that the phone , after using it for three months and considering its price , is really cost - effective "" .",23,0,86
dataset/preprocessed/training-data/sentiment_analysis/40,"1 Attention mechanism , which has been successfully used in machine translation , can enforce a model to pay more attention to the important part of a sentence .",24,0,29
dataset/preprocessed/training-data/sentiment_analysis/40,There are already some works using attention in sentiment analysis to exploit this advantage .,25,0,15
dataset/preprocessed/training-data/sentiment_analysis/40,Another observation is that some types of sentence structures are particularly challenging for target sentiment analysis .,26,0,17
dataset/preprocessed/training-data/sentiment_analysis/40,"For example , in "" Except Patrick , all other actors do n't play well "" , the word "" except "" and the phrase "" do n't play well "" produce a positive sentiment on "" Patrick "" .",27,0,40
dataset/preprocessed/training-data/sentiment_analysis/40,"It 's hard to synthesize these features just by LSTM , since their positions are dispersed .",28,0,17
dataset/preprocessed/training-data/sentiment_analysis/40,"Single attention based methods ( e.g. ) are also not capable to overcome such difficulty , because attending multiple words with one attention may hide the characteristic of each attended word .",29,0,32
dataset/preprocessed/training-data/sentiment_analysis/40,"In this paper , we propose a novel framework to solve the above problems in target sentiment analysis .",30,0,19
dataset/preprocessed/training-data/sentiment_analysis/40,"Specifically , our framework first adopts a bidirectional LSTM ( BLSTM ) to produce the memory ( i.e. the states of time steps generated by LSTM ) from the input , as bidirectional recurrent neural networks ( RNNs ) were found effective for a similar purpose in machine translation .",31,0,50
dataset/preprocessed/training-data/sentiment_analysis/40,"The memory slices are then weighted according to their relative positions to the target , so that different targets from the same sentence have their own tailor - made memories .",32,0,31
dataset/preprocessed/training-data/sentiment_analysis/40,"After that , we pay multiple attentions on the position - weighted memory and nonlinearly combine the attention results with a recurrent network , i.e. GRUs .",33,0,27
dataset/preprocessed/training-data/sentiment_analysis/40,"Finally , we apply softmax on the output of the GRU network to predict the sentiment on the target .",34,0,20
dataset/preprocessed/training-data/sentiment_analysis/40,Our framework introduces a novel way of applying multiple - attention mechanism to synthesize important features in difficult sentence structures .,35,0,21
dataset/preprocessed/training-data/sentiment_analysis/40,"It 's sort of analogous to the cognition procedure of a person , who might first notice part of the important information at the beginning , then notices more as she reads through , and finally combines the information from multiple attentions to draw a conclusion .",36,0,47
dataset/preprocessed/training-data/sentiment_analysis/40,"For the above sentence , our model may attend the word "" except "" first , and then attends the phrase "" do n't play well "" , finally combines them to generate a positive feature for "" Patrick "" .",37,0,41
dataset/preprocessed/training-data/sentiment_analysis/40,"also adopted the idea of multiple attentions , but they used the result of a previous attention to help the next attention attend more accurate information .",38,0,27
dataset/preprocessed/training-data/sentiment_analysis/40,"Their vector fed to softmax for classification is only from the final attention , which is essentially a linear combination of input embeddings ( they did not have a memory component ) .",39,0,33
dataset/preprocessed/training-data/sentiment_analysis/40,"Thus , the above limitation of single attention based methods also holds for .",40,0,14
dataset/preprocessed/training-data/sentiment_analysis/40,"In contrast , our model combines the results of multiple attentions with a GRU network , which has different behaviors inherited from RNNs , such as forgetting , maintaining , and non-linearly transforming , and thus allows a better prediction accuracy .",41,0,42
dataset/preprocessed/training-data/sentiment_analysis/40,"We evaluate our approach on four datasets : the first two come from SemEval 2014 , containing reviews of restaurant domain and laptop domain ; the third one is a collection of tweets , collected by ; to examine whether our framework is language - insensitive ( since languages show differences in quite a few aspects in expressing sentiments ) , we prepared a dataset of Chinese news comments with people mentions as opinion targets .",42,0,76
dataset/preprocessed/training-data/sentiment_analysis/40,"The experimental results show that our model performs well for different types of data , and consistently outperforms the state - of - the - art methods .",43,0,28
dataset/preprocessed/training-data/sentiment_analysis/40,The task of aspect sentiment classification belongs to entity - level sentiment analysis .,45,0,14
dataset/preprocessed/training-data/sentiment_analysis/40,Conventional representative methods for this task include rulebased methods and statisticbased methods .,46,0,13
dataset/preprocessed/training-data/sentiment_analysis/40,"extracted 2 - tuples of ( opinion target , opinion word ) from comments and then identified the sentiment of opinion targets .",47,0,23
dataset/preprocessed/training-data/sentiment_analysis/40,adopted Probabilistic Soft Logic to handle the task .,48,0,9
dataset/preprocessed/training-data/sentiment_analysis/40,There are also statistic - based approaches which employ SVM .,49,0,11
dataset/preprocessed/training-data/sentiment_analysis/39,SentiHood : Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods,2,1,11
dataset/preprocessed/training-data/sentiment_analysis/39,"In this paper , we introduce the task of targeted aspect - based sentiment analysis .",4,1,16
dataset/preprocessed/training-data/sentiment_analysis/39,The goal is to extract fine - grained information with respect to entities mentioned in user comments .,5,0,18
dataset/preprocessed/training-data/sentiment_analysis/39,This work extends both aspect - based sentiment analysis that assumes a single entity per document and targeted sentiment analysis that assumes a single sentiment towards a target entity .,6,0,30
dataset/preprocessed/training-data/sentiment_analysis/39,"In particular , we identify the sentiment towards each aspect of one or more entities .",7,0,16
dataset/preprocessed/training-data/sentiment_analysis/39,"As a testbed for this task , we introduce the SentiHood dataset , extracted from a question answering ( QA ) platform where urban neighbourhoods are discussed by users .",8,0,30
dataset/preprocessed/training-data/sentiment_analysis/39,In this context units of text often mention several aspects of one or more neighbourhoods .,9,0,16
dataset/preprocessed/training-data/sentiment_analysis/39,"This is the first time that a generic social media platform in this case a QA platform , is used for fine - grained opinion mining .",10,0,27
dataset/preprocessed/training-data/sentiment_analysis/39,Text coming from QA platforms is far less constrained compared to text from review specific platforms which current datasets are based on .,11,0,23
dataset/preprocessed/training-data/sentiment_analysis/39,"We develop several strong baselines , relying on logistic regression and state - of - the - art recurrent neural networks .",12,0,22
dataset/preprocessed/training-data/sentiment_analysis/39,This work is licensed under a Creative Commons Attribution 4.0 International Licence .,13,0,13
dataset/preprocessed/training-data/sentiment_analysis/39,Licence details :,14,0,3
dataset/preprocessed/training-data/sentiment_analysis/39,Sentiment analysis is an important task in natural language processing .,16,1,11
dataset/preprocessed/training-data/sentiment_analysis/39,"It has received not only a lot of interest in academia but also in industry , in particular for identifying customer satisfaction on products and services .",17,0,27
dataset/preprocessed/training-data/sentiment_analysis/39,Early research in the field of sentiment analysis only focused on identifying the over all sentiment or polarity of a given text .,18,0,23
dataset/preprocessed/training-data/sentiment_analysis/39,The underlying assumption of this work was that there is one over all polarity in the whole text .,19,0,19
dataset/preprocessed/training-data/sentiment_analysis/39,"Aspect - based sentiment analysis ( ABSA ) relates to the task of extracting fine - grained information by identifying the polarity towards different aspects of an entity in the same unit of text , and recognizing the polarity associated with each aspect separately .",20,1,45
dataset/preprocessed/training-data/sentiment_analysis/39,"The datasets for this task were mostly based on specialized review platforms such as Yelp where it is assumed that only one entity is discussed in one review snippet , but the opinion on multiple aspects can be expressed .",21,0,40
dataset/preprocessed/training-data/sentiment_analysis/39,This task is particularly useful because a user can assess the aggregated sentiment for each individual aspect of a given product or service and get a more fine - grained understanding of its quality .,22,0,35
dataset/preprocessed/training-data/sentiment_analysis/39,Another line of research in this field is targeted ( a.k.a. target - dependent ) sentiment analysis .,23,0,18
dataset/preprocessed/training-data/sentiment_analysis/39,Targeted sentiment analysis investigates the classification of opinion polarities towards certain target entity mentions in given sentences ( often a tweet ) .,24,1,23
dataset/preprocessed/training-data/sentiment_analysis/39,"For instance in the sentence "" People everywhere love Windows & vista .",25,0,13
dataset/preprocessed/training-data/sentiment_analysis/39,"Bill Gates "" , polarity towards Bill Gates is "" Neutral "" but the positive sentiment towards",26,0,17
dataset/preprocessed/training-data/sentiment_analysis/39,Windows & vista will interfere with identifying it if the usual methods for sentiment analysis task are employed .,27,0,19
dataset/preprocessed/training-data/sentiment_analysis/39,However this task assumes only the over all sentiment for each entity .,28,0,13
dataset/preprocessed/training-data/sentiment_analysis/39,"Moreover , the existing corpora for this task so far has contained only a single target entity per unit of text .",29,0,22
dataset/preprocessed/training-data/sentiment_analysis/39,"Both settings are obviously limited , and there exists many scenarios in which sentiments towards different aspects of several entities are discussed in the same unit of text .",30,0,29
dataset/preprocessed/training-data/sentiment_analysis/39,"As a running example , we use urban are as : choosing which are a to live or to visit is an important task when moving or visiting a new city .",31,0,32
dataset/preprocessed/training-data/sentiment_analysis/39,Currently there are no dedicated platforms for reviewing and rating aspects of neighbourhoods of a city .,32,0,17
dataset/preprocessed/training-data/sentiment_analysis/39,However we can find many discussions and threads on several blogs and question answering platforms that discuss aspects of are as in many cities around the world .,33,0,28
dataset/preprocessed/training-data/sentiment_analysis/39,"In general , these conversations are very comprehensible : they often contain specific information about several aspects of several neighbourhoods .",34,0,21
dataset/preprocessed/training-data/sentiment_analysis/39,One example is the following ( are a names are highlighted in bold and aspect related terms are underlined ) :,35,0,21
dataset/preprocessed/training-data/sentiment_analysis/39,""" Other places to look at in South London are Streatham ( good range of shops and restaurants , maybe a bit far out of central London but you get more for your money ) Brixton ( good transport links , trendy , can be a bit edgy )",36,0,49
dataset/preprocessed/training-data/sentiment_analysis/39,"Clapham ( good transport , good restaurants / pubs , can feel a bit dull , expensive ) ... """,37,0,20
dataset/preprocessed/training-data/sentiment_analysis/39,The example above does not perfectly fit into the existing tasks in sentiment analysis mentioned earlier .,38,0,17
dataset/preprocessed/training-data/sentiment_analysis/39,"In this work , we introduce a new task that not only subsumes the existing sub-fields of targeted and aspect - based sentiment analysis but it also makes less assumptions on the number of entities that can be discussed in the unit of text .",39,0,45
dataset/preprocessed/training-data/sentiment_analysis/39,"To compare with the existing aspect - based sentiment analysis task , take the following example from the restaurant dataset used by SemEval shared ABSA task .",40,0,27
dataset/preprocessed/training-data/sentiment_analysis/39,""" The design of the space is good but the service is horrid ! "" .",41,0,16
dataset/preprocessed/training-data/sentiment_analysis/39,"The ABSA task aims to identify that a positive sentiment towards the ambiance aspect is expressed ( opinion target expression is "" space "" ) .",42,0,26
dataset/preprocessed/training-data/sentiment_analysis/39,"Moreover , a negative sentiment is expressed towards the service aspect ( opinion target expression is "" service "" ) .",43,0,21
dataset/preprocessed/training-data/sentiment_analysis/39,"In this example , it is assumed that both of these opinions are expressed about a single restaurant which is not mentioned explicitly .",44,0,24
dataset/preprocessed/training-data/sentiment_analysis/39,"However , take the following synthetic example that ABSA is not addressing :",45,0,13
dataset/preprocessed/training-data/sentiment_analysis/39,""" The design of the space is good in Boqueria but the service is horrid , on the other hand , the staff in Gremio are very friendly and the food is always delicious . """,46,0,36
dataset/preprocessed/training-data/sentiment_analysis/39,"In this example , more than one restaurant are discussed and restaurants for which opinions are expressed , are explicitly mentioned .",47,0,22
dataset/preprocessed/training-data/sentiment_analysis/39,We call these target entities .,48,0,6
dataset/preprocessed/training-data/sentiment_analysis/39,"Current ABSA task can only recognise that positive and negative opinions towards aspect "" service "" are expressed .",49,0,19
dataset/preprocessed/training-data/sentiment_analysis/43,Multi - grained Attention Network for Aspect - Level Sentiment Classification,2,1,11
dataset/preprocessed/training-data/sentiment_analysis/43,We propose a novel multi-grained attention network ( MGAN ) model for aspect level sentiment classification .,4,1,17
dataset/preprocessed/training-data/sentiment_analysis/43,"Existing approaches mostly adopt coarse - grained attention mechanism , which may bring information loss if the aspect has multiple words or larger context .",5,0,25
dataset/preprocessed/training-data/sentiment_analysis/43,"We propose a fine - grained attention mechanism , which can capture the word - level interaction between aspect and context .",6,0,22
dataset/preprocessed/training-data/sentiment_analysis/43,And then we leverage the fine - grained and coarsegrained attention mechanisms to compose the MGAN framework .,7,0,18
dataset/preprocessed/training-data/sentiment_analysis/43,"Moreover , unlike previous works which train each aspect with its context separately , we design an aspect alignment loss to depict the aspect - level interactions among the aspects that have the same context .",8,0,36
dataset/preprocessed/training-data/sentiment_analysis/43,"We evaluate the proposed approach on three datasets : laptop and restaurant are from SemEval 2014 , and the last one is atwitter dataset .",9,0,25
dataset/preprocessed/training-data/sentiment_analysis/43,Experimental results show that the multi-grained attention network consistently outperforms the state - of - the - art methods on all three datasets .,10,0,24
dataset/preprocessed/training-data/sentiment_analysis/43,"We also conduct experiments to evaluate the effectiveness of aspect alignment loss , which indicates the aspect - level interactions can bring extra useful information and further improve the performance .",11,0,31
dataset/preprocessed/training-data/sentiment_analysis/43,"Aspect level sentiment classification is a fundamental task in sentiment analysis , which aims to infer the sentiment polarity ( e.g. positive , neutral , negative ) of sentence with respect to the aspects .",13,0,35
dataset/preprocessed/training-data/sentiment_analysis/43,"For example , in sentence "" I like coming back to Mac OS but this laptop is lacking in speaker quality compared to my $ 400 old HP laptop "" , the polarity of the sentence towards the aspect "" Mac OS "" is positive while the polarity is negative in terms of aspect "" speaker quality "" .",14,0,59
dataset/preprocessed/training-data/sentiment_analysis/43,* corresponding author .,15,0,4
dataset/preprocessed/training-data/sentiment_analysis/43,"Many statistical methods , such as support vector machine ( SVM ) , are employed with welldesigned handcrafted features .",16,0,20
dataset/preprocessed/training-data/sentiment_analysis/43,"In recent years , neural network models are studied to automatically learn low - dimensional representations for aspects and their context .",17,0,22
dataset/preprocessed/training-data/sentiment_analysis/43,Attention mechanism is also be studied to characterize the effect of aspect on enforcing the model to pay more attention on the important words of the context .,18,0,28
dataset/preprocessed/training-data/sentiment_analysis/43,Previous works mainly employed the simple averaged aspect vector to learn the attention weights on the context words .,19,0,19
dataset/preprocessed/training-data/sentiment_analysis/43,"further proposed the bidirectional attention mechanism , which interactively learns the attention weights on context / aspect words , with respect to the averaged vector of aspect / context , respectively .",20,0,32
dataset/preprocessed/training-data/sentiment_analysis/43,"These above attention methods are all at the coarse - grained level , which simply averages the aspect / context vector to guide learning the attention weights on the context / aspect words .",21,0,34
dataset/preprocessed/training-data/sentiment_analysis/43,"The simple average pooling mechanism might cause information loss , especially for the aspect with multiple words or larger context .",22,0,21
dataset/preprocessed/training-data/sentiment_analysis/43,"For example , in sentence "" I like coming back to Mac OS but this laptop is lacking in speaker quality compared to my $ 400 old HP laptop "" , the simple averaged vector of long context might lose information when steering the attention weights on aspect words .",23,0,50
dataset/preprocessed/training-data/sentiment_analysis/43,"Similarly , the simple averaged vector of aspect ( i.e. "" speaker quality "" ) may deviate from the intuitive core meaning ( i.e. "" quality "" ) when enforcing the model to pay varying attentions on the context words .",24,0,41
dataset/preprocessed/training-data/sentiment_analysis/43,"From another perspective , previous works all regard the aspect and its context words as one instance , and train each instance separately .",25,0,24
dataset/preprocessed/training-data/sentiment_analysis/43,"However , they do not consider the relationship among the instances that have the same context words .",26,0,18
dataset/preprocessed/training-data/sentiment_analysis/43,The aspect - level interactions among the instances with same context might bring extra useful information .,27,0,17
dataset/preprocessed/training-data/sentiment_analysis/43,"Considering the above example , intuitively , the aspect "" speaker quality "" should pay more attention on "" lacking "" and less attention on "" like "" , compared with aspect "" Mac OS "" , since they have different sentiment polarities .",28,0,44
dataset/preprocessed/training-data/sentiment_analysis/43,"In this paper , we propose a multi -grained attention network to address the above two issues in aspect level sentiment classification .",29,0,23
dataset/preprocessed/training-data/sentiment_analysis/43,"Specifically , we propose a fine - grained attention mechanism ( i.e. F- Aspect2Context and F - Context2Aspect ) , which is employed to characterize the word - level interactions between aspect and context words , and relieve the information loss occurred in coarse - grained attention mechanism .",30,0,49
dataset/preprocessed/training-data/sentiment_analysis/43,"In addition , we utilize the bidirectional coarsegrained attention ( i.e. C- Aspect2Context and C - Context2Aspect ) and combine them with finegrained attention vectors to compose the multigrained attention network for the final sentiment polarity prediction , which can leverage the advantages of them .",31,0,46
dataset/preprocessed/training-data/sentiment_analysis/43,"More importantly , in order to make use of the valuable aspect - level interaction information , we design an aspect alignment loss in the objective function to enhance the difference of the attention weights towards the aspects which have the same context and different sentiment polarities .",32,0,48
dataset/preprocessed/training-data/sentiment_analysis/43,"As far as we know , we are the first to explore the interactions among the aspects with the same context .",33,0,22
dataset/preprocessed/training-data/sentiment_analysis/43,"To evaluate the proposed approach , we conduct experiments on three datasets : laptop and restaurant are from the SemEval 2014 Task 4 and the third one is a tweet collection .",34,0,32
dataset/preprocessed/training-data/sentiment_analysis/43,Experimental results show that our method achieves the best performance on all three datasets .,35,0,15
dataset/preprocessed/training-data/sentiment_analysis/43,"Aspect - level sentiment analysis is a branch of sentiment classification , which requires considering both the sentence and aspect information .",37,0,22
dataset/preprocessed/training-data/sentiment_analysis/43,"Traditional approaches regard this task as the text classification problem and design effective features , which are utilized in statistical learning algorithms for training a classifier .",38,0,27
dataset/preprocessed/training-data/sentiment_analysis/43,"proposed to use SVM based on n-gram features , parse features and lexicon features , which achieved the best perfor - mance in SemEval 2014 .",39,0,26
dataset/preprocessed/training-data/sentiment_analysis/43,designed sentiment - specific word embedding and sentiment lexicons as rich features for prediction .,40,0,15
dataset/preprocessed/training-data/sentiment_analysis/43,These methods highly depend on the effectiveness of the laborious feature engineering work and easily reach the performance bottleneck .,41,0,20
dataset/preprocessed/training-data/sentiment_analysis/43,"In recent works , there are growing studies on neural network based methods due to their capability of encoding original features as continuous and low-dimensional vectors without feature engineering .",42,0,30
dataset/preprocessed/training-data/sentiment_analysis/43,"Recursive Neural Network are studied to conduct semantic compositions on tree structures , and generate representations for prediction .",43,0,19
dataset/preprocessed/training-data/sentiment_analysis/43,"Methods on LSTM ( Hochreiter and Schmidhuber , 1997 ) were proposed to model the context information and use an aggregated vector for prediction .",44,0,25
dataset/preprocessed/training-data/sentiment_analysis/43,"TD - LSTM ) adopted LSTM to model the left context and right context of the aspect , and concatenate them as the representation for prediction .",45,0,27
dataset/preprocessed/training-data/sentiment_analysis/43,"However , these works only focused on modeling the contexts without considering the aspects , which performed an important role in estimate the sentiment polarity .",46,0,26
dataset/preprocessed/training-data/sentiment_analysis/43,Attention mechanisms are studied to enhance the influence of aspects on the final representation for prediction .,47,0,17
dataset/preprocessed/training-data/sentiment_analysis/43,Many approaches adopted the averaged aspect vector to learn the attention weights on the hidden vectors of context words .,48,0,20
dataset/preprocessed/training-data/sentiment_analysis/43,"further proposed bidirectional attention mechanism , which also learns the attention weights on aspect words towards the averaged vector of context words .",49,0,23
dataset/preprocessed/training-data/sentiment_analysis/16,Target - Sensitive Memory Networks for Aspect Sentiment Classification,2,1,9
dataset/preprocessed/training-data/sentiment_analysis/16,Aspect sentiment classification ( ASC ) is a fundamental task in sentiment analysis .,4,1,14
dataset/preprocessed/training-data/sentiment_analysis/16,"Given an aspect / target and a sentence , the task classifies the sentiment polarity expressed on the target in the sentence .",5,0,23
dataset/preprocessed/training-data/sentiment_analysis/16,Memory networks ( MNs ) have been used for this task recently and have achieved state - of - the - art results .,6,0,24
dataset/preprocessed/training-data/sentiment_analysis/16,"In MNs , attention mechanism plays a crucial role in detecting the sentiment context for the given target .",7,0,19
dataset/preprocessed/training-data/sentiment_analysis/16,"However , we found an important problem with the current MNs in performing the ASC task .",8,1,17
dataset/preprocessed/training-data/sentiment_analysis/16,Simply improving the attention mechanism will not solve it .,9,0,10
dataset/preprocessed/training-data/sentiment_analysis/16,"The problem is referred to as target - sensitive sentiment , which means that the sentiment polarity of the ( detected ) context is dependent on the given target and it can not be inferred from the context alone .",10,0,40
dataset/preprocessed/training-data/sentiment_analysis/16,"To tackle this problem , we propose the targetsensitive memory networks ( TMNs ) .",11,0,15
dataset/preprocessed/training-data/sentiment_analysis/16,Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated .,12,0,17
dataset/preprocessed/training-data/sentiment_analysis/16,Aspect sentiment classification ( ASC ) is a core problem of sentiment analysis .,14,0,14
dataset/preprocessed/training-data/sentiment_analysis/16,"Given an aspect and a sentence containing the aspect , ASC classifies the sentiment polarity expressed in the sentence about the aspect , namely , positive , neutral , or negative .",15,0,32
dataset/preprocessed/training-data/sentiment_analysis/16,"Aspects are also called opinion targets ( or simply targets ) , which are usually product / service features in customer reviews .",16,0,23
dataset/preprocessed/training-data/sentiment_analysis/16,"In this paper , we use aspect and target interchangeably .",17,0,11
dataset/preprocessed/training-data/sentiment_analysis/16,"In practice , aspects can be specified by the user or extracted automatically using an aspect extraction technique .",18,0,19
dataset/preprocessed/training-data/sentiment_analysis/16,"In this work , we assume the aspect terms are given and only focus on the classification task .",19,0,19
dataset/preprocessed/training-data/sentiment_analysis/16,"Due to their impressive results in many NLP tasks , neural networks have been applied to ASC ( see the survey ) .",20,0,23
dataset/preprocessed/training-data/sentiment_analysis/16,"Memory networks ( MNs ) , a type of neural networks which were first proposed for question answering , have achieved the state - of - the - art results in ASC .",21,0,33
dataset/preprocessed/training-data/sentiment_analysis/16,A key factor for their success is the attention mechanism .,22,0,11
dataset/preprocessed/training-data/sentiment_analysis/16,"However , we found that using existing MNs to deal with ASC has an important problem and simply relying on attention modeling can not solve it .",23,0,27
dataset/preprocessed/training-data/sentiment_analysis/16,"That is , their performance degrades when the sentiment of a context word is sensitive to the given target .",24,0,20
dataset/preprocessed/training-data/sentiment_analysis/16,Let us consider the following sentences :,25,0,7
dataset/preprocessed/training-data/sentiment_analysis/16,( 1 ) The screen resolution is excellent but the price is ridiculous .,26,0,14
dataset/preprocessed/training-data/sentiment_analysis/16,( 2 ) The screen resolution is excellent but the price is high .,27,0,14
dataset/preprocessed/training-data/sentiment_analysis/16,( 3 ) The price is high .,28,0,8
dataset/preprocessed/training-data/sentiment_analysis/16,( 4 ) The screen resolution is high .,29,0,9
dataset/preprocessed/training-data/sentiment_analysis/16,"In sentence ( 1 ) , the sentiment expressed on aspect screen resolution ( or resolution for short ) is positive , whereas the sentiment on aspect price is negative .",30,0,31
dataset/preprocessed/training-data/sentiment_analysis/16,"For the sake of predicting correct sentiment , a crucial step is to first detect the sentiment context about the given aspect / target .",31,0,25
dataset/preprocessed/training-data/sentiment_analysis/16,We call this step targeted - context detection .,32,0,9
dataset/preprocessed/training-data/sentiment_analysis/16,Memory networks ( MNs ) can deal with this step quite well because the sentiment context of a given aspect can be captured by the internal attention mechanism in MNs .,33,0,31
dataset/preprocessed/training-data/sentiment_analysis/16,"Concretely , in sentence ( 1 ) the word "" excellent "" can be identified as the sentiment context when resolution is specified .",34,0,24
dataset/preprocessed/training-data/sentiment_analysis/16,"Likewise , the context word "" ridiculous "" will be placed with a high attention when price is the target .",35,0,21
dataset/preprocessed/training-data/sentiment_analysis/16,"With the correct targeted - context detected , a trained MN , which recognizes "" excellent "" as positive sentiment and "" ridiculous "" as negative sentiment , will infer correct sentiment polarity for the given target .",36,0,38
dataset/preprocessed/training-data/sentiment_analysis/16,"This is relatively easy as "" excellent "" and "" ridiculous "" are both target - independent sentiment words , i.e. , the words themselves already indicate clear sentiments .",37,0,30
dataset/preprocessed/training-data/sentiment_analysis/16,"As illustrated above , the attention mechanism addressing the targeted - context detection problem is very useful for ASC , and it helps classify many sentences like sentence ( 1 ) accurately .",38,0,33
dataset/preprocessed/training-data/sentiment_analysis/16,This also led to existing and potential research in improving attention modeling ( discussed in Section 5 ) .,39,0,19
dataset/preprocessed/training-data/sentiment_analysis/16,"However , we observed that simply focusing on tackling the target - context detection problem and learning better attention are not sufficient to solve the problem found in sentences ( 2 ) , ( 3 ) and ( 4 ) .",40,0,41
dataset/preprocessed/training-data/sentiment_analysis/16,"Sentence ( 2 ) is similar to sentence ( 1 ) except that the ( sentiment ) context modifying aspect / target price is "" high "" .",41,0,28
dataset/preprocessed/training-data/sentiment_analysis/16,"In this case , when "" high "" is assigned the correct attention for the aspect price , the model also needs to capture the sentiment interaction between "" high "" and price in order to identify the correct sentiment polarity .",42,0,42
dataset/preprocessed/training-data/sentiment_analysis/16,"This is not as easy as sentence ( 1 ) because "" high "" itself indicates no clear sentiment .",43,0,20
dataset/preprocessed/training-data/sentiment_analysis/16,"Instead , its sentiment polarity is dependent on the given target .",44,0,12
dataset/preprocessed/training-data/sentiment_analysis/16,"Looking at sentences ( 3 ) and ( 4 ) , we further see the importance of this problem and also why relying on attention mechanism alone is insufficient .",45,0,30
dataset/preprocessed/training-data/sentiment_analysis/16,"In these two sentences , sentiment contexts are both "" high "" ( i.e. , same attention ) , but sentence ( 3 ) is negative and sentence ( 4 ) is positive simply because their target aspects are different .",46,0,41
dataset/preprocessed/training-data/sentiment_analysis/16,"Therefore , focusing on improving attention will not help in these cases .",47,0,13
dataset/preprocessed/training-data/sentiment_analysis/16,We will give a theoretical insight about this problem with MNs in Section 3 .,48,0,15
dataset/preprocessed/training-data/sentiment_analysis/16,"In this work , we aim to solve this problem .",49,0,11
dataset/preprocessed/training-data/sentiment_analysis/10,DialogueRNN : An Attentive RNN for Emotion Detection in Conversations,2,1,10
dataset/preprocessed/training-data/sentiment_analysis/10,"Emotion detection in conversations is a necessary step for a number of applications , including opinion mining over chat history , social media threads , debates , argumentation mining , understanding consumer feedback in live conversations , and soon .",4,0,40
dataset/preprocessed/training-data/sentiment_analysis/10,Currently systems do not treat the parties in the conversation individually by adapting to the speaker of each utterance .,5,0,20
dataset/preprocessed/training-data/sentiment_analysis/10,"In this paper , we describe a new method based on recurrent neural networks that keeps track of the individual party states throughout the conversation and uses this information for emotion classification .",6,0,33
dataset/preprocessed/training-data/sentiment_analysis/10,Our model outperforms the stateof - the - art by a significant margin on two different datasets .,7,0,18
dataset/preprocessed/training-data/sentiment_analysis/10,"Emotion detection in conversations has been gaining increasing attention from the research community due to its applications in many important tasks such as opinion mining over chat history and social media threads in YouTube , Facebook , Twitter , and soon .",9,0,42
dataset/preprocessed/training-data/sentiment_analysis/10,"In this paper , we present a method based on recurrent neural networks ( RNN ) that can cater to these needs by processing the huge amount of available conversational data .",10,0,32
dataset/preprocessed/training-data/sentiment_analysis/10,"Current systems , including the state of the art , do not distinguish different parties in a conversation in a meaningful way .",11,0,23
dataset/preprocessed/training-data/sentiment_analysis/10,They are not aware of the speaker of a given utterance .,12,0,12
dataset/preprocessed/training-data/sentiment_analysis/10,"In contrast , we model individual parties with party states , as the conversation flows , by relying on the utterance , the context , and current party state .",13,0,30
dataset/preprocessed/training-data/sentiment_analysis/10,"Our model is based on the assumption that there are three major aspects relevant to the emotion in a conversation : the speaker , the context from the preceding utterances , and the emotion of the preceding utterances .",14,0,39
dataset/preprocessed/training-data/sentiment_analysis/10,"These three aspects are not necessarily independent , but their separate modeling significantly outperforms the state - of - the - art ) .",15,0,24
dataset/preprocessed/training-data/sentiment_analysis/10,"In dyadic conversations , the parties have distinct roles .",16,0,10
dataset/preprocessed/training-data/sentiment_analysis/10,"Hence , to extract the context , it is crucial to consider the preceding turns of both the speaker and the listener at a given moment .",17,0,27
dataset/preprocessed/training-data/sentiment_analysis/10,Our proposed DialogueRNN system employs three gated recurrent units ( GRU ) to model these aspects .,18,0,17
dataset/preprocessed/training-data/sentiment_analysis/10,"The incoming utterance is fed into two GRUs called global GRU and party GRU to update the context and party Copyright 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",19,0,34
dataset/preprocessed/training-data/sentiment_analysis/10,All rights reserved .,20,0,4
dataset/preprocessed/training-data/sentiment_analysis/10,"state , respectively .",21,0,4
dataset/preprocessed/training-data/sentiment_analysis/10,The global GRU encodes corresponding party information while encoding an utterance .,22,0,12
dataset/preprocessed/training-data/sentiment_analysis/10,Attending over this GRU gives contextual representation that has information of all preceding utterances by different parties in the conversation .,23,0,21
dataset/preprocessed/training-data/sentiment_analysis/10,The speaker state depends on this context through attention and the speaker 's previous state .,24,0,16
dataset/preprocessed/training-data/sentiment_analysis/10,"This ensures that at time t , the speaker state directly gets information from the speaker 's previous state and global GRU which has information on the preceding parties .",25,0,30
dataset/preprocessed/training-data/sentiment_analysis/10,"Finally , the updated speaker state is fed into the emotion GRU to decode the emotion representation of the given utterance , which is used for emotion classification .",26,0,29
dataset/preprocessed/training-data/sentiment_analysis/10,"At time t , the emotion GRU cell gets the emotion representation of t ? 1 and the speaker state of t .",27,0,23
dataset/preprocessed/training-data/sentiment_analysis/10,"The emotion GRU , along with the global GRU , plays a pivotal role in inter-party relation modeling .",28,0,19
dataset/preprocessed/training-data/sentiment_analysis/10,"On the other hand , party GRU models relation between two sequential states of the same party .",29,0,18
dataset/preprocessed/training-data/sentiment_analysis/10,"In DialogueRNN , all these three different types of GRUs are connected in a recurrent manner .",30,0,17
dataset/preprocessed/training-data/sentiment_analysis/10,We believe that Dialogue RNN outperforms state - of - the - art contextual emotion classifiers such as ) because of better context representation .,31,0,25
dataset/preprocessed/training-data/sentiment_analysis/10,The rest of the paper is organized as follows :,32,0,10
dataset/preprocessed/training-data/sentiment_analysis/10,"Section 2 discusses related work ; Section 3 provides detailed description of our model ; Sections 4 and 5 present the experimental results ; finally , Section 6 concludes the paper .",33,0,32
dataset/preprocessed/training-data/sentiment_analysis/10,"Emotion recognition has attracted attention in various fields such as natural language processing , psychology , cognitive science , and soon .",35,0,22
dataset/preprocessed/training-data/sentiment_analysis/10,found correlation between emotion and facial cues .,36,0,8
dataset/preprocessed/training-data/sentiment_analysis/10,fused acoustic information with visual cues for emotion recognition .,37,0,10
dataset/preprocessed/training-data/sentiment_analysis/10,"introduced text - based emotion recognition , developed in the work of .",38,0,13
dataset/preprocessed/training-data/sentiment_analysis/10,used contextual information for emotion recognition in multimodal setting .,39,0,10
dataset/preprocessed/training-data/sentiment_analysis/10,"Recently , successfully used RNN - based deep networks for multimodal emotion recognition , which was followed by other works .",40,0,21
dataset/preprocessed/training-data/sentiment_analysis/10,Reproducing human interaction requires deep understanding of conversation .,41,0,9
dataset/preprocessed/training-data/sentiment_analysis/10,gued that emotional dynamics in a conversation is an interpersonal phenomenon .,42,0,12
dataset/preprocessed/training-data/sentiment_analysis/10,"Hence , our model incorporates inter-personal interactions in an effective way .",43,0,12
dataset/preprocessed/training-data/sentiment_analysis/10,"Further , since conversations have a natural temporal nature , we adopt the temporal nature through recurrent network .",44,0,19
dataset/preprocessed/training-data/sentiment_analysis/10,"Memory networks has been successful in several NLP are as , including question answering , machine translation , speech recognition , and soon .",45,0,24
dataset/preprocessed/training-data/sentiment_analysis/10,"Thus , used memory networks for emotion recognition in dyadic conversations , where two distinct memory networks enabled inter -speaker interaction , yielding state - of - the - art performance .",46,0,32
dataset/preprocessed/training-data/sentiment_analysis/10,"Let there be M parties / participants p 1 , p 2 , . . . , p M ( M = 2 for the datasets we used ) in a conversation .",49,0,33
dataset/preprocessed/training-data/sentiment_analysis/46,A Multi-sentiment - resource Enhanced Attention Network for Sentiment Classification,2,1,10
dataset/preprocessed/training-data/sentiment_analysis/46,Deep learning approaches for sentiment classification do not fully exploit sentiment linguistic knowledge .,4,0,14
dataset/preprocessed/training-data/sentiment_analysis/46,"In this paper , we propose a Multi-sentiment - resource Enhanced Attention Network ( MEAN ) to alleviate the problem by integrating three kinds of sentiment linguistic knowledge ( e.g. , sentiment lexicon , negation words , intensity words ) into the deep neural network via attention mechanisms .",5,0,49
dataset/preprocessed/training-data/sentiment_analysis/46,"By using various types of sentiment resources , MEAN utilizes sentiment - relevant information from different representation subspaces , which makes it more effective to capture the over all semantics of the sentiment , negation and intensity words for sentiment prediction .",6,0,42
dataset/preprocessed/training-data/sentiment_analysis/46,The experimental results demonstrate that MEAN has robust superiority over strong competitors .,7,0,13
dataset/preprocessed/training-data/sentiment_analysis/46,"Sentiment classification is an important task of natural language processing ( NLP ) , aiming to classify the sentiment polarity of a given text as positive , negative , or more fine - grained classes .",9,0,36
dataset/preprocessed/training-data/sentiment_analysis/46,It has obtained considerable attention due to its broad applications in natural language processing .,10,0,15
dataset/preprocessed/training-data/sentiment_analysis/46,"Most existing studies setup sentiment classifiers using supervised machine learning approaches , such as support vector machine ( SVM ) , convolutional neural network ( CNN ) , long short - term memory ( LSTM ) , Tree - LSTM , and attention - based methods .",11,0,47
dataset/preprocessed/training-data/sentiment_analysis/46,"Despite the remarkable progress made by the previous work , we argue that sentiment analysis still remains a challenge .",12,0,20
dataset/preprocessed/training-data/sentiment_analysis/46,"Sentiment resources including sentiment lexicon , negation words , intensity words play a crucial role in traditional sentiment classification approaches .",13,0,21
dataset/preprocessed/training-data/sentiment_analysis/46,"Despite its usefulness , to date , the sentiment linguistic knowledge has been underutilized in most recent deep neural network models ( e.g. , CNNs and LSTMs ) .",14,0,29
dataset/preprocessed/training-data/sentiment_analysis/46,"In this work , we propose a Multi- sentimentresource Enhanced Attention Network ( MEAN ) for sentence - level sentiment classification to integrate many kinds of sentiment linguistic knowledge into deep neural networks via multi -path attention mechanism .",15,0,39
dataset/preprocessed/training-data/sentiment_analysis/46,"Specifically , we first design a coupled word embedding module to model the word representation from character - level and word - level semantics .",16,0,25
dataset/preprocessed/training-data/sentiment_analysis/46,This can help to capture the morphological information such as prefixes and suffixes of words .,17,0,16
dataset/preprocessed/training-data/sentiment_analysis/46,"Then , we propose a multisentiment - resource attention module to learn more comprehensive and meaningful sentiment - specific sentence representation by using the three types of sentiment resource words as attention sources attending to the context words respectively .",18,0,40
dataset/preprocessed/training-data/sentiment_analysis/46,"In this way , we can attend to different sentimentrelevant information from different representation subspaces implied by different types of sentiment sources and capture the over all semantics of the sentiment , negation and intensity words for sentiment prediction .",19,0,40
dataset/preprocessed/training-data/sentiment_analysis/46,The main contributions of this paper are summarized as follows .,20,0,11
dataset/preprocessed/training-data/sentiment_analysis/46,"First , we design a coupled word embedding obtained from character - level embedding and word - level embedding to capture both the character - level morphological information and word - level semantics .",21,0,34
dataset/preprocessed/training-data/sentiment_analysis/46,"Second , we propose a multi-sentiment - resource attention module to learn more comprehensive sentiment - specific sentence representation from multiply subspaces implied by three kinds of sentiment resources including sentiment lexicon , intensity words , negation words .",22,0,39
dataset/preprocessed/training-data/sentiment_analysis/46,"Finally , the experimental results show that MEAN consistently outperforms competitive methods .",23,0,13
dataset/preprocessed/training-data/sentiment_analysis/46,"Our proposed MEAN model consists of three key components : coupled word embedding module , multi-sentiment - resource attention module , sentence classifier module .",25,0,25
dataset/preprocessed/training-data/sentiment_analysis/46,"In the rest of this section , we will elaborate these three parts in details .",26,0,16
dataset/preprocessed/training-data/sentiment_analysis/46,The over all framework is shown in .,27,0,8
dataset/preprocessed/training-data/sentiment_analysis/46,Coupled Word Embedding,28,0,3
dataset/preprocessed/training-data/sentiment_analysis/46,"To exploit the sentiment - related morphological information implied by some prefixes and suffixes of words ( such as "" Non - "" , "" In - "" , "" Im - "" ) , we design a coupled word embedding learned from characterlevel embedding and word - level embedding .",29,0,51
dataset/preprocessed/training-data/sentiment_analysis/46,We first design a character - level convolution neural network ( Char - CNN ) to obtain character - level embedding .,30,0,22
dataset/preprocessed/training-data/sentiment_analysis/46,"Different from , the designed Char - CNN is a fully convolutional network without max - pooling layer to capture better semantic information in character chunk .",31,0,27
dataset/preprocessed/training-data/sentiment_analysis/46,"Specifically , we first input onehot - encoding character sequences to a 1 1 convolution layer to enhance the semantic nonlinear representation ability of our model , and the output is then fed into a multi-gram ( i.e. different window sizes ) convolution layer to capture different local character chunk information .",32,0,52
dataset/preprocessed/training-data/sentiment_analysis/46,"For word - level embedding , we use pretrained word vectors , GloVe , to map each word to a low - dimensional vector space .",33,0,26
dataset/preprocessed/training-data/sentiment_analysis/46,"Finally , each word is represented as a concatenation of the character - level embedding and word - level embedding .",34,0,21
dataset/preprocessed/training-data/sentiment_analysis/46,"This is performed on the context words and the three types of sentiment resource words 1 , resulting in four final coupled word embedding matrices :",35,0,26
dataset/preprocessed/training-data/sentiment_analysis/46,"Here , t , m , k , pare the length of the corresponding items respectively , and d is the embedding dimension .",36,0,24
dataset/preprocessed/training-data/sentiment_analysis/46,Wis normalized to better calculate the following word correlation .,38,0,10
dataset/preprocessed/training-data/sentiment_analysis/46,Multi-sentiment - resource,39,0,3
dataset/preprocessed/training-data/sentiment_analysis/46,"After obtaining the coupled word embedding , we propose a multi-sentiment - resource attention mechanism to help select the crucial sentimentresource - relevant context words to build the sentiment - specific sentence representation .",41,0,34
dataset/preprocessed/training-data/sentiment_analysis/46,"Concretely , we use the three kinds of sentiment resource words as attention sources to attend to the context words respectively , which is beneficial to capture different sentiment - relevant context words corresponding to different types of sentiment sources .",42,0,41
dataset/preprocessed/training-data/sentiment_analysis/46,"For example , using sentiment words as attention source attending to the context words helps form the sentiment - word - enhanced sentence representation .",43,0,25
dataset/preprocessed/training-data/sentiment_analysis/46,"Then , we combine the three kinds of sentiment - resource - enhanced sentence representations to learn the final sentiment - specific sentence representation .",44,0,25
dataset/preprocessed/training-data/sentiment_analysis/46,"We design three types of attention mechanisms : sentiment attention , intensity attention , negation attention to model the three kinds of sentiment resources , respectively .",45,0,27
dataset/preprocessed/training-data/sentiment_analysis/46,"In the following , we will elaborate the three types of attention mechanisms in details .",46,0,16
dataset/preprocessed/training-data/sentiment_analysis/46,"First , inspired by , we expect to establish the word - level relationship between the context words and different kinds of sentiment resource words .",47,0,26
dataset/preprocessed/training-data/sentiment_analysis/46,"To be specific , we define the dot products among the context words and the three kinds of sentiment resource words as correlation matrices .",48,0,25
dataset/preprocessed/training-data/sentiment_analysis/46,"Mathematically , the detailed formulation is described as follows .",49,0,10
dataset/preprocessed/training-data/sentiment_analysis/26,A Helping Hand : Transfer Learning for Deep Sentiment Analysis,2,1,10
dataset/preprocessed/training-data/sentiment_analysis/26,"Deep convolutional neural networks excel at sentiment polarity classification , but tend to require substantial amounts of training data , which moreover differs quite significantly between domains .",4,0,28
dataset/preprocessed/training-data/sentiment_analysis/26,"In this work , we present an approach to feed generic cues into the training process of such networks , leading to better generalization abilities given limited training data .",5,0,30
dataset/preprocessed/training-data/sentiment_analysis/26,"We propose to induce sentiment embeddings via supervision on extrinsic data , which are then fed into the model via a dedicated memorybased component .",6,0,25
dataset/preprocessed/training-data/sentiment_analysis/26,We observe significant gains in effectiveness on a range of different datasets in seven different languages .,7,0,17
dataset/preprocessed/training-data/sentiment_analysis/26,"Over the past decades , sentiment analysis has grown from an academic endeavour to an essential analytics tool .",9,1,19
dataset/preprocessed/training-data/sentiment_analysis/26,"Across the globe , people are voicing their opinion in online social media , product review sites , booking platforms , blogs , etc .",10,0,25
dataset/preprocessed/training-data/sentiment_analysis/26,"Hence , it is important to keep abreast of ongoing developments in all pertinent markets , accounting for different domains as well as different languages .",11,0,26
dataset/preprocessed/training-data/sentiment_analysis/26,"In recent years , deep neural architectures based on convolutional or recurrent layers have become established as the preeminent models for supervised sentiment polarity classification .",12,1,26
dataset/preprocessed/training-data/sentiment_analysis/26,"At the same time , it is also frequently observed that deep neural networks tend to be particularly data - hungry .",13,0,22
dataset/preprocessed/training-data/sentiment_analysis/26,"This is a problem in many real - world settings , where large amounts of training examples maybe too costly to obtain for every target domain .",14,0,27
dataset/preprocessed/training-data/sentiment_analysis/26,"A model trained on movie reviews , for instance , will fare very poorly on the task of assessing restaurant or hotel reviews , let alone tweets about politicians .",15,0,30
dataset/preprocessed/training-data/sentiment_analysis/26,"In this paper , we investigate how extrinsic signals can be incorporated into deep neural networks for sentiment analysis .",16,0,20
dataset/preprocessed/training-data/sentiment_analysis/26,Numerous papers have found the use of regular pre-trained word vector representations to be beneficial for sentiment analysis .,17,0,19
dataset/preprocessed/training-data/sentiment_analysis/26,"In our paper , we instead consider word embeddings specifically specialized for the task of sentiment analysis , studying how they can lead to stronger and more consistent gains , despite the fact that the embeddings were obtained using out - of - domain data .",18,0,46
dataset/preprocessed/training-data/sentiment_analysis/26,"An intuitive solution would be to concatenate regular embeddings , which provide semantic relatedness cues , with sentiment polarity cues thatare captured in additional dimensions .",19,0,26
dataset/preprocessed/training-data/sentiment_analysis/26,We instead propose a bespoke convolutional neural network architecture with a separate memory module dedicated to the sentiment embeddings .,20,0,20
dataset/preprocessed/training-data/sentiment_analysis/26,Our empirical study shows that the sentiment embeddings can lead to consistent gains across different datasets in a diverse set of domains and languages if a suitable neural network architecture is used .,21,0,33
dataset/preprocessed/training-data/sentiment_analysis/26,Sentiment Embedding Computation,23,0,3
dataset/preprocessed/training-data/sentiment_analysis/26,Our goal is to incorporate external cues into a deep neural network such that the network is able to generalize better even when training data is scarce .,24,0,28
dataset/preprocessed/training-data/sentiment_analysis/26,"While in computer vision , weights pre-trained on ImageNet are often used for transfer learning , the most popular way to incorporate external information into deep neural networks for text is to draw on word embeddings trained on vast amounts of word context information .",25,0,45
dataset/preprocessed/training-data/sentiment_analysis/26,"Indeed , the semantic relatedness signals provided by such representations often lead to slightly improved results in polarity classification tasks .",26,0,21
dataset/preprocessed/training-data/sentiment_analysis/26,"However , the co-occurrence - based objectives of word2vec and Glo Ve do not consider sentiment specifically .",27,0,18
dataset/preprocessed/training-data/sentiment_analysis/26,We thus seek to examine how complementary sentiment - specific information from an external source can give rise to further gains .,28,0,22
dataset/preprocessed/training-data/sentiment_analysis/26,Transfer Learning .,29,0,3
dataset/preprocessed/training-data/sentiment_analysis/26,"To this end , our goal is to induce sentiment embeddings that capture sentiment polarity signals in multiple domains and hence maybe useful across a range of different sentiment analysis tasks .",30,0,32
dataset/preprocessed/training-data/sentiment_analysis/26,The multi-domain nature of these distinguish them from the kinds of generic polarity scores captured in sentiment polarity lexicons .,31,0,20
dataset/preprocessed/training-data/sentiment_analysis/26,"We achieve this via transfer learning from trained models , benefiting from supervision on a series of sentiment polarity tasks from different domains .",32,0,24
dataset/preprocessed/training-data/sentiment_analysis/26,"Given a training collection consisting of n binary classification tasks ( e.g. , with documents inn different domains ) , we learn n corresponding polarity prediction models .",33,0,28
dataset/preprocessed/training-data/sentiment_analysis/26,"From these , we then extract token - level scores thatare tied to specific prediction outcomes .",34,0,17
dataset/preprocessed/training-data/sentiment_analysis/26,"Specifically , we train n linear models f i ( x ) = w ix + bi for tasks i = 1 , . . . , n.",35,0,28
dataset/preprocessed/training-data/sentiment_analysis/26,"Then , each vocabulary word index j is assigned a new ndimensional word vector x j = ( w 1 , j , , w n , j ) that incorporates the linear coefficients for that word across the different linear models .",36,0,43
dataset/preprocessed/training-data/sentiment_analysis/26,A minor challenge is that navely using bag - ofword features can lead to counter - intuitive weights .,37,0,19
dataset/preprocessed/training-data/sentiment_analysis/26,"If a word such as "" pleased "" in one domain mainly occurs after the word "" not "" , while the reviews in another domain primarily used "" pleased "" in its unnegated form , then "" pleased "" would be assessed as possessing opposite polarities in different domains .",38,0,51
dataset/preprocessed/training-data/sentiment_analysis/26,"To avoid this , we assume that features are preprocessed to better reflect whether words occur in a negated context .",39,0,21
dataset/preprocessed/training-data/sentiment_analysis/26,"In our experiments , we simply treat occurrences of "" not word "" as a single feature "" not word "" .",40,0,22
dataset/preprocessed/training-data/sentiment_analysis/26,"Of course , one can replace this heuristic with much more sophisticated techniques that fully account for the scope of a wider range of negation constructions .",41,0,27
dataset/preprocessed/training-data/sentiment_analysis/26,Graph - Based Extension .,42,0,5
dataset/preprocessed/training-data/sentiment_analysis/26,Most sentiment - related resources are available for the English language .,43,0,12
dataset/preprocessed/training-data/sentiment_analysis/26,"To produce vectors for other languages in our experiments , we rely on cross-lingual projection via graph - based propagation .",44,0,21
dataset/preprocessed/training-data/sentiment_analysis/26,"At this point , we have a set of initial sentiment embedding vectors ?",45,0,14
dataset/preprocessed/training-data/sentiment_analysis/26,Rn for words x ? V 0 .,47,0,8
dataset/preprocessed/training-data/sentiment_analysis/26,"We assume that we have a lexical knowledge graph G L = ( V , AL ) with anode set consisting of an extended multilingual vocabulary V ?",48,0,28
dataset/preprocessed/training-data/sentiment_analysis/26,"V 0 and a set of weighted directed arcs AL = { ( x 1 , x 1 , w 1 ) , . . . , ( x m , x m , w m ) } .",49,0,39
dataset/preprocessed/training-data/sentiment_analysis/21,Sentiment Classification using Document Embeddings trained with Cosine Similarity,2,1,9
dataset/preprocessed/training-data/sentiment_analysis/21,"In document - level sentiment classification , each document must be mapped to a fixed length vector .",4,1,18
dataset/preprocessed/training-data/sentiment_analysis/21,"Document embedding models map each document to a dense , lowdimensional vector in continuous vector space .",5,0,17
dataset/preprocessed/training-data/sentiment_analysis/21,This paper proposes training document embeddings using cosine similarity instead of dot product .,6,0,14
dataset/preprocessed/training-data/sentiment_analysis/21,"Experiments on the IMDB dataset show that accuracy is improved when using cosine similarity compared to using dot product , while using feature combination with Nave Bayes weighted bag of n-grams achieves a new state of the art accuracy of 97.42 % .",7,0,43
dataset/preprocessed/training-data/sentiment_analysis/21,Code to reproduce all experiments is available at https://github.com/tanthongtan/dv-cosine.,8,0,9
dataset/preprocessed/training-data/sentiment_analysis/21,"In document classification tasks such as sentiment classification ( in this paper we focus on binary sentiment classification of long movie reviews , i.e. determining whether each review is positive or negative ) , the choice of document representation is usually more important than the choice of classifier .",10,1,49
dataset/preprocessed/training-data/sentiment_analysis/21,"The task of text representation aims at mapping variable length texts into fixed length vectors , as to be valid inputs to a classifier .",11,0,25
dataset/preprocessed/training-data/sentiment_analysis/21,"Document embedding models achieve this by mapping each document to a dense , real - valued vector .",12,0,18
dataset/preprocessed/training-data/sentiment_analysis/21,This paper aims to improve existing document embedding models by training document embeddings using cosine similarity instead of dot product .,13,0,21
dataset/preprocessed/training-data/sentiment_analysis/21,"For example , in the basic model of trying to predict given a document - the words / n - grams in the document , instead of trying to maximize the dot product between a document vector and vectors of the words / n - grams in the document over the training set , we 'll be trying to maximize the cosine similarity instead .",14,0,65
dataset/preprocessed/training-data/sentiment_analysis/21,The motivation behind this is twofold .,15,0,7
dataset/preprocessed/training-data/sentiment_analysis/21,"Firstly , cosine similarity serves as a regularization mechanism ; by ignoring vector magnitudes , there is less incentive to increase the magnitudes of the input and output vectors , whereas in the case of dot product , vectors of frequent document - n - gram pairs can be made to have a high dot product simply by increasing the magnitudes of each vector .",16,0,65
dataset/preprocessed/training-data/sentiment_analysis/21,The weights learned should be smaller over all .,17,0,9
dataset/preprocessed/training-data/sentiment_analysis/21,"Secondly , as cosine similarity is widely used to measure document similarity , we believe our method should more directly maximize the cosine similarity between similar document vectors .",18,0,29
dataset/preprocessed/training-data/sentiment_analysis/21,"The angle between similar documents should be lower , and that may encode useful information for distinguishing between different types of documents .",19,0,23
dataset/preprocessed/training-data/sentiment_analysis/21,"We 'll compare the performance of our model on the IMDB dataset with dot product and to determine if our model serves anything beyond simple regularization , we 'll also compare it to dot product using L2 regularization .",20,0,39
dataset/preprocessed/training-data/sentiment_analysis/21,"Here we review methods of text representation , in which there are two main categories : bag of words models and neural embedding models .",22,0,25
dataset/preprocessed/training-data/sentiment_analysis/21,"The bag of words model ( Joachims , 1998 ) represents text as a fixed length vector of length equal to the number of distinct n-grams in the vocabulary .",23,0,30
dataset/preprocessed/training-data/sentiment_analysis/21,"Naive Bayes - Support Vector Machine ( NB - SVM ) utilizes nave bayes weighted bag of n-grams vectors for representing texts , feeding these vectors into a logistic regression or support vector machine classifier .",24,0,36
dataset/preprocessed/training-data/sentiment_analysis/21,"The first example of a neural embedding model is word embeddings which was proposed by , while objective functions utilizing the negative sampling technique for efficient training of word embeddings were proposed in 2013 by .",25,0,36
dataset/preprocessed/training-data/sentiment_analysis/21,"The aim of word embeddings is to map each word to are al vector , whereby the dot product between two vectors represents the amount of similarity in meaning between the words they represent .",26,0,35
dataset/preprocessed/training-data/sentiment_analysis/21,"There are two versions of word2vec : continuous bag of words ( CBOW ) , in which a neural network is trained to predict the next word in apiece of text given the word 's context , and skip - gram , where it will try to predict a word 's context given the word itself .",27,0,57
dataset/preprocessed/training-data/sentiment_analysis/21,In a 2017 paper Arora et al .,28,0,8
dataset/preprocessed/training-data/sentiment_analysis/21,"produce Sentence Embeddings by computing the weighted average of word vectors , where each word is weighted using smooth inverse frequency , and removing the first principle component .",29,0,29
dataset/preprocessed/training-data/sentiment_analysis/21,"( Le and Mikolov , 2014 ) maybe seen as a modification to word embeddings in order to embed as vectors paragraphs as opposed to words .",31,0,27
dataset/preprocessed/training-data/sentiment_analysis/21,"Paragraph vector comes in two flavors : the Distributed Memory Model of Paragraph Vectors ( PV - DM ) , and the Distributed Bag of Words version of Paragraph Vector ( PV - DBOW ) .",32,0,36
dataset/preprocessed/training-data/sentiment_analysis/21,PV - DM is basically the same as CBOW except that a paragraph vector is additionally averaged or concatenated along with the context and that whole thing is used to predict the next word .,33,0,35
dataset/preprocessed/training-data/sentiment_analysis/21,In the PV - DBOW model a paragraph vector alone is used / trained to predict the words in the paragraph .,34,0,22
dataset/preprocessed/training-data/sentiment_analysis/21,"Document Vector by predicting n-grams ( DV- ngram ) trains paragraph vectors to predict not only the words in the paragraph , but n-grams in the paragraph as well .",35,0,30
dataset/preprocessed/training-data/sentiment_analysis/21,"Weighted Neural Bag of n-grams ( W - Neural - BON ) uses an objective function similar to the one in DV - ngram , except that each log probability term is weighted using a weighing scheme which is similar to taking the absolute values of naive bayes weights .",36,0,50
dataset/preprocessed/training-data/sentiment_analysis/21,"In , they introduce three main methods of embedding n-grams .",37,0,11
dataset/preprocessed/training-data/sentiment_analysis/21,"The first is context guided n-gram representation ( CGNR ) , which is training n-gram vectors to predict its context ngrams .",38,0,22
dataset/preprocessed/training-data/sentiment_analysis/21,"The second is label guided n-gram representation ( LGNR ) , which is predicting given an n-gram the label of the document to which it belongs .",39,0,27
dataset/preprocessed/training-data/sentiment_analysis/21,"The last is text guided n-gram representation ( TGNR ) , which is predicting given an n-gram the document to which it belongs .",40,0,24
dataset/preprocessed/training-data/sentiment_analysis/21,Embeddings from Language Models ( ELMo ) learns contextualized word embeddings by training a bidirectional LSTM on the language modelling task of predicting the next word as well as the previous word .,41,0,33
dataset/preprocessed/training-data/sentiment_analysis/21,"Bidirectional Encoder Representations from Transformers ( BERT ) uses the masked language model objective , which is predicting the masked word given the left and right context , in order to pre-train a multi - layer bidirectional Transformer .",42,0,39
dataset/preprocessed/training-data/sentiment_analysis/21,BERT also jointly pre-trains text - pair representations by using a next sentence prediction objective .,43,0,16
dataset/preprocessed/training-data/sentiment_analysis/21,For the rest of this section we 'll look at other research which replaces dot product with cosine similarity .,44,0,20
dataset/preprocessed/training-data/sentiment_analysis/21,"In the context of fully - connected neural networks and convolutional neural networks , uses cosine similarity instead of dot product in computing a layer 's pre-activation as a regularization mechanism .",45,0,32
dataset/preprocessed/training-data/sentiment_analysis/21,"Using a special dataset where each instance is a paraphrase pair , trains word vectors in such away that the cosine similarity between the resultant document vectors of a paraphrase pair is directly maximized .",46,0,35
dataset/preprocessed/training-data/sentiment_analysis/21,"In learning neural n-gram and document embeddings , a dot product between the input vector and the output vector is generally used to compute the similarity measure between the two vectors , i.e. ' similar ' vectors should have a high dot product .",48,0,44
dataset/preprocessed/training-data/sentiment_analysis/21,In this paper we explore using cosine similarity instead of dot product in computing the similarity measure between the input and output vectors .,49,0,24
dataset/preprocessed/training-data/sentiment_analysis/35,Exploiting Coarse - to - Fine Task Transfer for Aspect - level Sentiment Classification,2,1,14
dataset/preprocessed/training-data/sentiment_analysis/35,"Aspect - level sentiment classification ( ASC ) aims at identifying sentiment polarities towards aspects in a sentence , where the aspect can behave as a general Aspect Category ( AC ) or a specific Aspect Term ( AT ) .",4,1,41
dataset/preprocessed/training-data/sentiment_analysis/35,"However , due to the especially expensive and labor - intensive labeling , existing public corpora in AT - level are all relatively small .",5,0,25
dataset/preprocessed/training-data/sentiment_analysis/35,"Meanwhile , most of the previous methods rely on complicated structures with given scarce data , which largely limits the efficacy of the neural models .",6,0,26
dataset/preprocessed/training-data/sentiment_analysis/35,"In this paper , we exploit a new direction named coarse - to - fine task transfer , which aims to leverage knowledge learned from a rich - resource source domain of the coarse - grained AC task , which is more easily accessible , to improve the learning in a low - resource target domain of the fine - grained AT task .",7,0,64
dataset/preprocessed/training-data/sentiment_analysis/35,"To resolve both the aspect granularity inconsistency and feature mismatch between domains , we propose a Multi - Granularity Alignment Network ( MGAN ) .",8,0,25
dataset/preprocessed/training-data/sentiment_analysis/35,"In MGAN , a novel Coarse 2 Fine attention guided by an auxiliary task can help the AC task modeling at the same finegrained level with the AT task .",9,0,30
dataset/preprocessed/training-data/sentiment_analysis/35,"To alleviate the feature false alignment , a contrastive feature alignment method is adopted to align aspect - specific feature representations semantically .",10,0,23
dataset/preprocessed/training-data/sentiment_analysis/35,"In addition , a large - scale multi-domain dataset for the AC task is provided .",11,0,16
dataset/preprocessed/training-data/sentiment_analysis/35,"Empirically , extensive experiments demonstrate the effectiveness of the MGAN .",12,0,11
dataset/preprocessed/training-data/sentiment_analysis/35,Source domain ( Restaurant - R1 ) Target domain ( Laptop - L ) resolution Aspect term :,13,0,18
dataset/preprocessed/training-data/sentiment_analysis/35,Polarity : Positive,14,0,3
dataset/preprocessed/training-data/sentiment_analysis/35,Aspect - level sentiment classification ( ASC ) aims to infer sentiment polarities over aspect categories ( AC ) or aspect terms ( AT ) distributed in sentences ) .,16,0,30
dataset/preprocessed/training-data/sentiment_analysis/35,"An aspect category implicitly appears in the sentence , which describes a general category of the entities .",17,0,18
dataset/preprocessed/training-data/sentiment_analysis/35,"For example , in the sentence "" The salmon is tasty while the waiter is very rude "" , the user speaks positively and negatively towards two aspect categories "" food "" and "" service "" , respectively .",18,0,39
dataset/preprocessed/training-data/sentiment_analysis/35,An aspect term characterizes a specific entity that explicitly occurs in the sentence .,19,0,14
dataset/preprocessed/training-data/sentiment_analysis/35,"Considering the same sentence "" The salmon is tasty while the waiter is very rude "" , the aspect terms are "" salmon "" and "" waiter "" , and the user expresses positive and negative sentiments over them , respectively .",20,0,42
dataset/preprocessed/training-data/sentiment_analysis/35,"In terms of the aspect granularity , the AC task is coarse - grained while the AT task is fine - grained .",21,0,23
dataset/preprocessed/training-data/sentiment_analysis/35,"To model aspect - oriented sentiment analysis , equipping Recurrent Neural Networks ( RNNs ) with the attention Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",22,1,33
dataset/preprocessed/training-data/sentiment_analysis/35,All rights reserved .,23,0,4
dataset/preprocessed/training-data/sentiment_analysis/35,"mechanism has become a mainstream approach , where RNNs aim to capture sequential patterns and the attention mechanism is to emphasize appropriate context features for encoding aspect - specific representations .",24,0,31
dataset/preprocessed/training-data/sentiment_analysis/35,"Typically , attention - based RNN models can achieve good performance only when large corpora are available .",25,0,18
dataset/preprocessed/training-data/sentiment_analysis/35,"However , AT - level datasets require the aspect terms to be comprehensively manually labeled or extracted by sequence labeling algorithms from the sentences , which is especially costly to obtain .",26,0,32
dataset/preprocessed/training-data/sentiment_analysis/35,"Thus , existing public AT - level datasets are all relatively small , which limits the potential of neural models .",27,0,21
dataset/preprocessed/training-data/sentiment_analysis/35,"Nonetheless , we observe that plentiful AC - level corpora are more easily accessible .",28,0,15
dataset/preprocessed/training-data/sentiment_analysis/35,This is because that aspect categories are usually in a small set of general aspects that can be pre-defined .,29,0,20
dataset/preprocessed/training-data/sentiment_analysis/35,"For example , commercial services such as review sites or social media can define a set of valuable aspect categories towards products or events in a particular domain ( e.g. , "" food "" , "" service "" , "" speed "" , and "" price "" in the Restaurant domain ) .",30,0,53
dataset/preprocessed/training-data/sentiment_analysis/35,"As a result , the mass collections of user preferences towards different aspect categories become practicable .",31,0,17
dataset/preprocessed/training-data/sentiment_analysis/35,"Motivated by this observation , we propose a new problem named coarse - to - fine task transfer across both domain and granularity , with the aim of borrowing knowledge from an abundant source domain of the coarse - grained AC task to a small - scale target domain of the fine - grained AT task .",32,0,57
dataset/preprocessed/training-data/sentiment_analysis/35,The challenges in fulfillment of this setting are two - fold : ( 1 ) task discrepancy : the two tasks concern with the aspects with different granularity .,33,0,29
dataset/preprocessed/training-data/sentiment_analysis/35,"Source aspects are coarse - grained aspect categories , which lack a priori position information in the context .",34,0,19
dataset/preprocessed/training-data/sentiment_analysis/35,"However , target aspects are fine - grained aspect terms , which have accurate position information .",35,0,17
dataset/preprocessed/training-data/sentiment_analysis/35,"Thus , inconsistent granularity in aspects causes the discrepancy between tasks ; ( 2 ) feature distribution discrepancy : generally the domains in the two tasks are different , which causes the distribution shift for both the aspects and its context between domains .",36,0,44
dataset/preprocessed/training-data/sentiment_analysis/35,"For example , in the source Restaurant domain , tasty and delicious are used to express positive sentiment towards the aspect category "" food "" , while lightweight and responsive often indicate positive sentiment towards the aspect term "" mouse "" in the target Laptop domain .",37,0,47
dataset/preprocessed/training-data/sentiment_analysis/35,"To resolve the challenges , we propose a novel framework named Multi - Granularity Alignment Network ( MGAN ) to simultaneously align aspect granularity and aspect- specific feature representations across domains .",38,0,32
dataset/preprocessed/training-data/sentiment_analysis/35,"Specifically , the MGAN consists of two networks for learning aspect - specific representations for the two domains , respectively .",39,0,21
dataset/preprocessed/training-data/sentiment_analysis/35,"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",40,0,67
dataset/preprocessed/training-data/sentiment_analysis/35,"Without any additional labeling , the C2F attention module can learn the coarse - to - fine process by an auxiliary task .",41,0,23
dataset/preprocessed/training-data/sentiment_analysis/35,"Actually , more specific aspect terms and their position information are most directly pertinent to the expression of sentiment .",42,0,20
dataset/preprocessed/training-data/sentiment_analysis/35,"The C2F module makes up these missing information for the source task , which effectively reduces the aspect granularity gap between tasks and facilitates the subsequent feature alignment .",43,0,29
dataset/preprocessed/training-data/sentiment_analysis/35,"Second , considering that a sentence may contain multiple aspects with different sentiments , thus capturing incorrect sentiment features towards the aspect can mislead feature alignment .",44,0,27
dataset/preprocessed/training-data/sentiment_analysis/35,"To prevent false alignment , we adopt the Contrastive Feature Alignment ( CFA ) ( Motiian et al. 2017 ) to semantically align aspect - specific representations .",45,0,28
dataset/preprocessed/training-data/sentiment_analysis/35,"The CFA considers both semantic alignment by maximally ensuring the equivalent distributions from different domains but the same class , and semantic separation by guaranteeing distributions from both different classes and domains to be as dissimilar as possible .",46,0,39
dataset/preprocessed/training-data/sentiment_analysis/35,"Moreover , we build a large - scale multidomain dataset named YelpAspect with 100K samples for each domain to serve as highly beneficial source domains .",47,0,26
dataset/preprocessed/training-data/sentiment_analysis/35,"Empirically , extensive experiments demonstrate that the proposed MGAN model can achieve superior performances on two AT - level datasets from SemEval ' 14 ABSA challenge and an ungrammatical AT - level twitter dataset .",48,0,35
dataset/preprocessed/training-data/sentiment_analysis/35,"Our contributions of this paper are four - fold : ( 1 ) to the best of our knowledge , a novel transfer setting cross both domain and granularity is first proposed for aspect - level sentiment analysis ; ( 2 ) a new large - scale , multi-domain AC - level dataset is constructed ; ( 3 ) the novel Coarse 2 Fine attention is proposed to effectively reduce the aspect granularity gap between tasks ; ( 4 ) empirical studies verify the effectiveness of the proposed model on three AT - level benchmarks .",49,0,96
dataset/preprocessed/training-data/sentiment_analysis/30,Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect - based Sentiment Analysis,2,1,14
dataset/preprocessed/training-data/sentiment_analysis/30,"While neural networks have been shown to achieve impressive results for sentence - level sentiment analysis , targeted aspect - based sentiment analysis ( TABSA ) - extraction of finegrained opinion polarity w.r.t. a pre-defined set of aspects - remains a difficult task .",4,1,44
dataset/preprocessed/training-data/sentiment_analysis/30,"Motivated by recent advances in memoryaugmented models for machine reading , we propose a novel architecture , utilising external "" memory chains "" with a delayed memory update mechanism to track entities .",5,0,33
dataset/preprocessed/training-data/sentiment_analysis/30,"On a TABSA task , the proposed model demonstrates substantial improvements over state - of the - art approaches , including those using external knowledge bases .",6,0,27
dataset/preprocessed/training-data/sentiment_analysis/30,Targeted aspect - based sentiment analysis ( TABSA ) is the task of identifying fine - grained opinion polarity towards a specific aspect associated with a given target .,9,0,29
dataset/preprocessed/training-data/sentiment_analysis/30,"The task requires classification of opinions on different entities across a range of different attributes , with the expectation that there will be no overt opinion expressed on a given entity for many attributes .",10,0,35
dataset/preprocessed/training-data/sentiment_analysis/30,"This can be seen in Example ( 1 ) , e.g. , where opinions on the aspects SAFETY and PRICE are expressed for entity LOC1 but not entity LOC2 : 2 ( 1 ) LOC1 is your best bet for secure although expensive and LOC2 is too far .",11,0,49
dataset/preprocessed/training-data/sentiment_analysis/30,"Note that in our dataset , all entity mentions have been pre-nomalised to LOCn , where n is an index .",13,0,21
dataset/preprocessed/training-data/sentiment_analysis/30,"The earliest work on ( T ) ABSA relied heavily on feature engineering , but more recent work based on deep learning has used models such as LSTMs to automatically learn aspect - specific word and sentence representations .",14,0,39
dataset/preprocessed/training-data/sentiment_analysis/30,"Despite these successes , keeping track of multiple entity - aspect pairs remains a difficult task , even for an LSTM .",15,0,22
dataset/preprocessed/training-data/sentiment_analysis/30,"As reported in , a target - dependent biLSTM is ineffective , both in terms of aspect detection and sentiment classification , compared to a simple logistic regression model with n-gram features .",16,0,33
dataset/preprocessed/training-data/sentiment_analysis/30,"Intuitively , we would expect that a model which better captures linguistic structure via the original word sequencing should perform better , which provides the motivation for this research .",17,0,30
dataset/preprocessed/training-data/sentiment_analysis/30,"More recently , successful works in ( T ) ABSA have explored the idea of leveraging external memory .",18,0,19
dataset/preprocessed/training-data/sentiment_analysis/30,"Their models are largely based on memory networks , originally developed for reasoning - focused machine reading comprehension tasks .",19,0,20
dataset/preprocessed/training-data/sentiment_analysis/30,"In contrast to memory networks , where each input sentence / word occupies a memory slot and is then accessed via attention independently , recent advances in machine reading suggest that processing inputs sequentially is beneficial to over all performance .",20,0,41
dataset/preprocessed/training-data/sentiment_analysis/30,"However , successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of inputs between the two tasks : on the Children 's Book Test corpus ( CBT ) , for example , competitive models take as input a window of text , centred around candidate entities , with crucial information contained within that window .",21,0,65
dataset/preprocessed/training-data/sentiment_analysis/30,"In TABSA , given the fine - grained nature of the task , it is common practice for models to operate at the word - rather than chunk / sentencelevel .",22,0,31
dataset/preprocessed/training-data/sentiment_analysis/30,"It is not uncommon to see examples like Example ( 1 ) , where the sentence starts with LOC1 , but the negative PRICE sentiment towards the entity is not expressed until much later .",23,0,35
dataset/preprocessed/training-data/sentiment_analysis/30,"Moreover , phrases such as best bet and although play the role of triggers , indicating that succeeding tokens bear aspect / sentiment signal .",24,0,25
dataset/preprocessed/training-data/sentiment_analysis/30,This key difference necessitates the ability to model the delayed activation of memory updates .,25,0,15
dataset/preprocessed/training-data/sentiment_analysis/30,"In this work , we propose a novel model architecture for TABSA , augmented with multiple "" memory chains "" , and equipped with a delayed memory update mechanism , to keep track of numerous entities independently .",26,0,38
dataset/preprocessed/training-data/sentiment_analysis/30,"We evaluate the effectiveness of the proposed model over the task of TABSA , and achieve substantial improvements over a number of baselines , including one incorporating external knowledge bases , setting a new state of the art in both sentiment classification and aspect detection .",27,0,46
dataset/preprocessed/training-data/sentiment_analysis/30,Task description .,29,0,3
dataset/preprocessed/training-data/sentiment_analysis/30,"In TABSA , a sentence s typically consists of a sequence of words : {w 1 , . . . , w i , . . . , w m } where w i denotes words interleaved with one or more targets ( t ) , which we assume to be pre-identified as with LOC1 and LOC2 in Example ( 1 ) .",30,0,63
dataset/preprocessed/training-data/sentiment_analysis/30,"Following , we frame the task as a 3 - class classification problem : given a sentence s , a pre-identified set of target entities T and fixed set of aspects A , predict the sentiment polarity y ?",31,0,39
dataset/preprocessed/training-data/sentiment_analysis/30,"{ positive , negative , none } over the full set of target - aspect pairs { ( t , a ) : t ? T , a ?",32,0,29
dataset/preprocessed/training-data/sentiment_analysis/30,"A }. For example , ( LOC1 , SAFETY ) has goldstandard polarity positive , while ( LOC1 , TRANSIT - LOCATION ) has polarity none .",33,0,27
dataset/preprocessed/training-data/sentiment_analysis/30,Proposed model .,34,0,3
dataset/preprocessed/training-data/sentiment_analysis/30,"To this end , we design a neural network architecture , capable of tracking and updating the states of entities at the right time with external memory , making it a natural fit for the task .",35,0,37
dataset/preprocessed/training-data/sentiment_analysis/30,"Specifically , our model maintains a number of "" memory chains "" h j , one for each entity with the key k j and dynamically updates the states ( h j ) of them as it progresses through the sentence with the help of the delay recurrence d j , taking previous activations into account .",36,0,57
dataset/preprocessed/training-data/sentiment_analysis/30,An illustration of our model is provided in . :,37,0,10
dataset/preprocessed/training-data/sentiment_analysis/30,"Illustration of our model with a single memory chain at time i. ? , ?",38,0,15
dataset/preprocessed/training-data/sentiment_analysis/30,"and GRU represent Equations , and , while circled nodes L , C , and + depict the location , content terms , Hadamard product , and addition , resp .",39,0,31
dataset/preprocessed/training-data/sentiment_analysis/30,Delayed memory update .,40,0,4
dataset/preprocessed/training-data/sentiment_analysis/30,"Update of each memory chain is controlled by a gating mechanism , consisting of three components : the "" content "" term w i h j i ?1 , the "" location "" term w i k j and the "" delay "" term vd j i where d j i carries knowledge regarding previous activation of the gate and v is a trainable parameter vector .",41,0,67
dataset/preprocessed/training-data/sentiment_analysis/30,"All three terms may lead to the activation of g j i , but differ in how they turn the gate on .",42,0,23
dataset/preprocessed/training-data/sentiment_analysis/30,"While the "" location "" term causes the gate to open for memory chains whose keys ( k j ) match the input , the "" content "" term triggers the activation when the content of the entities ( h j i?1 ) matches the input .",43,0,47
dataset/preprocessed/training-data/sentiment_analysis/30,The delay term models how and when the gate was turned on in the past with a GRU and how past activations should influence the current one .,44,0,28
dataset/preprocessed/training-data/sentiment_analysis/30,"More formally , with arrows denoting processing direction , the update gate is defined as :",45,0,16
dataset/preprocessed/training-data/sentiment_analysis/30,"g j i is the update gate value for the j - th memory at time i , 3 k j is the embedding for the jth entity ( key ) , ? ? h j i?1 is the hidden memory representation responsible for keeping track of the state of the j - th entity ( content ) , and ?",47,0,61
dataset/preprocessed/training-data/sentiment_analysis/30,is the sigmoid activation function .,48,0,6
dataset/preprocessed/training-data/sentiment_analysis/30,The delay recurrence,49,0,3
dataset/preprocessed/training-data/sentiment_analysis/23,Discriminative Neural Sentence Modeling by Tree - Based Convolution,2,1,9
dataset/preprocessed/training-data/sentiment_analysis/23,This paper proposes a tree - based convolutional neural network ( TBCNN ) for discriminative sentence modeling .,4,0,18
dataset/preprocessed/training-data/sentiment_analysis/23,Our models leverage either constituency trees or dependency trees of sentences .,5,0,12
dataset/preprocessed/training-data/sentiment_analysis/23,"The treebased convolution process extracts sentences ' structural features , and these features are aggregated by max pooling .",6,0,19
dataset/preprocessed/training-data/sentiment_analysis/23,"Such architecture allows short propagation paths between the output layer and underlying feature detectors , which enables effective structural feature learning and extraction .",7,0,24
dataset/preprocessed/training-data/sentiment_analysis/23,We evaluate our models on two tasks : sentiment analysis and question classification .,8,0,14
dataset/preprocessed/training-data/sentiment_analysis/23,"In both experiments , TBCNN outperforms previous state - of the - art results , including existing neural networks and dedicated feature / rule engineering .",9,0,26
dataset/preprocessed/training-data/sentiment_analysis/23,"We also make efforts to visualize the tree - based convolution process , shedding light on how our models work .",10,0,21
dataset/preprocessed/training-data/sentiment_analysis/23,"Discriminative sentence modeling aims to capture sentence meanings , and classify sentences according to certain criteria ( e.g. , sentiment ) .",12,0,22
dataset/preprocessed/training-data/sentiment_analysis/23,"It is related to various tasks of interest , and has attracted much attention in the NLP community .",13,0,19
dataset/preprocessed/training-data/sentiment_analysis/23,"Feature engineering - for example , n-gram features , dependency subtree features , or more dedicated ones - can play an important role in modeling sentences .",14,0,27
dataset/preprocessed/training-data/sentiment_analysis/23,"Kernel machines , e.g. , SVM , are exploited in and by specifying a certain measure of similarity between sentences , without explicit feature representation .",15,0,26
dataset/preprocessed/training-data/sentiment_analysis/23,"Recent advances of neural networks bring new techniques in understanding natural languages , and have exhibited considerable potential .",16,0,19
dataset/preprocessed/training-data/sentiment_analysis/23,"and propose unsupervised approaches to learn word embeddings , mapping discrete words to real - valued vectors in a meaning space .",17,0,22
dataset/preprocessed/training-data/sentiment_analysis/23,extend such approaches to learn sentences ' and paragraphs ' representations .,18,0,12
dataset/preprocessed/training-data/sentiment_analysis/23,"Compared with human engineering , neural networks serve as away of automatic feature learning .",19,0,15
dataset/preprocessed/training-data/sentiment_analysis/23,Two widely used neural sentence models are convolutional neural networks ( CNNs ) and recursive neural networks ( RNNs ) .,20,0,21
dataset/preprocessed/training-data/sentiment_analysis/23,"CNNs can extract words ' neighboring features effectively with short propagation paths , but they do not capture inherent sentence structures ( e.g. , parsing trees ) .",21,0,28
dataset/preprocessed/training-data/sentiment_analysis/23,"RNNs encode , to some extent , structural information by recursive semantic composition along a parsing tree .",22,0,18
dataset/preprocessed/training-data/sentiment_analysis/23,"However , they may have difficulties in learning deep dependencies because of long propagation paths ) .",23,0,17
dataset/preprocessed/training-data/sentiment_analysis/23,"( CNNs / RNNs and a variant , recurrent networks , will be reviewed in Section 2 . )",24,0,19
dataset/preprocessed/training-data/sentiment_analysis/23,"A curious question is whether we can combine the advantages of CNNs and RNNs , i.e. , whether we can exploit sentence structures ( like RNNs ) effectively with short propagation paths ( like CNNs ) .",25,0,37
dataset/preprocessed/training-data/sentiment_analysis/23,"In this paper , we propose a novel neural architecture for discriminative sentence modeling , called the Tree - Based Convolutional Neural Network ( TBCNN ) .",26,0,27
dataset/preprocessed/training-data/sentiment_analysis/23,"Our models can leverage different sentence parsing trees , e.g. , constituency trees and dependency trees .",27,0,17
dataset/preprocessed/training-data/sentiment_analysis/23,"The model variants are denoted as c- TBCNN and d - TBCNN , respectively .",28,0,15
dataset/preprocessed/training-data/sentiment_analysis/23,"The idea of tree - based convolution is to apply a set of subtree feature detectors , sliding over the entire parsing tree of a sentence ; then pooling aggregates these extracted feature vectors by taking the maximum value in each dimension .",29,0,43
dataset/preprocessed/training-data/sentiment_analysis/23,"One merit of such architecture is that all features , along the tree , have short propagation paths to the output layer , and hence structural information can be learned effectively .",30,0,32
dataset/preprocessed/training-data/sentiment_analysis/23,"TBCNNs are evaluated on two tasks , sentiment analysis and question classification ; our models have outperformed previous state - of - the - art results in both experiments .",31,0,30
dataset/preprocessed/training-data/sentiment_analysis/23,"To understand how TBCNNs work , we also visualize the network by plotting the convolution process .",32,0,17
dataset/preprocessed/training-data/sentiment_analysis/23,We make our code and results available on our project website .,33,0,12
dataset/preprocessed/training-data/sentiment_analysis/23,Background and Related Work,35,0,4
dataset/preprocessed/training-data/sentiment_analysis/23,"In this section , we present the background and related work regarding two prevailing neural architectures for discriminative sentence modeling .",36,0,21
dataset/preprocessed/training-data/sentiment_analysis/23,Convolutional Neural Networks,37,0,3
dataset/preprocessed/training-data/sentiment_analysis/23,"Convolutional neural networks ( CNNs ) , early used for image processing , turnout to be effective with natural languages as well .",38,0,23
dataset/preprocessed/training-data/sentiment_analysis/23,depicts a classic convolution process on a sentence .,39,0,9
dataset/preprocessed/training-data/sentiment_analysis/23,"A set of fixed - width - window feature detectors slide over the sentence , and output the extracted features .",40,0,21
dataset/preprocessed/training-data/sentiment_analysis/23,"Let t be the window size , and x 1 , , x t ?",41,0,15
dataset/preprocessed/training-data/sentiment_analysis/23,R ne be n e - dimensional word embeddings .,42,0,10
dataset/preprocessed/training-data/sentiment_analysis/23,"The output of convolution , evaluated at the current position , is",43,0,12
dataset/preprocessed/training-data/sentiment_analysis/23,where y ?,44,0,3
dataset/preprocessed/training-data/sentiment_analysis/23,R nc ( n c is the number of feature detectors ) .,45,0,13
dataset/preprocessed/training-data/sentiment_analysis/23,W ? R nc ( tne ) and b ?,46,0,10
dataset/preprocessed/training-data/sentiment_analysis/23,R nc are parameters ; f is the activation function .,47,0,11
dataset/preprocessed/training-data/sentiment_analysis/23,Semicolons represent column vector concatenation .,48,0,6
dataset/preprocessed/training-data/sentiment_analysis/23,"After convolution , the extracted features are pooled to a fixedsize vector for classification .",49,0,15
dataset/preprocessed/training-data/sentiment_analysis/34,Mazajak : An Online Arabic Sentiment Analyser,2,1,7
dataset/preprocessed/training-data/sentiment_analysis/34,Sentiment analysis ( SA ) is one of the most useful natural language processing applications .,4,1,16
dataset/preprocessed/training-data/sentiment_analysis/34,"Literature is flooding with many papers and systems addressing this task , but most of the work is focused on English .",5,0,22
dataset/preprocessed/training-data/sentiment_analysis/34,"In this paper , we present "" Mazajak "" , an online system for Arabic SA .",6,0,17
dataset/preprocessed/training-data/sentiment_analysis/34,"The system is based on a deep learning model , which achieves state - of - theart results on many Arabic dialect datasets including SemEval 2017 and ASTD .",7,0,29
dataset/preprocessed/training-data/sentiment_analysis/34,The availability of such system should assist various applications and research that rely on sentiment analysis as a tool .,8,0,20
dataset/preprocessed/training-data/sentiment_analysis/34,Sentiment analysis ( SA ) can be defined as the process of extracting and analysing the sentiment and polarity in a given piece of text .,10,0,26
dataset/preprocessed/training-data/sentiment_analysis/34,It is one of the tasks in the larger natural language processing ( NLP ) field .,11,0,17
dataset/preprocessed/training-data/sentiment_analysis/34,"The rapid and wide increase in the use of social media platforms , and the reliance on online shopping and marketing resulted in a flood of information .",12,0,28
dataset/preprocessed/training-data/sentiment_analysis/34,Many researchers started analysing and mining data for the task of public opinion mining .,13,0,15
dataset/preprocessed/training-data/sentiment_analysis/34,Sentiment analysis is one of the vital approaches to extract public opinion from large corpora of text .,14,1,18
dataset/preprocessed/training-data/sentiment_analysis/34,Companies can benefit from understanding the feedback of their costumers and their opinions .,15,0,14
dataset/preprocessed/training-data/sentiment_analysis/34,Governments as well can use it to understand the reaction of people to their policies and actions .,16,0,18
dataset/preprocessed/training-data/sentiment_analysis/34,"Work on SA started in early 2000s , particularly with the work of , where they studied the sentiment of movies ' reviews .",17,1,24
dataset/preprocessed/training-data/sentiment_analysis/34,The work has developed since then and it spanned different topics and fields such as social media .,18,0,18
dataset/preprocessed/training-data/sentiment_analysis/34,SA gained a lot of interest from researchers who recognised its importance and benefits .,19,0,15
dataset/preprocessed/training-data/sentiment_analysis/34,"However , most of the work is focused on English whereas Arabic did not receive much attention until recently , but it still lacks behind due to the many challenges of the Arabic language ; including the large variety in dialects and the complex morphology of the language .",20,0,49
dataset/preprocessed/training-data/sentiment_analysis/34,"Recently , the world witnessed a strong revolution in deep learning which was the driving force for many improvements in many fields .",21,0,23
dataset/preprocessed/training-data/sentiment_analysis/34,"The work on English NLP started utilising deep learning models from an early stage , then followed by Arabic NLP .",22,0,21
dataset/preprocessed/training-data/sentiment_analysis/34,The utilis ation of deep learning for Arabic SA started to receive more attention recently showing significant improvement in performance .,23,0,21
dataset/preprocessed/training-data/sentiment_analysis/34,"While there is a considerable amount of work that studies Arabic SA , to the best of our knowledge , there is no existing open - source tool for Arabic SA that could be used directly .",24,0,37
dataset/preprocessed/training-data/sentiment_analysis/34,"The only work that we are aware of is SentiStrength 1 , which is mainly developed for English , but supports other languages including Arabic .",25,0,26
dataset/preprocessed/training-data/sentiment_analysis/34,"However , it uses a basic dictionary - based approach that works with Arabic MSA and terribly fails with dialects which is the main language used in social media .",26,0,30
dataset/preprocessed/training-data/sentiment_analysis/34,"In this paper , we present Mazajak 2 , an Online Arabic sentiment analysis system that utilises deep learning and massive Arabic word embeddings .",27,0,25
dataset/preprocessed/training-data/sentiment_analysis/34,The system is available as an online API that can be used by other researchers .,28,0,16
dataset/preprocessed/training-data/sentiment_analysis/34,"The literature of Arabic SA has many attempts to tackle the problem , however most of the work is based on conventional machine learning algorithms with few attempts to use deep learning .",30,0,33
dataset/preprocessed/training-data/sentiment_analysis/34,A recent publication presents a comprehensive survey on Arabic SA .,31,0,11
dataset/preprocessed/training-data/sentiment_analysis/34,"In , the authors proposed an aspect - based SA system for Arabic hotel reviews , in which they used SVM and recurrent neural networks ( RNNs ) .",32,0,29
dataset/preprocessed/training-data/sentiment_analysis/34,"In another work , the authors applied SA on tweets using Naive Bayes ( NB ) and KNN , they achieved relatively good results .",33,0,25
dataset/preprocessed/training-data/sentiment_analysis/34,also created a large lexicon of Arabic terms extracted from news articles .,34,0,13
dataset/preprocessed/training-data/sentiment_analysis/34,"Based on their lexicon , they built an SA system and tested it on data collected from Twitter .",35,0,19
dataset/preprocessed/training-data/sentiment_analysis/34,"In , the authors aimed to tackle the problem of dialects .",36,0,12
dataset/preprocessed/training-data/sentiment_analysis/34,They built a slang sentimental words and idioms lexicon ( SSWIL ) and conducted some experiments using SVM and the new lexicon .,37,0,23
dataset/preprocessed/training-data/sentiment_analysis/34,"In the realm of social media analysis , the work in ( Abdulla et al. , 2013 ) introduced a dataset of 2000 tweets , which the authors used to conduct an experiment with lexicon - based and ML - based systems .",38,0,43
dataset/preprocessed/training-data/sentiment_analysis/34,They found that combining both approaches would achieve better results .,39,0,11
dataset/preprocessed/training-data/sentiment_analysis/34,proposed an SA system for social media .,40,0,8
dataset/preprocessed/training-data/sentiment_analysis/34,"In their work , they experimented and studied a large variety of features .",41,0,14
dataset/preprocessed/training-data/sentiment_analysis/34,They also studied the effect of the dialects and morphological richness of Arabic .,42,0,14
dataset/preprocessed/training-data/sentiment_analysis/34,"Moreover , In ( Abdul - Mageed , 2017a , b ) , the authors studied the different ways to handle the Arabic morphological richness for SA .",43,0,28
dataset/preprocessed/training-data/sentiment_analysis/34,"They studied the effect of segmentation in representing the lexical input , also they tried to study the weight and importance of these segments for SA .",44,0,27
dataset/preprocessed/training-data/sentiment_analysis/34,"In SemEval 2017 , a sentiment analysis task was presented that included Arabic .",45,0,14
dataset/preprocessed/training-data/sentiment_analysis/34,were ranked first in SemEval 2017 task for Arabic SA .,46,0,11
dataset/preprocessed/training-data/sentiment_analysis/34,"They used a set of hand - engineered and lexicon - based features , the classifier of choice was a complement NB classifier .",47,0,24
dataset/preprocessed/training-data/sentiment_analysis/34,"The second rank in the same task was for the work of Jabreel and Moreno ( 2017 ) , who introduced a rich set of features thatare mostly based on bag of words ( BoW ) model in addition to some features extracted from word embeddings .",48,0,47
dataset/preprocessed/training-data/sentiment_analysis/34,They used SVM as their classification algorithm .,49,0,8
dataset/preprocessed/training-data/sentiment_analysis/32,Interactive Attention Networks for Aspect - Level Sentiment Classification,2,1,9
dataset/preprocessed/training-data/sentiment_analysis/32,Aspect - level sentiment classification aims at identifying the sentiment polarity of specific target in its context .,4,0,18
dataset/preprocessed/training-data/sentiment_analysis/32,Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating target - specific representations .,5,1,30
dataset/preprocessed/training-data/sentiment_analysis/32,"However , these studies always ignore the separate modeling of targets .",6,0,12
dataset/preprocessed/training-data/sentiment_analysis/32,"In this paper , we argue that both targets and contexts deserve special treatment and need to be learned their own representations via interactive learning .",7,0,26
dataset/preprocessed/training-data/sentiment_analysis/32,"Then , we propose the interactive attention networks ( IAN ) to interactively learn attentions in the contexts and targets , and generate the representations for targets and contexts separately .",8,0,31
dataset/preprocessed/training-data/sentiment_analysis/32,"With this design , the IAN model can well represent a target and its collocative context , which is helpful to sentiment classification .",9,0,24
dataset/preprocessed/training-data/sentiment_analysis/32,Experimental results on Se -m Eval 2014 Datasets demonstrate the effectiveness of our model .,10,0,15
dataset/preprocessed/training-data/sentiment_analysis/32,Aspect - level sentiment classification is a fine - grained task in sentiment analysis which aims to identify the sentiment polarity of targets in their context .,12,0,27
dataset/preprocessed/training-data/sentiment_analysis/32,"For example , Given the mentioned targets : staff , pizza and beef cubes , and their context sentence "" a group of friendly staff , the pizza is not bad , but the beef cubes are not worth the money ! "" , the sentiment polarity for the three targets , staff , pizza and beef cubes , are positive , neutral and negative respectively .",13,0,67
dataset/preprocessed/training-data/sentiment_analysis/32,Aspect - level sentiment classification is a fundamental task in natural language processing and catches many researchers ' attention .,14,0,20
dataset/preprocessed/training-data/sentiment_analysis/32,"Traditional approaches mainly focus on designing a set of features such as bag - of - words , sentiment lexicon to train a classifier ( e.g. , SVM ) for aspect - level sentiment classification .",15,0,36
dataset/preprocessed/training-data/sentiment_analysis/32,"However , feature engineering is labor intensive and almost reaches its performance bottleneck .",16,0,14
dataset/preprocessed/training-data/sentiment_analysis/32,"With the development of deep learning techniques , some researchers have designed effective neural networks to automatically generate useful lowdimensional representations from targets and their contexts and obtain a promising result on the aspect - level sentiment classification task .",17,0,40
dataset/preprocessed/training-data/sentiment_analysis/32,As Jiang et al.,18,0,4
dataset/preprocessed/training-data/sentiment_analysis/32,"point out that 40 % of sentiment classification errors are caused by not considering targets in sentiment classification , recent work tends to especially strengthen the effect of targets when modeling the contexts .",19,0,34
dataset/preprocessed/training-data/sentiment_analysis/32,propose an adaptive recursive neural network ( RNN ) to propagate the sentiments from context words to specific targets based on syntactic relations on tweet data .,20,0,27
dataset/preprocessed/training-data/sentiment_analysis/32,"separate the whole context into three components , i.e. , target , its left context and right context , and then use sentiment lexicon and neural pooling functions to generate the target - dependent features .",21,0,36
dataset/preprocessed/training-data/sentiment_analysis/32,divide the contexts into left part with target and right part with target and use two long short - term memory ( LSTM ) models to model the two parts respectively .,22,0,32
dataset/preprocessed/training-data/sentiment_analysis/32,Then the composed targetspecific representations from both parts are used for sentiment classification .,23,0,14
dataset/preprocessed/training-data/sentiment_analysis/32,Wang et al.,24,0,3
dataset/preprocessed/training-data/sentiment_analysis/32,design aspect embeddings for targets and concatenate them with word representations to generate the final representations using LSTM networks and attention mechanism .,25,0,23
dataset/preprocessed/training-data/sentiment_analysis/32,The studies above have realized the importance of targets and developed various methods with the goal of precisely modeling contexts via generating target - specific representations .,26,0,27
dataset/preprocessed/training-data/sentiment_analysis/32,"However , they all ignore the separate modeling of targets , especially with the aid of contexts .",27,0,18
dataset/preprocessed/training-data/sentiment_analysis/32,"In our opinion , only the coordination of targets and their contexts can really enhance the performance of sentiment classification .",28,0,21
dataset/preprocessed/training-data/sentiment_analysis/32,"Let us take "" The picture quality is clear - cut but the battery life is too short "" as an example .",29,0,23
dataset/preprocessed/training-data/sentiment_analysis/32,"When "" short "" is collocated with "" battery life "" , the sentiment tends to be negative .",30,0,19
dataset/preprocessed/training-data/sentiment_analysis/32,"But when "" short "" is used with "" spoon "" in the context "" Short fat noodle spoon , relatively deep some curva "" , the sentiment can be neutral .",31,0,32
dataset/preprocessed/training-data/sentiment_analysis/32,"Then , the next problem is how to simultaneously model targets and contexts precisely .",32,0,15
dataset/preprocessed/training-data/sentiment_analysis/32,"First , target and context can determine representations of each other .",33,0,12
dataset/preprocessed/training-data/sentiment_analysis/32,"For example , when we see the target "" picture quality "" , context word "" clear - cut "" is naturally associated with the target .",34,0,27
dataset/preprocessed/training-data/sentiment_analysis/32,"And it is vice versa - "" picture quality "" is first connected with "" clear - cut "" .",35,0,20
dataset/preprocessed/training-data/sentiment_analysis/32,"In such cases , we argue that targets and contexts can be modeled separately but learned from their interaction .",36,0,20
dataset/preprocessed/training-data/sentiment_analysis/32,"Second , our commonsense is that the context is composed of many words .",37,0,14
dataset/preprocessed/training-data/sentiment_analysis/32,"In fact , targets are also not limited to only one word .",38,0,13
dataset/preprocessed/training-data/sentiment_analysis/32,"No matter targets or contexts , different words may have different contributions to the final representation .",39,0,17
dataset/preprocessed/training-data/sentiment_analysis/32,"For example , it is easy to know that "" picture "" plays a more important role in the representation of the target "" picture quality "" which is described by "" clear - cut "" .",40,0,37
dataset/preprocessed/training-data/sentiment_analysis/32,"Thus , we first propose that both targets and contexts should be computed their attention weights to capture their important information respectively .",41,0,23
dataset/preprocessed/training-data/sentiment_analysis/32,"Based on the two points analyzed above , we propose an interactive attention network ( IAN ) model which is based on long - short term memory networks ( LSTM ) and attention mechanism .",42,0,35
dataset/preprocessed/training-data/sentiment_analysis/32,IAN utilizes the attention mechanism associated with a target to get important information from the context and compute context representation for sentiment classification .,43,0,24
dataset/preprocessed/training-data/sentiment_analysis/32,"Further , IAN makes use of the interactive information from context to supervise the modeling of the target which is helpful to judging sentiment .",44,0,25
dataset/preprocessed/training-data/sentiment_analysis/32,"Finally , with both target representation and context representation concatenated , IAN predicts the sentiment polarity for the target within its context .",45,0,23
dataset/preprocessed/training-data/sentiment_analysis/32,"Experiments on SemEval 2014 demonstrate that IAN can precisely model both targets and contexts , and achieve the stateof - the - art performance .",46,0,25
dataset/preprocessed/training-data/sentiment_analysis/32,"In this section , we first introduce the architecture of interactive attention networks ( IAN ) model for aspect - level sentiment classification .",48,0,24
dataset/preprocessed/training-data/sentiment_analysis/32,"Next , we show the training details of IAN .",49,0,10
dataset/preprocessed/training-data/sentiment_analysis/48,Variational Semi-supervised Aspect - term Sentiment Analysis via Transformer,2,1,9
dataset/preprocessed/training-data/sentiment_analysis/48,Aspect - term sentiment analysis ( ATSA ) is a long - standing challenge in natural language processing .,4,0,19
dataset/preprocessed/training-data/sentiment_analysis/48,It requires fine - grained semantical reasoning about a target entity appeared in the text .,5,0,16
dataset/preprocessed/training-data/sentiment_analysis/48,"As manual annotation over the aspects is laborious and time - consuming , the amount of labeled data is limited for supervised learning .",6,0,24
dataset/preprocessed/training-data/sentiment_analysis/48,This paper proposes a semisupervised method for the ATSA problem by using the Variational Autoencoder based on Transformer .,7,0,19
dataset/preprocessed/training-data/sentiment_analysis/48,The model learns the latent distribution via variational inference .,8,0,10
dataset/preprocessed/training-data/sentiment_analysis/48,"By disentangling the latent representation into the aspect - specific sentiment and the lexical context , our method induces the underlying sentiment prediction for the unlabeled data , which then benefits the ATSA classifier .",9,0,35
dataset/preprocessed/training-data/sentiment_analysis/48,"Our method is classifier - agnostic , i.e. , the classifier is an independent module and various supervised models can be integrated .",10,0,23
dataset/preprocessed/training-data/sentiment_analysis/48,Experimental results are obtained on the SemEval 2014 task 4 and show that our method is effective with different five specific classifiers and outperforms these models by a significant margin .,11,0,31
dataset/preprocessed/training-data/sentiment_analysis/48,"Aspect based sentiment analysis ( ABSA ) has two sub - tasks , namely aspect - term sentiment analysis ( ATSA ) and aspect - category sentiment analysis ( ACSA ) .",13,1,32
dataset/preprocessed/training-data/sentiment_analysis/48,"ACSA is to infer the sentiment polarity with regard to the predefined categories , e.g. , the aspect food , price , ambience .",14,1,24
dataset/preprocessed/training-data/sentiment_analysis/48,"On the other hand , ATSA aims at classifying the sentiment polarity of a given aspect word or phrase in the text .",15,1,23
dataset/preprocessed/training-data/sentiment_analysis/48,"For example , given a review about a restaurant "" the [ pizza ] aspect is the best if you like thin crusted pizza , however , the [ service ] aspect is awful . "" , the sentiment implications with regard to "" pizza "" and "" service "" are contrary .",16,0,53
dataset/preprocessed/training-data/sentiment_analysis/48,"For the aspect * * : equal contribution "" pizza "" , the sentiment polarity is "" positive "" while "" negative "" for the aspect "" service "" .",17,0,30
dataset/preprocessed/training-data/sentiment_analysis/48,"In contrast to document - level sentiment analysis , ATSA requires more fine - grained reasoning about the textual context .",18,0,21
dataset/preprocessed/training-data/sentiment_analysis/48,The task is worthy of investigation as it can obtain the attitude with regard to a specific entity which we are interested in .,19,0,24
dataset/preprocessed/training-data/sentiment_analysis/48,"The task is widely applicated in analyzing the comments , such as opinion generation .",20,0,15
dataset/preprocessed/training-data/sentiment_analysis/48,"Recently , many attempts focus on supervised learning and pay much attention to the interaction between the aspect and the context .",21,0,22
dataset/preprocessed/training-data/sentiment_analysis/48,"However , the amount of labeled data is quite limited as the annotation about the aspects is laborious .",22,0,19
dataset/preprocessed/training-data/sentiment_analysis/48,"Currently available data sets , e.g. Se -m",23,0,8
dataset/preprocessed/training-data/sentiment_analysis/48,"Eval , only has around 2 K unique sentences and 3K sentence - aspect pairs , which is insufficient to fully exploit the power of the deep models .",24,0,29
dataset/preprocessed/training-data/sentiment_analysis/48,"Fortunately , a large amount of unlabeled data is available for free and can be accessed easily from the websites .",25,0,21
dataset/preprocessed/training-data/sentiment_analysis/48,It will be of great significance if numerous unlabeled samples can be utilized to further facilitate the supervised ATSA classifier .,26,0,21
dataset/preprocessed/training-data/sentiment_analysis/48,"Therefore , the semi-supervised ATSA is a promising research topic .",27,0,11
dataset/preprocessed/training-data/sentiment_analysis/48,"In ATSA , achieving the sentiment of the aspectterm is semantically complicated and it is nontrivial for a model to capture sentimental similarity of the aspects , which causes the difficulties for semi-supervised learning .",28,0,35
dataset/preprocessed/training-data/sentiment_analysis/48,"In this paper , we proposed a classifier - agnostic framework which named Aspect - term Semi-supervised Variational Autoencoder ( Kingma and Welling , 2014 ) based on Transformer ( ASVAET ) .",29,0,33
dataset/preprocessed/training-data/sentiment_analysis/48,The variational autoencoder offers the flexibility to customize the model structure .,30,0,12
dataset/preprocessed/training-data/sentiment_analysis/48,"In other words , the proposed framework is compatible with other supervised neural networks to boost their performance .",31,0,19
dataset/preprocessed/training-data/sentiment_analysis/48,"Our proposed model learns the latent representation ar Xiv : 1810.10437v3 [ cs. CL ] 5 Sep 2019 of the input data and disentangles the representations into two independent parts , i.e. , the aspectterm sentiment and the representation of the lexical context .",32,0,44
dataset/preprocessed/training-data/sentiment_analysis/48,"By regarding the aspect sentiment polarity of the unlabeled data as the discrete latent variable , the model implicitly induces the sentiment polarity via the variational inference .",33,0,28
dataset/preprocessed/training-data/sentiment_analysis/48,"Specifically , the representation of the lexical context is extracted by the encoder and the aspect - term sentiment polarity is inferred from the specific ATSA classifier .",34,0,28
dataset/preprocessed/training-data/sentiment_analysis/48,The decoder takes these two representations as inputs and reconstructs the original sentence by two unidirectional language models .,35,0,19
dataset/preprocessed/training-data/sentiment_analysis/48,"In contrast to the conventional auto - regressive models , the latent representations have their specific meanings and are obtained from the encoder and the classifier to the input examples .",36,0,31
dataset/preprocessed/training-data/sentiment_analysis/48,"Therefore , it is also possible to condition the sentence generation on the sentiment and lexical information w.r.t. a certain target entity .",37,0,23
dataset/preprocessed/training-data/sentiment_analysis/48,"In addition , by separating the representation of the input sentence , the classifier becomes an independent module in our framework , which endows the method with the ability to integrate different classifiers .",38,0,34
dataset/preprocessed/training-data/sentiment_analysis/48,The method is presented in detail in Sec. 3 .,39,0,10
dataset/preprocessed/training-data/sentiment_analysis/48,Experimental results are obtained on the two classical datasets from SemEval 2014 task,40,0,13
dataset/preprocessed/training-data/sentiment_analysis/48,4 . Five recent available models are implemented as the classifier in ASVAET .,41,0,14
dataset/preprocessed/training-data/sentiment_analysis/48,Our method is able to utilize the unlabeled data and consistently improve the performance against the supervised models .,42,0,19
dataset/preprocessed/training-data/sentiment_analysis/48,"Compared with other semisupervised methods , i.e. , in - domain word embedding pre-training and self - training , the proposed method also demonstrates better performance .",43,0,27
dataset/preprocessed/training-data/sentiment_analysis/48,"We also evaluate the effectiveness of labeled data and sharing embeddings , and show that the structure can provide the separation between lexical context and sentiment polarity in the latent space .",44,0,32
dataset/preprocessed/training-data/sentiment_analysis/48,Sentiment analysis is a traditional research hotspot in the NLP field .,46,0,12
dataset/preprocessed/training-data/sentiment_analysis/48,"Rather than obtaining the sentimental inclination of the entire text , ATSA instead aims to extract the sentimental expression w.r.t. a target entity .",47,0,24
dataset/preprocessed/training-data/sentiment_analysis/48,"With the release of online completions , abundant methods were proposed to explore the limits of current models .",48,0,19
dataset/preprocessed/training-data/sentiment_analysis/48,proposed to make use of bidirectional Long Short - Term Memory ( LSTM ) to encode the sentence from the left and right to the aspect - term .,49,0,29
dataset/preprocessed/training-data/sentiment_analysis/44,Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment Analysis,2,1,12
dataset/preprocessed/training-data/sentiment_analysis/44,Discourse Parsing and Sentiment Analysis are two fundamental tasks in Natural Language Processing that have been shown to be mutually beneficial .,4,0,22
dataset/preprocessed/training-data/sentiment_analysis/44,"In this work , we design and compare two Neural models for jointly learning both tasks .",5,0,17
dataset/preprocessed/training-data/sentiment_analysis/44,"In the proposed approach , we first create a vector representation for all the text segments in the input sentence .",6,0,21
dataset/preprocessed/training-data/sentiment_analysis/44,"Next , we apply three different Recursive Neural Net models : one for discourse structure prediction , one for discourse relation prediction and one for sentiment analysis .",7,0,28
dataset/preprocessed/training-data/sentiment_analysis/44,"Finally , we combine these Neural Nets in two different joint models :",8,0,13
dataset/preprocessed/training-data/sentiment_analysis/44,Multi - tasking and Pre-training .,9,0,6
dataset/preprocessed/training-data/sentiment_analysis/44,Our results on two standard corpora indicate that both methods result in improvements in each task but Multi - tasking has a bigger impact than Pre-training .,10,0,27
dataset/preprocessed/training-data/sentiment_analysis/44,"Specifically for Discourse Parsing , we see improvements in the prediction on the set of contrastive relations .",11,0,18
dataset/preprocessed/training-data/sentiment_analysis/44,"This paper focuses on studying two fundamental NLP tasks , Discourse Parsing and Sentiment Analysis .",13,0,16
dataset/preprocessed/training-data/sentiment_analysis/44,"The importance of these tasks and their wide applications ( e.g. , , ) has initiated much interest in studying both , but no method yet exists that can come close to human performance in solving them .",14,0,38
dataset/preprocessed/training-data/sentiment_analysis/44,"Discourse parsing is the task of parsing apiece of text into a tree ( called a Discourse Tree ) , the leaves of which are typically clauses ( called Elementary Discourse Units or EDUs in short ) and nodes ( Discourse Units ) represent text spans thatare concatenations of their corresponding sub - trees ' text spans 1 .",15,0,59
dataset/preprocessed/training-data/sentiment_analysis/44,"Nodes also have labels identifying discourse relationships ( "" contrast "" , "" evidence "" , etc. ) between their corresponding subtrees .",16,0,23
dataset/preprocessed/training-data/sentiment_analysis/44,The relation also specifies nucliearity of the children .,17,0,9
dataset/preprocessed/training-data/sentiment_analysis/44,Nuclei are the core parts of the relation and Satellites are the supportive ones .,18,0,15
dataset/preprocessed/training-data/sentiment_analysis/44,A Relation can take one of the following forms :,19,0,10
dataset/preprocessed/training-data/sentiment_analysis/44,( 1 ) Satellite - Nucleus : First Discourse Unit is Satellite and second Discourse Unit is Nucleus .,20,0,19
dataset/preprocessed/training-data/sentiment_analysis/44,( 2 ) Nucleus - Satellite : First Discourse Unit is Nucleus and second Discourse Unit is Satellite .,21,0,19
dataset/preprocessed/training-data/sentiment_analysis/44,( 3 ) Nucleus - Nucleus : Both Discourse Units are Nuclei .,22,0,13
dataset/preprocessed/training-data/sentiment_analysis/44,In this approach relation identification and nuclearity assignment is done simultaneously .,23,0,12
dataset/preprocessed/training-data/sentiment_analysis/44,shows the Discourse,24,0,3
dataset/preprocessed/training-data/sentiment_analysis/44,Tree of a sample sentence .,25,0,6
dataset/preprocessed/training-data/sentiment_analysis/44,"In this sentence , the Discourse Unit "" There are slow and repetitive parts , "" holds a "" Contrast "" relationship with "" but it has just enough spice to keep it interesting . "" .",26,0,37
dataset/preprocessed/training-data/sentiment_analysis/44,"Furthermore , we can see that the former Discourse Unit is the satellite of the relation and the later part is the Nucleus .",27,0,24
dataset/preprocessed/training-data/sentiment_analysis/44,Discourse Parsing is such a critical task in NLP because previous work has shown that information :,28,0,17
dataset/preprocessed/training-data/sentiment_analysis/44,"The Sentiment annotation ( over Discourse Tree structure ) of a sentence from Sentiment Treebank dataset contained in the resulting Discourse Tree can benefit many other NLP tasks including but not restricted to automatic summarization ( e.g. , , , ) , machine translation ( e.g. , , ) and question answering ( e.g. , ) .",29,0,57
dataset/preprocessed/training-data/sentiment_analysis/44,"In contrast to traditional syntactic and semantic parsing , Discourse Parsing can generate structures that cover not only a single sentence but also multi-sentential text .",30,0,26
dataset/preprocessed/training-data/sentiment_analysis/44,"However , the focus of this paper is on sentence level Discourse Parsing , leaving the study of extensions to multi-sentential text as future work .",31,0,26
dataset/preprocessed/training-data/sentiment_analysis/44,The second fundamental task we consider in this work is assigning a contextual polarity label to text ( sentiment analysis ) .,32,0,22
dataset/preprocessed/training-data/sentiment_analysis/44,Analyzing the over all polarity of a sentence is a challenging task due to the ambiguities that can be introduced by combinations of words and phrases .,33,0,27
dataset/preprocessed/training-data/sentiment_analysis/44,"For example in the movie review excerpt shown in , the phrase "" There are slow and repetitive parts "" has a negative sentiment .",34,0,25
dataset/preprocessed/training-data/sentiment_analysis/44,"However when it is combined with the positive phrase "" but it has just enough spice to keep it interesting "" , it results in an over all positive sentence .",35,0,31
dataset/preprocessed/training-data/sentiment_analysis/44,"It has been suggested that the information extracted from Discourse Trees can help with Sentiment Analysis and likewise , knowing the sentiment of two pieces of text might help with the identification of discourse relationships between them .",36,0,38
dataset/preprocessed/training-data/sentiment_analysis/44,"For instance , taking the sentence in as an example , knowing that the two text spans "" There are slow and repetitive parts "" and "" but it has just enough spice to keep it interesting "" are in a Contrast relationship to each other , also signals that the sentiment of the two text spans is less likely to be of the same type",37,0,66
dataset/preprocessed/training-data/sentiment_analysis/44,"Likewise , knowing that the sentiment of the former text span is "" very negative "" , while the sentiment of the later text span is "" very positive "" , helps to narrow down the choice of discourse relation between these two text spans to the Contrastive group which contains relations Contrast , Comparison , Antithesis , Antithesis - e , Consequence - s , Concession and Problem - Solution .",39,0,72
dataset/preprocessed/training-data/sentiment_analysis/44,"To the best of our knowledge there is no previous work that learns both of these tasks in a joint model , using deep learning architectures .",40,0,27
dataset/preprocessed/training-data/sentiment_analysis/44,The main contribution of this paper is to address this gap by investigating how the two tasks can benefit from each other at the sentence level within a deep learning joint model .,41,0,33
dataset/preprocessed/training-data/sentiment_analysis/44,More specific contributions include :,42,0,5
dataset/preprocessed/training-data/sentiment_analysis/44,( i ),43,0,3
dataset/preprocessed/training-data/sentiment_analysis/44,"The development of three independent recursive neural nets : two for the key sub - tasks of discourse parsing , namely structure prediction and relation prediction ; the third net for sentiment prediction .",44,0,34
dataset/preprocessed/training-data/sentiment_analysis/44,"( ii ) The design and experimental comparison of two alternative neural joint models , Multitasking and Pre-training , that have been shown to be effective in previous work for combining other tasks in NLP ( e.g. , , , ) .",45,0,42
dataset/preprocessed/training-data/sentiment_analysis/44,Our results indicate that a joint model performs better than individual models in either of the tasks with Multi - tasking outperforming Pre-training .,46,0,24
dataset/preprocessed/training-data/sentiment_analysis/44,"Upon closer inspection , we also find that the improvement of Multi - tasking system in Relation prediction is mainly for the Contrastive set of relations , which confirms our hypothesis that knowing the sentiment of two text spans can help narrow down the choice of discourse relations that holds between them .",47,0,53
dataset/preprocessed/training-data/sentiment_analysis/44,"Traditionally , Discourse Parsing and Sentiment Analysis have been approached by applying machine learning methods with predetermined , engineered features that were carefully chosen by studying the properties of the text .",49,0,32
dataset/preprocessed/training-data/sentiment_analysis/38,Learning to Generate Reviews and Discovering Sentiment,2,0,7
dataset/preprocessed/training-data/sentiment_analysis/38,We explore the properties of byte - level recurrent language models .,4,0,12
dataset/preprocessed/training-data/sentiment_analysis/38,"When given sufficient amounts of capacity , training data , and compute time , the representations learned by these models include disentangled features corresponding to high - level concepts .",5,0,30
dataset/preprocessed/training-data/sentiment_analysis/38,"Specifically , we find a single unit which performs sentiment analysis .",6,0,12
dataset/preprocessed/training-data/sentiment_analysis/38,"These representations , learned in an unsupervised manner , achieve state of the art on the binary subset of the Stanford Sentiment Treebank .",7,0,24
dataset/preprocessed/training-data/sentiment_analysis/38,They are also very data efficient .,8,0,7
dataset/preprocessed/training-data/sentiment_analysis/38,"When using only a handful of labeled examples , our approach matches the performance of strong baselines trained on full datasets .",9,0,22
dataset/preprocessed/training-data/sentiment_analysis/38,We also demonstrate the sentiment unit has a direct influence on the generative process of the model .,10,0,18
dataset/preprocessed/training-data/sentiment_analysis/38,Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment .,11,0,19
dataset/preprocessed/training-data/sentiment_analysis/38,Introduction and Motivating Work,12,0,4
dataset/preprocessed/training-data/sentiment_analysis/38,Representation learning ) plays a critical role in many modern machine learning systems .,13,1,14
dataset/preprocessed/training-data/sentiment_analysis/38,Representations map raw data to more useful forms and the choice of representation is an important component of any application .,14,0,21
dataset/preprocessed/training-data/sentiment_analysis/38,"Broadly speaking , there are two are as of research emphasizing different details of how to learn useful representations .",15,0,20
dataset/preprocessed/training-data/sentiment_analysis/38,"The supervised training of high - capacity models on large labeled datasets is critical to the recent success of deep learning techniques for a wide range of applications such as image classification , speech recognition , and machine translation ) .",16,0,41
dataset/preprocessed/training-data/sentiment_analysis/38,Analysis of the task specific representations learned by these models reveals many fascinating properties .,17,0,15
dataset/preprocessed/training-data/sentiment_analysis/38,"Image classifiers learn a broadly useful hierarchy of feature detectors rerepresenting raw pixels as edges , textures , and objects .",18,0,21
dataset/preprocessed/training-data/sentiment_analysis/38,"In the field of computer vision , it is now commonplace to reuse these representations on a broad suite of related tasks - one of the most successful examples of transfer learning to date .",19,0,35
dataset/preprocessed/training-data/sentiment_analysis/38,There is also along history of unsupervised representation learning .,20,0,10
dataset/preprocessed/training-data/sentiment_analysis/38,Much of the early research into modern deep learning was developed and validated via this approach ) .,21,0,18
dataset/preprocessed/training-data/sentiment_analysis/38,"Unsupervised learning is promising due to its ability to scale beyond only the subsets and domains of data that can be cleaned and labeled given resource , privacy , or other constraints .",22,0,33
dataset/preprocessed/training-data/sentiment_analysis/38,This advantage is also its difficulty .,23,0,7
dataset/preprocessed/training-data/sentiment_analysis/38,"While supervised approaches have clear objectives that can be directly optimized , unsupervised approaches rely on proxy tasks such as reconstruction , density estimation , or generation , which do not directly encourage useful representations for specific tasks .",24,0,39
dataset/preprocessed/training-data/sentiment_analysis/38,"As a result , much work has gone into designing objectives , priors , and architectures meant to encourage the learning of useful representations .",25,0,25
dataset/preprocessed/training-data/sentiment_analysis/38,We refer readers to for a detailed review .,26,0,9
dataset/preprocessed/training-data/sentiment_analysis/38,"Despite these difficulties , there are notable applications of unsupervised learning .",27,0,12
dataset/preprocessed/training-data/sentiment_analysis/38,Pre-trained word vectors are a vital part of many modern NLP systems .,28,0,13
dataset/preprocessed/training-data/sentiment_analysis/38,"These representations , learned by modeling word co-occurrences , increase the data efficiency and generalization capability of NLP systems .",29,0,20
dataset/preprocessed/training-data/sentiment_analysis/38,Topic modelling can also discover factors within a corpus of text which align to human interpretable concepts such as art or education .,30,0,23
dataset/preprocessed/training-data/sentiment_analysis/38,"How to learn representations of phrases , sentences , and documents is an open are a of research .",31,0,19
dataset/preprocessed/training-data/sentiment_analysis/38,"Inspired by the success of word vectors , propose skipthought vectors , a method of training a sentence encoder by predicting the preceding and following sentence .",32,0,27
dataset/preprocessed/training-data/sentiment_analysis/38,The representation learned by this objective performs competitively on a broad suite of evaluated tasks .,33,0,16
dataset/preprocessed/training-data/sentiment_analysis/38,More advanced training techniques such as layer normalization further improve results .,34,0,12
dataset/preprocessed/training-data/sentiment_analysis/38,"However , skip - thought vectors are still outperformed by supervised models which directly optimize the desired performance metric on a specific dataset .",35,0,24
dataset/preprocessed/training-data/sentiment_analysis/38,"This is the case for both text classification ar Xiv:1704.01444v2LG ] 6 Apr 2017 tasks , which measure whether a specific concept is well encoded in a representation , and more general semantic similarity tasks .",36,0,36
dataset/preprocessed/training-data/sentiment_analysis/38,"This occurs even when the datasets are relatively small by modern standards , often consisting of only a few thousand labeled examples .",37,0,23
dataset/preprocessed/training-data/sentiment_analysis/38,"In contrast to learning a generic representation on one large dataset and then evaluating on other tasks / datasets , proposed using similar unsupervised objectives such as sequence autoencoding and language modeling to first pretrain a model on a dataset and then finetune it for a given task .",38,0,49
dataset/preprocessed/training-data/sentiment_analysis/38,This approach outperformed training the same model from random initialization and achieved state of the art on several text classification datasets .,39,0,22
dataset/preprocessed/training-data/sentiment_analysis/38,Combining language modelling with topic modelling and fitting a small supervised feature extractor on top has also achieved strong results on in - domain document level sentiment analysis .,40,0,29
dataset/preprocessed/training-data/sentiment_analysis/38,"Considering this , we hypothesize two effects maybe combining to result in the weaker performance of purely unsupervised approaches .",41,0,20
dataset/preprocessed/training-data/sentiment_analysis/38,Skip - thought vectors were trained on a corpus of books .,42,0,12
dataset/preprocessed/training-data/sentiment_analysis/38,"But some of the classification tasks they are evaluated on , such as sentiment analysis of reviews of consumer goods , do not have much overlap with the text of novels .",43,0,32
dataset/preprocessed/training-data/sentiment_analysis/38,"We propose this distributional issue , combined with the limited capacity of current models , results in representational underfitting .",44,0,20
dataset/preprocessed/training-data/sentiment_analysis/38,"Current generic distributed sentence representations maybe very lossy - good at capturing the gist , but poor with the precise semantic or syntactic details which are critical for applications .",45,0,30
dataset/preprocessed/training-data/sentiment_analysis/38,The experimental and evaluation protocols maybe underestimating the quality of unsupervised representation learning for sentences and documents due to certain seemingly insignificant design decisions .,46,0,25
dataset/preprocessed/training-data/sentiment_analysis/38,also raises concern about current evaluation tasks in their recent work which provides a thorough survey of architectures and objectives for learning unsupervised sentence representations - including the above mentioned skip - thoughts .,47,0,34
dataset/preprocessed/training-data/sentiment_analysis/38,"In this work , we test whether this is the case .",48,0,12
dataset/preprocessed/training-data/sentiment_analysis/38,We focus in on the task of sentiment analysis and attempt to learn an unsupervised representation that accurately contains this concept .,49,1,22
dataset/preprocessed/training-data/sentiment_analysis/9,Graphical Abstract A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction Highlights A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction,2,1,48
dataset/preprocessed/training-data/sentiment_analysis/9,Proposing a model for the joint task of aspect term extraction and aspect polarity classification .,4,0,16
dataset/preprocessed/training-data/sentiment_analysis/9,"The model proposed is Chinese language - oriented and applicable to the English language , with the ability to handle both Chinese and English reviews .",5,0,26
dataset/preprocessed/training-data/sentiment_analysis/9,The model also integrates the domain - adapted BERT model for enhancement .,6,0,13
dataset/preprocessed/training-data/sentiment_analysis/9,The model achieves state - of - the - art performance on seven ABSA datasets .,7,0,16
dataset/preprocessed/training-data/sentiment_analysis/9,Aspect - based sentiment analysis ( ABSA ) task is a multi - grained task of natural language processing and consists of two subtasks : aspect term extraction ( ATE ) and aspect polarity classification ( APC ) .,8,1,39
dataset/preprocessed/training-data/sentiment_analysis/9,Most of the existing work focuses on the subtask of aspect term polarity inferring and ignores the significance of aspect term extraction .,9,1,23
dataset/preprocessed/training-data/sentiment_analysis/9,"Besides , the existing researches do not pay attention to the research of the Chinese - oriented ABSA task .",10,0,20
dataset/preprocessed/training-data/sentiment_analysis/9,"Based on the local context focus ( LCF ) mechanism , this paper firstly proposes a multi-task learning model for Chineseoriented aspect - based sentiment analysis , namely LCF - ATEPC .",11,0,32
dataset/preprocessed/training-data/sentiment_analysis/9,"Compared with existing models , this model equips the capability of extracting aspect term and inferring aspect term polarity synchronously , moreover , this model is effective to analyze both Chinese and English comments simultaneously and the experiment on a multilingual mixed dataset proved its availability .",12,0,47
dataset/preprocessed/training-data/sentiment_analysis/9,"By integrating the domain - adapted BERT model , the LCF - ATEPC model achieved the state - of the - art performance of aspect term extraction and aspect polarity classification in four Chinese review datasets .",13,1,37
dataset/preprocessed/training-data/sentiment_analysis/9,"Besides , the experimental results on the most commonly used SemEval - 2014 task 4 Restaurant and Laptop datasets outperform the state - of - the - art performance on the ATE",14,0,32
dataset/preprocessed/training-data/sentiment_analysis/9,"Aspect - based sentiment analysis ; Pontiki , Galanis , Papageorgiou , Androutsopoulos , Manandhar , AL - Smadi , Al - Ayyoub , Zhao , Qin , De Clercq , Hoste , Apidianaki , Tannier , Loukachevitch , Kotelnikov , Bel , Jimnez - Zafra and Eryigit ( 2016 ) ( ABSA ) is a fine - grained task compared with traditional sentiment analysis , which requires the model to be able to automatic extract the aspects and predict the polarities of all the aspects .",16,1,87
dataset/preprocessed/training-data/sentiment_analysis/9,"For example , given a restaurant review :",17,0,8
dataset/preprocessed/training-data/sentiment_analysis/9,""" The dessert at this restaurant is delicious but the service is poor , "" the full - designed model for ABSA needs to extract the aspects "" dessert "" and "" service "" and correctly reason about their polarity .",18,0,41
dataset/preprocessed/training-data/sentiment_analysis/9,"In this review , the consumers ' opinions on "" dessert "" and "" service "" are not consistent , with positive and negative sentiment polarity respectively .",19,0,28
dataset/preprocessed/training-data/sentiment_analysis/9,"Generally , aspects and their polarity need to be manually labeled before running the aspect polarity classification procedure in the supervised deep learning models .",20,0,25
dataset/preprocessed/training-data/sentiment_analysis/9,"However , most of the proposed models for aspect - based sentiment analysis tasks only focus on improving the classification accuracy of aspect polarity and ignore the research of aspect term extraction .",21,0,33
dataset/preprocessed/training-data/sentiment_analysis/9,"Therefore , when conducting transfer learning on aspect - based sentiment analysis , those proposed models often fall into the dilemma of lacking aspect extraction method on targeted tasks because there is not enough research support .",22,0,37
dataset/preprocessed/training-data/sentiment_analysis/9,The APC task is a kind of classification problem .,23,1,10
dataset/preprocessed/training-data/sentiment_analysis/9,"The researches concerning APC tasks is more abundant than the ATE task , and a large number of deep learning - based models have been proposed to solve APC problems , such as the models ; ; ; based on long short - term memory ( LSTM ) and the methodologies based on transformer .",24,1,55
dataset/preprocessed/training-data/sentiment_analysis/9,"The purpose of the APC task is to predict the exact sentiment polarity of different aspects in their context , rather than to fuzzily analyze the over all sentiment polarity on the sentence - level or document - level .",25,0,40
dataset/preprocessed/training-data/sentiment_analysis/9,"In the APC task , the polarities are most usually classified into three categories : positive , negative , and neutral .",26,0,22
dataset/preprocessed/training-data/sentiment_analysis/9,"It is obvious that the sentiment polarity classified based on aspects can better mine the fine - grained emotional tendency in reviews or tweets , thus providing a more accurate reference for decision - makers .",27,0,36
dataset/preprocessed/training-data/sentiment_analysis/9,"Similar to the named entity recognition ( NER ) task , the ATE task is a sequence labeling task , which aims to extract aspects from the reviews or tweet .",28,0,31
dataset/preprocessed/training-data/sentiment_analysis/9,"In most researches ; , the ATE task is studied independently , away from the APC task .",29,0,18
dataset/preprocessed/training-data/sentiment_analysis/9,The ATE task first segments a review into separate tokens and then infers whether the tokens belong to any aspect .,30,0,21
dataset/preprocessed/training-data/sentiment_analysis/9,"The tokens maybe labeled in different forms in different studies , but most of the studies have adopted the IOB 2 label to annotate tokens .",31,0,26
dataset/preprocessed/training-data/sentiment_analysis/9,"Aiming to automatically extract aspects from the text efficiently and analyze the sentiment polarity of aspects simultaneously , this paper proposes a multi-task learning model for aspect - based sentiment analysis .",32,0,32
dataset/preprocessed/training-data/sentiment_analysis/9,Multilingual processing is an important research orientation of natural language processing .,33,0,12
dataset/preprocessed/training-data/sentiment_analysis/9,The LCF - ATEPC 3 model proposed in this paper is a novel multilingual and multi-task - oriented model .,34,0,20
dataset/preprocessed/training-data/sentiment_analysis/9,"Apart from achieving state - of - the - art performance in commonly used SemEval - 2014 task4 datasets , the experimental results in four Chinese review datasets also validate that this model has a strong ability to expand and adapt to the needs of multilingual task .",35,0,48
dataset/preprocessed/training-data/sentiment_analysis/9,"The proposed model is based on multi-head self - attention ( MHSA ) and integrates the pre-trained and the local context focus mechanism , namely LCF - ATEPC .",36,0,29
dataset/preprocessed/training-data/sentiment_analysis/9,"By training on a small amount of annotated data of aspect and their polarity , the model can be adapted to a large - scale dataset , automatically extracting the aspects and predicting the sentiment polarities .",37,0,37
dataset/preprocessed/training-data/sentiment_analysis/9,"In this way , the model can discover the unknown aspects and avoids the tedious and huge cost of manually annotating all aspects and polarities .",38,0,26
dataset/preprocessed/training-data/sentiment_analysis/9,It is of great significance for the field - specific aspect - based sentiment analysis .,39,0,16
dataset/preprocessed/training-data/sentiment_analysis/9,The main contributions of this article are as follows :,40,0,10
dataset/preprocessed/training-data/sentiment_analysis/9,"For the first time , this paper studies the multi -task model of APC subtask and ATE subtask for multilingual reviews , which provides a new idea for the research of Chinese aspect extraction .",42,0,35
dataset/preprocessed/training-data/sentiment_analysis/9,"2 . This paper firstly applies self - attention and local context focus techniques to aspect word extraction task , and fully explore their potential in aspect term extraction task .",43,0,31
dataset/preprocessed/training-data/sentiment_analysis/9,"3 . The LCF - ATEPC model proposed in this paper integrates the pre-trained BERT model , significantly improves both the performance of ATE task and APC subtask , and achieves new state - of - the - art performance especially the F 1 score of ATE task .",44,0,49
dataset/preprocessed/training-data/sentiment_analysis/9,"Besides , we adopted the domain - adapted BERT model trained on the domain - related",45,0,16
dataset/preprocessed/training-data/sentiment_analysis/9,"The labels adopted in this paper are : , ,",46,0,10
dataset/preprocessed/training-data/sentiment_analysis/9,The codes for this paper are available at https://github.com/yangheng95/LCF-ATEPC,48,0,9
dataset/preprocessed/training-data/sentiment_analysis/9,corpus to the ABSA joint - task learning model .,49,0,10
dataset/preprocessed/training-data/sentiment_analysis/19,Improved Sentence Modeling using Suffix Bidirectional LSTM,2,1,7
dataset/preprocessed/training-data/sentiment_analysis/19,"Recurrent neural networks have become ubiquitous in computing representations of sequential data , especially textual data in natural language processing .",4,1,21
dataset/preprocessed/training-data/sentiment_analysis/19,"In particular , Bidirectional LSTMs are at the heart of several neural models achieving state - of - the - art performance in a wide variety of tasks in NLP .",5,0,31
dataset/preprocessed/training-data/sentiment_analysis/19,"However , BiLSTMs are known to suffer from sequential bias the contextual representation of a token is heavily influenced by tokens close to it in a sentence .",6,0,28
dataset/preprocessed/training-data/sentiment_analysis/19,We propose a general and effective improvement to the BiLSTM model which encodes each suffix and prefix of a sequence of tokens in both forward and reverse directions .,7,0,29
dataset/preprocessed/training-data/sentiment_analysis/19,We call our model Suffix Bidirectional LSTM or SuBiLSTM .,8,0,10
dataset/preprocessed/training-data/sentiment_analysis/19,This introduces an alternate bias that favors long range dependencies .,9,0,11
dataset/preprocessed/training-data/sentiment_analysis/19,We apply SuBiLSTMs to several tasks that require sentence modeling .,10,0,11
dataset/preprocessed/training-data/sentiment_analysis/19,"We demonstrate that using SuBiLSTM instead of a BiLSTM in existing models leads to improvements in performance in learning general sentence representations , text classification , textual entailment and paraphrase detection .",11,0,32
dataset/preprocessed/training-data/sentiment_analysis/19,Using SuBiLSTM we achieve new state - of - the - art results for fine - grained sentiment classification and question classification .,12,1,23
dataset/preprocessed/training-data/sentiment_analysis/19,Let s be a sequence with n tokens .,13,0,9
dataset/preprocessed/training-data/sentiment_analysis/19,"We use s[i : j ] to denote the sequence of embeddings of the tokens from s [ i ] to s [ j ] , where j maybe less than i .",14,0,33
dataset/preprocessed/training-data/sentiment_analysis/19,L p represent a LSTM that ar Xiv : 1805.07340v2 [ cs.LG ],16,0,13
dataset/preprocessed/training-data/sentiment_analysis/19,Recurrent Neural Networks ( RNN ) ) have emerged as a powerful tool for modeling sequential data .,18,1,18
dataset/preprocessed/training-data/sentiment_analysis/19,"RNNs have largely given way to more sophisticated recurrent architectures like Long Short - Term Memory and the simpler Gated Recurrent Unit ) , owing to their superior gradient propagation properties .",20,0,32
dataset/preprocessed/training-data/sentiment_analysis/19,"The importance of LSTMs in natural language processing , where a sentence as a sequence of tokens represents a fundamental unit , has risen exponentially over the past few years .",21,0,31
dataset/preprocessed/training-data/sentiment_analysis/19,A LSTM processing a sentence in the forward direction produces distributed representations of its prefixes .,22,0,16
dataset/preprocessed/training-data/sentiment_analysis/19,A Bidirectional LSTM ( BiLSTM in short ) additionally processes the sentence in the reverse direction ( starting from the last token ) producing representations of the suffixes ( in the reverse direction ) .,23,0,35
dataset/preprocessed/training-data/sentiment_analysis/19,"For every token tin the sentence , a BiLSTM thus produces a contextual representation oft based on its prefix and suffix in the sentence .",24,0,25
dataset/preprocessed/training-data/sentiment_analysis/19,"Despite their sophisticated design , it is well known that LSTMs suffer from sequential bias Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",25,0,30
dataset/preprocessed/training-data/sentiment_analysis/19,All rights reserved ..,26,0,4
dataset/preprocessed/training-data/sentiment_analysis/19,The hidden state of a LSTM is heavily influenced by the last few tokens it has processed .,27,0,18
dataset/preprocessed/training-data/sentiment_analysis/19,"This implies that the contextual representation oft is highly influenced by the tokens close to it in the sequential order , with tokens farther away being less influential .",28,0,29
dataset/preprocessed/training-data/sentiment_analysis/19,"Computing contextual representations that capture long range dependencies is a challenging research problem , with numerous applications .",29,0,18
dataset/preprocessed/training-data/sentiment_analysis/19,"In this paper , we propose a simple , general and effective technique to compute contextual representations that capture long range dependencies .",30,0,23
dataset/preprocessed/training-data/sentiment_analysis/19,"For each token t , we encode both its prefix and suffix in both the forward and reverse direction .",31,0,20
dataset/preprocessed/training-data/sentiment_analysis/19,"Notably , the encoding of the suffix in the forward direction is biased towards tokens sequentially farther away to the right oft .",32,0,23
dataset/preprocessed/training-data/sentiment_analysis/19,"Similarly , the encoding of the prefix in the reverse direction is biased towards tokens sequentially farther away to the left oft .",33,0,23
dataset/preprocessed/training-data/sentiment_analysis/19,"Further , we combine the prefix and suffix representations by a simple max - pooling operation to produce a richer contextual representation of t in both the forward and reverse direction .",34,0,32
dataset/preprocessed/training-data/sentiment_analysis/19,We call our model Suffix BiLSTM or SuBiLSTM in short .,35,0,11
dataset/preprocessed/training-data/sentiment_analysis/19,A SuBiLSTM has the same representation length as a BiLSTM with the same hidden dimension .,36,0,16
dataset/preprocessed/training-data/sentiment_analysis/19,We consider two versions of SuBiLSTMs - a tied version where the suffixes and prefixes in each direction are encoded using the same LSTM and an untied version where two different LSTMs are used .,37,0,35
dataset/preprocessed/training-data/sentiment_analysis/19,"Note that , as in a BiLSTM , we always use different LSTMs for the forward and reverse direction .",38,0,20
dataset/preprocessed/training-data/sentiment_analysis/19,"In general a SuBiLSTM can be used as a drop in replacement in any model that uses the intermediate states of a BiLSTM , without changing any other parts of the model .",39,0,33
dataset/preprocessed/training-data/sentiment_analysis/19,"However , the main motivation for introducing SuBiLSTMs is to apply it to problems that require whole sentence modeling e.g. text classification , where the richer contextual information can be helpful .",40,0,32
dataset/preprocessed/training-data/sentiment_analysis/19,"We demonstrate the effectiveness of SuBiLSTM on several sentence modeling tasks in NLP - general sentence representation , text classification , textual entailment and paraphrase detection .",41,0,27
dataset/preprocessed/training-data/sentiment_analysis/19,"In each of these tasks , we show gains by simply replacing BiLSTMs in strong base models , achieving a new state - of - the - art in fine grained sentiment classification and question classification .",42,0,37
dataset/preprocessed/training-data/sentiment_analysis/19,Here ; is the concatenation operator .,43,0,7
dataset/preprocessed/training-data/sentiment_analysis/19,This defines the SuBiL - STM model .,44,0,8
dataset/preprocessed/training-data/sentiment_analysis/19,We also define another representation where the two LSTMs encoding the sequence in the same direction are the same or their weights are tied .,45,0,25
dataset/preprocessed/training-data/sentiment_analysis/19,"This defines the SuBiLSTM - Tied model , which concretely is",46,0,11
dataset/preprocessed/training-data/sentiment_analysis/19,"In contrast to SuBiLSTM , a standard BiLSTM uses the following contextual representation of s.",47,0,15
dataset/preprocessed/training-data/sentiment_analysis/19,"For a fixed hidden dimension , SuBiLSTM and SuBiLSTM - Tied have the same representation length as a BiLSTM .",48,0,20
dataset/preprocessed/training-data/sentiment_analysis/19,"Importantly , SuBiLSTM - Tied uses the same number of parameters as a BiLSTM , while SuBiLSTM uses twice as many .",49,0,22
dataset/preprocessed/training-data/sentiment_analysis/22,Hierarchical Attention Based Position-aware Network for Aspect-level Sentiment Analysis,2,1,9
dataset/preprocessed/training-data/sentiment_analysis/22,Aspect - level sentiment analysis aims to identify the sentiment of a specific target in its context .,4,0,18
dataset/preprocessed/training-data/sentiment_analysis/22,Previous works have proved that the interactions between aspects and the contexts are important .,5,0,15
dataset/preprocessed/training-data/sentiment_analysis/22,"On this basis , we also propose a succinct hierarchical attention based mechanism to fuse the information of targets and the contextual words .",6,0,24
dataset/preprocessed/training-data/sentiment_analysis/22,"In addition , most existing methods ignore the position information of the aspect when encoding the sentence .",7,0,18
dataset/preprocessed/training-data/sentiment_analysis/22,"In this paper , we argue that the position - aware representations are beneficial to this task .",8,0,18
dataset/preprocessed/training-data/sentiment_analysis/22,"Therefore , we propose a hierarchical attention based position - aware network ( HAPN ) , which introduces position embeddings to learn the position - aware representations of sentences and further generate the target - specific representations of contextual words .",9,0,41
dataset/preprocessed/training-data/sentiment_analysis/22,The experimental results on SemEval 2014 dataset show that our approach outperforms the state - of - theart methods .,10,0,20
dataset/preprocessed/training-data/sentiment_analysis/22,"Aspect - level sentiment analysis is a fine - grained task in sentiment analysis , which aims to identify the sentiment polarity ( i.e. , negative , neutral , or positive ) of a specific opinion target expressed in a comment / review by a reviewer .",12,1,47
dataset/preprocessed/training-data/sentiment_analysis/22,"For example , given a sentence "" The price is reasonable although the service is poor "" , the sentiment polarity for aspects "" price "" and "" service "" are positive and negative respectively .",13,0,36
dataset/preprocessed/training-data/sentiment_analysis/22,"Traditional methods for aspect - level sentiment analysis mainly focus on designing a set of features ( such as bag - of - words , sentiment lexicons , and linguistic features ) to train a classifier for sentiment classification .",14,0,40
dataset/preprocessed/training-data/sentiment_analysis/22,"However , such kind of feature engineering work often relies on human ingenuity , which is a timeconsuming process and lacks generalization .",15,0,23
dataset/preprocessed/training-data/sentiment_analysis/22,"In recent years , more and more neural network based models have been proposed and obtained the stateof - the - art results .",16,0,24
dataset/preprocessed/training-data/sentiment_analysis/22,"As previous research reveals that 40 % of sentiment classification errors are caused by not considering targets in sentiment classification , recent works tend to focus on fusing the information of the targets and the contexts .",17,0,37
dataset/preprocessed/training-data/sentiment_analysis/22,and both concatenated the aspect embeddings and embeddings of each word as inputs to a LSTM based model so as to introduce the information of the target into the model .,18,0,31
dataset/preprocessed/training-data/sentiment_analysis/22,adopted circular convolution and circular correlation to model the similarity between aspect and contextual words .,19,0,16
dataset/preprocessed/training-data/sentiment_analysis/22,and both employed a bidirectional attention operation to achieve the representations of targets and contextual words determined by each other .,20,0,21
dataset/preprocessed/training-data/sentiment_analysis/22,introduced an attention - over- attention based network to model the aspects and contexts in a joint way and explicitly capture the interaction between aspects and the context .,21,0,29
dataset/preprocessed/training-data/sentiment_analysis/22,"As described above , the existing studies show that the interactions between aspects and the context are important to the aspect - level sentiment analysis .",22,0,26
dataset/preprocessed/training-data/sentiment_analysis/22,"Leveraging this idea , we also propose a succinct hierarchical attention based mechanism to fuse the information of targets and the contextual words , which aims to generate the target - specific representations of each word .",23,0,37
dataset/preprocessed/training-data/sentiment_analysis/22,"In addition , most of the above methods ignore the position information of the aspect when",24,0,16
dataset/preprocessed/training-data/sentiment_analysis/22,Hierarchical Attention Based Position - aware Network for Aspect - level Sentiment Analysis,25,0,13
dataset/preprocessed/training-data/sentiment_analysis/22,"Lishuang Li , Yang Liu and AnQiao Zhou School of Computer Science and Technology , Dalian University of Technology lilishuang314@163.com encoding the sentence .",26,0,24
dataset/preprocessed/training-data/sentiment_analysis/22,We argue that the position of a candidate aspect is important for the sentence modelling .,27,0,16
dataset/preprocessed/training-data/sentiment_analysis/22,"For instance , consider the sentence "" I bought a mobile phone , it s camera is wonderful but the battery life is a bit short "" .",28,0,28
dataset/preprocessed/training-data/sentiment_analysis/22,"For the candidate aspect "" battery life "" , "" wonderful "" and "" short "" are both likely to be considered as its adjunct word .",29,0,27
dataset/preprocessed/training-data/sentiment_analysis/22,"In this case , if we encode the position information into the representation of each word effectively , we would have more confidence in concluding that the "" short "" is the adjunct word of "" battery life "" and predict the sentiment as negative .",30,0,46
dataset/preprocessed/training-data/sentiment_analysis/22,"Then , the next problem is how to introduce the position information .",31,0,13
dataset/preprocessed/training-data/sentiment_analysis/22,"In some previous works , they weighted the representation of each word according to the position , and the words close to the aspect could be paid more attention .",32,0,30
dataset/preprocessed/training-data/sentiment_analysis/22,"However , this operation is not always reasonable and sometimes the adjunct word maybe faraway from the target word .",33,0,20
dataset/preprocessed/training-data/sentiment_analysis/22,"Thus , we introduce position embeddings when modelling the sentence and further generate the positionaware representations .",34,0,17
dataset/preprocessed/training-data/sentiment_analysis/22,"In other words , the position information is considered as a kind of features and embedded into position embeddings .",35,0,20
dataset/preprocessed/training-data/sentiment_analysis/22,The model will learn to exploit both of the semantic information and the position clues .,36,0,16
dataset/preprocessed/training-data/sentiment_analysis/22,"Based on the analysis above , in this paper , we propose a hierarchical attention based positionaware network ( HAPN ) for aspect - level sentiment classification .",37,0,28
dataset/preprocessed/training-data/sentiment_analysis/22,A position - aware encoding layer is introduced for modelling the sentence to achieve the position - aware abstract representation of each word .,38,0,24
dataset/preprocessed/training-data/sentiment_analysis/22,"On this basis , a succinct fusion mechanism is further proposed to fuse the information of aspects and the contexts , achieving the final sentence representation .",39,0,27
dataset/preprocessed/training-data/sentiment_analysis/22,"Finally , we feed the achieved sentence representation into a softmax layer to predict the sentiment polarity .",40,0,18
dataset/preprocessed/training-data/sentiment_analysis/22,"We evaluate our approach on SemEval 2014 dataset , containing reviews of restaurant domain and laptop domain .",41,0,18
dataset/preprocessed/training-data/sentiment_analysis/22,"The experimental results demonstrate that the proposed approach is effective for aspect - level sentiment classification , and it outperforms state - of - the - art approaches with remarkable gains .",42,0,32
dataset/preprocessed/training-data/sentiment_analysis/22,We make our source code public at https://github.com/DUT-LiuYang/Aspect-Sentiment-Analysis.,43,0,8
dataset/preprocessed/training-data/sentiment_analysis/22,Many approaches have been proposed to address the problem of aspect - level sentiment analysis .,45,0,16
dataset/preprocessed/training-data/sentiment_analysis/22,"Traditional approaches to this task normally exploited a diverse set of strategies to convert classification clues ( i.e. , sentiment lexicons , bagof - word ) into feature vectors .",46,0,30
dataset/preprocessed/training-data/sentiment_analysis/22,"Although these methods have achieved comparable performance , their models highly depend on the effectiveness of the handcraft features which are labor intensive and lack generalization .",47,0,27
dataset/preprocessed/training-data/sentiment_analysis/22,"Therefore , many neural network based models have been proposed in recent years .",48,0,14
dataset/preprocessed/training-data/sentiment_analysis/22,And most current state - of - the - art works in aspect - based sentiment analysis pay more attention to fusing the information of the targets and contextual words .,49,0,31
dataset/preprocessed/training-data/sentiment_analysis/1,EEG - Based Emotion Recognition Using Regularized Graph Neural Networks,2,1,10
dataset/preprocessed/training-data/sentiment_analysis/1,EEG signals measure the neuronal activities on different brain regions via electrodes .,4,0,13
dataset/preprocessed/training-data/sentiment_analysis/1,Many existing studies on EEG - based emotion recognition do not exploit the topological structure of EEG signals .,5,0,19
dataset/preprocessed/training-data/sentiment_analysis/1,"In this paper , we propose a regularized graph neural network ( RGNN ) for EEG - based emotion recognition , which is biologically supported and captures both local and global inter-channel relations .",6,0,34
dataset/preprocessed/training-data/sentiment_analysis/1,"Specifically , we model the inter-channel relations in EEG signals via an adjacency matrix in our graph neural network where the connection and sparseness of the adjacency matrix are supported by the neurosicience theories of human brain organization .",7,0,39
dataset/preprocessed/training-data/sentiment_analysis/1,"In addition , we propose two regularizers , namely node - wise domain adversarial training ( NodeDAT ) and emotion - aware distribution learning ( EmotionDL ) , to improve the robustness of our model against cross - subject EEG variations and noisy labels , respectively .",8,0,47
dataset/preprocessed/training-data/sentiment_analysis/1,"To thoroughly evaluate our model , we conduct extensive experiments in both subject - dependent and subject - independent classification settings on two public datasets : SEED and SEED - IV .",9,0,32
dataset/preprocessed/training-data/sentiment_analysis/1,"Our model obtains better performance than competitive baselines such as SVM , DBN , DGCNN , BiDANN , and the state - of - the - art BiHDM in most experimental settings .",10,0,33
dataset/preprocessed/training-data/sentiment_analysis/1,Our model analysis demonstrates that the proposed biologically supported adjacency matrix and two regularizers contribute consistent and significant gain to the performance .,11,0,23
dataset/preprocessed/training-data/sentiment_analysis/1,"Investigations on the neuronal activities reveal that pre-frontal , parietal and occipital regions maybe the most informative regions for emotion recognition , which is consistent with relevant prior studies .",12,0,30
dataset/preprocessed/training-data/sentiment_analysis/1,"In addition , experimental results suggest that global inter-channel relations between the left and right hemispheres are important for emotion recognition and local inter-channel relations between ( FP1 , AF3 ) , ( F6 , F8 ) and ( FP2 , AF4 ) may also provide useful information .",13,0,49
dataset/preprocessed/training-data/sentiment_analysis/1,"E MOTION recognition is an important subarea of affective computing , which focuses on recognizing human emotions based on a variety of modalities , such as audio- visual expressions , body language , physiological signals , etc .",15,1,38
dataset/preprocessed/training-data/sentiment_analysis/1,"Compared to other modalities , physiological signals , such as electroencephalogram ( EEG ) , electrocardiogram ( ECG ) , electromyogram ( EMG ) , galvanic skin response ( GSR ) , etc. , have the advantage of being difficult to hide or disguise .",16,0,45
dataset/preprocessed/training-data/sentiment_analysis/1,"In recent years , due to the rapid development of noninvasive , easy - to - use and inexpensive EEG recording devices , EEG - based emotion recognition has received an increasing amount of attention in both research and applications .",17,0,41
dataset/preprocessed/training-data/sentiment_analysis/1,Emotion models can be broadly categorized into discrete models and dimensional models .,18,0,13
dataset/preprocessed/training-data/sentiment_analysis/1,"The former categorizes emotions into discrete entities , e.g. , anger , disgust , fear , happiness , sadness , and surprise in Ekman 's theory .",19,0,27
dataset/preprocessed/training-data/sentiment_analysis/1,"The latter describes emotions using their underlying dimensions , e.g. , valence , arousal and dominance , which measures emotions from unpleasant to pleasant , passive to active , and submissive to dominant , respectively .",20,0,36
dataset/preprocessed/training-data/sentiment_analysis/1,EEG signals measure voltage fluctuations from the cortex in the brain and have been shown to reveal important information about human emotional states .,21,0,24
dataset/preprocessed/training-data/sentiment_analysis/1,"For example , greater relative left frontal EEG activity has been observed P. Zhong , D. when experiencing positive emotions .",22,0,21
dataset/preprocessed/training-data/sentiment_analysis/1,The voltage fluctuations on different brain regions are measured by electrodes attached to the scalp .,23,0,16
dataset/preprocessed/training-data/sentiment_analysis/1,Each electrode collects EEG signals in one channel .,24,0,9
dataset/preprocessed/training-data/sentiment_analysis/1,"The collected EEG signals are often analyzed in specific frequency bands for each channel , namely delta ( 1 - 4 Hz ) , thet a ( 4 -7 Hz ) , alpha , beta , and gamma ( > 30 Hz ) .",25,0,44
dataset/preprocessed/training-data/sentiment_analysis/1,Many existing EEG - based emotion recognition methods are primarily based on the supervised machine learning approach wherein features are extracted from preprocessed EEG signals in each channel over a time window and then a classifier is trained on the extracted features to recognize emotions .,26,0,46
dataset/preprocessed/training-data/sentiment_analysis/1,Wang et al.,27,0,3
dataset/preprocessed/training-data/sentiment_analysis/1,"compared power spectral density features ( PSD ) , wavelet features and nonlinear dynamical features with a Support Vector Machine ( SVM ) classifier .",28,0,25
dataset/preprocessed/training-data/sentiment_analysis/1,"Zheng and Lu investigated critical frequency bands and channels using PSD , differential entropy ( DE ) and PSD asymmetry features , and obtained robust accuracy using deep belief networks ( DBN ) .",29,0,34
dataset/preprocessed/training-data/sentiment_analysis/1,"However , most existing EEGbased emotion recognition approaches do not address the following three challenges :",30,0,16
dataset/preprocessed/training-data/sentiment_analysis/1,"1 ) the topological structure of EEG signals are not effectively exploited to learn more discriminative EEG representations ; 2 ) EEG signals vary significantly across different subjects , which hinders the generalizability of the trained classifiers ; and 3 ) participants may not always generate the intended emotions when watching emotion - eliciting stimuli .",31,0,56
dataset/preprocessed/training-data/sentiment_analysis/1,"Consequently , the emotion labels in the collected EEG data are noisy and may not be consistent with the actual elicited emotions .",32,0,23
dataset/preprocessed/training-data/sentiment_analysis/1,There have been several attempts to address the first challenge .,33,0,11
dataset/preprocessed/training-data/sentiment_analysis/1,Zhang et al. and Zhang et al .,34,0,8
dataset/preprocessed/training-data/sentiment_analysis/1,"incorporated spatial relations in EEG signals using convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) , respectively .",35,0,23
dataset/preprocessed/training-data/sentiment_analysis/1,"However , their approaches require a 2D representation of EEG channels on the scalp , which may cause information loss during flattening because channels are actually arranged in the 3D space .",36,0,32
dataset/preprocessed/training-data/sentiment_analysis/1,"In addition , their approach of using CNNs and RNNs to capture inter-channel relations has difficulty in learning long - range dependencies .",37,0,23
dataset/preprocessed/training-data/sentiment_analysis/1,Graph neural networks ( GNN ) has been applied in to capture inter-channel relations using an adjacency matrix .,38,0,19
dataset/preprocessed/training-data/sentiment_analysis/1,"However , similar to CNNs and RNNs , their approach only considers relations between the nearest channels , which thus may lose valuable information between distant channels , such as PSD asymmetry between channels on the left and right hemispheres in the frontal region , which has been shown as informative in valence prediction .",39,0,55
dataset/preprocessed/training-data/sentiment_analysis/1,A recent work applies RNNs to learn EEG representations in the two hemispheres separately and then adopts the asymmetric differences between them to recognize emotions .,40,0,26
dataset/preprocessed/training-data/sentiment_analysis/1,"However , their approach is limited to using only the bi-hemispherical discrepancies and ignores other useful features such as neuronal activities recorded from each channel .",41,0,26
dataset/preprocessed/training-data/sentiment_analysis/1,"In recent years , several studies , investigated the transferability of EEG - based emotion recognition models across subjects .",42,0,20
dataset/preprocessed/training-data/sentiment_analysis/1,Lan et al.,43,0,3
dataset/preprocessed/training-data/sentiment_analysis/1,"compared several domain adaptation techniques such as maximum independence domain adaptation ( MIDA ) , transfer component analysis ( TCA ) , subspace alignment ( SA ) , etc .",44,0,30
dataset/preprocessed/training-data/sentiment_analysis/1,They found that the subject - independent classification accuracy can be improved by around 10 % .,45,0,17
dataset/preprocessed/training-data/sentiment_analysis/1,Li et al. applied domain adversarial learning to lower the influence of individual subject on EEG data and obtained improved performance as well .,46,0,24
dataset/preprocessed/training-data/sentiment_analysis/1,"However , their approaches do not exploit any graph structure and only leads to small performance improvement ( see Section 7.1 ) .",47,0,23
dataset/preprocessed/training-data/sentiment_analysis/1,"To the best of our knowledge , no attempt has been made to address the problem of noisy labels in EEG - based emotion recognition .",48,0,26
dataset/preprocessed/training-data/sentiment_analysis/1,"In this paper , we propose a regularized graph neural network ( RGNN ) aiming to address all three aforementioned challenges .",49,0,22
dataset/preprocessed/training-data/sentiment_analysis/5,A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition,2,1,9
dataset/preprocessed/training-data/sentiment_analysis/5,The neuroscience study [ 1 ] has revealed the discrepancy of emotion expression between left and right hemispheres of human brain .,4,0,22
dataset/preprocessed/training-data/sentiment_analysis/5,"Inspired by this study , in this paper , we propose a novel bi-hemispheric discrepancy model ( BiHDM ) to learn the asymmetric differences between two hemispheres for electroencephalograph ( EEG ) emotion recognition .",5,1,35
dataset/preprocessed/training-data/sentiment_analysis/5,"Concretely , we first employ four directed recurrent neural networks ( RNNs ) based on two spatial orientations to traverse electrode signals on two separate brain regions , which enables the model to obtain the deep representations of all the EEG electrodes ' signals while keeping the intrinsic spatial dependence .",6,0,51
dataset/preprocessed/training-data/sentiment_analysis/5,Then we design a pairwise subnetwork to capture the discrepancy information between two hemispheres and extract higher - level features for final classification .,7,0,24
dataset/preprocessed/training-data/sentiment_analysis/5,"Besides , in order to reduce the domain shift between training and testing data , we use a domain discriminator that adversarially induces the over all feature learning module to generate emotion - related but domain - invariant feature , which can further promote EEG emotion recognition .",8,0,48
dataset/preprocessed/training-data/sentiment_analysis/5,"We conduct experiments on three public EEG emotional datasets , and the experiments show that the new state - of - the - art results can be achieved .",9,0,29
dataset/preprocessed/training-data/sentiment_analysis/5,"Emotion , as a common mental phenomenon , is closely related to our daily life .",11,0,16
dataset/preprocessed/training-data/sentiment_analysis/5,"Although it is easy to sense other people 's emotion in human - human interaction , it is still difficult for machines to understand the complicated emotions of human beings .",12,0,31
dataset/preprocessed/training-data/sentiment_analysis/5,"As the first step to make machines capture human emotions , emotion recognition has received substantial attention from human - machine - interaction ( HMI ) and pattern recognition research communities in recent years , , .",13,1,37
dataset/preprocessed/training-data/sentiment_analysis/5,"Human emotional expressions are mostly based on verbal behavior methods ( e.g. , speech ) , and nonverbal behavior methods ( e.g. , facial expression ) .",14,0,27
dataset/preprocessed/training-data/sentiment_analysis/5,"Thus , a large body of literature concentrates on learning the emotional components from speech and facial expression data .",15,0,20
dataset/preprocessed/training-data/sentiment_analysis/5,"However , from the viewpoint of neuroscience , humans emotion originates from a variety of brain cortex regions , such as the orbital frontal cortex , ventral medial prefrontal cortex , and amygdala , which provides us a potential approach to decode emotion by recording the continuous human brain activity signals over these brain regions .",16,0,56
dataset/preprocessed/training-data/sentiment_analysis/5,"For example , by placing the EEG electrodes on the scalp , we can record the neural activities in the brain , which can be used to recognize human emotions .",17,0,31
dataset/preprocessed/training-data/sentiment_analysis/5,Most existing EEG emotion recognition methods focus on two fundamental challenges .,18,0,12
dataset/preprocessed/training-data/sentiment_analysis/5,One is how to extract discriminative features related to emotions .,19,0,11
dataset/preprocessed/training-data/sentiment_analysis/5,"Typically , EEG features can be extracted from the time domain , frequency domain , and time - frequency domain .",20,0,21
dataset/preprocessed/training-data/sentiment_analysis/5,"In , Jenke et al . evaluated all the existing features by using machine learning techniques on a self - recorded dataset .",21,0,23
dataset/preprocessed/training-data/sentiment_analysis/5,The other challenge is how to classify the features correctly .,22,0,11
dataset/preprocessed/training-data/sentiment_analysis/5,"Many EEG emotion recognition models and methods have been proposed over the past years , .",23,0,16
dataset/preprocessed/training-data/sentiment_analysis/5,"For example , Zheng et al. proposed a group sparse canonical correlation analysis method for simultaneous EEG channel selection and emotion recognition .",24,0,23
dataset/preprocessed/training-data/sentiment_analysis/5,Li et al.,25,0,3
dataset/preprocessed/training-data/sentiment_analysis/5,fused the information propagation patterns and activation difference in the brain to improve the performance of emotional recognition .,26,0,19
dataset/preprocessed/training-data/sentiment_analysis/5,These techniques have shown excellent performance on some EEG emotional datasets .,27,0,12
dataset/preprocessed/training-data/sentiment_analysis/5,"Recently , many researchers have attempted to consider the neuroscience findings of emotion as the prior knowledge to extract features or develop models , effectively enhancing the performance of EEG emotion recognition .",28,0,33
dataset/preprocessed/training-data/sentiment_analysis/5,"For example , Hinrikus et al.",29,0,6
dataset/preprocessed/training-data/sentiment_analysis/5,used EEG spectral asymmetry index for the depression detection .,30,0,10
dataset/preprocessed/training-data/sentiment_analysis/5,"It is well realized through neuroscience study that although the anatomy of human brain looks like symmetric , the left and right hemispheres have different responses to emotions .",31,0,29
dataset/preprocessed/training-data/sentiment_analysis/5,"For example , from the view of neuroscience , Dimond et al. , Davidson et al. , and Herrington et al.",32,0,21
dataset/preprocessed/training-data/sentiment_analysis/5,"have studied the asymmetry of emotion expression , and Schwartz et al. ,",33,0,13
dataset/preprocessed/training-data/sentiment_analysis/5,"Wager et al. , and Costanzo et al. have discussed the emotion lateralization .",34,0,14
dataset/preprocessed/training-data/sentiment_analysis/5,"Furthermore , the literature of EEG emotion recognition has seen the use of asymmetry to classify EEG emotional signal .",35,0,20
dataset/preprocessed/training-data/sentiment_analysis/5,"Lin et al. investigated the relationships between emotional states and brain activities , and extracted power spectrum density , differential asymmetry power , and rational asymmetry power as the features .",36,0,31
dataset/preprocessed/training-data/sentiment_analysis/5,"Motivated by their previous findings of critical brain are as for emotion recognition , Zheng et al.",37,0,17
dataset/preprocessed/training-data/sentiment_analysis/5,selected six symmetrical temporal lobe electrodes as the critical channels for EEG emotion recognition .,38,0,15
dataset/preprocessed/training-data/sentiment_analysis/5,Li et al.,39,0,3
dataset/preprocessed/training-data/sentiment_analysis/5,separately extracted two brain hemispheric features and achieved the state - of - the - art classification performance .,40,0,19
dataset/preprocessed/training-data/sentiment_analysis/5,The above researches demonstrate that it is a promising and fruitful way to integrate the unique characteristics of EEG signal into the machine learning algorithms .,41,0,26
dataset/preprocessed/training-data/sentiment_analysis/5,It will bean interesting and meaningful topic of how to utilize this discrepancy property of two brain hemispheres to improve EEG emotion recognition .,42,0,24
dataset/preprocessed/training-data/sentiment_analysis/5,"Thus , in this paper , we propose a novel neural network model BiHDM to learn the bi-hemispheric discrepancy for EEG emotion recognition .",43,0,24
dataset/preprocessed/training-data/sentiment_analysis/5,"BiHDM aims to obtain the deep discrepant features between the left and right hemispheres , which is expected to contain more discriminative information to recognize the EEG emotion signals .",44,0,30
dataset/preprocessed/training-data/sentiment_analysis/5,"To achieve this goal , we need to solve two major problems , i.e. , how to extract the features for each hemispheric EEG data and meanwhile measure the difference between them .",45,0,33
dataset/preprocessed/training-data/sentiment_analysis/5,"Unlike other data structures such as skeletal action data , in which the position of each node varies with time , the EEG data consists of several electrodes thatare set under the predefined coordinates on the scalp .",46,0,38
dataset/preprocessed/training-data/sentiment_analysis/5,"Hence , to avoid losing this intrinsic graph structural information of EEG data , we can simplify the graph structure learning process by using the horizontal and vertical traversing RNNs , which will construct a complete relationship graph and generate discriminative deep features for all the EEG electrodes .",47,0,49
dataset/preprocessed/training-data/sentiment_analysis/5,"After obtaining these deep features of each electrodes , we can extract the asymmetric discrepancy information between two hemispheres by performing specific pairwise operations for any paired symmetric electrodes .",48,0,30
dataset/preprocessed/training-data/sentiment_analysis/5,The concrete process is as follows :,49,0,7
dataset/preprocessed/training-data/sentiment_analysis/27,Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect - level Sentiment Classification,2,1,13
dataset/preprocessed/training-data/sentiment_analysis/27,Aspect - level sentiment classification aims to distinguish the sentiment polarities over one or more aspect terms in a sentence .,4,0,21
dataset/preprocessed/training-data/sentiment_analysis/27,"Existing approaches mostly model different aspects in one sentence independently , which ignore the sentiment dependencies between different aspects .",5,0,20
dataset/preprocessed/training-data/sentiment_analysis/27,"However , we find such dependency information between different aspects can bring additional valuable information .",6,0,16
dataset/preprocessed/training-data/sentiment_analysis/27,"In this paper , we propose a novel aspect - level sentiment classification model based on graph convolutional networks ( GCN ) which can effectively capture the sentiment dependencies between multi-aspects in one sentence .",7,0,35
dataset/preprocessed/training-data/sentiment_analysis/27,"Our model firstly introduces bidirectional attention mechanism with position encoding to model aspect - specific representations between each aspect and its context words , then employs GCN over the attention mechanism to capture the sentiment dependencies between different aspects in one sentence .",8,0,43
dataset/preprocessed/training-data/sentiment_analysis/27,We evaluate the proposed approach on the SemEval 2014 datasets .,9,0,11
dataset/preprocessed/training-data/sentiment_analysis/27,Experiments show that our model outperforms the state - of - the - art methods .,10,0,16
dataset/preprocessed/training-data/sentiment_analysis/27,"We also conduct experiments to evaluate the effectiveness of GCN module , which indicates that the dependencies between different aspects is highly helpful in aspect - level sentiment classification .",11,0,30
dataset/preprocessed/training-data/sentiment_analysis/27,Aspect - level sentiment classification is a fundamental natural language processing task that gets lots of attention in recent years .,13,0,21
dataset/preprocessed/training-data/sentiment_analysis/27,"It is a fine - grained task in sentiment analysis , which aims to infer the sentiment polarities of aspects in their context .",14,1,24
dataset/preprocessed/training-data/sentiment_analysis/27,"For example , in the sentence "" The price is reasonable although the service is poor "" , the sentiment polarities for the two aspect terms , "" price "" and "" service "" , are positive and negative respectively .",15,0,41
dataset/preprocessed/training-data/sentiment_analysis/27,An aspect term ( or simply aspect ) is usually an entity or an entity aspect .,16,0,17
dataset/preprocessed/training-data/sentiment_analysis/27,"Aspect - level sentiment classification is much more complicated than sentence - level sentiment classification , because identifying the parts of sentence describing the corresponding aspects is difficult .",17,0,29
dataset/preprocessed/training-data/sentiment_analysis/27,"Traditional approaches mainly focus on statistical methods to design a set of handcrafted features to train a classifier ( e.g. , Support Vector Machine ) .",18,0,26
dataset/preprocessed/training-data/sentiment_analysis/27,"However , such kind of feature - based work is labor - intensive .",19,0,14
dataset/preprocessed/training-data/sentiment_analysis/27,"In recent years , neural network models are of growing interest for their capacity to automatically generate useful low dimensional representations from aspects and their contexts , and achieve great accuracy on the aspect - level sentiment classification without careful engineering of features .",20,0,44
dataset/preprocessed/training-data/sentiment_analysis/27,"Especially , by the ability to effectively identify which words in the sentence are more important on a given aspect , attention mechanisms implemented by neural networks are widely used in aspect - level sentiment classification .",21,0,37
dataset/preprocessed/training-data/sentiment_analysis/27,"The setting is romantic , but the food is horrible , the service is pathetic .",22,0,16
dataset/preprocessed/training-data/sentiment_analysis/27,setting food service aspect - 1 aspect - 2 aspect - 3 opposite similar .,23,0,15
dataset/preprocessed/training-data/sentiment_analysis/27,An example to illustrate the usefulness of the sentiment dependencies between multiple aspects .,24,0,14
dataset/preprocessed/training-data/sentiment_analysis/27,"The dependencies can be inferred by some knowledge in the sentence , e.g. , conjunction .",25,0,16
dataset/preprocessed/training-data/sentiment_analysis/27,"The evidence of the usefulness of the sentiment dependencies is that we can easily guess the true sentiment of "" food "" even if we mask the word "" horrible "" .",26,0,32
dataset/preprocessed/training-data/sentiment_analysis/27,mechanism with a gated recurrent unit network to capture the relevance between each context word and the aspect .,27,0,19
dataset/preprocessed/training-data/sentiment_analysis/27,Ma et al. design a model which learns the representations of the aspect and context interactively with two attention mechanisms .,28,0,21
dataset/preprocessed/training-data/sentiment_analysis/27,Song et al.,29,0,3
dataset/preprocessed/training-data/sentiment_analysis/27,"propose an attentional encoder network , which employ multi-head attention for the modeling between context and aspect .",30,0,18
dataset/preprocessed/training-data/sentiment_analysis/27,These attention - based models have proven to be successful and effective in learning aspect - specific representations .,31,0,19
dataset/preprocessed/training-data/sentiment_analysis/27,"Despite these advances , the studies above still remain problems .",32,0,11
dataset/preprocessed/training-data/sentiment_analysis/27,"They all build models with each aspect individually ignoring the sentiment dependencies information between mul - tiple aspects , which will lose some additional valuable information .",33,0,27
dataset/preprocessed/training-data/sentiment_analysis/27,"For example , as we can see from the example given in , the sentiment polarity of the first aspect "" setting "" is positive .",34,0,26
dataset/preprocessed/training-data/sentiment_analysis/27,"From the conjunction "" but "" , we are easy to know that the second aspect "" food "" has opposite sentiment polarity with "" setting "" .",35,0,28
dataset/preprocessed/training-data/sentiment_analysis/27,"By this sentiment dependency relation , we can guess the polarity of aspect "" food "" is negative .",36,0,19
dataset/preprocessed/training-data/sentiment_analysis/27,"Similarly , from the second comma , we conjecture that the sentiment polarity of the last aspect "" service "" is likely the same as "" food "" .",37,0,29
dataset/preprocessed/training-data/sentiment_analysis/27,"Therefore , the sentiment dependencies are helpful to infer the sentiment polarities of aspects in one sentence .",38,0,18
dataset/preprocessed/training-data/sentiment_analysis/27,"In this paper , we propose a novel method to model Sentiment Dependencies with Graph Convolutional Networks ( SDGCN ) for aspect - level sentiment classification .",39,0,27
dataset/preprocessed/training-data/sentiment_analysis/27,"GCN is a simple and effective convolutional neural network operating on graphs , which can catch inter-dependent information from rich relational data .",40,0,23
dataset/preprocessed/training-data/sentiment_analysis/27,"For every node in graph , GCN encodes relevant information about its neighborhoods as a new feature representation vector .",41,0,20
dataset/preprocessed/training-data/sentiment_analysis/27,"In our case , an aspect is treated as a node , and an edge represents the sentiment dependency relation of two nodes .",42,0,24
dataset/preprocessed/training-data/sentiment_analysis/27,Our model learns the sentiment dependencies of aspects via this graph structure .,43,0,13
dataset/preprocessed/training-data/sentiment_analysis/27,"As far as we know , our work is the first to consider the sentiment dependencies between aspects in one sentence for aspect - level sentiment classification task .",44,0,29
dataset/preprocessed/training-data/sentiment_analysis/27,"Furthermore , in order to capture the aspect - specific representations , our model applies bidirectional attention mechanism with position encoding before GCN .",45,0,24
dataset/preprocessed/training-data/sentiment_analysis/27,We evaluate the proposed approach on the SemEval 2014 datasets .,46,0,11
dataset/preprocessed/training-data/sentiment_analysis/27,Experiments show that our model outperforms the state - of - the - art methods .,47,0,16
dataset/preprocessed/training-data/sentiment_analysis/27,The main contributions of this paper are presented as follows :,48,0,11
dataset/preprocessed/training-data/sentiment_analysis/27,"To the best of our knowledge , this is the first study to consider the sentiment dependencies between aspects in one sentence for aspect - level sentiment classification .",49,0,29
dataset/preprocessed/training-data/sentiment_analysis/47,Left - Center - Right Separated Neural Network for Aspect - based Sentiment Analysis with Rotatory Attention,2,1,17
dataset/preprocessed/training-data/sentiment_analysis/47,Deep learning techniques have achieved success in aspect - based sentiment analysis in recent years .,4,0,16
dataset/preprocessed/training-data/sentiment_analysis/47,"However , there are two important issues that still remain to be further studied , i.e. , 1 ) how to efficiently represent the target especially when the target contains multiple words ; 2 ) how to utilize the interaction between target and left / right contexts to capture the most important words in them .",5,0,56
dataset/preprocessed/training-data/sentiment_analysis/47,"In this paper , we propose an approach , called left - centerright separated neural network with rotatory attention ( LCR - Rot ) , to better address the two problems .",6,0,32
dataset/preprocessed/training-data/sentiment_analysis/47,Our approach has two characteristics :,7,0,6
dataset/preprocessed/training-data/sentiment_analysis/47,"1 ) it has three separated LSTMs , i.e. , left , center and right LSTMs , corresponding to three parts of a review ( left context , target phrase and right context ) ;",8,0,35
dataset/preprocessed/training-data/sentiment_analysis/47,2 ) it has a rotatory attention mechanism which models the relation between target and left / right contexts .,9,0,20
dataset/preprocessed/training-data/sentiment_analysis/47,The target2context attention is used to capture the most indicative sentiment words in left / right contexts .,10,0,18
dataset/preprocessed/training-data/sentiment_analysis/47,"Subsequently , the context2target attention is used to capture the most important word in the target .",11,0,17
dataset/preprocessed/training-data/sentiment_analysis/47,This leads to a two - side representation of the target : left - aware target and right - aware target .,12,0,22
dataset/preprocessed/training-data/sentiment_analysis/47,We compare our approach on three benchmark datasets with ten related methods proposed recently .,13,0,15
dataset/preprocessed/training-data/sentiment_analysis/47,The results show that our approach significantly outperforms the state - of - the - art techniques .,14,0,18
dataset/preprocessed/training-data/sentiment_analysis/47,"Aspect - based sentiment analysis is a fine - grained classification task in sentiment analysis , identifying sentiment polarity of a sentence expressed toward a target .",16,1,27
dataset/preprocessed/training-data/sentiment_analysis/47,"In the early studies , methods for the aspect - based sentiment classification task were similar as that used in standard sentiment classification task .",17,1,25
dataset/preprocessed/training-data/sentiment_analysis/47,"Researchers normally designed a set of features ( such as bag - of - words , sentiment lexicons , and linguistic features ) to train a statistical learning algorithm for sentiment classification ; * The corresponding author of this paper ..",18,0,41
dataset/preprocessed/training-data/sentiment_analysis/47,"However , such kind of feature engineering work was labor - intensive and almost reached its performance bottleneck .",19,0,19
dataset/preprocessed/training-data/sentiment_analysis/47,"In recently years , more and more researchers have adopted more advanced deep learning algorithms .",20,0,16
dataset/preprocessed/training-data/sentiment_analysis/47,"By taking advantage of the powerful representation ability , well - designed neural networks can automatically generate meaningful low - dimensional representations for the targets and their contexts , and obtained the state - of the - art results in aspect - based sentiment classification task .",21,0,47
dataset/preprocessed/training-data/sentiment_analysis/47,"As we have mentioned , aspect - based sentiment classification differs from traditional sentiment classification in that the former is target - related .",22,0,24
dataset/preprocessed/training-data/sentiment_analysis/47,pointed out that 40 % of the classification errors are caused by ignoring the target information in twitter sentiment classification .,23,0,21
dataset/preprocessed/training-data/sentiment_analysis/47,The sentiment polarity of a sentence is strongly related to its target in aspect - based sentiment analysis .,24,0,19
dataset/preprocessed/training-data/sentiment_analysis/47,"Taking the following sentence Example 1 : "" I am pleased with the life of battery , but the windows 8 operating system is so bad . "" for example , the target set is { the life of battery , windows 8 operating system } .",25,0,47
dataset/preprocessed/training-data/sentiment_analysis/47,"As far the target the life of battery is considered , the expected sentiment is positive ; by contrast , as far as the target windows 8 operating system is considered , the correct sentiment should be negative .",26,0,39
dataset/preprocessed/training-data/sentiment_analysis/47,"That is , in one review sentence , the sentiment toward different targets could be opposite .",27,0,17
dataset/preprocessed/training-data/sentiment_analysis/47,"Along with the deepening of research work , incorporating the target information into the model gradually becomes a consensus in aspect - based sentiment classification in recent years .",28,0,29
dataset/preprocessed/training-data/sentiment_analysis/47,"However , the previous way of modeling targets and contexts still have some shortcomings .",29,0,15
dataset/preprocessed/training-data/sentiment_analysis/47,"For one thing , according to our statistics , more than 25 % of the target on the Restaurant and 35 % of the target on the Laptop datasets contain at least two words , but almost all researchers ignore the case of target phrase that contains multiple words , and just used the average of target constituting word vectors to represent target .",30,0,64
dataset/preprocessed/training-data/sentiment_analysis/47,"For instance , proposed a target - connection long short - term memory ( LSTM ) model , which utilizes the connections between target and each context word when composing the representation of a sentence .",31,0,36
dataset/preprocessed/training-data/sentiment_analysis/47,"For another , the rep-resentations of targets and contexts are influenced by each other which is paid not enough attention .",32,0,21
dataset/preprocessed/training-data/sentiment_analysis/47,"Taking Example 1 for example , with respect to the target "" the life of battery "" , "" pleased "" should be paid with more attention than the other targets not related words ( such as "" bad "" ) in the context ; as for targets , "" life "" and "" battery "" should pay more attention in the representation of target "" the life of battery "" .",33,0,72
dataset/preprocessed/training-data/sentiment_analysis/47,"We can see that the representations of contexts are related to targets , meanwhile it is natural that targets are influenced by their contexts .",34,0,25
dataset/preprocessed/training-data/sentiment_analysis/47,"In summary , when employing deep neural networks for aspect - based sentiment classification , the following two problems remain to be further studied :",35,0,25
dataset/preprocessed/training-data/sentiment_analysis/47,Problem 1 : how to more efficiently represent the target especially when the target contains multiple words ?,36,0,18
dataset/preprocessed/training-data/sentiment_analysis/47,Problem 2 : how to utilize the interaction between targets and contexts to capture the most important words in the representation of targets and contexts ?,37,0,26
dataset/preprocessed/training-data/sentiment_analysis/47,"With the attempt to better address the two problems , in this paper we propose a left - center - right separated neural network with rotatory attention mechanism ( LCR - Rot ) .",38,0,34
dataset/preprocessed/training-data/sentiment_analysis/47,"Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( left context , target phrase and right context ) .",39,0,48
dataset/preprocessed/training-data/sentiment_analysis/47,"On this basis , we further propose a rotatory attention mechanism to take into account the interaction between targets and contexts to better represent targets and contexts .",40,0,28
dataset/preprocessed/training-data/sentiment_analysis/47,The target2context attention is used to capture the most indicative sentiment words in left / right contexts .,41,0,18
dataset/preprocessed/training-data/sentiment_analysis/47,"Subsequently , the context2target attention is used to capture the most important word in the target .",42,0,17
dataset/preprocessed/training-data/sentiment_analysis/47,This leads to a two - side representation of the target : left - aware target and right - aware target .,43,0,22
dataset/preprocessed/training-data/sentiment_analysis/47,"Finally , we concatenate the component representations as the final representation of the sentence and feed it into a softmax layer to predict the sentiment polarity .",44,0,27
dataset/preprocessed/training-data/sentiment_analysis/47,The key characteristics of our work can be summarized as follows :,45,0,12
dataset/preprocessed/training-data/sentiment_analysis/47,"With respect to Problem 1 , the target phrase is modeled with two - side representation which is combination of left - aware target and right - aware target .",47,0,30
dataset/preprocessed/training-data/sentiment_analysis/47,It better support the multi-word targets and leads to a significant improvement of classification performance ;,48,0,16
dataset/preprocessed/training-data/sentiment_analysis/18,Effective Attention Modeling for Aspect - Level Sentiment Classification,2,1,9
dataset/preprocessed/training-data/sentiment_analysis/18,Aspect - level sentiment classification aims to determine the sentiment polarity of a review sentence towards an opinion target .,4,0,20
dataset/preprocessed/training-data/sentiment_analysis/18,A sentence could contain multiple sentiment - target pairs ; thus the main challenge of this task is to separate different opinion contexts for different targets .,5,0,27
dataset/preprocessed/training-data/sentiment_analysis/18,"To this end , attention mechanism has played an important role in previous state - of - the - art neural models .",6,0,23
dataset/preprocessed/training-data/sentiment_analysis/18,The mechanism is able to capture the importance of each context word towards a target by modeling their semantic associations .,7,0,21
dataset/preprocessed/training-data/sentiment_analysis/18,We build upon this line of research and propose two novel approaches for improving the effectiveness of attention .,8,0,19
dataset/preprocessed/training-data/sentiment_analysis/18,"First , we propose a method for target representation that better captures the semantic meaning of the opinion target .",9,0,20
dataset/preprocessed/training-data/sentiment_analysis/18,"Second , we introduce an attention model that incorporates syntactic information into the attention mechanism .",10,0,16
dataset/preprocessed/training-data/sentiment_analysis/18,"We experiment on attention - based LSTM ( Long Short - Term Memory ) models using the datasets from SemEval 2014 , 2015 , and 2016 .",11,0,27
dataset/preprocessed/training-data/sentiment_analysis/18,The experimental results show that the conventional attention - based LSTM can be substantially improved by incorporating the two approaches .,12,0,21
dataset/preprocessed/training-data/sentiment_analysis/18,This work is licenced under a Creative Commons Attribution 4.0 International Licence .,13,0,13
dataset/preprocessed/training-data/sentiment_analysis/18,Licence details : http:// creativecommons.org/licenses/by/4.0/,14,0,5
dataset/preprocessed/training-data/sentiment_analysis/18,Aspect - level sentiment classification is an important task in fine - grained sentiment analysis .,16,1,16
dataset/preprocessed/training-data/sentiment_analysis/18,"Given a sentence and an opinion target ( also called aspect expression ) occurring in the sentence , the task aims to determine the sentiment polarity of the sentence towards the opinion target .",17,0,34
dataset/preprocessed/training-data/sentiment_analysis/18,An opinion target or target for short refers to a word or a phrase ( a sequence of words ) describing an aspect of an entity .,18,0,27
dataset/preprocessed/training-data/sentiment_analysis/18,"For example , in the sentence "" This little place has acute interior decor and affordable prices "" , the targets are interior decor and prices , and they belong to the aspects ambience and price respectively .",19,0,38
dataset/preprocessed/training-data/sentiment_analysis/18,"Compared to document - level or sentence - level sentiment classification , the main challenge of aspectlevel sentiment classification is to differentiate sentiments towards different targets when there are multiple targets in a sentence .",20,0,35
dataset/preprocessed/training-data/sentiment_analysis/18,"For instance , the sentence "" The appetizers are ok , but the service is slow . "" expresses a neutral sentiment on the target appetizers and a negative sentiment on the target service .",21,0,35
dataset/preprocessed/training-data/sentiment_analysis/18,"To this end , attention mechanism has played an important role in state - of - the - art neural models for this task .",22,0,25
dataset/preprocessed/training-data/sentiment_analysis/18,"It assigns a positive weight att i for each context word w i , which can be interpreted as the probability that w i is the right word to focus on when inferring the sentiment polarity of the given target .",23,0,41
dataset/preprocessed/training-data/sentiment_analysis/18,The weight att i is generally computed as a function of the hidden representation hi of w i and the target representation t as follows :,24,0,26
dataset/preprocessed/training-data/sentiment_analysis/18,It has been shown that adding an attention model substantially improves the accuracy of aspect - level sentiment classification .,25,0,20
dataset/preprocessed/training-data/sentiment_analysis/18,Our work builds upon this line of research .,26,0,9
dataset/preprocessed/training-data/sentiment_analysis/18,We propose two novel approaches for improving the effectiveness of attention models .,27,0,13
dataset/preprocessed/training-data/sentiment_analysis/18,The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression .,28,0,22
dataset/preprocessed/training-data/sentiment_analysis/18,The target representation is crucial since attention weights are computed based on it as shown in Eq.,29,0,17
dataset/preprocessed/training-data/sentiment_analysis/18,"In representing the target , we are mapping a word or a phrase into a vector in Rd .",31,0,19
dataset/preprocessed/training-data/sentiment_analysis/18,"Ideally , targets that are semantically similar should be mapped to vectors that are close together in Rd .",32,0,19
dataset/preprocessed/training-data/sentiment_analysis/18,"However , previous neural attention models simply map a target by averaging its component word vectors .",33,0,17
dataset/preprocessed/training-data/sentiment_analysis/18,"This may work fine for targets that only contain one word but may fail to capture the semantics of more complex expressions , as also mentioned by .",34,0,28
dataset/preprocessed/training-data/sentiment_analysis/18,"For example , we can not obtain a good representation for "" hot dog "" by averaging the word vectors of "" hot "" and "" dog "" .",35,0,29
dataset/preprocessed/training-data/sentiment_analysis/18,Hot would be close to words like warm or cold and dog would be close to animals like cat .,36,0,20
dataset/preprocessed/training-data/sentiment_analysis/18,The average would not be close to other food like burgers or spaghetti .,37,0,14
dataset/preprocessed/training-data/sentiment_analysis/18,"Another example is "" hong kong style food "" .",38,0,10
dataset/preprocessed/training-data/sentiment_analysis/18,"As it consists of many words , the averaged word vector could be faraway from "" food "" in vector space .",39,0,22
dataset/preprocessed/training-data/sentiment_analysis/18,"To address this problem , inspired by , we instead model each target as a mixture of K aspect embeddings where we would like each embedded aspect to represent a cluster of closely related targets .",40,0,36
dataset/preprocessed/training-data/sentiment_analysis/18,We use an autoencoder structure to learn both the aspect embeddings as well as the representation of the target as a weighted combination of the aspect embeddings .,41,0,28
dataset/preprocessed/training-data/sentiment_analysis/18,The weight vector represents the probability distribution over aspects for the given target .,42,0,14
dataset/preprocessed/training-data/sentiment_analysis/18,The autoencoder structure is jointly trained with a neural attention - based sentiment classifier to provide a good target representation as well as a high accuracy on the predicted sentiment .,43,0,31
dataset/preprocessed/training-data/sentiment_analysis/18,"We found the learned embeddings to be semantically meaningful , i.e. , embeddings of words that are semantically related appear close to the same aspect embedding .",44,0,27
dataset/preprocessed/training-data/sentiment_analysis/18,"For example , embeddings of the words service , servers , staff , and courteous appear close to the same aspect embedding , which we interpret to represent the aspect service .",45,0,32
dataset/preprocessed/training-data/sentiment_analysis/18,Our second approach exploits syntactic information to construct a syntax - based attention model .,46,0,15
dataset/preprocessed/training-data/sentiment_analysis/18,The attention models used in previous works give equal importance to all context words .,47,0,15
dataset/preprocessed/training-data/sentiment_analysis/18,"In that case , the computed attention weights rely entirely on the semantic associations between context words and the target .",48,0,21
dataset/preprocessed/training-data/sentiment_analysis/18,"However , this may not be sufficient for differentiating opinions words for different targets .",49,0,15
dataset/preprocessed/training-data/sentiment_analysis/15,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,2,1,10
dataset/preprocessed/training-data/sentiment_analysis/15,Semantic word spaces have been very useful but can not express the meaning of longer phrases in a principled way .,4,0,21
dataset/preprocessed/training-data/sentiment_analysis/15,Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition .,5,1,25
dataset/preprocessed/training-data/sentiment_analysis/15,"To remedy this , we introduce a Sentiment Treebank .",6,0,10
dataset/preprocessed/training-data/sentiment_analysis/15,"It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality .",7,0,24
dataset/preprocessed/training-data/sentiment_analysis/15,"To address them , we introduce the Recursive Neural Tensor Network .",8,0,12
dataset/preprocessed/training-data/sentiment_analysis/15,"When trained on the new treebank , this model outperforms all previous methods on several metrics .",9,0,17
dataset/preprocessed/training-data/sentiment_analysis/15,It pushes the state of the art in single sentence positive / negative classification from 80 % up to 85.4 % .,10,0,22
dataset/preprocessed/training-data/sentiment_analysis/15,"The accuracy of predicting fine - grained sentiment labels for all phrases reaches 80.7 % , an improvement of 9.7 % over bag of features baselines .",11,0,27
dataset/preprocessed/training-data/sentiment_analysis/15,"Lastly , it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases .",12,0,29
dataset/preprocessed/training-data/sentiment_analysis/15,Semantic vector spaces for single words have been widely used as features .,14,0,13
dataset/preprocessed/training-data/sentiment_analysis/15,"Because they can not capture the meaning of longer phrases properly , compositionality in semantic vector spaces has recently received a lot of attention .",15,0,25
dataset/preprocessed/training-data/sentiment_analysis/15,"However , progress is held back by the current lack of large and labeled compositionality resources and :",16,0,18
dataset/preprocessed/training-data/sentiment_analysis/15,"Example of the Recursive Neural Tensor Network accurately predicting 5 sentiment classes , very negative to very positive ( -- , - , 0 , + , + + ) , at every node of a parse tree and capturing the negation and it s scope in this sentence .",17,0,50
dataset/preprocessed/training-data/sentiment_analysis/15,models to accurately capture the underlying phenomena presented in such data .,18,0,12
dataset/preprocessed/training-data/sentiment_analysis/15,"To address this need , we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus .",19,0,32
dataset/preprocessed/training-data/sentiment_analysis/15,The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language .,20,0,28
dataset/preprocessed/training-data/sentiment_analysis/15,"The corpus is based on the dataset introduced by and consists of 11,855 single sentences extracted from movie reviews .",21,0,20
dataset/preprocessed/training-data/sentiment_analysis/15,"It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees , each annotated by 3 human judges .",22,0,27
dataset/preprocessed/training-data/sentiment_analysis/15,This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena .,23,0,18
dataset/preprocessed/training-data/sentiment_analysis/15,shows one of the many examples with clear compositional structure .,24,0,11
dataset/preprocessed/training-data/sentiment_analysis/15,The granularity and size of this dataset will enable the community to train compositional models that are based on supervised and structured machine learning techniques .,25,0,26
dataset/preprocessed/training-data/sentiment_analysis/15,"While there are several datasets with document and chunk labels available , there is a need to better capture sentiment from short comments , such as Twitter data , which provide less over all signal per document .",26,0,38
dataset/preprocessed/training-data/sentiment_analysis/15,"In order to capture the compositional effects with higher accuracy , we propose a new model called the Recursive Neural Tensor Network ( RNTN ) .",27,0,26
dataset/preprocessed/training-data/sentiment_analysis/15,Recursive Neural Tensor Networks take as input phrases of any length .,28,0,12
dataset/preprocessed/training-data/sentiment_analysis/15,They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor - based composition function .,29,0,30
dataset/preprocessed/training-data/sentiment_analysis/15,"We compare to several supervised , compositional models such as standard recursive neural networks ( RNN ) , matrix - vector RNNs , and baselines such as neural networks that ignore word order , Naive Bayes ( NB ) , bi-gram NB and SVM .",30,0,45
dataset/preprocessed/training-data/sentiment_analysis/15,All models get a significant boost when trained with the new dataset but the RNTN obtains the highest performance with 80.7 % accuracy when predicting finegrained sentiment for all nodes .,31,0,31
dataset/preprocessed/training-data/sentiment_analysis/15,"Lastly , we use a test set of positive and negative sentences and their respective negations to show that , unlike bag of words models , the RNTN accurately captures the sentiment change and scope of negation .",32,0,38
dataset/preprocessed/training-data/sentiment_analysis/15,RNTNs also learn that sentiment of phrases following the contrastive conjunction ' but ' dominates .,33,0,16
dataset/preprocessed/training-data/sentiment_analysis/15,"The complete training and testing code , a live demo and the Stanford Sentiment Treebank dataset are available at http://nlp.stanford.edu/ sentiment .",34,0,22
dataset/preprocessed/training-data/sentiment_analysis/15,"This work is connected to five different are as of NLP research , each with their own large amount of related work to which we can not do full justice given space constraints .",36,0,34
dataset/preprocessed/training-data/sentiment_analysis/15,Semantic Vector Spaces .,37,0,4
dataset/preprocessed/training-data/sentiment_analysis/15,The dominant approach in semantic vector spaces uses distributional similarities of single words .,38,0,14
dataset/preprocessed/training-data/sentiment_analysis/15,"Often , co-occurrence statistics of a word and its context are used to describe each word , such as tf- idf .",39,0,22
dataset/preprocessed/training-data/sentiment_analysis/15,Variants of this idea use more complex frequencies such as how often a word appears in a certain syntactic context .,40,0,21
dataset/preprocessed/training-data/sentiment_analysis/15,"However , distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts .",41,0,20
dataset/preprocessed/training-data/sentiment_analysis/15,One possibility to remedy this is to use neural word vectors .,42,0,12
dataset/preprocessed/training-data/sentiment_analysis/15,These vectors can be trained in an unsupervised fashion to capture distributional similarities but then also be fine - tuned and trained to specific tasks such as sentiment detection .,43,0,30
dataset/preprocessed/training-data/sentiment_analysis/15,The models in this paper can use purely supervised word representations learned entirely on the new corpus .,44,0,18
dataset/preprocessed/training-data/sentiment_analysis/15,Compositionality in Vector Spaces .,45,0,5
dataset/preprocessed/training-data/sentiment_analysis/15,Most of the compositionality algorithms and related datasets capture two word compositions .,46,0,13
dataset/preprocessed/training-data/sentiment_analysis/15,"Mitchell and Lapata ( 2010 ) use e.g. two - word phrases and analyze similarities computed by vector addition , multiplication and others .",47,0,24
dataset/preprocessed/training-data/sentiment_analysis/15,"Some related models such as holographic reduced representations , quantum logic , discrete - continuous models and the recent compositional matrix space model have not been experimentally validated on larger corpora .",48,0,32
dataset/preprocessed/training-data/sentiment_analysis/15,"compute matrix representations for longer phrases and define composition as matrix multiplication , and also evaluate on sentiment .",49,0,19
dataset/preprocessed/training-data/sentiment_analysis/8,Multimodal Speech Emotion Recognition and Ambiguity Resolution,2,1,7
dataset/preprocessed/training-data/sentiment_analysis/8,Identifying emotion from speech is a nontrivial task pertaining to the ambiguous definition of emotion itself .,4,1,17
dataset/preprocessed/training-data/sentiment_analysis/8,"In this work , we adopt a featureengineering based approach to tackle the task of speech emotion recognition .",5,0,19
dataset/preprocessed/training-data/sentiment_analysis/8,"Formalizing our problem as a multi-class classification problem , we compare the performance of two categories of models .",6,0,19
dataset/preprocessed/training-data/sentiment_analysis/8,"For both , we extract eight hand - crafted features from the audio signal .",7,0,15
dataset/preprocessed/training-data/sentiment_analysis/8,"In the first approach , the extracted features are used to train six traditional machine learning classifiers , whereas the second approach is based on deep learning wherein a baseline feed - forward neural network and an LSTM - based classifier are trained over the same features .",8,0,48
dataset/preprocessed/training-data/sentiment_analysis/8,"In order to resolve ambiguity in communication , we also include features from the text domain .",9,0,17
dataset/preprocessed/training-data/sentiment_analysis/8,"We report accuracy , f- score , precision and recall for the different experiment settings we evaluated our models in .",10,0,21
dataset/preprocessed/training-data/sentiment_analysis/8,"Overall , we show that lighter machine learning based models trained over a few hand - crafted features are able to achieve performance comparable to the current deep learning based stateof - the - art method for emotion recognition .",11,0,40
dataset/preprocessed/training-data/sentiment_analysis/8,"Communication is the key to human existence and more often than not , we have to deal with ambiguous situations .",13,0,21
dataset/preprocessed/training-data/sentiment_analysis/8,"For instance , the phrase "" This is awesome "" could be said under either happy or sad settings .",14,0,20
dataset/preprocessed/training-data/sentiment_analysis/8,"Humans are able to resolve ambiguity in most cases because we can efficiently comprehend information from multiple domains ( henceforth , referred to as modalities ) , namely , speech , text and visual .",15,0,35
dataset/preprocessed/training-data/sentiment_analysis/8,"With the rise of deep learning algorithms , there have been multiple attempts to tackle the task of Speech Emotion Recognition ( SER ) as in [ 2 ] and .",16,1,31
dataset/preprocessed/training-data/sentiment_analysis/8,"However , this rise has made practitioners rely more on the power of the deep learning models as opposed to using domain knowledge to construct meaningful features and building models that perform well as well as are interpretable .",17,0,39
dataset/preprocessed/training-data/sentiment_analysis/8,"In this work , we explore the implication of hand - crafted features for SER and compare the performance of lighter machine learning models with the heavily data - reliant deep learning models .",18,1,34
dataset/preprocessed/training-data/sentiment_analysis/8,"Furthermore , we also combine features from the textual modality to understand the correlation between different modalities and aid ambiguity resolution .",19,0,22
dataset/preprocessed/training-data/sentiment_analysis/8,"More formally , we pose our task as a multi-class classification problem and employ the two classes of models to solve that .",20,0,23
dataset/preprocessed/training-data/sentiment_analysis/8,"For both the approaches , we first extract handcrafted features from the time domain of the audio signal and train the respective models .",21,0,24
dataset/preprocessed/training-data/sentiment_analysis/8,"In the first approach , we train traditional machine learning classifiers , namely , Random Forests , Gradient Boosting , Support Vector Machines , Naive - Bayes and Logistic Regression .",22,0,31
dataset/preprocessed/training-data/sentiment_analysis/8,"In the second approach , we build a Multi - Layer Perceptron and an LSTM classifier to recognize emotion given a speech signal .",23,0,24
dataset/preprocessed/training-data/sentiment_analysis/8,"The models are evaluated on the IEMOCAP dataset under different settings , namely , Audio- only , Text - only and Audio + Text 1 .",24,0,26
dataset/preprocessed/training-data/sentiment_analysis/8,The rest of the paper is organized as follows :,25,0,10
dataset/preprocessed/training-data/sentiment_analysis/8,"Section II describes existing methods in the literature for the task of speech emotion recognition ; Section III gives an overview of the dataset used in this work and the pre-processing steps applied before feature extraction ; Section IV describes the proposed models and implementation details ; Results are reported in Section V , followed by the conclusion and future scope of this work in Section VI .",26,0,68
dataset/preprocessed/training-data/sentiment_analysis/8,"In this section , we review some of the work that has been done in the field of speech emotion recognition ( SER ) .",29,0,25
dataset/preprocessed/training-data/sentiment_analysis/8,The task of SER is not new and has been studied for quite sometime in literature .,30,0,17
dataset/preprocessed/training-data/sentiment_analysis/8,A majority of the early approaches ( [ 6 ] ) used Hidden Markov Models ( HMMs ) for identifying emotion from speech .,31,0,24
dataset/preprocessed/training-data/sentiment_analysis/8,Recent introduction of deep neural networks to the domain has also significantly improved the state - of - the - art performance .,32,0,23
dataset/preprocessed/training-data/sentiment_analysis/8,"For instance , and use recurrent autoencoders to solve the task .",33,0,12
dataset/preprocessed/training-data/sentiment_analysis/8,"Recently , methods have also been proposed to efficiently combine features from multiple domains , such as , Tensor Fusion Networks and Low - Rank Matrix Multiplication , instead of trivial concatenation .",34,0,33
dataset/preprocessed/training-data/sentiment_analysis/8,"This work aims to provide a comparative study between 1 ) deep learning based models thatare trained end - to - end , and 2 ) lighter machine learning and deep learning based models trained over handcrafted features .",35,0,39
dataset/preprocessed/training-data/sentiment_analysis/8,We also investigate the information residing in multiple modalities and how their combination affects the performance .,36,0,17
dataset/preprocessed/training-data/sentiment_analysis/8,"In this work , we use the IEMOCAP released in 2008 by researchers at the University of Southern California ( USC ) .",39,0,23
dataset/preprocessed/training-data/sentiment_analysis/8,It contains five recorded sessions of conversations from ten speakers and amounts to nearly 12 hours of audio- visual information along with transcriptions .,40,0,24
dataset/preprocessed/training-data/sentiment_analysis/8,"It is annotated with eight categorical emotion labels , namely , anger , happiness , sadness , neutral , surprise , fear , frustration and excited .",41,0,27
dataset/preprocessed/training-data/sentiment_analysis/8,"It also contains dimensional labels such as values of the activation and valence from 1 to 5 ; however , they are not used in this work .",42,0,28
dataset/preprocessed/training-data/sentiment_analysis/8,The dataset is already split into multiple utterances for each session and we further split each utterance file to obtain wav files for each sentence .,43,0,26
dataset/preprocessed/training-data/sentiment_analysis/8,This was done using the start timestamp and end timestamp provided for the transcribed sentences .,44,0,16
dataset/preprocessed/training-data/sentiment_analysis/8,This results in a total of ? 10 K audio files which are then used to extract features .,45,0,19
dataset/preprocessed/training-data/sentiment_analysis/8,This section describes the data pre-processing steps followed by a detailed description of the features extracted and the two models applied to the classification problem .,48,0,26
dataset/preprocessed/training-data/sentiment_analysis/6,Context - Dependent Sentiment Analysis in User- Generated Videos,2,1,9
dataset/preprocessed/training-data/sentiment_analysis/6,"Multimodal sentiment analysis is a developing area of research , which involves the identification of sentiments in videos .",4,1,19
dataset/preprocessed/training-data/sentiment_analysis/6,"Current research considers utterances as independent entities , i.e. , ignores the interdependencies and relations among the utterances of a video .",5,0,22
dataset/preprocessed/training-data/sentiment_analysis/6,"In this paper , we propose a LSTM - based model that enables utterances to capture contextual information from their surroundings in the same video , thus aiding the classification process .",6,0,32
dataset/preprocessed/training-data/sentiment_analysis/6,Our method shows 5 - 10 % performance improvement over the state of the art and high robustness to generalizability .,7,0,21
dataset/preprocessed/training-data/sentiment_analysis/6,"Sentiment analysis is a ' suitcase ' research problem that requires tackling many NLP sub - tasks , e.g. , aspect extraction , named entity recognition , concept extraction , sarcasm detection , personality recognition , and more .",9,1,39
dataset/preprocessed/training-data/sentiment_analysis/6,"Sentiment analysis can be performed at different granularity levels , e.g. , subjectivity detection simply classifies data as either subjective ( opinionated ) or objective ( neutral ) , while polarity detection focuses on determining whether subjective data indicate positive or negative sentiment .",10,0,44
dataset/preprocessed/training-data/sentiment_analysis/6,"Emotion recognition further breaks down the inferred polarity into a set of emotions conveyed by the subjective data , e.g. , positive sentiment can be caused by joy or anticipation , while negative sentiment can be caused by fear or disgust .",11,1,42
dataset/preprocessed/training-data/sentiment_analysis/6,"Even though the primary focus of this paper is to classify sentiment in videos , we also show the performance of the proposed method for the finergrained task of emotion recognition .",12,0,32
dataset/preprocessed/training-data/sentiment_analysis/6,"Emotion recognition and sentiment analysis have become a new trend in social media , helping users and companies to automatically extract the opinions expressed in user - generated content , especially videos .",13,0,33
dataset/preprocessed/training-data/sentiment_analysis/6,"Thanks to the high availability of computers and smartphones , and the rapid rise of social media , consumers tend to record their reviews and opinions about products or films and upload them on social media platforms , such as YouTube and Facebook .",14,0,44
dataset/preprocessed/training-data/sentiment_analysis/6,"Such videos often contain comparisons , which can aid prospective buyers make an informed decision .",15,0,16
dataset/preprocessed/training-data/sentiment_analysis/6,The primary advantage of analyzing videos over text is the surplus of behavioral cues present in vocal and visual modalities .,16,0,21
dataset/preprocessed/training-data/sentiment_analysis/6,"The vocal modulations and facial expressions in the visual data , along with textual data , provide important cues to better identify affective states of the opinion holder .",17,0,29
dataset/preprocessed/training-data/sentiment_analysis/6,"Thus , a combination of text and video data helps to create a more robust emotion and sentiment analysis model .",18,0,21
dataset/preprocessed/training-data/sentiment_analysis/6,An utterance is a unit of speech bound by breathes or pauses .,19,0,13
dataset/preprocessed/training-data/sentiment_analysis/6,Utterance - level sentiment analysis focuses on tagging every utterance of a video with a sentiment label ( instead of assigning a unique label to the whole video ) .,20,0,30
dataset/preprocessed/training-data/sentiment_analysis/6,"In particular , utterance - level sentiment analysis is useful to understand the sentiment dynamics of different aspects of the topics covered by the speaker throughout his / her speech .",21,0,31
dataset/preprocessed/training-data/sentiment_analysis/6,"Recently , a number of approaches to multimodal sentiment analysis , producing interesting results , have been proposed .",22,1,19
dataset/preprocessed/training-data/sentiment_analysis/6,"However , there are major issues that remain unaddressed .",23,0,10
dataset/preprocessed/training-data/sentiment_analysis/6,Not considering the relation and dependencies among the utterances is one of such issues .,24,0,15
dataset/preprocessed/training-data/sentiment_analysis/6,State - of - the - art approaches in this are a treat utterances independently and ignore the order of utterances in a video .,25,0,25
dataset/preprocessed/training-data/sentiment_analysis/6,Every utterance in a video is spoken at a distinct time and in a particular order .,26,0,17
dataset/preprocessed/training-data/sentiment_analysis/6,"Thus , a video can be treated as a sequence of utterances .",27,0,13
dataset/preprocessed/training-data/sentiment_analysis/6,"Like any other sequence classification problem , sequential utterances of a video may largely be contextually correlated and , hence , influence each other 's sentiment distribution .",28,0,28
dataset/preprocessed/training-data/sentiment_analysis/6,"In our paper , we give importance to the order in which utterances appear in a video .",29,0,18
dataset/preprocessed/training-data/sentiment_analysis/6,We treat surrounding utterances as the context of the utterance that is aimed to be classified .,30,0,17
dataset/preprocessed/training-data/sentiment_analysis/6,"For example , the MOSI dataset contains a video , in which a girl reviews the movie ' Green Hornet ' .",31,0,22
dataset/preprocessed/training-data/sentiment_analysis/6,"At one point , she says "" The Green Hornet did something similar "" .",32,0,15
dataset/preprocessed/training-data/sentiment_analysis/6,"Normally , doing something similar , i.e. , monotonous or repetitive might be perceived as negative .",33,0,17
dataset/preprocessed/training-data/sentiment_analysis/6,"However , the nearby utterances "" It engages the audience more "" , "" they took a new spin on it "" , "" and I just loved it "" indicate a positive context .",34,0,35
dataset/preprocessed/training-data/sentiment_analysis/6,"The hypothesis of the independence of tokens is quite popular in information retrieval and data mining , e.g. , bag - of - words model , but it has a lot limitations .",35,0,33
dataset/preprocessed/training-data/sentiment_analysis/6,"In this paper , we discard such an oversimplifying hypothesis and develop a framework based on long shortterm memory ( LSTM ) that takes a sequence of utterances as input and extracts contextual utterancelevel features .",36,0,36
dataset/preprocessed/training-data/sentiment_analysis/6,"The other uncovered major issues in the literature are the role of speaker - dependent versus speaker - independent models , the impact of each modality across the dataset , and generalization ability of a multimodal sentiment classifier .",37,0,39
dataset/preprocessed/training-data/sentiment_analysis/6,Leaving these issues unaddressed has presented difficulties in effective comparison of different multimodal sentiment analysis methods .,38,0,17
dataset/preprocessed/training-data/sentiment_analysis/6,"In this work , we address all of these issues .",39,0,11
dataset/preprocessed/training-data/sentiment_analysis/6,"Our model preserves the sequential order of utterances and enables consecutive utterances to share information , thus providing contextual information to the utterance - level sentiment classification process .",40,0,29
dataset/preprocessed/training-data/sentiment_analysis/6,Experimental results show that the proposed framework has outperformed the state of the art on three benchmark datasets by 5 - 10 % .,41,0,24
dataset/preprocessed/training-data/sentiment_analysis/6,The paper is organized as follows :,42,0,7
dataset/preprocessed/training-data/sentiment_analysis/6,"Section 2 provides a brief literature review on multimodal sentiment analysis ; Section 3 describes the proposed method in detail ; experimental results and discussion are shown in Section 4 ; finally , Section 5 concludes the paper .",43,0,39
dataset/preprocessed/training-data/sentiment_analysis/6,"The opportunity to capture people 's opinions has raised growing interest both within the scientific community , for the new research challenges , and in the business world , due to the remarkable benefits to be had from financial market prediction .",45,0,42
dataset/preprocessed/training-data/sentiment_analysis/6,Text - based sentiment analysis systems can be broadly categorized into knowledge - based and statistics - based approaches .,46,0,20
dataset/preprocessed/training-data/sentiment_analysis/6,"While the use of knowledge bases was initially more popular for the identification of polarity in text , sentiment analysis researchers have recently been using statistics - based approaches , with a special focus on supervised statistical methods .",47,0,39
dataset/preprocessed/training-data/sentiment_analysis/6,"In 1974 , Ekman carried out extensive studies on facial expressions which showed that universal facial expressions are able to provide sufficient clues to detect emotions .",48,0,27
dataset/preprocessed/training-data/sentiment_analysis/6,"Recent studies on speech - based emotion analysis have focused on identifying relevant acoustic features , such as fundamental frequency ( pitch ) , intensity of utterance , bandwidth , and duration .",49,0,33
dataset/preprocessed/training-data/sentiment_analysis/41,Attention - based LSTM for Aspect - level Sentiment Classification,2,1,10
dataset/preprocessed/training-data/sentiment_analysis/41,Aspect - level sentiment classification is a finegrained task in sentiment analysis .,4,1,13
dataset/preprocessed/training-data/sentiment_analysis/41,"Since it provides more complete and in - depth results , aspect - level sentiment analysis has received much attention these years .",5,0,23
dataset/preprocessed/training-data/sentiment_analysis/41,"In this paper , we reveal that the sentiment polarity of a sentence is not only determined by the content but is also highly related to the concerned aspect .",6,0,30
dataset/preprocessed/training-data/sentiment_analysis/41,"For instance , "" The appetizers are ok , but the service is slow . "" , for aspect taste , the polarity is positive while for service , the polarity is negative .",7,0,34
dataset/preprocessed/training-data/sentiment_analysis/41,"Therefore , it is worthwhile to explore the connection between an aspect and the content of a sentence .",8,0,19
dataset/preprocessed/training-data/sentiment_analysis/41,"To this end , we propose an Attention - based Long Short - Term Memory Network for aspect - level sentiment classification .",9,0,23
dataset/preprocessed/training-data/sentiment_analysis/41,The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input .,10,0,19
dataset/preprocessed/training-data/sentiment_analysis/41,We experiment on the SemEval 2014 dataset and results show that our model achieves state - of the - art performance on aspect - level sentiment classification .,11,0,28
dataset/preprocessed/training-data/sentiment_analysis/41,"Sentiment analysis , also known as opinion mining , is a key NLP task that receives much attention these years .",13,0,21
dataset/preprocessed/training-data/sentiment_analysis/41,Aspect - level sentiment analysis is a fine - grained task that can provide complete and in - depth results .,14,0,21
dataset/preprocessed/training-data/sentiment_analysis/41,"In this paper , we deal with aspect - level sentiment classification and we find that the sentiment polarity of a sentence is highly dependent on both content and aspect .",15,1,31
dataset/preprocessed/training-data/sentiment_analysis/41,"For example , the sentiment polarity of "" Staffs are not that friendly , but the taste covers all . "" will be positive if the aspect is food but negative when considering the aspect service .",16,0,37
dataset/preprocessed/training-data/sentiment_analysis/41,Polarity could be opposite when different aspects are considered .,17,0,10
dataset/preprocessed/training-data/sentiment_analysis/41,"Neural networks have achieved state - of - the - art performance in a variety of NLP tasks such as machine translation , paraphrase identification , question answering and text summarization .",18,0,32
dataset/preprocessed/training-data/sentiment_analysis/41,"However , neural network models are still in infancy to deal with aspectlevel sentiment classification .",19,0,16
dataset/preprocessed/training-data/sentiment_analysis/41,"In some works , target dependent sentiment classification can be benefited from taking into account target information , such as in Target - Dependent LSTM ( TD - LSTM ) and Target - Connection LSTM ( TC - LSTM ) .",20,0,41
dataset/preprocessed/training-data/sentiment_analysis/41,"However , those models can only take into consideration the target but not aspect information which is proved to be crucial for aspect - level classification .",21,0,27
dataset/preprocessed/training-data/sentiment_analysis/41,"Attention has become an effective mechanism to obtain superior results , as demonstrated in image recognition , machine translation , reasoning about entailment and sentence summarization .",22,0,27
dataset/preprocessed/training-data/sentiment_analysis/41,"Even more , neural attention can improve the ability to read comprehension .",23,0,13
dataset/preprocessed/training-data/sentiment_analysis/41,"In this paper , we propose an attention mechanism to enforce the model to attend to the important part of a sentence , in response to a specific aspect .",24,0,30
dataset/preprocessed/training-data/sentiment_analysis/41,We design an aspect - tosentence attention mechanism that can concentrate on the key part of a sentence given the aspect .,25,0,22
dataset/preprocessed/training-data/sentiment_analysis/41,We explore the potential correlation of aspect and sentiment polarity in aspect - level sentiment classification .,26,0,17
dataset/preprocessed/training-data/sentiment_analysis/41,"In order to capture important information in response to a given aspect , we design an attentionbased LSTM .",27,0,19
dataset/preprocessed/training-data/sentiment_analysis/41,"We evaluate our approach on a benchmark dataset , which contains restaurants and laptops data .",28,0,16
dataset/preprocessed/training-data/sentiment_analysis/41,The main contributions of our work can be summarized as follows :,29,0,12
dataset/preprocessed/training-data/sentiment_analysis/41,We propose attention - based Long Short - Term memory for aspect - level sentiment classification .,30,0,17
dataset/preprocessed/training-data/sentiment_analysis/41,The models are able to attend different parts of a sentence when different aspects are concerned .,31,0,17
dataset/preprocessed/training-data/sentiment_analysis/41,Results show that the attention mechanism is effective .,32,0,9
dataset/preprocessed/training-data/sentiment_analysis/41,"Since aspect plays a key role in this task , we propose two ways to take into account aspect information during attention : one way is to concatenate the aspect vector into the sentence hidden representations for computing attention weights , and another way is to additionally append the aspect vector into the input word vectors .",33,0,57
dataset/preprocessed/training-data/sentiment_analysis/41,"Experimental results indicate that our approach can improve the performance compared with several baselines , and further examples demonstrate the attention mechanism works well for aspect - level sentiment classification .",34,0,31
dataset/preprocessed/training-data/sentiment_analysis/41,The rest of our paper is structured as follows :,35,0,10
dataset/preprocessed/training-data/sentiment_analysis/41,"Section 2 discusses related works , Section 3 gives a detailed description of our attention - based proposals , Section 4 presents extensive experiments to justify the effectiveness of our proposals , and Section 5 summarizes this work and the future direction .",36,0,43
dataset/preprocessed/training-data/sentiment_analysis/41,"In this section , we will review related works on aspect - level sentiment classification and neural networks for sentiment classification briefly .",38,0,23
dataset/preprocessed/training-data/sentiment_analysis/41,Sentiment Classification at Aspect - level,39,0,6
dataset/preprocessed/training-data/sentiment_analysis/41,Aspect - level sentiment classification is typically considered as a classification problem in the liter - ature .,40,0,18
dataset/preprocessed/training-data/sentiment_analysis/41,"As we mentioned before , aspect - level sentiment classification is a fine - grained classification task .",41,0,18
dataset/preprocessed/training-data/sentiment_analysis/41,"The majority of current approaches attempt to detecting the polarity of the entire sentence , regardless of the entities mentioned or aspects .",42,0,23
dataset/preprocessed/training-data/sentiment_analysis/41,Traditional approaches to solve those problems are to manually design a set of features .,43,0,15
dataset/preprocessed/training-data/sentiment_analysis/41,"With the abundance of sentiment lexicons , the lexicon - based features were built for sentiment analysis .",44,0,18
dataset/preprocessed/training-data/sentiment_analysis/41,"Most of these studies focus on building sentiment classifiers with features , which include bag - of - words and sentiment lexicons , using SVM .",45,0,26
dataset/preprocessed/training-data/sentiment_analysis/41,"However , the results highly depend on the quality of features .",46,0,12
dataset/preprocessed/training-data/sentiment_analysis/41,"In addition , feature engineering is labor intensive .",47,0,9
dataset/preprocessed/training-data/sentiment_analysis/41,Sentiment Classification with Neural Networks,48,0,5
dataset/preprocessed/training-data/sentiment_analysis/41,"Since a simple and effective approach to learn distributed representations was proposed , neural networks advance sentiment analysis substantially .",49,0,20
dataset/preprocessed/training-data/sentiment_analysis/11,Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos,2,1,10
dataset/preprocessed/training-data/sentiment_analysis/11,Emotion recognition in conversations is crucial for the development of empathetic machines .,4,1,13
dataset/preprocessed/training-data/sentiment_analysis/11,Present methods mostly ignore the role of inter-speaker dependency relations while classifying emotions in conversations .,5,0,16
dataset/preprocessed/training-data/sentiment_analysis/11,"In this paper , we address recognizing utterance - level emotions in dyadic conversational videos .",6,0,16
dataset/preprocessed/training-data/sentiment_analysis/11,"We propose a deep neural framework , termed conversational memory network , which leverages contextual information from the conversation history .",7,0,21
dataset/preprocessed/training-data/sentiment_analysis/11,"The framework takes a multimodal approach comprising audio , visual and textual features with gated recurrent units to model past utterances of each speaker into memories .",8,0,27
dataset/preprocessed/training-data/sentiment_analysis/11,Such memories are then merged using attention - based hops to capture inter-speaker dependencies .,9,0,15
dataset/preprocessed/training-data/sentiment_analysis/11,Experiments show an accuracy improvement of 3?4 % over the state of the art .,10,0,15
dataset/preprocessed/training-data/sentiment_analysis/11,Development of machines with emotional intelligence has been a long - standing goal of AI .,12,0,16
dataset/preprocessed/training-data/sentiment_analysis/11,"With the increasing infusion of interactive systems in our lives , the need for empathetic machines with emotional understanding is paramount .",13,0,22
dataset/preprocessed/training-data/sentiment_analysis/11,Previous research in affective computing has looked at dialogues as an essential basis to learn emotional dynamics .,14,0,18
dataset/preprocessed/training-data/sentiment_analysis/11,"Since the advent of Web 2.0 , dialogue videos have proliferated across the internet through platforms like movies , webinars , and video chats .",15,0,25
dataset/preprocessed/training-data/sentiment_analysis/11,"Emotion detection from such resources can benefit numerous fields like counseling , public opinion mining , financial forecasting , and intelligent systems such as smart homes and chatbots .",16,1,29
dataset/preprocessed/training-data/sentiment_analysis/11,"In this paper , we analyze emotion detection in videos of dyadic conversations .",17,1,14
dataset/preprocessed/training-data/sentiment_analysis/11,A dyadic conversation is a form of a dialogue between two entities .,18,0,13
dataset/preprocessed/training-data/sentiment_analysis/11,"We propose a conversational memory network ( CMN ) , which uses a multimodal approach for emotion detection in utterances ( a unit of speech bound by breathes or pauses ) of such conversational videos .",19,0,36
dataset/preprocessed/training-data/sentiment_analysis/11,Emotional dynamics in a conversation is known to be driven by two prime factors : self and interspeaker emotional influence .,20,0,21
dataset/preprocessed/training-data/sentiment_analysis/11,"Self - influence relates to the concept of emotional inertia , i.e. , the degree to which a person 's feelings carryover from one moment to another .",21,0,28
dataset/preprocessed/training-data/sentiment_analysis/11,Inter- speaker emotional influence is another trait where the other person acts as an influencer in the speaker 's emotional state .,22,0,22
dataset/preprocessed/training-data/sentiment_analysis/11,"Conversely , speakers also tend to mirror emotions of their counterparts .",23,0,12
dataset/preprocessed/training-data/sentiment_analysis/11,provides an example from the dataset showing the presence of these two traits in a dialogue .,24,0,17
dataset/preprocessed/training-data/sentiment_analysis/11,Existing works in the literature do not capitalize on these two factors .,25,0,13
dataset/preprocessed/training-data/sentiment_analysis/11,Context - free systems infer emotions based only on the current utterance in the conversation .,26,0,16
dataset/preprocessed/training-data/sentiment_analysis/11,"Whereas , state - of - the - art context - based networks like , use long short - term memory ( LSTM ) networks to model speaker - based context that suffers from incapability of long - range summarization and unweighted influence from context , leading to model bias .",27,0,51
dataset/preprocessed/training-data/sentiment_analysis/11,Our proposed CMN incorporates these factors by using emotional context information present in the conversation history .,28,0,17
dataset/preprocessed/training-data/sentiment_analysis/11,It improves speakerbased emotion modeling by using memory networks which are efficient in capturing long - term dependencies and summarizing task - specific details using attention models .,29,0,28
dataset/preprocessed/training-data/sentiment_analysis/11,"Specifically , the memory cells of CMN are continuous vectors that store the context information found in the utterance histories .",30,0,21
dataset/preprocessed/training-data/sentiment_analysis/11,CMN also models interplay of these memories to capture interspeaker dependencies .,31,0,12
dataset/preprocessed/training-data/sentiment_analysis/11,"CMN first extracts multimodal features ( audio , visual , and text ) for all utterances in a video .",32,0,20
dataset/preprocessed/training-data/sentiment_analysis/11,"In order to detect the emotion of a particular utterance , say u i , it gathers its histories by collecting previous utterances within a context window .",33,0,28
dataset/preprocessed/training-data/sentiment_analysis/11,Separate histories are created for both speakers .,34,0,8
dataset/preprocessed/training-data/sentiment_analysis/11,These histories are then modeled into memory cells using gated recurrent units ( GRUs ) .,35,0,16
dataset/preprocessed/training-data/sentiment_analysis/11,"After that , CMN reads both the speaker 's memories and employs attention mechanism on them , in order to find the most useful historical utterances to classify u i .",36,0,31
dataset/preprocessed/training-data/sentiment_analysis/11,The memories are then merged with u i using an addition operation weighted by the attention scores .,37,0,18
dataset/preprocessed/training-data/sentiment_analysis/11,This is done to model inter-speaker influences and dynamics .,38,0,10
dataset/preprocessed/training-data/sentiment_analysis/11,"The whole cycle is repeated for multiple hops and finally , this merged representation of utterance u i is used to classify its emotion category .",39,0,26
dataset/preprocessed/training-data/sentiment_analysis/11,The contributions of this paper can be summarized as follows :,40,0,11
dataset/preprocessed/training-data/sentiment_analysis/11,"We propose an architecture , termed CMN , for emotion detection in a dyadic conversation that considers utterance histories of both the speaker to model emotional dynamics .",42,0,28
dataset/preprocessed/training-data/sentiment_analysis/11,The architecture is extensible to multi-speaker conversations in formats such as textual dialogues or conversational videos .,43,0,17
dataset/preprocessed/training-data/sentiment_analysis/11,"When applied to videos , we adopt a multimodal approach to extract diverse features from utterances .",45,0,17
dataset/preprocessed/training-data/sentiment_analysis/11,It also makes our model robust to missing information .,46,0,10
dataset/preprocessed/training-data/sentiment_analysis/11,CMN provides a significant increase in accuracy of 3 ?,48,0,10
dataset/preprocessed/training-data/sentiment_analysis/11,4 % over previous state - of - the - art networks .,49,0,13
dataset/preprocessed/training-data/sentiment_analysis/36,Transformation Networks for Target - Oriented Sentiment Classification,2,1,8
dataset/preprocessed/training-data/sentiment_analysis/36,Target - oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence .,5,0,18
dataset/preprocessed/training-data/sentiment_analysis/36,"RNN with attention seems a good fit for the characteristics of this task , and indeed it achieves the state - of - the - art performance .",6,0,28
dataset/preprocessed/training-data/sentiment_analysis/36,"After re-examining the drawbacks of attention mechanism and the obstacles that block CNN to perform well in this classification task , we propose a new model to overcome these issues .",7,0,31
dataset/preprocessed/training-data/sentiment_analysis/36,"Instead of attention , our model employs a CNN layer to extract salient features from the transformed word representations originated from a bi-directional RNN layer .",8,0,26
dataset/preprocessed/training-data/sentiment_analysis/36,"Between the two layers , we propose a component to generate target - specific representations of words in the sentence , meanwhile incorporate a mechanism for preserving the original contextual information from the RNN layer .",9,0,36
dataset/preprocessed/training-data/sentiment_analysis/36,Experiments show that our model achieves a new state - of - the - art performance on a few benchmarks .,10,0,21
dataset/preprocessed/training-data/sentiment_analysis/36,"Target - oriented ( also mentioned as "" target - level "" or "" aspect - level "" in some works ) sentiment classification aims to determine sentiment polarities over "" opinion targets "" that explicitly appear in the sentences .",13,0,41
dataset/preprocessed/training-data/sentiment_analysis/36,"For example , in the sentence "" I am pleased with the fast logon , and the long battery life "" , the user mentions two targets * The work was done when Xin Li was an intern at Tencent AI Lab .",14,0,43
dataset/preprocessed/training-data/sentiment_analysis/36,"This project is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region , China ( Project Code : 14203414 ) .",15,0,29
dataset/preprocessed/training-data/sentiment_analysis/36,"Our code is open - source and available at https :// github.com/lixin4ever/TNet "" log on "" and "" better life "" , and expresses positive sentiments over them .",16,0,29
dataset/preprocessed/training-data/sentiment_analysis/36,"The task is usually formulated as predicting a sentiment category for a ( target , sentence ) pair .",17,0,19
dataset/preprocessed/training-data/sentiment_analysis/36,"Recurrent Neural Networks ( RNNs ) with attention mechanism , firstly proposed in machine translation , is the most commonly - used technique for this task .",18,0,27
dataset/preprocessed/training-data/sentiment_analysis/36,"For example , ; ; ;; and employ attention to measure the semantic relatedness between each context word and the target , and then use the induced attention scores to aggregate contextual features for prediction .",19,0,36
dataset/preprocessed/training-data/sentiment_analysis/36,"In these works , the attention weight based combination of word - level features for classification may introduce noise and downgrade the prediction accuracy .",20,0,25
dataset/preprocessed/training-data/sentiment_analysis/36,"For example , in "" This dish is my favorite and I always get it and never get tired of it . "" , these approaches tend to involve irrelevant words such as "" never "" and "" tired "" when they highlight the opinion modifier "" favorite "" .",21,0,50
dataset/preprocessed/training-data/sentiment_analysis/36,"To some extent , this drawback is rooted in the attention mechanism , as also observed in machine translation and image captioning .",22,0,23
dataset/preprocessed/training-data/sentiment_analysis/36,"Another observation is that the sentiment of a target is usually determined by key phrases such as "" is my favorite "" .",23,0,23
dataset/preprocessed/training-data/sentiment_analysis/36,"By this token , Convolutional Neural Networks ( CNNs ) - whose capability for extracting the informative n-gram features ( also called "" active local features "" ) as sentence representations has been verified in - should be a suitable model for this classification problem .",24,0,46
dataset/preprocessed/training-data/sentiment_analysis/36,"However , CNN likely fails in cases where a sentence expresses different sentiments over multiple targets , such as "" great food but the service was dreadful ! "" .",25,0,30
dataset/preprocessed/training-data/sentiment_analysis/36,One reason is that CNN can not fully explore the target information as done by RNN - based meth-ods .,26,0,20
dataset/preprocessed/training-data/sentiment_analysis/36,"2 Moreover , it is hard for vanilla CNN to differentiate opinion words of multiple targets .",27,0,17
dataset/preprocessed/training-data/sentiment_analysis/36,"Precisely , multiple active local features holding different sentiments ( e.g. , "" great food "" and "" service was dreadful "" ) maybe captured for a single target , thus it will hinder the prediction .",28,0,37
dataset/preprocessed/training-data/sentiment_analysis/36,"We propose a new architecture , named Target - Specific Transformation Networks ( TNet ) , to solve the above issues in the task of target sentiment classification .",29,0,29
dataset/preprocessed/training-data/sentiment_analysis/36,TNet firstly encodes the context information into word embeddings and generates the contextualized word representations with LSTMs .,30,0,18
dataset/preprocessed/training-data/sentiment_analysis/36,"To integrate the target information into the word representations , TNet introduces a novel Target - Specific Transformation ( TST ) component for generating the target - specific word representations .",31,0,31
dataset/preprocessed/training-data/sentiment_analysis/36,"Contrary to the previous attention - based approaches which apply the same target representation to determine the attention scores of individual context words , TST firstly generates different representations of the target conditioned on individual context words , then it consolidates each context word with its tailor - made target representation to obtain the transformed word representation .",32,0,58
dataset/preprocessed/training-data/sentiment_analysis/36,"Considering the context word "" long "" and the target "" battery life "" in the above example , TST firstly measures the associations between "" long "" and individual target words .",33,0,33
dataset/preprocessed/training-data/sentiment_analysis/36,"Then it uses the association scores to generate the target representation conditioned on "" long "" .",34,0,17
dataset/preprocessed/training-data/sentiment_analysis/36,"After that , TST transforms the representation of "" long "" into its target - specific version with the new target representation .",35,0,23
dataset/preprocessed/training-data/sentiment_analysis/36,"Note that "" long "" could also indicate a negative sentiment ( say for "" startup time "" ) , and the above TST is able to differentiate them .",36,0,30
dataset/preprocessed/training-data/sentiment_analysis/36,"As the context information carried by the representations from the LSTM layer will be lost after the non-linear TST , we design a contextpreserving mechanism to contextualize the generated target - specific word representations .",37,0,35
dataset/preprocessed/training-data/sentiment_analysis/36,Such mechanism also allows deep transformation structure to learn abstract features 3 .,38,0,13
dataset/preprocessed/training-data/sentiment_analysis/36,"To help the CNN feature extractor locate sentiment indicators more accurately , we adopt a proximity strategy to scale the input of convolutional layer with positional relevance between a word and the target .",39,0,34
dataset/preprocessed/training-data/sentiment_analysis/36,"One method could be concatenating the target representation with each word representation , but the effect as shown in is limited .",41,0,22
dataset/preprocessed/training-data/sentiment_analysis/36,Abstract features usually refer to the features ultimately useful for the task .,43,0,13
dataset/preprocessed/training-data/sentiment_analysis/36,"In summary , our contributions are as follows :",44,0,9
dataset/preprocessed/training-data/sentiment_analysis/36,"TNet adapts CNN to handle target - level sentiment classification , and its performance dominates the state - of - the - art models on benchmark datasets .",45,0,28
dataset/preprocessed/training-data/sentiment_analysis/36,A novel Target - Specific Transformation component is proposed to better integrate target information into the word representations .,46,0,19
dataset/preprocessed/training-data/sentiment_analysis/36,"A context - preserving mechanism is designed to forward the context information into a deep transformation architecture , thus , the model can learn more abstract contextualized word features from deeper networks .",47,0,33
dataset/preprocessed/training-data/sentiment_analysis/36,"Given a target - sentence pair ( w ? , w ) , where w ? = {w ? 1 , w ? 2 , ... , w ? m } is a sub-sequence of w = {w 1 , w 2 , ... , w n } , and the corresponding word embeddings x ? = {x ? 1 , x ? 2 , ... , x ? m } and x = {x 1 , x 2 , ... , x n } , the aim of target sentiment classification is to predict the sentiment polarity y ?",49,0,100
dataset/preprocessed/training-data/sentiment_analysis/14,Emo2 Vec : Learning Generalized Emotion Representation by Multi- task Training,2,1,11
dataset/preprocessed/training-data/sentiment_analysis/14,"In this paper , we propose Emo2 Vec which encodes emotional semantics into vectors .",4,0,15
dataset/preprocessed/training-data/sentiment_analysis/14,"We train Emo2 Vec by multi-task learning six different emotion - related tasks , including emotion / sentiment analysis , sarcasm classification , stress detection , abusive language classification , insult detection , and personality recognition .",5,0,37
dataset/preprocessed/training-data/sentiment_analysis/14,"Our evaluation of Emo2 Vec shows that it outperforms existing affect - related representations , such as Sentiment - Specific Word Embedding and DeepMoji embeddings with much smaller training corpora .",6,0,31
dataset/preprocessed/training-data/sentiment_analysis/14,"When concatenated with GloVe , Emo2 Vec achieves competitive performances to state - of - the - art results on several tasks using a simple logistic regression classifier .",7,0,29
dataset/preprocessed/training-data/sentiment_analysis/14,"Recent work on word representation has been focusing on embedding syntactic and semantic information into fixed - sized vectors based on the distributional hypothesis , and have proven to be useful in many natural language tasks .",9,0,37
dataset/preprocessed/training-data/sentiment_analysis/14,"However , despite the rising popularity regarding the use of word embeddings , they often fail to capture the emotional semantics the words convey .",10,0,25
dataset/preprocessed/training-data/sentiment_analysis/14,"For example , the Glo Ve vector captures the semantic meaning of "" headache "" , as it is closer to words of ill symptoms like "" fever "" and "" toothache "" , but misses the emotional association that the word carries .",11,0,44
dataset/preprocessed/training-data/sentiment_analysis/14,"The word "" headache "" in the sentence "" You are giving me a headache "" does not really mean that the speaker will get a headache , but instead implies the negative emotion of the speaker .",12,0,38
dataset/preprocessed/training-data/sentiment_analysis/14,"To include affective information into the word representation , proposed Sentiment - Specific Word Embeddings ( SSWE ) which encodes both positive / negative sentiment and syntactic contextual information in a vector space .",13,0,34
dataset/preprocessed/training-data/sentiment_analysis/14,This work demonstrates the effectiveness of incorporating sentiment labels in a wordlevel information for sentiment - related tasks compared to other word embeddings .,14,0,24
dataset/preprocessed/training-data/sentiment_analysis/14,"However , they only focus on binary labels , which weakens their generalization ability on other affect tasks .",15,0,19
dataset/preprocessed/training-data/sentiment_analysis/14,"instead uses emotion lexicons to tune the vector space , which gives them better results .",16,0,16
dataset/preprocessed/training-data/sentiment_analysis/14,"Nevertheless , this method requires human - labeled lexicons and can not scale to large amounts of data .",17,0,19
dataset/preprocessed/training-data/sentiment_analysis/14,"achieves good results on affect tasks by training a two - layer bidirectional Long Short - Term Memory ( bi - LSTM ) model , named DeepMoji , to predict emoji of the input document using a huge dataset of 1.2 billions of tweets .",18,0,45
dataset/preprocessed/training-data/sentiment_analysis/14,"However , collecting billions of tweets is expensive and time consuming for researchers .",19,0,14
dataset/preprocessed/training-data/sentiment_analysis/14,"Furthermore , most works in sentiment and emotion analysis have focused solely on a single task .",20,0,17
dataset/preprocessed/training-data/sentiment_analysis/14,"Nevertheless , as emotion is a complex concept , we believe that all emotion involving situations such as stress , hate speech , sarcasm , and insult , should be included for a deeper understanding of emotion .",21,0,38
dataset/preprocessed/training-data/sentiment_analysis/14,"Thus , one way to achieve this is through a multi-task training framework , as we present here .",22,0,19
dataset/preprocessed/training-data/sentiment_analysis/14,"1 ) We propose Emo2Vec 1 which are word - level representations that encode emotional semantics into fixed - sized , real - valued vectors .",24,0,26
dataset/preprocessed/training-data/sentiment_analysis/14,2 ) We propose to learn Emo2Vec with a multi-task learning framework by including six different emotion - related tasks .,25,0,21
dataset/preprocessed/training-data/sentiment_analysis/14,"3 ) Compared to existing affect - related embeddings , Emo2 Vec achieves better results on more than ten datasets with much less training data ( 1.9 M vs 1.2B documents ) .",26,0,33
dataset/preprocessed/training-data/sentiment_analysis/14,"Furthermore , with a simple logistic regression classifier , Emo2 Vec reaches competitive performance to state - of - the - art results on several",27,0,25
dataset/preprocessed/training-data/sentiment_analysis/14,We train Emo2 Vec using an end - to - end multi-task learning framework with one larger dataset and several small task - specific datasets .,29,0,26
dataset/preprocessed/training-data/sentiment_analysis/14,"The model is divided into two parts : a shared embedding layer ( i.e. Emo2 Vec ) , and task - specific classifiers .",30,0,24
dataset/preprocessed/training-data/sentiment_analysis/14,"All datasets share the same word - level representations ( i.e. Emo2 Vec ) , thus forcing the model to encode shared knowledge into a single matrix .",31,0,28
dataset/preprocessed/training-data/sentiment_analysis/14,"For the larger dataset , a Convolutional Neural Network ( CNN ) model is used to capture complex linguistic features present in the corpus .",32,0,25
dataset/preprocessed/training-data/sentiment_analysis/14,"On the other hand , the classifier of each small dataset is a simple logistic regression .",33,0,17
dataset/preprocessed/training-data/sentiment_analysis/14,"We define D = {d L , d 1 , d 2 , , d n } as the set of n + 1 datasets , where d L is the larger dataset and the other d i are the small datasets .",35,0,43
dataset/preprocessed/training-data/sentiment_analysis/14,"We denote a sentence X i with i ? { L , 1 , 2 , , n} as [ w i , 1 , w i , 2 , , w i , N i ] where w i , j is the j - th word in the i - th sample and Ni is the number of words .",36,0,62
dataset/preprocessed/training-data/sentiment_analysis/14,"All the models ' parameters are defined as M ? = {T , CNN , LR ? 1 , . . . , LR ?n } , where T ?",37,0,30
dataset/preprocessed/training-data/sentiment_analysis/14,"R | V |k is the Emo2 Vec matrix , | V | is the vocabulary size and k is the embedding dimension , CNN is a Convolutional Neural Network model and LR ?",38,0,34
dataset/preprocessed/training-data/sentiment_analysis/14,i for i ?,39,0,4
dataset/preprocessed/training-data/sentiment_analysis/14,"[ 1 , n] is a logistic regression classifier parameterized by ?",40,0,12
dataset/preprocessed/training-data/sentiment_analysis/14,i which is specific for the dataset d i .,41,0,10
dataset/preprocessed/training-data/sentiment_analysis/14,"We denote the embedded representation of a word w i , j withe w i , j .",42,0,18
dataset/preprocessed/training-data/sentiment_analysis/14,The CNN architecture used is illustrated in 2 .,44,0,9
dataset/preprocessed/training-data/sentiment_analysis/14,"Firstly , 1 - D convolution is used to extract n- gram features from the input embeddings .",45,0,18
dataset/preprocessed/training-data/sentiment_analysis/14,"Specifically , the j - th filter denoted by F j , is convolved with embeddings of words in a sliding window of size k j , giving a feature value c j , t .",46,0,36
dataset/preprocessed/training-data/sentiment_analysis/14,J filters are learned trough this process :,47,0,8
dataset/preprocessed/training-data/sentiment_analysis/14,where * is the 1 - D convolution operation .,48,0,10
dataset/preprocessed/training-data/sentiment_analysis/14,This is followed by a layer of ReLU activation for non-linearity .,49,0,12
dataset/preprocessed/training-data/sentiment_analysis/17,Improved Semantic Representations From Tree - Structured Long Short - Term Memory Networks,2,1,13
dataset/preprocessed/training-data/sentiment_analysis/17,"Because of their superior ability to preserve sequence information overtime , Long Short - Term Memory ( LSTM ) networks , a type of recurrent neural network with a more complex computational unit , have obtained strong results on a variety of sequence modeling tasks .",4,0,46
dataset/preprocessed/training-data/sentiment_analysis/17,The only underlying LSTM structure that has been explored so far is a linear chain .,5,0,16
dataset/preprocessed/training-data/sentiment_analysis/17,"However , natural language exhibits syntactic properties that would naturally combine words to phrases .",6,0,15
dataset/preprocessed/training-data/sentiment_analysis/17,"We introduce the Tree - LSTM , a generalization of LSTMs to tree - structured network topologies .",7,0,18
dataset/preprocessed/training-data/sentiment_analysis/17,"Tree - LSTMs outperform all existing systems and strong LSTM baselines on two tasks : predicting the semantic relatedness of two sentences ( Sem Eval 2014 , Task 1 ) and sentiment classification ( Stanford Sentiment Treebank ) .",8,1,39
dataset/preprocessed/training-data/sentiment_analysis/17,"Most models for distributed representations of phrases and sentences - that is , models where realvalued vectors are used to represent meaning - fall into one of three classes : bag - of - words models , sequence models , and tree - structured models .",10,0,46
dataset/preprocessed/training-data/sentiment_analysis/17,"In bag - of - words models , phrase and sentence representations are independent of word order ; for example , they can be generated by averaging constituent word representations .",11,0,31
dataset/preprocessed/training-data/sentiment_analysis/17,"In contrast , sequence models construct sentence representations as an order - sensitive function of the sequence of tokens .",12,0,20
dataset/preprocessed/training-data/sentiment_analysis/17,"Lastly , tree - structured models compose each phrase and sentence representation from its constituent subphrases according to a given syntactic structure over the sentence ) .",13,0,27
dataset/preprocessed/training-data/sentiment_analysis/17,x 2 x 3 x 4 y 1 y 2 y 3 y 4,16,0,14
dataset/preprocessed/training-data/sentiment_analysis/17,x 5 x 6 y 1 y 2 y 3 y 4 y 6 : Top :,21,0,17
dataset/preprocessed/training-data/sentiment_analysis/17,A chain - structured LSTM network .,22,0,7
dataset/preprocessed/training-data/sentiment_analysis/17,A tree - structured LSTM network with arbitrary branching factor .,24,0,11
dataset/preprocessed/training-data/sentiment_analysis/17,"Order-insensitive models are insufficient to fully capture the semantics of natural language due to their inability to account for differences in meaning as a result of differences in word order or syntactic structure ( e.g. , "" cats climb trees "" vs. "" trees climb cats "" ) .",25,0,49
dataset/preprocessed/training-data/sentiment_analysis/17,We therefore turn to ordersensitive sequential or tree - structured models .,26,0,12
dataset/preprocessed/training-data/sentiment_analysis/17,"In particular , tree - structured models are a linguistically attractive option due to their relation to syntactic interpretations of sentence structure .",27,0,23
dataset/preprocessed/training-data/sentiment_analysis/17,"A natural question , then , is the following : to what extent ( if at all ) can we do better with tree - structured models as opposed to sequential models for sentence representation ?",28,0,36
dataset/preprocessed/training-data/sentiment_analysis/17,"In this paper , we work towards addressing this question by directly comparing a type of sequential model that has recently been used to achieve state - of - the - art results in several NLP tasks against its tree - structured generalization .",29,0,44
dataset/preprocessed/training-data/sentiment_analysis/17,"Due to their capability for processing arbitrarylength sequences , recurrent neural networks ( RNNs ) are a natural choice for sequence modeling tasks .",30,0,24
dataset/preprocessed/training-data/sentiment_analysis/17,"Recently , RNNs with Long Short - Term Memory ( LSTM ) units ) have re-emerged as a popular architecture due to their representational power and effectiveness at capturing long - term dependencies .",31,0,34
dataset/preprocessed/training-data/sentiment_analysis/17,"LSTM networks , which we review in Sec. 2 , have been successfully applied to a variety of sequence modeling and prediction tasks , notably machine translation , speech recognition , image caption generation , and program execution .",32,0,39
dataset/preprocessed/training-data/sentiment_analysis/17,"In this paper , we introduce a generalization of the standard LSTM architecture to tree - structured network topologies and show its superiority for representing sentence meaning over a sequential LSTM .",33,0,32
dataset/preprocessed/training-data/sentiment_analysis/17,"While the standard LSTM composes its hidden state from the input at the current time step and the hidden state of the LSTM unit in the previous time step , the tree - structured LSTM , or Tree - LSTM , composes its state from an input vector and the hidden states of arbitrarily many child units .",34,0,58
dataset/preprocessed/training-data/sentiment_analysis/17,The standard LSTM can then be considered a special case of the Tree - LSTM where each internal node has exactly one child .,35,0,24
dataset/preprocessed/training-data/sentiment_analysis/17,"In our evaluations , we demonstrate the empirical strength of Tree - LSTMs as models for representing sentences .",36,0,19
dataset/preprocessed/training-data/sentiment_analysis/17,We evaluate the Tree - LSTM architecture on two tasks : semantic relatedness prediction on sentence pairs and sentiment classification of sentences drawn from movie reviews .,37,0,27
dataset/preprocessed/training-data/sentiment_analysis/17,Our experiments show that Tree - LSTMs outperform existing systems and sequential LSTM baselines on both tasks .,38,0,18
dataset/preprocessed/training-data/sentiment_analysis/17,Implementations of our models and experiments are available at https :// github.com/stanfordnlp/treelstm.,39,0,12
dataset/preprocessed/training-data/sentiment_analysis/17,Long Short - Term Memory Networks,40,0,6
dataset/preprocessed/training-data/sentiment_analysis/17,Recurrent neural networks ( RNNs ) are able to process input sequences of arbitrary length via the recursive application of a transition function on a hidden state vector ht .,42,0,30
dataset/preprocessed/training-data/sentiment_analysis/17,"At each time step t , the hidden state ht is a function of the input vector x t that the network receives at time t and it s previous hidden state h t?1 .",43,0,35
dataset/preprocessed/training-data/sentiment_analysis/17,"For example , the input vector x t could be a vector representation of the t- th word in body of text .",44,0,23
dataset/preprocessed/training-data/sentiment_analysis/17,The hidden state ht ?,45,0,5
dataset/preprocessed/training-data/sentiment_analysis/17,Rd can be interpreted as a d-dimensional distributed representation of the sequence of tokens observed up to time t.,46,0,19
dataset/preprocessed/training-data/sentiment_analysis/17,"Commonly , the RNN transition function is an affine transformation followed by a pointwise nonlinearity such as the hyperbolic tangent function :",47,0,22
dataset/preprocessed/training-data/sentiment_analysis/17,"Unfortunately , a problem with RNNs with transition functions of this form is that during training , components of the gradient vector can grow or decay exponentially overlong sequences .",48,0,30
dataset/preprocessed/training-data/sentiment_analysis/17,This problem with exploding or vanishing gradients makes it difficult for the RNN model to learn long - distance correlations in a sequence .,49,0,24
dataset/preprocessed/training-data/sentiment_analysis/29,Aspect Level Sentiment Classification with Attention - over - Attention Neural Networks,2,1,12
dataset/preprocessed/training-data/sentiment_analysis/29,Aspect - level sentiment classification aims to identify the sentiment expressed towards some aspects given context sentences .,4,1,18
dataset/preprocessed/training-data/sentiment_analysis/29,"In this paper , we introduce an attention - over- attention ( AOA ) neural network for aspect level sentiment classification .",5,0,22
dataset/preprocessed/training-data/sentiment_analysis/29,Our approach models aspects and sentences in a joint way and explicitly captures the interaction between aspects and context sentences .,6,0,21
dataset/preprocessed/training-data/sentiment_analysis/29,"With the AOA module , our model jointly learns the representations for aspects and sentences , and automatically focuses on the important parts in sentences .",7,0,26
dataset/preprocessed/training-data/sentiment_analysis/29,Our experiments on laptop and restaurant datasets demonstrate our approach outperforms previous LSTM - based architectures .,8,0,17
dataset/preprocessed/training-data/sentiment_analysis/29,"Unlike document level sentiment classification task , aspect level sentiment classification is a more fine - grained classification task .",10,0,20
dataset/preprocessed/training-data/sentiment_analysis/29,"It aims at identifying the sentiment polarity ( e.g. positive , negative , neutral ) of one specific aspect in its context sentence .",11,0,24
dataset/preprocessed/training-data/sentiment_analysis/29,"For example , given a sentence "" great food but the service was dreadful "" the sentiment polarity for aspects "" food "" and "" service "" are positive and negative respectively .",12,0,33
dataset/preprocessed/training-data/sentiment_analysis/29,Aspect sentiment classification overcomes one limitation of document level sentiment classification when multiple aspects appear in one sentence .,13,0,19
dataset/preprocessed/training-data/sentiment_analysis/29,"In our previous example , there are two aspects and the general sentiment of the whole sentence is mixed with positive and negative polarity .",14,0,25
dataset/preprocessed/training-data/sentiment_analysis/29,"If we ignore the aspect information , it is hard to determine the polarity for a specified target .",15,0,19
dataset/preprocessed/training-data/sentiment_analysis/29,Such error commonly exists in the general sentiment classification tasks .,16,0,11
dataset/preprocessed/training-data/sentiment_analysis/29,"In one recent work , Jiang et al. manually evaluated a Twitter sentiment classifier and showed that 40 % of sentiment classification errors are because of not considering targets .",17,0,30
dataset/preprocessed/training-data/sentiment_analysis/29,Many methods have been proposed to deal with aspect level sentiment classification .,18,0,13
dataset/preprocessed/training-data/sentiment_analysis/29,The typical way is to build a machine learning classifier by supervised training .,19,0,14
dataset/preprocessed/training-data/sentiment_analysis/29,"Among these machine learning - based approaches , there are mainly two different types .",20,0,15
dataset/preprocessed/training-data/sentiment_analysis/29,One is to build a classifier based on manually created features .,21,0,12
dataset/preprocessed/training-data/sentiment_analysis/29,The other type is based on neural networks using end - to - end training without any prior knowledge .,22,0,20
dataset/preprocessed/training-data/sentiment_analysis/29,"Because of its capacity of learning representations from data without feature engineering , neural networks are becoming popular in this task .",23,0,22
dataset/preprocessed/training-data/sentiment_analysis/29,"Because of advantages of neural networks , we approach this aspect level sentiment classification problem based on long short - term memory ( LSTM ) neural networks .",24,0,28
dataset/preprocessed/training-data/sentiment_analysis/29,"Previous LSTM - based methods mainly focus on modeling texts separately , while our approach models aspects and texts simultaneously using LSTMs .",25,0,23
dataset/preprocessed/training-data/sentiment_analysis/29,"Furthermore , the target representation and text representation generated from LSTMs interact with each other by an attention - over - attention ( AOA ) module .",26,0,27
dataset/preprocessed/training-data/sentiment_analysis/29,AOA automatically generates mutual attentions not only from aspect - to - text but also text - to - aspect .,27,0,21
dataset/preprocessed/training-data/sentiment_analysis/29,This is inspired by the observation that only few words in a sentence contribute to the sentiment towards an aspect .,28,0,21
dataset/preprocessed/training-data/sentiment_analysis/29,"Many times , those sentiment bearing words are highly correlated with the aspects .",29,0,14
dataset/preprocessed/training-data/sentiment_analysis/29,"In our previous example , there are two aspects "" appetizers "" and "" service "" in the sentence "" the appetizers are ok , but the service is slow . """,30,0,32
dataset/preprocessed/training-data/sentiment_analysis/29,"Based on our language experience , we know that the negative word "" slow "" is more likely to describe "" service "" but not the "" appetizers "" .",31,0,30
dataset/preprocessed/training-data/sentiment_analysis/29,"Similarly , for an aspect phrase , we also need to focus on the most important part .",32,0,18
dataset/preprocessed/training-data/sentiment_analysis/29,That is why we choose AOA to attend to the most important parts in both aspect and sentence .,33,0,19
dataset/preprocessed/training-data/sentiment_analysis/29,"Compared to previous methods , our model performs better on the laptop and restaurant datasets from SemEval 2014 2 Related work Sentiment Classification Sentiment classification aims at detecting the sentiment polarity for text .",34,0,34
dataset/preprocessed/training-data/sentiment_analysis/29,There are various approaches proposed for this research question .,35,0,10
dataset/preprocessed/training-data/sentiment_analysis/29,Most existing works use machine learning algorithms to classify texts in a supervision fashion .,36,0,15
dataset/preprocessed/training-data/sentiment_analysis/29,Algorithms like Naive Bayes and Support Vector Machine ( SVM ) are widely used in this problem .,37,0,18
dataset/preprocessed/training-data/sentiment_analysis/29,The majority of these approaches either rely on n-gram features or manually designed features .,38,0,15
dataset/preprocessed/training-data/sentiment_analysis/29,Multiple sentiment lexicons are built for this purpose .,39,0,9
dataset/preprocessed/training-data/sentiment_analysis/29,"In the recent years , sentiment classification has been advanced by neural networks significantly .",40,0,15
dataset/preprocessed/training-data/sentiment_analysis/29,Neural network based approaches automatically learn feature representations and do not require intensive feature engineering .,41,0,16
dataset/preprocessed/training-data/sentiment_analysis/29,Researchers proposed a variety of neural network architectures .,42,0,9
dataset/preprocessed/training-data/sentiment_analysis/29,"Classical methods include Convolutional Neural Networks , Recurrent Neural Networks , Recursive Neural Networks .",43,0,15
dataset/preprocessed/training-data/sentiment_analysis/29,These approaches have achieved promising results on sentiment analysis .,44,0,10
dataset/preprocessed/training-data/sentiment_analysis/29,"Aspect Level Sentiment Classification Aspect level sentiment classification is a branch of sentiment classification , the goal of which is to identify the sentiment polarity of one specific aspect in a sentence .",45,0,33
dataset/preprocessed/training-data/sentiment_analysis/29,"Some early works designed several rule based models for aspect level sentiment classification , such as .",46,0,17
dataset/preprocessed/training-data/sentiment_analysis/29,"Nasukawa et al. first perform dependency parsing on sentences , then they use predefined rules to determine the sentiment about aspects .",47,0,22
dataset/preprocessed/training-data/sentiment_analysis/29,Jiang et al .,48,0,4
dataset/preprocessed/training-data/sentiment_analysis/29,improve the target - dependent sentiment classification by creating several target - dependent features based on the sentences ' grammar structures .,49,0,22
dataset/preprocessed/training-data/sentiment_analysis/24,An Interactive Multi - Task Learning Network for End - to - End Aspect - Based Sentiment Analysis,2,1,18
dataset/preprocessed/training-data/sentiment_analysis/24,Aspect - based sentiment analysis produces a list of aspect terms and their corresponding sentiments for a natural language sentence .,4,0,21
dataset/preprocessed/training-data/sentiment_analysis/24,"This task is usually done in a pipeline manner , with aspect term extraction performed first , followed by sentiment predictions toward the extracted aspect terms .",5,0,27
dataset/preprocessed/training-data/sentiment_analysis/24,"While easier to develop , such an approach does not fully exploit joint information from the two subtasks and does not use all available sources of training information that might be helpful , such as document - level labeled sentiment corpus .",6,0,42
dataset/preprocessed/training-data/sentiment_analysis/24,"In this paper , we propose an interactive multi-task learning network ( IMN ) which is able to jointly learn multiple related tasks simultaneously at both the token level as well as the document level .",7,0,36
dataset/preprocessed/training-data/sentiment_analysis/24,"Unlike conventional multi-task learning methods that rely on learning common features for the different tasks , IMN introduces a message passing architecture where information is iteratively passed to different tasks through a shared set of latent variables .",8,0,38
dataset/preprocessed/training-data/sentiment_analysis/24,Experimental results demonstrate superior performance of the proposed method against multiple baselines on three benchmark datasets .,9,0,17
dataset/preprocessed/training-data/sentiment_analysis/24,Aspect - based sentiment analysis ( ABSA ) aims to determine people 's attitude towards specific aspects in a review .,11,1,21
dataset/preprocessed/training-data/sentiment_analysis/24,"This is done by extracting explicit aspect mentions , referred to as aspect term extraction ( AE ) , and detecting the sentiment orientation towards each extracted aspect term , referred to as aspect - level sentiment classification ( AS ) .",12,1,42
dataset/preprocessed/training-data/sentiment_analysis/24,"For example , in the sentence "" Great food but the service is dreadful "" , the aspect terms are "" food "" and "" service "" , and the sentiment orientations towards them are positive and negative respectively .",13,0,40
dataset/preprocessed/training-data/sentiment_analysis/24,"In previous works , AE and AS are typically treated separately and the over all task is performed in a pipeline manner , which may not fully exploit the joint information between the two tasks .",14,0,36
dataset/preprocessed/training-data/sentiment_analysis/24,"Recently , two studies have shown that integrated models can achieve comparable results to pipeline methods .",15,0,17
dataset/preprocessed/training-data/sentiment_analysis/24,Both works formulate the problem as a single sequence labeling task with a unified tagging scheme,16,0,16
dataset/preprocessed/training-data/sentiment_analysis/24,"However , in their methods , the two tasks are only linked through unified tags , while the correlation between them is not explicitly modeled .",18,0,26
dataset/preprocessed/training-data/sentiment_analysis/24,"Furthermore , the methods only learn from aspect - level instances , the size of which is usually small , and do not exploit available information from other sources such as related documentlevel labeled sentiment corpora , which contain useful sentiment - related linguistic knowledge and are much easier to obtain in practice .",19,0,54
dataset/preprocessed/training-data/sentiment_analysis/24,"In this work , we propose an interactive multitask learning network ( IMN ) , which solves both tasks simultaneously , enabling the interactions between both tasks to be better exploited .",20,0,32
dataset/preprocessed/training-data/sentiment_analysis/24,"Furthermore , IMN allows AE and AS to be trained together with related document - level tasks , exploiting the knowledge from larger document - level corpora .",21,0,28
dataset/preprocessed/training-data/sentiment_analysis/24,IMN introduces a novel message passing mechanism that allows informative interactions between tasks .,22,0,14
dataset/preprocessed/training-data/sentiment_analysis/24,"Specifically , it sends useful information from different tasks back to a shared latent representation .",23,0,16
dataset/preprocessed/training-data/sentiment_analysis/24,The information is then combined with the shared latent representation and made available to all tasks for further processing .,24,0,20
dataset/preprocessed/training-data/sentiment_analysis/24,"This operation is performed iteratively , allowing the information to be modified and propagated across multiple links as the number of iterations increases .",25,0,24
dataset/preprocessed/training-data/sentiment_analysis/24,"In contrast to most multi-task learning schemes which share information through learning a common feature representation , IMN not only allows shared features , but also explicitly models the interactions between tasks through the message passing mechanism , allowing different tasks to better influence each other .",26,0,47
dataset/preprocessed/training-data/sentiment_analysis/24,"In addition , IMN allows fined - grained tokenlevel classification tasks to be trained together with document - level classification tasks .",27,0,22
dataset/preprocessed/training-data/sentiment_analysis/24,"We incorporated two document - level classification tasks - sentiment classification ( DS ) and domain classification ( DD ) - to be jointly trained with AE and AS , allowing the aspect - level tasks to benefit from document - level information .",28,0,44
dataset/preprocessed/training-data/sentiment_analysis/24,"In our experiments , we show that the proposed method is able to outperform multiple pipeline and integrated baselines on three benchmark datasets 2 .",29,0,25
dataset/preprocessed/training-data/sentiment_analysis/24,Aspect - Based Sentiment Analysis .,31,0,6
dataset/preprocessed/training-data/sentiment_analysis/24,"Existing approaches typically decompose ABSA into two subtasks , and solve them in a pipeline setting .",32,0,17
dataset/preprocessed/training-data/sentiment_analysis/24,Both AE and AS have been extensively studied in the literature .,33,0,12
dataset/preprocessed/training-data/sentiment_analysis/24,"However , treating each task independently has several dis advantages .",34,0,11
dataset/preprocessed/training-data/sentiment_analysis/24,"In a pipeline setting , errors from the first step tend to be propagated to the second step , leading to a poorer over all performance .",35,0,27
dataset/preprocessed/training-data/sentiment_analysis/24,"In addition , this approach is unable to exploit the commonalities and associations between tasks , which may help reduce the amount of training data required to train both tasks .",36,0,31
dataset/preprocessed/training-data/sentiment_analysis/24,Some previous works have attempted to develop integrated solutions .,37,0,10
dataset/preprocessed/training-data/sentiment_analysis/24,proposed to model the problem as a sequence labeling task with a unified tagging scheme .,38,0,16
dataset/preprocessed/training-data/sentiment_analysis/24,"However , their results were discouraging .",39,0,7
dataset/preprocessed/training-data/sentiment_analysis/24,"Recently , two works have shown some promising results in this direction with more sophisticated network structures .",40,0,18
dataset/preprocessed/training-data/sentiment_analysis/24,"However , in their models , the two subtasks are still only linked through a unified tagging scheme , while the interactions between them are not explicitly mod -2 Our source code can be obtained from https :// github.com/ruidan/IMN-E2E-ABSA eled .",41,0,41
dataset/preprocessed/training-data/sentiment_analysis/24,"To address this issue , a better network structure allowing further task interactions is needed .",42,0,16
dataset/preprocessed/training-data/sentiment_analysis/24,Multi - Task Learning .,43,0,5
dataset/preprocessed/training-data/sentiment_analysis/24,"One straightforward approach to perform AE and AS simultaneously is multi-task learning , where one conventional framework is to employ a shared network and two task - specific network to derive a shared feature space and two task - specific feature spaces .",44,0,43
dataset/preprocessed/training-data/sentiment_analysis/24,Multitask learning frameworks have been employed successfully in various natural language processing ( NLP ) tasks .,45,0,17
dataset/preprocessed/training-data/sentiment_analysis/24,"By learning semantically related tasks in parallel using a shared representation , multi-task learning could capture the correlations between tasks and improve the model generalization ability in certain cases .",46,0,30
dataset/preprocessed/training-data/sentiment_analysis/24,"For ABSA , have shown that aspectlevel sentiment classification can be significantly improved through joint training with documentlevel sentiment classification .",47,0,21
dataset/preprocessed/training-data/sentiment_analysis/24,"However , conventional multi-task learning still does not explicitly model the interactions between tasks - the two tasks only interact with each other through error back - propoagation to contribute to the learned features and such implicit interactions are not controllable .",48,0,42
dataset/preprocessed/training-data/sentiment_analysis/24,"Unlike existing methods , our proposed IMN not only allows the representations to be shared , but also explicitly models the interactions between tasks , by using an iterative message passing scheme .",49,0,33
dataset/preprocessed/training-data/sentiment_analysis/42,Aspect Level Sentiment Classification with Deep Memory Network,2,1,8
dataset/preprocessed/training-data/sentiment_analysis/42,We introduce a deep memory network for aspect level sentiment classification .,4,0,12
dataset/preprocessed/training-data/sentiment_analysis/42,"Unlike feature - based SVM and sequential neural models such as LSTM , this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect .",5,0,32
dataset/preprocessed/training-data/sentiment_analysis/42,"Such importance degree and text representation are calculated with multiple computational layers , each of which is a neural attention model over an external memory .",6,0,26
dataset/preprocessed/training-data/sentiment_analysis/42,"Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state - of - art feature based SVM system , and substantially better than LSTM and attention - based LSTM architectures .",7,0,35
dataset/preprocessed/training-data/sentiment_analysis/42,On both datasets we show that multiple computational layers could improve the performance .,8,0,14
dataset/preprocessed/training-data/sentiment_analysis/42,"Moreover , our approach is also fast .",9,0,8
dataset/preprocessed/training-data/sentiment_analysis/42,The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation .,10,0,18
dataset/preprocessed/training-data/sentiment_analysis/42,Aspect level sentiment classification is a fundamental task in the field of sentiment analysis .,12,1,15
dataset/preprocessed/training-data/sentiment_analysis/42,"Given a sentence and an aspect occurring in the sentence , this task aims at inferring the sentiment polarity ( e.g. positive , negative , neutral ) of the aspect .",13,0,31
dataset/preprocessed/training-data/sentiment_analysis/42,"For example , in sentence "" great food but the service was dreadful ! "" , the sentiment polarity of aspect "" food "" is positive while the polarity of aspect * Corresponding author .",14,0,35
dataset/preprocessed/training-data/sentiment_analysis/42,""" service "" is negative .",15,0,6
dataset/preprocessed/training-data/sentiment_analysis/42,Researchers typically use machine learning algorithms and build sentiment classifier in a supervised manner .,16,0,15
dataset/preprocessed/training-data/sentiment_analysis/42,Representative approaches in literature include feature based Support Vector Machine and neural network models .,17,0,15
dataset/preprocessed/training-data/sentiment_analysis/42,"Neural models are of growing interest for their capacity to learn text representation from data without careful engineering of features , and to capture semantic relations between aspect and context words in a more scalable way than feature based SVM .",18,0,41
dataset/preprocessed/training-data/sentiment_analysis/42,"Despite these advantages , conventional neural models like long short - term memory ( LSTM ) capture context information in an implicit way , and are incapable of explicitly exhibiting important context clues of an aspect .",19,0,37
dataset/preprocessed/training-data/sentiment_analysis/42,We believe that only some subset of context words are needed to infer the sentiment towards an aspect .,20,0,19
dataset/preprocessed/training-data/sentiment_analysis/42,"For example , in sentence "" great food but the service was dreadful ! "" , "" dreadful "" is an important clue for the aspect "" service "" but "" great "" is not needed .",21,0,37
dataset/preprocessed/training-data/sentiment_analysis/42,"Standard LSTM works in a sequential way and manipulates each context word with the same operation , so that it can not explicitly reveal the importance of each context word .",22,0,31
dataset/preprocessed/training-data/sentiment_analysis/42,A desirable solution should be capable of explicitly capturing the importance of context words and using that information to buildup features for the sentence after given an aspect word .,23,0,30
dataset/preprocessed/training-data/sentiment_analysis/42,"Furthermore , a human asked to do this task will selectively focus on parts of the contexts , and acquire information where it is needed to buildup an internal representation towards an aspect in his / her mind .",24,0,39
dataset/preprocessed/training-data/sentiment_analysis/42,"In pursuit of this goal , we develop deep memory network for aspect level sentiment classification , which is inspired by the recent success of computational models with attention mechanism and explicit memory .",25,0,34
dataset/preprocessed/training-data/sentiment_analysis/42,"Our approach is data - driven , computationally efficient and does not rely on syntactic parser or sentiment lexicon .",26,0,20
dataset/preprocessed/training-data/sentiment_analysis/42,The approach consists of multiple computational layers with shared parameters .,27,0,11
dataset/preprocessed/training-data/sentiment_analysis/42,"Each layer is a content - and location - based attention model , which first learns the importance / weight of each context word and then utilizes this information to calculate continuous text representation .",28,0,35
dataset/preprocessed/training-data/sentiment_analysis/42,The text representation in the last layer is regarded as the feature for sentiment classification .,29,0,16
dataset/preprocessed/training-data/sentiment_analysis/42,"As every component is differentiable , the entire model could be efficiently trained end - toend with gradient descent , where the loss function is the cross - entropy error of sentiment classification .",30,0,34
dataset/preprocessed/training-data/sentiment_analysis/42,We apply the proposed approach to laptop and restaurant datasets from SemEval 2014 .,31,0,14
dataset/preprocessed/training-data/sentiment_analysis/42,Experimental results show that our approach performs comparable to atop system using feature - based SVM .,32,0,17
dataset/preprocessed/training-data/sentiment_analysis/42,"On both datasets , our approach outperforms both LSTM and attention - based LSTM models in terms of classification accuracy and running speed .",33,0,24
dataset/preprocessed/training-data/sentiment_analysis/42,"Lastly , we show that using multiple computational layers over external memory could achieve improved performance .",34,0,17
dataset/preprocessed/training-data/sentiment_analysis/42,Our approach is inspired by the recent success of memory network in question answering .,37,0,15
dataset/preprocessed/training-data/sentiment_analysis/42,We describe the background on memory network in this part .,38,0,11
dataset/preprocessed/training-data/sentiment_analysis/42,Memory network is a general machine learning framework introduced by Weston et al ..,39,0,14
dataset/preprocessed/training-data/sentiment_analysis/42,"Its central idea is inference with a long - term memory component , which could be read , written to , and jointly learned with the goal of using it for prediction .",40,0,33
dataset/preprocessed/training-data/sentiment_analysis/42,"Formally , a memory network consists of a memory m and four components I , G , O and R , where m is an array of objects such as an array of vectors .",41,0,35
dataset/preprocessed/training-data/sentiment_analysis/42,"Among these four components , I converts input to internal feature representation , G updates old memories with new input , O generates an out - put representation given a new input and the current memory state , R outputs a response based on the output representation .",42,0,48
dataset/preprocessed/training-data/sentiment_analysis/42,Let us take question answering as an example to explain the workflow of memory network .,43,0,16
dataset/preprocessed/training-data/sentiment_analysis/42,"Given a list of sentences and a question , the task aims to find evidences from these sentences and generate an answer , e.g. a word .",44,0,27
dataset/preprocessed/training-data/sentiment_analysis/42,"During inference , I component reads one sentence s i at a time and encodes it into a vector representation .",45,0,21
dataset/preprocessed/training-data/sentiment_analysis/42,Then G component updates apiece of memory mi based on current sentence representation .,46,0,14
dataset/preprocessed/training-data/sentiment_analysis/42,"After all sentences are processed , we get a memory matrix m which stores the semantics of these sentences , each row representing a sentence .",47,0,26
dataset/preprocessed/training-data/sentiment_analysis/42,"Given a question q , memory network encodes it into vector representation e q , and then O component uses e q to select question related evidences from memory m and generates an output vector o .",48,0,37
dataset/preprocessed/training-data/sentiment_analysis/42,"Finally , R component takes o as the input and outputs the final response .",49,0,15
dataset/preprocessed/training-data/sentiment_analysis/31,Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification,2,1,9
dataset/preprocessed/training-data/sentiment_analysis/31,We introduce a novel parameterized convolutional neural network for aspect level sentiment classification .,4,0,14
dataset/preprocessed/training-data/sentiment_analysis/31,"Using parameterized filters and parameterized gates , we incorporate aspect information into convolutional neural networks ( CNN ) .",5,0,19
dataset/preprocessed/training-data/sentiment_analysis/31,"Experiments demonstrate that our parameterized filters and parameterized gates effectively capture the aspectspecific features , and our CNN - based models achieve excellent results on SemEval 2014 datasets .",6,0,29
dataset/preprocessed/training-data/sentiment_analysis/31,Continuous growing of user generated text in social media platforms such as Twitter drives sentiment classification increasingly popular .,8,1,19
dataset/preprocessed/training-data/sentiment_analysis/31,"The goal of sentiment classification is to detect whether apiece of text expresses a positive , a negative , or a neutral sentiment .",9,0,24
dataset/preprocessed/training-data/sentiment_analysis/31,"The majority of the literature focuses on general sentiment analysis ( document level ) , not involving a specific topic or entity .",10,0,23
dataset/preprocessed/training-data/sentiment_analysis/31,"When there are multiple aspects about an entity in a sentence , it is hard to determine the sentiment as a whole .",11,0,23
dataset/preprocessed/training-data/sentiment_analysis/31,"Differing from general sentiment classification , aspect level sentiment classification identifies opinions from text about specific entities and their aspects .",12,1,21
dataset/preprocessed/training-data/sentiment_analysis/31,"For example , given a sentence "" great food but the service was dreadful "" , the sentiment polarity about aspect "" food "" is positive while the sentiment polarity about "" service "" is negative .",13,0,37
dataset/preprocessed/training-data/sentiment_analysis/31,"If we ignore the aspect information , it is hard to determine the sentiment for a target aspect , which accounts for a large portion of sentiment classification error .",14,0,30
dataset/preprocessed/training-data/sentiment_analysis/31,"Recently , machine learning based approaches are becoming popular for this task .",15,0,13
dataset/preprocessed/training-data/sentiment_analysis/31,Representative approaches in literature include Support Vector Machine ( SVM ) with manually created features and neural network based models .,16,0,21
dataset/preprocessed/training-data/sentiment_analysis/31,"Because of neural networks ' capacity of learning representations from data without feature engineering , they are of growing interest for this natural language processing task .",17,0,27
dataset/preprocessed/training-data/sentiment_analysis/31,The mainstream neural methods are either based on long short - term memory or memory networks .,18,0,17
dataset/preprocessed/training-data/sentiment_analysis/31,"None of them are using convolutional neural networks ( CNN ) , which are good at capturing local patterns .",19,0,20
dataset/preprocessed/training-data/sentiment_analysis/31,"In the present work , we propose two simple yet effective convolutional neural networks with aspect information incorporated .",20,0,19
dataset/preprocessed/training-data/sentiment_analysis/31,The over all architecture differs significantly from previous work .,21,0,10
dataset/preprocessed/training-data/sentiment_analysis/31,"Specifically , we design two novel neural units that take target aspects into account .",22,0,15
dataset/preprocessed/training-data/sentiment_analysis/31,"One is parameterized filter , the other is parameterized gate .",23,0,11
dataset/preprocessed/training-data/sentiment_analysis/31,These units both are generated from aspect - specific features and are further applied on the sentence .,24,0,18
dataset/preprocessed/training-data/sentiment_analysis/31,Our experiments show that our two model variants work surprisingly well on this type of task .,25,0,17
dataset/preprocessed/training-data/sentiment_analysis/31,Aspect level sentiment classification is a branch of sentiment classification .,27,0,11
dataset/preprocessed/training-data/sentiment_analysis/31,It aims at identifying the sentiment polarity of one aspect target in a context sentence .,28,0,16
dataset/preprocessed/training-data/sentiment_analysis/31,One typical early work tries to identify the aspect level sentiment polarity based on predefined language rules .,29,0,18
dataset/preprocessed/training-data/sentiment_analysis/31,Nasukawa and Yi first perform dependency parsing on sentences .,30,0,10
dataset/preprocessed/training-data/sentiment_analysis/31,Then rules are applied to determine the sentiment about aspects .,31,0,11
dataset/preprocessed/training-data/sentiment_analysis/31,Standard machine learning algorithms like SVM are also widely used on this task .,32,0,14
dataset/preprocessed/training-data/sentiment_analysis/31,"Jiang et al. create several targetdependent features , then they feed these targetdependent features with content features into an SVM classifier .",33,0,22
dataset/preprocessed/training-data/sentiment_analysis/31,"In recent years , aspect level sentiment classification is dominated by neural network based approaches .",34,0,16
dataset/preprocessed/training-data/sentiment_analysis/31,The majority of published works rely on the architecture of long short - term memory ( LSTM ) . use an attention vector generated from aspect embedding to better capture the important parts in sentences .,35,0,36
dataset/preprocessed/training-data/sentiment_analysis/31,introduce a wordaspect fusion operation to learn associative relationships between aspects and sentences .,36,0,14
dataset/preprocessed/training-data/sentiment_analysis/31,use an attention - over- attention layer to further improve the performance .,37,0,13
dataset/preprocessed/training-data/sentiment_analysis/31,Another type of neural architectures known as memory network has also been used in this task .,38,0,17
dataset/preprocessed/training-data/sentiment_analysis/31,takes an aspect term as a query sent to external memory .,39,0,12
dataset/preprocessed/training-data/sentiment_analysis/31,Their model consists of multiple computational layers .,40,0,8
dataset/preprocessed/training-data/sentiment_analysis/31,Each layer is an attention model .,41,0,7
dataset/preprocessed/training-data/sentiment_analysis/31,One recent work Dyadic Mem NN places associative layers on top of memory networks to improve the performance .,42,0,19
dataset/preprocessed/training-data/sentiment_analysis/31,The over all architecture in this paper differs significantly with all these previous works .,43,0,15
dataset/preprocessed/training-data/sentiment_analysis/31,"To the best of our knowledge , this paper is the first attempt using convolutional neural networks for aspect level sentiment classification .",44,0,23
dataset/preprocessed/training-data/sentiment_analysis/31,Parameterized Convolutional Neural Networks,45,0,4
dataset/preprocessed/training-data/sentiment_analysis/31,"In this section , we introduce our method for aspect level sentiment classification , which is based on convolutional neural networks .",46,0,22
dataset/preprocessed/training-data/sentiment_analysis/31,"We first describe CNN for general sentiment classification , then we introduce our two model variants Parameterized Filters for Convolutional Neural Networks ( PF - CNN ) and Parameterized Gated Convolutional Neural Networks ( PG - CNN ) .",47,0,39
dataset/preprocessed/training-data/sentiment_analysis/31,"In aspect level sentiment classification , we are given a sentence s = [ w 1 , w 2 , ... , w i , ... , w n ] and an aspect target t = [ w i , w i + 1 , ... , w i +m ?1 ] .",49,0,53
dataset/preprocessed/training-data/sentiment_analysis/37,IARM : Inter-Aspect Relation Modeling with Memory Networks in Aspect - Based Sentiment Analysis,2,1,14
dataset/preprocessed/training-data/sentiment_analysis/37,Sentiment analysis has immense implications in modern businesses through user-feedback mining .,4,1,12
dataset/preprocessed/training-data/sentiment_analysis/37,Large product - based enterprises like Samsung and Apple make crucial business decisions based on the large quantity of user reviews and suggestions available in different e-commerce websites and social media platforms like Amazon and Facebook .,5,0,37
dataset/preprocessed/training-data/sentiment_analysis/37,Sentiment analysis caters to these needs by summarizing user sentiment behind a particular object .,6,0,15
dataset/preprocessed/training-data/sentiment_analysis/37,"In this paper , we present a novel approach of incorporating the neighboring aspects related information into the sentiment classification of the target aspect using memory networks .",7,0,28
dataset/preprocessed/training-data/sentiment_analysis/37,Our method outperforms the state of the art by 1.6 % on average in two distinct domains .,8,0,18
dataset/preprocessed/training-data/sentiment_analysis/37,"Sentiment analysis plays a huge role in userfeedback extraction from different popular ecommerce websites like Amazon , eBay , etc .",10,0,21
dataset/preprocessed/training-data/sentiment_analysis/37,"Large enterprises are not only interested in the over all user sentiment about a given product , but the sentiment behind the finer aspects of a product is also very important to them .",11,0,34
dataset/preprocessed/training-data/sentiment_analysis/37,"Companies allocate their resources to research , development , and marketing based on these factors .",12,0,16
dataset/preprocessed/training-data/sentiment_analysis/37,Aspect - based sentiment analysis ( ABSA ) caters to these needs .,13,1,13
dataset/preprocessed/training-data/sentiment_analysis/37,Users tend to express their opinion on different aspects of a given product .,14,0,14
dataset/preprocessed/training-data/sentiment_analysis/37,"For example , the sentence "" Everything is so easy to use , Mac software is just so much simpler than Microsoft software . "" expresses sentiment behind three aspects : "" use "" , "" Mac software "" , and "" Microsoft software "" to be positive , positive , and negative respectively .",15,0,55
dataset/preprocessed/training-data/sentiment_analysis/37,This leads to two tasks to be solved : aspect extraction and aspect sentiment polarity detection .,16,0,17
dataset/preprocessed/training-data/sentiment_analysis/37,"In this paper , we tackle the latter problem by modeling the relation among different aspects in a sentence .",17,0,20
dataset/preprocessed/training-data/sentiment_analysis/37,Recent works on ABSA does not consider the neighboring aspects in a sentence during classification .,18,0,16
dataset/preprocessed/training-data/sentiment_analysis/37,"For instance , in the sentence "" The menu is very limited - I think we counted 4 or 5 entries . "" , the sub-sentence "" I think ... entries "" does not reflect the true sentiment behind containing aspect "" entries "" , unless the other aspect "" menu "" is considered .",19,0,55
dataset/preprocessed/training-data/sentiment_analysis/37,"Here , the negative sentiment of "" menu "" induces "" entries "" to have the same sentiment .",20,0,19
dataset/preprocessed/training-data/sentiment_analysis/37,We hypothesize that our architecture iteratively models the influence from the other aspects to generate accurate target aspect representation .,21,0,20
dataset/preprocessed/training-data/sentiment_analysis/37,"In sentences containing multiple aspects , the main challenge an Aspect - Based - Sentiment - Analysis ( ABSA ) classifier faces is to correctly connect an aspect to the corresponding sentimentbearing phrase ( typically adjective ) .",22,0,38
dataset/preprocessed/training-data/sentiment_analysis/37,"Let us consider this sentence "" Coffee is a better deal than overpriced cosi sandwiches "" .",23,0,17
dataset/preprocessed/training-data/sentiment_analysis/37,"Here , we find two aspects : "" coffee "" and "" cosi sandwiches "" .",24,0,16
dataset/preprocessed/training-data/sentiment_analysis/37,"It is clear in this sentence that the sentiment of "" coffee "" is expressed by the sentimentally charged word "" better "" ; on the other hand , "" overpriced "" carries the sentiment of "" cosi sandwiches "" .",25,0,41
dataset/preprocessed/training-data/sentiment_analysis/37,The aim of the ABSA classifier is to learn these connections between the aspects and their sentiment bearing phrases .,26,1,20
dataset/preprocessed/training-data/sentiment_analysis/37,"In this work , we argue that during sentiment prediction of an aspect ( say "" coffee "" in this case ) , the knowledge of the existence and representation of the other aspects ( "" cosi sandwiches "" ) in the sentence is beneficial .",27,0,46
dataset/preprocessed/training-data/sentiment_analysis/37,The sentiment of an aspect in a sentence can influence the succeeding aspects due to the presence of conjunctions .,28,0,20
dataset/preprocessed/training-data/sentiment_analysis/37,"In particular , for sentences containing conjunctions like and , not only , also , but , however , though , etc. , aspects tend to share their sentiments .",29,0,30
dataset/preprocessed/training-data/sentiment_analysis/37,"In the sentence "" Food is usually very good , though I wonder about freshness of raw vegetables "" , the aspect "" raw vegetables "" does not have any trivial sentiment marker linked to it .",30,0,37
dataset/preprocessed/training-data/sentiment_analysis/37,"However , the positive sentiment of "" food "" , due to the word "" "" good "" , and presence of conjunction "" though "" determines the sentiment of "" raw vegetables "" to be negative .",31,0,38
dataset/preprocessed/training-data/sentiment_analysis/37,"Thus , aspects when arranged as a sequence , reveal high correlation and interplay of sentiments .",32,0,17
dataset/preprocessed/training-data/sentiment_analysis/37,"To model these scenarios , firstly , following , we independently generate aspect - aware sentence representations for all the aspects using gated recurrent unit ( GRU ) and attention mechanism .",33,0,32
dataset/preprocessed/training-data/sentiment_analysis/37,"Then , we employ memory networks to repeatedly match the target aspect representation with the other aspects to generate more accurate representation of the target aspect .",34,0,27
dataset/preprocessed/training-data/sentiment_analysis/37,This refined representation is fed to a softmax classifier for final classification .,35,0,13
dataset/preprocessed/training-data/sentiment_analysis/37,We empirically show below that our method outperforms the current state of the art by 1.6 % on average in two distinct domains : restaurant and laptop .,36,0,28
dataset/preprocessed/training-data/sentiment_analysis/37,The rest of the paper structured as follows :,37,0,9
dataset/preprocessed/training-data/sentiment_analysis/37,"Section 2 discusses previous works ; Section 3 delves into the method we present ; Section 4 mentions the dataset , baselines , and experimental settings ; Section 5 presents and analyzes the results ; finally , Section 6 concludes this paper .",38,0,43
dataset/preprocessed/training-data/sentiment_analysis/37,"Sentiment analysis is becoming increasingly important due to the rise of the need to process textual data in wikis , micro-blogs , and other social media platforms .",40,0,28
dataset/preprocessed/training-data/sentiment_analysis/37,"Sentiment analysis requires solving several related NLP problems , like aspect extraction .",41,0,13
dataset/preprocessed/training-data/sentiment_analysis/37,Aspect based sentiment analysis ( ABSA ) is a key task of sentiment analysis which focuses on classifying sentiment of each aspect in the sentences .,42,0,26
dataset/preprocessed/training-data/sentiment_analysis/37,"In this paper , we focus on ABSA , which is a key task of sentiment analysis that aims to classify sentiment of each aspect individually in a sentence .",43,0,30
dataset/preprocessed/training-data/sentiment_analysis/37,"In recent days , thanks to the increasing progress of deep neural network research , novel frameworks have been proposed , achieving notable performance improvement in aspect - based sentiment analysis .",44,0,32
dataset/preprocessed/training-data/sentiment_analysis/37,The common way of doing ABSA is feeding the aspect - aware sentence representation to the neural network for classification .,45,0,21
dataset/preprocessed/training-data/sentiment_analysis/37,This was first proposed by where they appended aspect embeddings with the each word embeddings of the sentence to generate aspect - aware sentence representation .,46,0,26
dataset/preprocessed/training-data/sentiment_analysis/37,This representation was further fed to an attention layer followed by softmax for final classification .,47,0,16
dataset/preprocessed/training-data/sentiment_analysis/37,"More recently , proposed a model where both context and aspect representations interact with each other 's attention mechanism to generate the over all representation .",48,0,26
dataset/preprocessed/training-data/sentiment_analysis/37,proposed word - aspect associations using circular correlation as an improvement over 's work .,49,0,15
dataset/preprocessed/training-data/sentiment_analysis/33,Convolutional Neural Networks with Recurrent Neural Filters,2,1,7
dataset/preprocessed/training-data/sentiment_analysis/33,We introduce a class of convolutional neural networks ( CNNs ) that utilize recurrent neural networks ( RNNs ) as convolution filters .,4,0,23
dataset/preprocessed/training-data/sentiment_analysis/33,"A convolution filter is typically implemented as a linear affine transformation followed by a nonlinear function , which fails to account for language compositionality .",5,0,25
dataset/preprocessed/training-data/sentiment_analysis/33,"As a result , it limits the use of high - order filters thatare often warranted for natural language processing tasks .",6,0,22
dataset/preprocessed/training-data/sentiment_analysis/33,"In this work , we model convolution filters with RNNs that naturally capture compositionality and long - term dependencies in language .",7,1,22
dataset/preprocessed/training-data/sentiment_analysis/33,We show that simple CNN architectures equipped with recurrent neural filters ( RNFs ) achieve results thatare on par with the best published ones on the Stanford Sentiment Treebank and two answer sentence selection datasets .,8,0,36
dataset/preprocessed/training-data/sentiment_analysis/33,"Convolutional neural networks ( CNNs ) have been shown to achieve state - of - the - art results on various natural language processing ( NLP ) tasks , such as sentence classification , question answering , and machine translation .",11,0,41
dataset/preprocessed/training-data/sentiment_analysis/33,"In an NLP system , a convolution operation is typically a sliding window function that applies a convolution filter to every possible window of words in a sentence .",12,0,29
dataset/preprocessed/training-data/sentiment_analysis/33,"Hence , the key components of CNNs are a set of convolution filters that compose low - level word features into higher - level representations .",13,0,26
dataset/preprocessed/training-data/sentiment_analysis/33,"Convolution filters are usually realized as linear systems , as their outputs are affine transformations of the inputs followed by some non-linear activation functions .",14,0,25
dataset/preprocessed/training-data/sentiment_analysis/33,"Prior work directly adopts a linear convolution filter to NLP problems by utilizing a concatenation of embeddings of a window of words as the input , which leverages word order information in a shallow additive way .",15,0,37
dataset/preprocessed/training-data/sentiment_analysis/33,"As early CNN architectures have mainly drawn inspiration from models of the primate visual system , the necessity of coping with language compositionality is largely overlooked in these systems .",16,0,30
dataset/preprocessed/training-data/sentiment_analysis/33,"Due to the linear nature of the convolution filters , they lack the ability to capture complex language phenomena , such as compositionality and long - term dependencies .",17,0,29
dataset/preprocessed/training-data/sentiment_analysis/33,"To overcome this , we propose to employ recurrent neural networks ( RNNs ) as convolution filters of CNN systems for various NLP tasks .",18,0,25
dataset/preprocessed/training-data/sentiment_analysis/33,"Our recurrent neural filters ( RNFs ) can naturally deal with language compositionality with a recurrent function that models word relations , and they are also able to implicitly model long - term dependencies .",19,0,35
dataset/preprocessed/training-data/sentiment_analysis/33,"RNFs are typically applied to word sequences of moderate lengths , which alleviates some well - known drawbacks of RNNs , including their vulnerability to the gradient vanishing and exploding problems .",20,0,32
dataset/preprocessed/training-data/sentiment_analysis/33,"As in conventional CNNs , the computation of the convolution operation with RNFs can be easily parallelized .",21,0,18
dataset/preprocessed/training-data/sentiment_analysis/33,"As a result , RNF - based CNN models can be 3 - 8 x faster than their RNN counterparts .",22,0,21
dataset/preprocessed/training-data/sentiment_analysis/33,We present two RNF - based CNN architectures for sentence classification and answer sentence selection problems .,23,0,17
dataset/preprocessed/training-data/sentiment_analysis/33,Experimental results on the Stanford Sentiment Treebank and the QASent and WikiQA datasets demonstrate that RNFs significantly improve CNN performance over linear filters by 4 - 5 % accuracies and 3 - 6 % MAP scores respectively .,24,0,38
dataset/preprocessed/training-data/sentiment_analysis/33,"Analysis results suggest that RNFs perform much better than linear filters in detecting longer key phrases , which provide stronger cues for categorizing the sentences .",25,0,26
dataset/preprocessed/training-data/sentiment_analysis/33,The aim of a convolution filter is to produce a local feature for a window of words .,27,0,18
dataset/preprocessed/training-data/sentiment_analysis/33,"We describe a novel approach to learning filters using RNNs , which is especially suitable for NLP problems .",28,0,19
dataset/preprocessed/training-data/sentiment_analysis/33,arXiv:1808.09315v1 [ cs.CL ] 28 Aug 2018,29,0,7
dataset/preprocessed/training-data/sentiment_analysis/33,We then present two CNN architectures equipped with RNFs for sentence classification and sentence matching tasks respectively .,30,0,18
dataset/preprocessed/training-data/sentiment_analysis/33,Recurrent neural filters,31,0,3
dataset/preprocessed/training-data/sentiment_analysis/33,"We briefly review the linear convolution filter implementation by , which has been widely adopted in later works .",32,0,19
dataset/preprocessed/training-data/sentiment_analysis/33,"Consider an m-gram word sequence [ x i , , x i+m?1 ] , where xi ?",33,0,17
dataset/preprocessed/training-data/sentiment_analysis/33,R k is a k-dimensional word vector .,34,0,8
dataset/preprocessed/training-data/sentiment_analysis/33,"A linear convolution filter is a function applied to the m-gram to produce a feature c i , j :",35,0,20
dataset/preprocessed/training-data/sentiment_analysis/33,"is the concatenation operator , b j is a bias term , and f is a non-linear activation function .",37,0,20
dataset/preprocessed/training-data/sentiment_analysis/33,We typically use multiple independent linear filters to yield a feature vector c i for the word sequence .,38,0,19
dataset/preprocessed/training-data/sentiment_analysis/33,Linear convolution filters make strong assumptions about language that could harm the performance of NLP systems .,39,0,17
dataset/preprocessed/training-data/sentiment_analysis/33,"First , linear filters assume local compositionality and ignore long - term dependencies in language .",40,0,16
dataset/preprocessed/training-data/sentiment_analysis/33,"Second , they use separate parameters for each value of the time index , which hinders parameter sharing for the same word type .",41,0,24
dataset/preprocessed/training-data/sentiment_analysis/33,The assumptions become more problematic if we increase the window size m .,42,0,13
dataset/preprocessed/training-data/sentiment_analysis/33,"We propose to address the limitations by employing RNNs to realize convolution filters , which we term recurrent neural filters ( RNFs ) .",43,0,24
dataset/preprocessed/training-data/sentiment_analysis/33,RNFs compose the words of the m-gram from left to right using the same recurrent unit :,44,0,17
dataset/preprocessed/training-data/sentiment_analysis/33,"where ht is a hidden state vector that encoded information about previously processed words , and the function RNN is a recurrent unit such as Long Short - Term Memory ( LSTM ) unit ( Hochreiter and Schmidhuber , 1997 ) or Gated Recurrent Unit ( GRU ) .",45,0,49
dataset/preprocessed/training-data/sentiment_analysis/33,We use the last hidden state h i +m?1 as the RNF output feature vector c i .,46,0,18
dataset/preprocessed/training-data/sentiment_analysis/33,"Features learned by RNFs are interdependent of each other , which permits the learning of complementary information about the word sequence .",47,0,22
dataset/preprocessed/training-data/sentiment_analysis/33,The left - to - right word composing procedure in RNFs preserves word order information and implicitly models long - term dependencies in language .,48,0,25
dataset/preprocessed/training-data/sentiment_analysis/33,RNFs can be treated as simple dropin replacements of linear filters and potentially adopted in numerous CNN architectures .,49,0,19
dataset/preprocessed/training-data/sentiment_analysis/0,MULTIMODAL SPEECH EMOTION RECOGNITION USING AUDIO AND TEXT,2,1,8
dataset/preprocessed/training-data/sentiment_analysis/0,"Speech emotion recognition is a challenging task , and extensive reliance has been placed on models that use audio features in building well - performing classifiers .",4,0,27
dataset/preprocessed/training-data/sentiment_analysis/0,"In this paper , we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data .",5,0,30
dataset/preprocessed/training-data/sentiment_analysis/0,"As emotional dialogue is composed of sound and spoken content , our model encodes the information from audio and text sequences using dual recurrent neural networks ( RNNs ) and then combines the information from these sources to predict the emotion class .",6,0,43
dataset/preprocessed/training-data/sentiment_analysis/0,"This architecture analyzes speech data from the signal level to the language level , and it thus utilizes the information within the data more comprehensively than models that focus on audio features .",7,0,33
dataset/preprocessed/training-data/sentiment_analysis/0,Extensive experiments are conducted to investigate the efficacy and properties of the proposed model .,8,0,15
dataset/preprocessed/training-data/sentiment_analysis/0,"Our proposed model outperforms previous state - of - the - art methods in assigning data to one of four emotion categories ( i.e. , angry , happy , sad and neutral ) when the model is applied to the IEMOCAP dataset , as reflected by accuracies ranging from 68.8 % to 71.8 % .",9,0,55
dataset/preprocessed/training-data/sentiment_analysis/0,"Recently , deep learning algorithms have successfully addressed problems in various fields , such as image classification , machine translation , speech recognition , text - to - speech generation and other machine learning related are as .",11,0,38
dataset/preprocessed/training-data/sentiment_analysis/0,"Similarly , substantial improvements in performance have been obtained when deep learning algorithms have been applied to statistical speech processing .",12,0,21
dataset/preprocessed/training-data/sentiment_analysis/0,"These fundamental improvements have led researchers to investigate additional topics related to human nature , which have long been objects of study .",13,0,23
dataset/preprocessed/training-data/sentiment_analysis/0,"One such topic involves understanding human emotions and reflecting it through machine intelligence , such as emotional dialogue models .",14,0,20
dataset/preprocessed/training-data/sentiment_analysis/0,"In developing emotionally aware intelligence , the very first step is building robust emotion classifiers that display good performance regardless of the application ; this outcome is considered to be one of the fundamental research goals in affective computing .",15,0,40
dataset/preprocessed/training-data/sentiment_analysis/0,"In particular , the speech emotion recognition task is one of the most important problems in the field of paralinguistics .",16,0,21
dataset/preprocessed/training-data/sentiment_analysis/0,"This field has recently broadened its applications , as it is a crucial factor in optimal humancomputer interactions , including dialog systems .",17,0,23
dataset/preprocessed/training-data/sentiment_analysis/0,"The goal of speech emotion recognition is to predict the emotional content of speech and to classify speech according to one of several labels ( i.e. , happy , sad , neutral , and angry ) .",18,0,37
dataset/preprocessed/training-data/sentiment_analysis/0,"Various types of deep learning methods have been applied to increase the performance of emotion classifiers ; however , this task is still considered to be challenging for several reasons .",19,0,31
dataset/preprocessed/training-data/sentiment_analysis/0,"First , insufficient data for training complex neural network - based models are available , due to the costs associated with human involvement .",20,0,24
dataset/preprocessed/training-data/sentiment_analysis/0,"Second , the characteristics of emotions must be learned from low - level speech signals .",21,0,16
dataset/preprocessed/training-data/sentiment_analysis/0,Feature - based models display limited skills when applied to this problem .,22,0,13
dataset/preprocessed/training-data/sentiment_analysis/0,"To overcome these limitations , we propose a model that uses high - level text transcription , as well as low - level audio signals , to utilize the information contained within low - resource datasets to a greater degree .",23,0,41
dataset/preprocessed/training-data/sentiment_analysis/0,"Given recent improvements in automatic speech recognition ( ASR ) technology , speech transcription can be carried out using audio signals with considerable skill .",24,0,25
dataset/preprocessed/training-data/sentiment_analysis/0,"The emotional content of speech is clearly indicated by the emotion words contained in a sentence , such as "" lovely "" and "" awesome , "" which carry strong emotions compared to generic ( non-emotion ) words , such as "" person "" and "" day . """,25,0,49
dataset/preprocessed/training-data/sentiment_analysis/0,"Thus , we hypothesize that the speech emotion recognition model will be benefit from the incorporation of high - level textual input .",26,0,23
dataset/preprocessed/training-data/sentiment_analysis/0,"In this paper , we propose a novel deep dual recurrent encoder model that simultaneously utilizes audio and text data in recognizing emotions from speech .",27,0,26
dataset/preprocessed/training-data/sentiment_analysis/0,Extensive experiments are conducted to investigate the efficacy and properties of the proposed model .,28,0,15
dataset/preprocessed/training-data/sentiment_analysis/0,"Our proposed model outperforms previous state - of - the - art methods by 68.8 % to 71.8 % when applied to the IEMOCAP dataset , which is one of the most well - studied datasets .",29,0,37
dataset/preprocessed/training-data/sentiment_analysis/0,"Based on an error analysis of the models , we show that our proposed model accurately identifies emotion classes .",30,0,20
dataset/preprocessed/training-data/sentiment_analysis/0,"Moreover , the neutral class misclassification bias frequently exhibited by previous models , which focus on audio features , is less pronounced in our model ..",31,0,26
dataset/preprocessed/training-data/sentiment_analysis/0,"Recently , researchers have proposed various neural network - based architectures to improve the performance of speech emotion recognition .",32,0,20
dataset/preprocessed/training-data/sentiment_analysis/0,An initial study utilized deep neural networks ( DNNs ) to extract high - level features from raw audio data and demonstrated its effectiveness in speech emotion recognition .,33,0,29
dataset/preprocessed/training-data/sentiment_analysis/0,"With the advancement of deep learning methods , more complex neuralbased architectures have been proposed .",34,0,16
dataset/preprocessed/training-data/sentiment_analysis/0,Convolutional neural network ( CNN ) - based models have been trained on information derived from raw audio signals using spectrograms or audio features such as Mel - frequency cepstral coefficients ( MFCCs ) and low - level descriptors ( LLDs ) .,35,0,43
dataset/preprocessed/training-data/sentiment_analysis/0,"These neural network - based models are combined to produce higher - complexity models , and these models achieved the best - recorded performance when applied to the IEMOCAP dataset .",36,0,31
dataset/preprocessed/training-data/sentiment_analysis/0,Another line of research has focused on adopting variant machine learning techniques combined with neural networkbased models .,37,0,18
dataset/preprocessed/training-data/sentiment_analysis/0,One researcher utilized the multiobject learning approach and used gender and naturalness as auxiliary tasks so that the neural network - based model learned more features from a given dataset .,38,0,31
dataset/preprocessed/training-data/sentiment_analysis/0,"Another researcher investigated transfer learning methods , leveraging external data from related domains .",39,0,14
dataset/preprocessed/training-data/sentiment_analysis/0,"As emotional dialogue is composed of sound and spoken content , researchers have also investigated the combination of acoustic features and language information , built belief network - based methods of identifying emotional key phrases , and assessed the emotional salience of verbal cues from both phoneme sequences and words .",40,0,51
dataset/preprocessed/training-data/sentiment_analysis/0,"However , none of these studies have utilized information from speech signals and text sequences simultaneously in an end - to - end learning neural network - based model to classify emotions .",41,0,33
dataset/preprocessed/training-data/sentiment_analysis/0,This section describes the methodologies thatare applied to the speech emotion recognition task .,43,0,14
dataset/preprocessed/training-data/sentiment_analysis/0,We start by introducing the recurrent encoder model for the audio and text modalities individually .,44,0,16
dataset/preprocessed/training-data/sentiment_analysis/0,We then propose a multimodal approach that encodes both audio and textual information simultaneously via a dual recurrent encoder .,45,0,20
dataset/preprocessed/training-data/sentiment_analysis/0,Audio Recurrent Encoder ( ARE ),46,0,6
dataset/preprocessed/training-data/sentiment_analysis/0,"Motivated by the architecture used in , we build an audio recurrent encoder ( ARE ) to predict the class of a given audio signal .",47,0,26
dataset/preprocessed/training-data/sentiment_analysis/0,"Once MFCC features have been extracted from an audio signal , a subset of the sequential features is fed into the RNN ( i.e. , gated recurrent units ( GRUs ) ) , which leads to the formation of the network 's internal hidden state ht to model the time series patterns .",48,0,53
dataset/preprocessed/training-data/sentiment_analysis/0,This internal hidden state is updated at each time step with the input data x t and the hidden state of the previous time step h t?1 as follows :,49,0,30
dataset/preprocessed/training-data/sentiment_analysis/50,Exploiting Document Knowledge for Aspect - level Sentiment Classification,2,1,9
dataset/preprocessed/training-data/sentiment_analysis/50,Attention - based long short - term memory ( LSTM ) networks have proven to be useful in aspect - level sentiment classification .,4,0,24
dataset/preprocessed/training-data/sentiment_analysis/50,"However , due to the difficulties in annotating aspect - level data , existing public datasets for this task are all relatively small , which largely limits the effectiveness of those neural models .",5,0,34
dataset/preprocessed/training-data/sentiment_analysis/50,"In this paper , we explore two approaches that transfer knowledge from documentlevel data , which is much less expensive to obtain , to improve the performance of aspect - level sentiment classification .",6,0,34
dataset/preprocessed/training-data/sentiment_analysis/50,"We demonstrate the effectiveness of our approaches on 4 public datasets from Se -m Eval 2014 , 2015 , and 2016 , and we show that attention - based LSTM benefits from document - level knowledge in multiple ways .",7,0,40
dataset/preprocessed/training-data/sentiment_analysis/50,"Given a sentence and an opinion target ( also called an aspect term ) occurring in the sentence , aspectlevel sentiment classification aims to determine the sentiment polarity in the sentence towards the opinion target .",9,0,36
dataset/preprocessed/training-data/sentiment_analysis/50,An opinion target or target for short refers to a word or a phrase describing an aspect of an entity .,10,0,21
dataset/preprocessed/training-data/sentiment_analysis/50,"For example , in the sentence "" This little place has acute interior decor but the prices are quite expensive "" , the targets are interior decor and prices , and they are associated with positive and negative sentiment respectively .",11,0,41
dataset/preprocessed/training-data/sentiment_analysis/50,"A sentence may contain multiple sentimenttarget pairs , thus one challenge is to separate different opinion contexts for different targets .",12,0,21
dataset/preprocessed/training-data/sentiment_analysis/50,"For this purpose , state - of - the - art neural methods adopt attention - based LSTM networks , where the LSTM aims to capture sequential patterns and the attention mechanism aims to emphasize target - specific contexts for encoding sentence representations .",13,0,44
dataset/preprocessed/training-data/sentiment_analysis/50,"Typically , LSTMs only show their potential when trained on large datasets .",14,0,13
dataset/preprocessed/training-data/sentiment_analysis/50,"However , aspect - level training data requires the annotation of all opinion targets in a sentence , which is costly to obtain in practice .",15,0,26
dataset/preprocessed/training-data/sentiment_analysis/50,"As such , existing public aspect - level datasets are all relatively small .",16,0,14
dataset/preprocessed/training-data/sentiment_analysis/50,Insufficient training data limits the effectiveness of neural models .,17,0,10
dataset/preprocessed/training-data/sentiment_analysis/50,"Despite the lack of aspect - level labeled data , enormous document - level labeled data are easily accessible online such as Amazon reviews .",18,0,25
dataset/preprocessed/training-data/sentiment_analysis/50,These reviews contain substantial linguistic patterns and come with sentiment labels naturally .,19,0,13
dataset/preprocessed/training-data/sentiment_analysis/50,"In this paper , we hypothesize that aspect - level sentiment classification can be improved by employing knowledge gained from document - level sentiment classification .",20,0,26
dataset/preprocessed/training-data/sentiment_analysis/50,"Specifically , we explore two transfer methods to incorporate this sort of knowledge - pretraining and multi-task learning .",21,0,19
dataset/preprocessed/training-data/sentiment_analysis/50,"In our experiments , we find that both methods are helpful and combining them achieves significant improvements over attentionbased LSTM models trained only on aspect - level data .",22,0,29
dataset/preprocessed/training-data/sentiment_analysis/50,We also illustrate by examples that additional knowledge from document - level data is beneficial in multiple ways .,23,0,19
dataset/preprocessed/training-data/sentiment_analysis/50,Our source code can be obtained from https://github.com/ruidan/Aspect-level-sentiment.,24,0,8
dataset/preprocessed/training-data/sentiment_analysis/50,Various neural models have been proposed for aspect - level sentiment classification .,26,0,13
dataset/preprocessed/training-data/sentiment_analysis/50,The main idea behind these works is to develop neural architectures thatare able to learn continuous features and capture the intricate relation between a target and context words .,27,0,29
dataset/preprocessed/training-data/sentiment_analysis/50,"However , to sufficiently train these models , substantial aspect - level annotated data is required , which is expensive to obtain in practice .",28,0,25
dataset/preprocessed/training-data/sentiment_analysis/50,We explore both pretraining and multi-task learning for transferring knowledge from document level to aspect level .,29,0,17
dataset/preprocessed/training-data/sentiment_analysis/50,Both methods are widely studied in the literature .,30,0,9
dataset/preprocessed/training-data/sentiment_analysis/50,Pretraining is a common technique used in computer vision where low - level neural layers can be usefully transferred to different tasks .,31,0,23
dataset/preprocessed/training-data/sentiment_analysis/50,"In natural language processing ( NLP ) , some efforts have been initiated on pretraining LSTMs for sequence - to - sequence models in both supervised and unsupervised settings , where promising results have been obtained .",32,0,37
dataset/preprocessed/training-data/sentiment_analysis/50,"On the other hand , multi-task learning simultaneously trains on samples in multiple tasks with a combined objective , which has improved model generalization ability in certain cases .",33,0,29
dataset/preprocessed/training-data/sentiment_analysis/50,"In the work of , the authors investigated the transferability of neural models in NLP applications with extensive experiments , showing that transferability largely depends on the semantic relatedness of the source and target tasks .",34,0,36
dataset/preprocessed/training-data/sentiment_analysis/50,"For our problem , we hypothesize that aspect - level sentiment classification can be improved by employing knowledge gained from document - level sentiment classification , as these two tasks are highly related semantically .",35,0,35
dataset/preprocessed/training-data/sentiment_analysis/50,Attention - based LSTM,37,0,4
dataset/preprocessed/training-data/sentiment_analysis/50,We first describe a conventional implementation of an attention - based LSTM model for this task .,38,0,17
dataset/preprocessed/training-data/sentiment_analysis/50,We use it as a baseline model and extend it with pretraining and multi-task learning approaches for incorporating document - level knowledge .,39,0,23
dataset/preprocessed/training-data/sentiment_analysis/50,"The inputs are a sentence s = ( w 1 , w 2 , ... , w n ) consisting of n words , and an opinion target x = ( x 1 , x 2 , ... , x m ) occurring in the sentence consisting of a subsequence of m words from s .",40,0,56
dataset/preprocessed/training-data/sentiment_analysis/50,Each word is associated with a continuous word embedding e w from an embedding matrix E ?,41,0,17
dataset/preprocessed/training-data/sentiment_analysis/50,"RV d , where V is the vocabulary size and d is the embedding dimension .",42,0,16
dataset/preprocessed/training-data/sentiment_analysis/50,"LSTM is used to capture sequential information , and outputs a sequence of hidden vectors :",43,0,16
dataset/preprocessed/training-data/sentiment_analysis/50,"[h 1 , ... , h n ] = LSTM ( [e w 1 , ... , e wn ] , ? lstm )",44,0,24
dataset/preprocessed/training-data/sentiment_analysis/50,( 1 ) An attention layer assigns a weight ?,45,0,10
dataset/preprocessed/training-data/sentiment_analysis/50,i to each word in the sentence .,46,0,8
dataset/preprocessed/training-data/sentiment_analysis/50,The final target - specific representation of the sentence sis then given by :,47,0,14
dataset/preprocessed/training-data/sentiment_analysis/50,i is computed as follows :,49,0,6
dataset/preprocessed/training-data/sentiment_analysis/45,Utilizing BERT for Aspect - Based Sentiment Analysis via Constructing Auxiliary Sentence,2,1,12
dataset/preprocessed/training-data/sentiment_analysis/45,"Aspect - based sentiment analysis ( ABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect , is a challenging subtask of sentiment analysis ( SA ) .",4,1,34
dataset/preprocessed/training-data/sentiment_analysis/45,"In this paper , we construct an auxiliary sentence from the aspect and convert ABSA to a sentence - pair classification task , such as question answering ( QA ) and natural language inference ( NLI ) .",5,0,38
dataset/preprocessed/training-data/sentiment_analysis/45,We fine - tune the pre-trained model from BERT and achieve new state - of the - art results on SentiHood and SemEval - 2014 Task 4 datasets .,6,0,29
dataset/preprocessed/training-data/sentiment_analysis/45,Sentiment analysis ( SA ) is an important task in natural language processing .,8,0,14
dataset/preprocessed/training-data/sentiment_analysis/45,"It solves the computational processing of opinions , emotions , and subjectivity - sentiment is collected , analyzed and summarized .",9,0,21
dataset/preprocessed/training-data/sentiment_analysis/45,"It has received much attention not only in academia but also in industry , providing real - time feedback through online reviews on websites such as Amazon , which can take advantage of customers ' opinions on specific products or services .",10,0,42
dataset/preprocessed/training-data/sentiment_analysis/45,The underlying assumption of this task is that the entire text has an over all polarity .,11,0,17
dataset/preprocessed/training-data/sentiment_analysis/45,"However , the users ' comments may contain different aspects , such as : "" This book is a hardcover version , but the price is a bit high . """,12,0,31
dataset/preprocessed/training-data/sentiment_analysis/45,"The polarity in ' appearance ' is positive , and the polarity regarding ' price ' is negative .",13,0,19
dataset/preprocessed/training-data/sentiment_analysis/45,Aspect - based sentiment analysis ( ABSA ) aims to identify fine - grained polarity towards a specific aspect .,14,0,20
dataset/preprocessed/training-data/sentiment_analysis/45,This task allows users to evaluate aggregated sentiments for each aspect of a given product or service and gain a more granular understanding of their quality .,15,0,27
dataset/preprocessed/training-data/sentiment_analysis/45,"Both SA and ABSA are sentence - level or document - level tasks , but one comment may refer to more than one object , and sentence - level tasks can not handle sentences with multiple targets .",16,1,38
dataset/preprocessed/training-data/sentiment_analysis/45,"Therefore , introduce the task of targeted aspect - based sentiment analysis ( TABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect associated with a given target .",17,1,35
dataset/preprocessed/training-data/sentiment_analysis/45,The task can be divided into two steps : ( 1 ) the first step is to determine the aspects associated with each target ; ( 2 ) the second step is to resolve the polarity of aspects to a given target .,18,0,43
dataset/preprocessed/training-data/sentiment_analysis/45,"The earliest work on ( T ) ABSA relied heavily on feature engineering , and subsequent neural network - based methods achieved higher accuracy .",19,0,25
dataset/preprocessed/training-data/sentiment_analysis/45,"Recently , incorporate useful commonsense knowledge into a deep neural network to further enhance the result of the model .",20,0,20
dataset/preprocessed/training-data/sentiment_analysis/45,optimize the memory network and apply it to their model to better capture linguistic structure .,21,0,16
dataset/preprocessed/training-data/sentiment_analysis/45,"More recently , the pre-trained language models , such as ELMo , OpenAI GPT , and BERT , have shown their effectiveness to alleviate the effort of feature engineering .",22,0,30
dataset/preprocessed/training-data/sentiment_analysis/45,"Especially , BERT has achieved excellent results in QA and NLI .",23,0,12
dataset/preprocessed/training-data/sentiment_analysis/45,"However , there is not much improvement in ( T ) ABSA task with the direct use of the pretrained BERT model ( see ) .",24,0,26
dataset/preprocessed/training-data/sentiment_analysis/45,We think this is due to the inappropriate use of the pre-trained BERT model .,25,0,15
dataset/preprocessed/training-data/sentiment_analysis/45,"Since the input representation of BERT can represent both a single text sentence and a pair of text sentences , we can convert ( T ) ABSA into a sentence - pair classification task and fine - tune the pre-trained BERT .",26,0,42
dataset/preprocessed/training-data/sentiment_analysis/45,"In this paper , we investigate several methods of constructing an auxiliary sentence and transform ( T ) ABSA into a sentence - pair classification task .",27,0,27
dataset/preprocessed/training-data/sentiment_analysis/45,We fine - tune the pre-trained model from BERT and achieve new state - of - the - art results on ( T ) ABSA task .,28,0,27
dataset/preprocessed/training-data/sentiment_analysis/45,"We also conduct a comparative experiment to verify that the classification based on a sentence - pair is better than the single - sentence classification with fine - tuned BERT , which means that the improvement is not only from BERT but also from our method .",29,0,47
dataset/preprocessed/training-data/sentiment_analysis/45,"In particular , our contribution is two - fold :",30,0,10
dataset/preprocessed/training-data/sentiment_analysis/45,We propose a new solution of ( T ) ABSA by converting it to a sentence - pair classification task .,32,0,21
dataset/preprocessed/training-data/sentiment_analysis/45,We fine - tune the pre-trained BERT model and achieve new state - of - the - art results on Senti - Hood and SemEval - 2014 Task 4 datasets .,34,0,31
dataset/preprocessed/training-data/sentiment_analysis/45,"In this section , we describe our method in detail .",36,0,11
dataset/preprocessed/training-data/sentiment_analysis/45,"In TABSA , a sentence s usually consists of a series of words : {w 1 , , w m } , and some of the words {w i 1 , , w i k } are pre-identified targets {t 1 , , t k } , following , we set the task as a 3 class classification problem : given the sentence s , a set of target entities T and a fixed aspect set A = { general , price , transitlocation , saf ety } , predict the sentiment polarity y ?",39,0,95
dataset/preprocessed/training-data/sentiment_analysis/45,"{ positive , negative , none } over the full set of the target - aspect pairs { ( t , a ) : t ? T , a ? A }. As we can see in , the gold standard polarity of ( LOCATION2 , price ) is negative , while the polarity of ( LOCATION1 , price ) is none .",40,0,63
dataset/preprocessed/training-data/sentiment_analysis/45,"In ABSA , the target - aspect pairs {t , a} become only aspects a .",42,0,16
dataset/preprocessed/training-data/sentiment_analysis/45,This setting is equivalent to learning subtasks 3 ( Aspect Category Detection ) and subtask 4 ( Aspect Category Polarity ) of SemEval - 2014 Task 4 2 at the same time .,43,0,33
dataset/preprocessed/training-data/sentiment_analysis/45,Construction of the auxiliary sentence,44,0,5
dataset/preprocessed/training-data/sentiment_analysis/45,"For simplicity , we mainly describe our method with TABSA as an example .",45,0,14
dataset/preprocessed/training-data/sentiment_analysis/45,We consider the following four methods to convert the TABSA task into a sentence pair classification task :,46,0,18
dataset/preprocessed/training-data/sentiment_analysis/45,"Example : LOCATION2 is central London so extremely expensive , LOCATION1 is often considered the coolest are a of London .",48,0,21
dataset/preprocessed/training-data/sentiment_analysis/45,transit - location,49,0,3
dataset/preprocessed/training-data/sentiment_analysis/12,Progressive Self - Supervised Attention Learning for Aspect - Level Sentiment Analysis,2,1,12
dataset/preprocessed/training-data/sentiment_analysis/12,"In aspect - level sentiment classification ( ASC ) , it is prevalent to equip dominant neural models with attention mechanisms , for the sake of acquiring the importance of each context word on the given aspect .",4,1,38
dataset/preprocessed/training-data/sentiment_analysis/12,"However , such a mechanism tends to excessively focus on a few frequent words with sentiment polarities , while ignoring infrequent ones .",5,0,23
dataset/preprocessed/training-data/sentiment_analysis/12,"In this paper , we propose a progressive self - supervised attention learning approach for neural ASC models , which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms .",6,1,35
dataset/preprocessed/training-data/sentiment_analysis/12,"Specifically , we iteratively conduct sentiment predictions on all training instances .",7,0,12
dataset/preprocessed/training-data/sentiment_analysis/12,"Particularly , at each iteration , the context word with the maximum attention weight is extracted as the one with active / misleading influence on the correct / incorrect prediction of every instance , and then the word itself is masked for subsequent iterations .",8,0,45
dataset/preprocessed/training-data/sentiment_analysis/12,"Finally , we augment the conventional training objective with a regularization term , which enables ASC models to continue equally focusing on the extracted active context words while decreasing weights of those misleading ones .",9,0,35
dataset/preprocessed/training-data/sentiment_analysis/12,"Experimental results on multiple datasets show that our proposed approach yields better attention mechanisms , leading to substantial improvements over the two stateof - the - art neural ASC models .",10,0,31
dataset/preprocessed/training-data/sentiment_analysis/12,Source code and trained models are available .,11,0,8
dataset/preprocessed/training-data/sentiment_analysis/12,"Aspect - level sentiment classification ( ASC ) , as an indispensable task in sentiment analysis , aims at inferring the sentiment polarity of an input sentence in a certain aspect .",13,1,32
dataset/preprocessed/training-data/sentiment_analysis/12,"In this regard , pre-vious representative models are mostly discriminative classifiers based on manual feature engineering , such as Support Vector Machine .",14,0,23
dataset/preprocessed/training-data/sentiment_analysis/12,"Recently , with the rapid development of deep learning , dominant ASC models have evolved into neural network ( NN ) based models , which are able to automatically learn the aspect - related semantic representation of an input sentence and thus exhibit better performance .",15,0,46
dataset/preprocessed/training-data/sentiment_analysis/12,"Usually , these NN - based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect .",16,0,25
dataset/preprocessed/training-data/sentiment_analysis/12,It can not be denied that attention mechanisms play vital roles in neural ASC models .,17,0,16
dataset/preprocessed/training-data/sentiment_analysis/12,"However , the existing attention mechanism in ASC suffers from a major drawback .",18,1,14
dataset/preprocessed/training-data/sentiment_analysis/12,"Specifically , it is prone to overly focus on a few frequent words with sentiment polarities and little attention is laid upon low - frequency ones .",19,0,27
dataset/preprocessed/training-data/sentiment_analysis/12,"As a result , the performance of attentional neural ASC models is still far from satisfaction .",20,0,17
dataset/preprocessed/training-data/sentiment_analysis/12,"We speculate that this is because there exist widely "" apparent patterns "" and "" inapparent patterns "" .",21,0,19
dataset/preprocessed/training-data/sentiment_analysis/12,"Here , "" apparent patterns "" are interpreted as high - frequency words with strong sentiment polarities and "" inapparent patterns "" are referred to as low - frequency ones in training data .",22,0,34
dataset/preprocessed/training-data/sentiment_analysis/12,"As mentioned in , NNs are easily affected by these two modes : "" apparent patterns "" tend to be overly learned while "" inapparent patterns "" often can not be fully learned .",23,0,34
dataset/preprocessed/training-data/sentiment_analysis/12,Here we use sentences in to explain this defect .,24,0,10
dataset/preprocessed/training-data/sentiment_analysis/12,"In the first three training sentences , given the fact that the context word "" small "" occurs frequently with negative sentiment , the atten - tion mechanism pays more attention to it and directly relates the sentences containing it with negative sentiment .",25,0,44
dataset/preprocessed/training-data/sentiment_analysis/12,"This inevitably causes another informative context word "" crowded "" to be partially neglected in spite of it also possesses negative sentiment .",26,0,23
dataset/preprocessed/training-data/sentiment_analysis/12,"Consequently , a neural ASC model incorrectly predicts the sentiment of the last two test sentences : in the first test sentence , the neural ASC model fails to capture the negative sentiment implicated by "" crowded "" ; while , in the second test sentence , the attention mechanism directly focuses on "" small "" though it is not related to the given aspect .",27,0,66
dataset/preprocessed/training-data/sentiment_analysis/12,"Therefore , we believe that the attention mechanism for ASC still leaves tremendous room for improvement .",28,0,17
dataset/preprocessed/training-data/sentiment_analysis/12,"One potential solution to the above - mentioned issue is supervised attention , which , however , is supposed to be manually annotated , requiring labor - intense work .",29,0,30
dataset/preprocessed/training-data/sentiment_analysis/12,"In this paper , we propose a novel progressive self - supervised attention learning approach for neural ASC models .",30,0,20
dataset/preprocessed/training-data/sentiment_analysis/12,"Our method is able to automatically and incrementally mine attention supervision information from a training corpus , which can be exploited to guide the training of attention mechanisms in ASC models .",31,0,32
dataset/preprocessed/training-data/sentiment_analysis/12,The basic idea behind our approach roots in the following fact : the context word with the maximum attention weight has the greatest impact on the sentiment prediction of an input sentence .,32,0,33
dataset/preprocessed/training-data/sentiment_analysis/12,"Thus , such a context word of a correctly predicted training instance should betaken into consideration during the model training .",33,0,21
dataset/preprocessed/training-data/sentiment_analysis/12,"In contrast , the context word in an incorrectly predicted training instance ought to be ignored .",34,0,17
dataset/preprocessed/training-data/sentiment_analysis/12,"To this end , we iteratively conduct sentiment predictions on all training instances .",35,0,14
dataset/preprocessed/training-data/sentiment_analysis/12,"Particularly , at each iteration , we extract the context word with the maximum attention weight from each training instance to form attention supervision information , which can be used to guide the training of attention mechanism : in the case of correct prediction , we will remain this word to be considered ; otherwise , the attention weight of this word is expected to be decreased .",36,0,68
dataset/preprocessed/training-data/sentiment_analysis/12,"Then , we mask all extracted context words of each training instance so far and then refollow the above process to discover more supervision information for attention mechanisms .",37,0,29
dataset/preprocessed/training-data/sentiment_analysis/12,"Finally , we augment the standard training objective with a regularizer , which enforces attention distributions of these mined context words to be consistent with their expected distributions .",38,0,29
dataset/preprocessed/training-data/sentiment_analysis/12,Our main contributions are three - fold :,39,0,8
dataset/preprocessed/training-data/sentiment_analysis/12,"( 1 ) Through in - depth analysis , we point out the existing drawback of the attention mechanism for ASC .",40,0,22
dataset/preprocessed/training-data/sentiment_analysis/12,( 2 ) We propose a novel incremental approach to automatically extract attention supervision information for neural ASC models .,41,0,20
dataset/preprocessed/training-data/sentiment_analysis/12,"To the best of our knowledge , our work is the first attempt to explore automatic attention supervision information mining for ASC .",42,0,23
dataset/preprocessed/training-data/sentiment_analysis/12,( 3 ) We apply our approach to two dominant neural ASC models : Memory Network ( MN ) and Transformation Network ( TNet ) .,43,0,26
dataset/preprocessed/training-data/sentiment_analysis/12,Experimental results on several benchmark datasets demonstrate the effectiveness of our approach .,44,0,13
dataset/preprocessed/training-data/sentiment_analysis/12,"In this section , we give brief introductions to MN and TNet , which both achieve satisfying performance and thus are chosen as the foundations of our work .",46,0,29
dataset/preprocessed/training-data/sentiment_analysis/12,"Here we introduce some notations to facilitate subsequent descriptions : x= ( x 1 , x 2 , ... , x N ) is the input sentence , t= ( t 1 , t 2 , ... , t T ) is the given target aspect , y , y p ?{ Positive , Negative , Neutral } denote the ground - truth and the predicted sentiment , respectively . MN . The framework illustration of MN is given in .",47,0,81
dataset/preprocessed/training-data/sentiment_analysis/12,"We first introduce an aspect embedding matrix converting each target aspect word t j into a vector representation , and then define the final vector representation v( t ) oft as the averaged aspect embedding of its words .",48,0,39
dataset/preprocessed/training-data/sentiment_analysis/12,"Meanwhile , another embedding matrix is used to project each context word x i to the continuous space stored in memory , denoted by mi .",49,0,26
dataset/preprocessed/training-data/sentiment_analysis/2,A Position - aware Bidirectional Attention Network for Aspect - level Sentiment Analysis,2,1,13
dataset/preprocessed/training-data/sentiment_analysis/2,Aspect - level sentiment analysis aims to distinguish the sentiment polarity of each specific aspect term in a given sentence .,4,0,21
dataset/preprocessed/training-data/sentiment_analysis/2,"Both industry and academia have realized the importance of the relationship between aspect term and sentence , and made attempts to model the relationship by designing a series of attention models .",5,0,32
dataset/preprocessed/training-data/sentiment_analysis/2,"However , most existing methods usually neglect the fact that the position information is also crucial for identifying the sentiment polarity of the aspect term .",6,0,26
dataset/preprocessed/training-data/sentiment_analysis/2,"When an aspect term occurs in a sentence , its neighboring words should be given more attention than other words with long distance .",7,0,24
dataset/preprocessed/training-data/sentiment_analysis/2,"Therefore , we propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional GRU .",8,0,19
dataset/preprocessed/training-data/sentiment_analysis/2,"PBAN not only concentrates on the position information of aspect terms , but also mutually models the relation between aspect term and sentence by employing bidirectional attention mechanism .",9,0,29
dataset/preprocessed/training-data/sentiment_analysis/2,The experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our proposed PBAN model .,10,0,16
dataset/preprocessed/training-data/sentiment_analysis/2,* Corresponding author : Yuexian Hou .,11,0,7
dataset/preprocessed/training-data/sentiment_analysis/2,This work is licenced under a Creative Commons Attribution 4.0 International Licence .,12,0,13
dataset/preprocessed/training-data/sentiment_analysis/2,Licence details : http://creativecommons.org/licenses/by/4.0/,13,0,4
dataset/preprocessed/training-data/sentiment_analysis/2,The aim of the target - dependent sentiment analysis is similar to aspect - level sentiment analysis .,15,0,18
dataset/preprocessed/training-data/sentiment_analysis/2,"Given a sentence and target / aspect term , the task calls for inferring the sentiment polarity of the sentence towards the target / aspect term .",16,0,27
dataset/preprocessed/training-data/sentiment_analysis/2,"Sentiment analysis , also known as opinion mining , is a vital task in Natural Language Processing ( NLP ) .",18,1,21
dataset/preprocessed/training-data/sentiment_analysis/2,"It divides the text into two or more classes according to the affective states and the subjective information of the text , and has received plenty of attention from both industry and academia .",19,0,34
dataset/preprocessed/training-data/sentiment_analysis/2,"In this paper , we address the aspect - level sentiment analysis , which is a fine - grained task in the field of sentiment analysis .",20,0,27
dataset/preprocessed/training-data/sentiment_analysis/2,"For instance , given the mentioned aspect terms { menu , server , specials } , and the sentence is "" The menu looked good , except for offering the Chilean Sea Bass , but the server does not offer up the specials that were written on the board outside . "" .",21,0,53
dataset/preprocessed/training-data/sentiment_analysis/2,"For aspect term menu , the sentiment polarity is positive , but for aspect term server , the polarity is negative while for specials , the polarity is neutral .",22,0,30
dataset/preprocessed/training-data/sentiment_analysis/2,One important challenge in aspect - level sentiment analysis is how to model the semantic relationship between aspect terms and sentences .,23,0,22
dataset/preprocessed/training-data/sentiment_analysis/2,Traditional approaches have defined rich features about content and syntactic structures so as to capture the sentiment polarity .,24,0,19
dataset/preprocessed/training-data/sentiment_analysis/2,However this kind of feature - based method is labor - intensive and highly depends on the quality of the features .,25,0,22
dataset/preprocessed/training-data/sentiment_analysis/2,"Compared with these methods , neural network architectures are capable of learning features without feature engineering , and have been widely used in a variety of NLP tasks such as machine translation , question answering and text classification .",26,0,39
dataset/preprocessed/training-data/sentiment_analysis/2,"Recently , with the development of the neural networks , they are also applied to target - dependent sentiment analysis 1 , such as Target - Dependent LSTM ( TD - LSTM ) and Target - Connection LSTM ( TC - LSTM ) .",27,0,44
dataset/preprocessed/training-data/sentiment_analysis/2,"However , these neural network - based methods can not effectively identify which words in the sentence are more important .",28,0,21
dataset/preprocessed/training-data/sentiment_analysis/2,"Fortunately , attention mechanisms are an effective way to solve this problem .",29,0,13
dataset/preprocessed/training-data/sentiment_analysis/2,"Attention , which is widely applied to Computer Vision ( CV ) and NLP fields , is an effective mechanism and has been demonstrated in image recognition , machine translation and reading comprehension .",30,0,34
dataset/preprocessed/training-data/sentiment_analysis/2,"Therefore , some researchers have designed attention networks to address the aspect - level sentiment analysis and have obtained comparable results , such as AE - LSTM , ATAE - LSTM and IAN .",31,0,34
dataset/preprocessed/training-data/sentiment_analysis/2,"However , these existing work ignores or does not explicitly model the position information of the aspect term in a sentence , which has been studied for improving performance in information retrieval ( IR ) .",32,0,36
dataset/preprocessed/training-data/sentiment_analysis/2,"In , the occurrence positions of the query terms were modeled via kernel functions and then integrated into traditional IR models to boost the retrieval performance .",33,0,27
dataset/preprocessed/training-data/sentiment_analysis/2,"By analyzing this aspect - level sentiment analysis task and the corresponding dataset , we find that when an aspect term occurs in a sentence , its neighboring words in the sentence should be given more attention than other words with long distance .",34,0,44
dataset/preprocessed/training-data/sentiment_analysis/2,"Let us take "" It 's a perfect place to have an amazing indian food . "" as an example , when the aspect term is indian food , its corresponding sentiment polarity is positive .",35,0,36
dataset/preprocessed/training-data/sentiment_analysis/2,"Intuitively , we can see that the neighboring word of the indian food ( i.e. "" amazing "" ) has a greater contribution to judge the sentiment polarity of the aspect term than other words with long distance such as "" to "" and "" have "" .",36,0,48
dataset/preprocessed/training-data/sentiment_analysis/2,"Sometimes this intuitive idea of judging the sentiment polarity maybe interpreted as a cognitive activity , which also can be rephrased in a quantum - like language model .",37,0,29
dataset/preprocessed/training-data/sentiment_analysis/2,"To be specific , sentiment polarity maybe interpreted as a quantum - like cognition state .",38,0,16
dataset/preprocessed/training-data/sentiment_analysis/2,"Inspired by this , we go one step further and propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional Gated Recurrent Units ( Bi - GRU ) .",39,0,33
dataset/preprocessed/training-data/sentiment_analysis/2,"In addition to utilizing the position information , PBAN also mutually models the relationship between the sentence and different words in the aspect term by adopting a bidirectional attention mechanism .",40,0,31
dataset/preprocessed/training-data/sentiment_analysis/2,"To be specific , our model consists of three components :",41,0,11
dataset/preprocessed/training-data/sentiment_analysis/2,"1 ) Obtaining position information of each word in corresponding sentence based on the current aspect term , then converting the position information into position embedding .",42,0,27
dataset/preprocessed/training-data/sentiment_analysis/2,2 ) The PBAN composes of two Bi - GRU networks focusing on extracting the aspectlevel features and sentence - level features respectively .,43,0,24
dataset/preprocessed/training-data/sentiment_analysis/2,3 ) Using the bidirectional attention mechanism to model the mutual relation between aspect term and its corresponding sentence .,44,0,20
dataset/preprocessed/training-data/sentiment_analysis/2,"We evaluate our models on SemEval 2014 Datasets , and the results show that our models are more effective than other previous methods .",45,0,24
dataset/preprocessed/training-data/sentiment_analysis/2,The main contributions of our work can be summarized as follows :,46,0,12
dataset/preprocessed/training-data/sentiment_analysis/2,( 1 ) We attempt to explicitly investigate the effectiveness of the position information of aspect term for aspect - level sentiment analysis .,47,0,24
dataset/preprocessed/training-data/sentiment_analysis/2,"( 2 ) We propose a position - aware bidirectional attention network ( PBAN ) based on Bi - GRU , which has been proved to be effective to improve the sentiment analysis performance .",48,0,35
dataset/preprocessed/training-data/sentiment_analysis/2,"( 3 ) We apply a bidirectional attention mechanism , which can enhance the mutual relation between the aspect term and its corresponding sentence , and prevent the irrelevant words from getting more attention .",49,0,35
dataset/preprocessed/training-data/sentiment_analysis/51,Fine - grained Sentiment Classification using BERT,2,1,7
dataset/preprocessed/training-data/sentiment_analysis/51,"Sentiment classification is an important process in understanding people 's perception towards a product , service , or topic .",4,1,20
dataset/preprocessed/training-data/sentiment_analysis/51,Many natural language processing models have been proposed to solve the sentiment classification problem .,5,0,15
dataset/preprocessed/training-data/sentiment_analysis/51,"However , most of them have focused on binary sentiment classification .",6,0,12
dataset/preprocessed/training-data/sentiment_analysis/51,"In this paper , we use a promising deep learning model called BERT to solve the fine - grained sentiment classification task .",7,0,23
dataset/preprocessed/training-data/sentiment_analysis/51,Experiments show that our model outperforms other popular models for this task without sophisticated architecture .,8,0,16
dataset/preprocessed/training-data/sentiment_analysis/51,We also demonstrate the effectiveness of transfer learning in natural language processing in the process .,9,0,16
dataset/preprocessed/training-data/sentiment_analysis/51,Sentiment classification is a form of text classification in which apiece of text has to be classified into one of the predefined sentiment classes .,11,0,25
dataset/preprocessed/training-data/sentiment_analysis/51,It is a supervised machine learning problem .,12,0,8
dataset/preprocessed/training-data/sentiment_analysis/51,"In binary sentiment classification , the possible classes are positive and negative .",13,0,13
dataset/preprocessed/training-data/sentiment_analysis/51,"In fine - grained sentiment classification , there are five classes ( very negative , negative , neutral , positive , and very positive ) .",14,0,26
dataset/preprocessed/training-data/sentiment_analysis/51,"Sentiment classification model , like any other machine learning model , requires its input to be a fixed - sized vector of numbers .",15,0,24
dataset/preprocessed/training-data/sentiment_analysis/51,"Therefore , we need to convert a text - sequence of words represented as ASCII or Unicode - into a fixedsized vector that encodes the meaningful information of the text .",16,0,31
dataset/preprocessed/training-data/sentiment_analysis/51,Many statistical and deep learning NLP models have been proposed just for that .,17,0,14
dataset/preprocessed/training-data/sentiment_analysis/51,"Recently , there has been an explosion of developments in NLP as well as other deep learning architectures .",18,0,19
dataset/preprocessed/training-data/sentiment_analysis/51,"While transfer learning ( pretraining and finetuning ) has become the de-facto standard in computer vision , NLP is yet to utilize this concept fully .",19,0,26
dataset/preprocessed/training-data/sentiment_analysis/51,"However , neural language models such as word vectors , paragraph vectors , and Glo Ve have started the transfer learning revolution in NLP .",20,0,25
dataset/preprocessed/training-data/sentiment_analysis/51,"Recently , Google researchers published BERT ( Bidirectional Encoder Representations from Transformers ) , a deep bidirectional language model based on the Transformer architecture , and advanced the state - of - the - art in many popular NLP tasks .",21,0,41
dataset/preprocessed/training-data/sentiment_analysis/51,"In this paper , we use the pretrained BERT model and finetune it for the fine - grained sentiment classification task on the Stanford Sentiment Treebank ( SST ) dataset .",22,0,31
dataset/preprocessed/training-data/sentiment_analysis/51,The rest of the paper is organized into six sections .,23,0,11
dataset/preprocessed/training-data/sentiment_analysis/51,"In Section II , we mention our motivation for this work .",24,0,12
dataset/preprocessed/training-data/sentiment_analysis/51,"In Section III , we discuss related works .",25,0,9
dataset/preprocessed/training-data/sentiment_analysis/51,"In Section IV , we describe the dataset we performed our experiments on .",26,0,14
dataset/preprocessed/training-data/sentiment_analysis/51,We explain our model architecture and methodology in detail in Section V .,27,0,13
dataset/preprocessed/training-data/sentiment_analysis/51,Then we present and analyze our results in Section VI .,28,0,11
dataset/preprocessed/training-data/sentiment_analysis/51,"Finally , we provide our concluding remarks in Section VII .",29,0,11
dataset/preprocessed/training-data/sentiment_analysis/51,"We have been working on replicating the different research paper results for sentiment analysis , especially on the finegrained Stanford Sentiment Treebank ( SST ) dataset .",32,0,27
dataset/preprocessed/training-data/sentiment_analysis/51,"After the popularity of BERT , researchers have tried to use it on different NLP tasks , including binary sentiment classification on SST - 2 ( binary ) dataset , and they were able to obtain state - of - the - art results as well .",33,0,47
dataset/preprocessed/training-data/sentiment_analysis/51,But we have n't yet found any experimentation done using BERT on the SST - 5 ( fine - grained ) dataset .,34,0,23
dataset/preprocessed/training-data/sentiment_analysis/51,"Because BERT is so powerful , fast , and easy to use for downstream tasks , it is likely to give promising results in SST - 5 dataset as well .",35,0,31
dataset/preprocessed/training-data/sentiment_analysis/51,This became the main motivation for pursuing this work .,36,0,10
dataset/preprocessed/training-data/sentiment_analysis/51,"Sentiment classification is one of the most popular tasks in NLP , and so there has been a lot of research and progress in solving this task accurately .",39,0,29
dataset/preprocessed/training-data/sentiment_analysis/51,"Most of the approaches have focused on binary sentiment classification , most probably because there are large public datasets for it such as the IMDb movie review dataset .",40,0,29
dataset/preprocessed/training-data/sentiment_analysis/51,"In this section , we only discuss some significant deep learning NLP approaches applied to sentiment classification .",41,0,18
dataset/preprocessed/training-data/sentiment_analysis/51,"The first step in sentiment classification of a text is the embedding , where a text is converted into a fixed - size vector .",42,0,25
dataset/preprocessed/training-data/sentiment_analysis/51,"Since the number of words in the vocabulary after tokenization and stemming is limited , researchers first tackled the problem of learning word embeddings .",43,0,25
dataset/preprocessed/training-data/sentiment_analysis/51,The first promising language model was proposed by Mikolov et al ..,44,0,12
dataset/preprocessed/training-data/sentiment_analysis/51,They trained continuous semantic representation of words from large unlabeled text that could be fine - tuned for downstream tasks .,45,0,21
dataset/preprocessed/training-data/sentiment_analysis/51,Pennington et al. used a co-occurrence matrix and only trained on nonzero elements to efficiently learn semantic word embeddings .,46,0,20
dataset/preprocessed/training-data/sentiment_analysis/51,Bojanowski et al. broke words into character n-grams for smaller vocabulary size and fast training .,47,0,16
dataset/preprocessed/training-data/sentiment_analysis/51,The next step is to combine a variable number of word vectors into a single fixed - size document vector .,48,0,21
dataset/preprocessed/training-data/sentiment_analysis/51,"The trivial way is to take the sum or the average , but they do n't lose the ordering information of words and thus do n't give good results .",49,0,30
dataset/preprocessed/training-data/sentiment_analysis/4,ICON : Interactive Conversational Memory Network for Multimodal Emotion Detection,2,1,10
dataset/preprocessed/training-data/sentiment_analysis/4,Emotion recognition in conversations is crucial for building empathetic machines .,4,1,11
dataset/preprocessed/training-data/sentiment_analysis/4,Current work in this domain do not explicitly consider the inter-personal influences that thrive in the emotional dynamics of dialogues .,5,0,21
dataset/preprocessed/training-data/sentiment_analysis/4,"To this end , we propose Interactive COnversational memory Network ( ICON ) , a multimodal emotion detection framework that extracts multimodal features from conversational videos and hierarchically models the selfand interspeaker emotional influences into global memories .",6,0,38
dataset/preprocessed/training-data/sentiment_analysis/4,Such memories generate contextual summaries which aid in predicting the emotional orientation of utterance - videos .,7,0,17
dataset/preprocessed/training-data/sentiment_analysis/4,Our model outperforms state - of - the - art networks on multiple classification and regression tasks in two benchmark datasets .,8,0,22
dataset/preprocessed/training-data/sentiment_analysis/4,Emotions play an important role in our daily life .,11,0,10
dataset/preprocessed/training-data/sentiment_analysis/4,A long - standing goal of AI has been to create affective agents that can detect and comprehend emotions .,12,0,20
dataset/preprocessed/training-data/sentiment_analysis/4,Research in affective computing has mainly focused on understanding affect ( emotions and sentiment ) in monologues .,13,0,18
dataset/preprocessed/training-data/sentiment_analysis/4,"However , with increasing interactions of humans with machines , researchers now aim at building agents that can seamlessly analyze affective content in conversations .",14,0,25
dataset/preprocessed/training-data/sentiment_analysis/4,"This can help in creating empathetic dialogue systems , thus improving the over all human - computer interaction experience .",15,0,20
dataset/preprocessed/training-data/sentiment_analysis/4,"Analyzing emotional dynamics in conversations , however , poses complex challenges .",16,1,12
dataset/preprocessed/training-data/sentiment_analysis/4,This is due to the presence of intricate dependencies between the affective states of speakers participating in the dialogue .,17,0,20
dataset/preprocessed/training-data/sentiment_analysis/4,"In this paper , we address the problem of emotion recognition in conversational videos .",18,0,15
dataset/preprocessed/training-data/sentiment_analysis/4,We specifically focus on dyadic conversations where two entities participate in a dialogue .,19,0,14
dataset/preprocessed/training-data/sentiment_analysis/4,"We propose Interactive COnversational memory Network ( ICON ) , a multimodal network for identifying emotions in utterance - videos .",20,0,21
dataset/preprocessed/training-data/sentiment_analysis/4,"Here , utterances are units of speech bounded by breaths or pauses of the speaker .",21,0,16
dataset/preprocessed/training-data/sentiment_analysis/4,Emotional dynamics in conversations consist of two important properties : self and inter-personal dependencies ( Morris and .,22,0,18
dataset/preprocessed/training-data/sentiment_analysis/4,"Self - dependencies , also known as emotional inertia , deal with the aspect of emotional influence that speakers have on themselves during conversations .",23,0,25
dataset/preprocessed/training-data/sentiment_analysis/4,"On the other hand , inter-personal dependencies relate to the emotional influences that the counterparts induce into a speaker .",24,0,20
dataset/preprocessed/training-data/sentiment_analysis/4,"Conversely , during the course of a dialogue , speakers also tend to mirror their counterparts to build rapport .",25,0,20
dataset/preprocessed/training-data/sentiment_analysis/4,demonstrates a sample conversation from the dataset involving both self and interpersonal dependencies .,26,0,14
dataset/preprocessed/training-data/sentiment_analysis/4,"While most conversational frameworks only focus on self dependencies , ICON leverages both such dependencies to generate affective summaries of conversations .",27,0,22
dataset/preprocessed/training-data/sentiment_analysis/4,"First , it extracts multimodal features from all utterancevideos .",28,0,10
dataset/preprocessed/training-data/sentiment_analysis/4,"Next , given a test utterance to be classified , ICON considers the preceding utterances of both speakers falling within a context - window and models their self - emotional influences using local gated recurrent units .",29,0,37
dataset/preprocessed/training-data/sentiment_analysis/4,"Furthermore , to incorporate inter -speaker influences , a global representation is generated using a GRU that intakes output of the local GRUs .",30,0,24
dataset/preprocessed/training-data/sentiment_analysis/4,"For each instance in the context - window , the output of this global GRU is stored as a memory cell .",31,0,22
dataset/preprocessed/training-data/sentiment_analysis/4,These memories are then subjected to multiple read / write cycles that include attention mechanism for generating contextual summaries of the conversational history .,32,0,24
dataset/preprocessed/training-data/sentiment_analysis/4,"At each iteration , the representation of the test utterance is improved with this summary representation and finally used for prediction .",33,0,22
dataset/preprocessed/training-data/sentiment_analysis/4,I do n't think I can do this anymore .,34,0,10
dataset/preprocessed/training-data/sentiment_analysis/4,[ frustrated ],35,0,3
dataset/preprocessed/training-data/sentiment_analysis/4,Well I guess you are n't trying hard enough .,36,0,10
dataset/preprocessed/training-data/sentiment_analysis/4,It s been three years .,37,0,6
dataset/preprocessed/training-data/sentiment_analysis/4,I have tried everything .,38,0,5
dataset/preprocessed/training-data/sentiment_analysis/4,[ frustrated ],39,0,3
dataset/preprocessed/training-data/sentiment_analysis/4,Maybe you 're not smart enough .,40,0,7
dataset/preprocessed/training-data/sentiment_analysis/4,[ neutral ],41,0,3
dataset/preprocessed/training-data/sentiment_analysis/4,Just go out and keep trying .,42,0,7
dataset/preprocessed/training-data/sentiment_analysis/4,[ neutral ],43,0,3
dataset/preprocessed/training-data/sentiment_analysis/4,I am smart enough .,44,0,5
dataset/preprocessed/training-data/sentiment_analysis/4,I am really good at what I do .,45,0,9
dataset/preprocessed/training-data/sentiment_analysis/4,I just do n't know how to make someone else see that .,46,0,13
dataset/preprocessed/training-data/sentiment_analysis/4,[ anger ],47,0,3
dataset/preprocessed/training-data/sentiment_analysis/4,"Pa is frustrated over her long term unemployment and seeks encouragement ( u1 , u 3 ) .",48,0,18
dataset/preprocessed/training-data/sentiment_analysis/4,"Pb , however , is pre-occupied and replies sarcastically ( u4 ) .",49,0,13
dataset/preprocessed/training-data/sentiment_analysis/13,BERT Post - Training for Review Reading Comprehension and Aspect - based Sentiment Analysis,2,1,14
dataset/preprocessed/training-data/sentiment_analysis/13,Question - answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making .,4,0,30
dataset/preprocessed/training-data/sentiment_analysis/13,"Inspired by the recent success of machine reading comprehension ( MRC ) on formal documents , this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions .",5,0,40
dataset/preprocessed/training-data/sentiment_analysis/13,We call this problem Review Reading Comprehension ( RRC ) .,6,0,11
dataset/preprocessed/training-data/sentiment_analysis/13,"To the best of our knowledge , no existing work has been done on RRC .",7,0,16
dataset/preprocessed/training-data/sentiment_analysis/13,"In this work , we first build an RRC dataset called ReviewRC based on a popular benchmark for aspectbased sentiment analysis .",8,0,22
dataset/preprocessed/training-data/sentiment_analysis/13,"Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC .",9,0,47
dataset/preprocessed/training-data/sentiment_analysis/13,"To show the generality of the approach , the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .",10,1,38
dataset/preprocessed/training-data/sentiment_analysis/13,Experimental results demonstrate that the proposed posttraining is highly effective 1 .,11,0,12
dataset/preprocessed/training-data/sentiment_analysis/13,"For online commerce , question - answering ( QA ) serves either as a standalone application of customer service or as a crucial component of a dialogue system that answers user questions .",13,0,33
dataset/preprocessed/training-data/sentiment_analysis/13,Many intelligent personal assistants ( such as Amazon Alexa and Google Assistant ) support online shopping by allowing the user to speak directly to the assistants .,14,0,27
dataset/preprocessed/training-data/sentiment_analysis/13,"One major hindrance for this mode of shopping is that such systems have limited capability to answer user questions about products ( or services ) , which are vital for customer decision making .",15,0,34
dataset/preprocessed/training-data/sentiment_analysis/13,"As such , an intelligent agent that can automatically answer customers ' questions is very important for the success of online businesses .",16,0,23
dataset/preprocessed/training-data/sentiment_analysis/13,"Given the ever - changing environment of products and services , it is very hard , if not impossible , to pre-compile an up - to - date and reliable knowledge base to cover a wide assortment of questions that customers may ask , such as in factoidbased KB - QA .",17,0,52
dataset/preprocessed/training-data/sentiment_analysis/13,"As a compromise , many online businesses leverage community question - answering ( CQA ) to crowdsource answers from existing customers .",18,0,22
dataset/preprocessed/training-data/sentiment_analysis/13,"However , the problem with this approach is that many questions are not answered , and if they are answered , the answers are delayed , which is not suitable for interactive QA .",19,0,34
dataset/preprocessed/training-data/sentiment_analysis/13,"In this paper , we explore the potential of using product reviews as a large source of user experiences that can be exploited to obtain answers to user questions .",20,0,30
dataset/preprocessed/training-data/sentiment_analysis/13,"Although there are existing studies that have used information retrieval ( IR ) techniques to find a whole review as the response to a user question , giving the whole review to the user is undesirable as it is quite time - consuming for the user to read it .",21,0,50
dataset/preprocessed/training-data/sentiment_analysis/13,"Inspired by the success of Machine Reading Comphrenesions ( MRC ) , we propose a novel task called Review Reading Comprehension ( RRC ) as following .",22,0,27
dataset/preprocessed/training-data/sentiment_analysis/13,Problem Definition :,23,0,3
dataset/preprocessed/training-data/sentiment_analysis/13,"Given a question q = ( q 1 , . . . , q m ) from a customer ( or user ) about a product and a review d = ( d 1 , . . . , d n ) for that product containing the information to answer q , find a sequence of tokens ( a text span ) a = ( d s , . . . , d e ) ind that answers q correctly , where 1 ? s ? n , 1 ? e ? n , and s ?",24,0,97
dataset/preprocessed/training-data/sentiment_analysis/13,A sample laptop review is shown in .,26,0,8
dataset/preprocessed/training-data/sentiment_analysis/13,We can see that customers may not only ask factoid Questions Q1 :,27,0,13
dataset/preprocessed/training-data/sentiment_analysis/13,Does it have an internal hard drive ?,28,0,8
dataset/preprocessed/training-data/sentiment_analysis/13,Q2 : How large is the internal hard drive ?,29,0,10
dataset/preprocessed/training-data/sentiment_analysis/13,Q3 : is the capacity of the internal hard drive OK ?,30,0,12
dataset/preprocessed/training-data/sentiment_analysis/13,Review Excellent value and a must buy for someone looking for a Macbook .,31,0,14
dataset/preprocessed/training-data/sentiment_analysis/13,You ca n't get any better than this price and it come with A1 an internal disk drive .,32,0,19
dataset/preprocessed/training-data/sentiment_analysis/13,All the newer MacBooks do not .,33,0,7
dataset/preprocessed/training-data/sentiment_analysis/13,Plus you get 500 GB A2 which is also a great A3 feature .,34,0,14
dataset/preprocessed/training-data/sentiment_analysis/13,"Also , the resale value on this will keep .",35,0,10
dataset/preprocessed/training-data/sentiment_analysis/13,"I highly recommend you get one before they are gone . questions such as the specs about some aspects of the laptop as in the first and second questions but also subjective or opinion questions about some aspects ( capacity of the hard drive ) , as in the third question .",36,0,52
dataset/preprocessed/training-data/sentiment_analysis/13,"RRC poses some domain challenges compared to the traditional MRC on Wikipedia , such as the need for rich product knowledge , informal text , and fine - grained opinions ( there is almost no subjective content in Wikipedia articles ) .",37,0,42
dataset/preprocessed/training-data/sentiment_analysis/13,Research also shows that yes / no questions are very frequent for products with complicated specifications .,38,0,17
dataset/preprocessed/training-data/sentiment_analysis/13,"To the best of our knowledge , no existing work has been done in RRC .",39,0,16
dataset/preprocessed/training-data/sentiment_analysis/13,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",40,0,41
dataset/preprocessed/training-data/sentiment_analysis/13,We detail ReviewRC in Sec.,41,0,5
dataset/preprocessed/training-data/sentiment_analysis/13,"5 . Given the wide spectrum of domains ( types of products or services ) in online businesses and the prohibitive cost of annotation , ReviewRC can only be considered to have a limited number of annotated examples for supervised training , which still leaves the domain challenges partially unresolved .",42,0,51
dataset/preprocessed/training-data/sentiment_analysis/13,This work adopts BERT ) as the base model as it achieves the state - of the - art performance on MRC .,43,0,23
dataset/preprocessed/training-data/sentiment_analysis/13,"Although BERT aims to learn contextualized representations across a wide range of NLP tasks ( to be task - agnostic ) , leveraging BERT alone still leaves the domain challenges un - 2 http://alt.qcri.org/semeval2016/",44,0,34
dataset/preprocessed/training-data/sentiment_analysis/13,"We choose these review datasets to align RRC with existing research on sentiment analysis. resolved ( as BERT is trained on Wikipedia articles and has almost no understanding of opinion text ) , and it also introduces another challenge of task - awareness ( the RRC task ) , called the task challenge .",46,0,54
dataset/preprocessed/training-data/sentiment_analysis/13,"This challenge arises when the taskagnostic BERT meets the limited number of finetuning examples in ReviewRC ( see Sec. 5 ) for RRC , which is insufficient to fine - tune BERT to ensure full task - awareness of the system 3 .",47,0,43
dataset/preprocessed/training-data/sentiment_analysis/13,"To address all the above challenges , we propose a novel joint post - training technique that takes BERT 's pre-trained weights as the initialization 4 for basic language understanding and adapt BERT with both domain knowledge and task ( MRC ) knowledge before fine - tuning using the domain end task annotated data for the domain RRC .",48,0,59
dataset/preprocessed/training-data/sentiment_analysis/13,"This technique leverages knowledge from two sources : unsupervised domain reviews and supervised ( yet out - of - domain ) MRC data 5 , where the former enhances domain - awareness and the latter strengthens MRC task - awareness .",49,0,41
dataset/preprocessed/training-data/sentiment_analysis/20,DataStories at SemEval-2017 Task 4 : Deep LSTM with Attention for Message - level and Topic - based Sentiment Analysis,2,1,20
dataset/preprocessed/training-data/sentiment_analysis/20,"In this paper we present two deep - learning systems that competed at SemEval - 2017 Task 4 "" Sentiment Analysis in Twitter "" .",4,1,25
dataset/preprocessed/training-data/sentiment_analysis/20,"We participated in all subtasks for English tweets , involving message - level and topic - based sentiment polarity classification and quantification .",5,0,23
dataset/preprocessed/training-data/sentiment_analysis/20,"We use Long Short - Term Memory ( LSTM ) networks augmented with two kinds of attention mechanisms , on top of word embeddings pre-trained on a big collection of Twitter messages .",6,0,33
dataset/preprocessed/training-data/sentiment_analysis/20,"Also , we present a text processing tool suitable for social network messages , which performs tokenization , word normalization , segmentation and spell correction .",7,0,26
dataset/preprocessed/training-data/sentiment_analysis/20,"Moreover , our approach uses no hand - crafted features or sentiment lexicons .",8,0,14
dataset/preprocessed/training-data/sentiment_analysis/20,"We ranked 1 st ( tie ) in Subtask A , and achieved very competitive results in the rest of the Subtasks .",9,0,23
dataset/preprocessed/training-data/sentiment_analysis/20,Both the word embeddings and our text processing tool 1 are available to the research community .,10,0,17
dataset/preprocessed/training-data/sentiment_analysis/20,"Sentiment analysis is an area in Natural Language Processing ( NLP ) , studying the identification and quantification of the sentiment expressed in text .",12,1,25
dataset/preprocessed/training-data/sentiment_analysis/20,"Sentiment analysis in Twitter is a particularly challenging task , because of the informal and "" creative "" writing style , with improper use of grammar , figurative language , misspellings and slang .",13,0,34
dataset/preprocessed/training-data/sentiment_analysis/20,"In previous runs of the Task , sentiment analysis was usually tackled using hand - crafted features and / or sentiment lexicons , feeding them to classifiers such as Naive Bayes or Support Vector Machines ( SVM ) .",14,0,39
dataset/preprocessed/training-data/sentiment_analysis/20,"These approaches require a laborious 1 github.com/cbaziotis/ekphrasis feature- engineering process , which may also need domain - specific knowledge , usually resulting both in redundant and missing features .",15,0,29
dataset/preprocessed/training-data/sentiment_analysis/20,"Whereas , artificial neural networks which perform feature learning , last year achieved very good results , outperforming the competition .",16,0,21
dataset/preprocessed/training-data/sentiment_analysis/20,"In this paper , we present two deep - learning systems that competed at SemEval - 2017 Task 4 .",17,0,20
dataset/preprocessed/training-data/sentiment_analysis/20,Our first model is designed for addressing the problem of messagelevel sentiment analysis .,18,0,14
dataset/preprocessed/training-data/sentiment_analysis/20,"We employ a 2 - layer Bidirectional LSTM , equipped with an attention mechanism .",19,0,15
dataset/preprocessed/training-data/sentiment_analysis/20,"For the topic - based sentiment analysis tasks , we propose a Siamese Bidirectional LSTM with a contextaware attention mechanism .",20,0,21
dataset/preprocessed/training-data/sentiment_analysis/20,"In contrast to top - performing systems of previous years , we do not rely on hand - crafted features , sentiment lexicons and we do not use model ensembles .",21,0,31
dataset/preprocessed/training-data/sentiment_analysis/20,We make the following contributions :,22,0,6
dataset/preprocessed/training-data/sentiment_analysis/20,"A text processing tool for text tokenization , word normalization , word segmentation and spell correction , geared towards Twitter .",23,0,21
dataset/preprocessed/training-data/sentiment_analysis/20,"A deep learning system for short - text sentiment analysis using an attention mechanism , in order to enforce the contribution of words that determine the sentiment of a message .",24,0,31
dataset/preprocessed/training-data/sentiment_analysis/20,"A deep learning system for topic - based sentiment analysis , with a context - aware attention mechanism utilizing the topic information .",25,0,23
dataset/preprocessed/training-data/sentiment_analysis/20,"Figure 1 provides a high - level overview of our approach , which consists of two main steps and an optional task - dependent third step : ( 1 ) the text processing , where we use our own text processing tool for preparing the data for our neural network , : High - level overview of our approach networks and ( 3 ) the quantification step for estimating the sentiment distribution for each topic .",27,0,76
dataset/preprocessed/training-data/sentiment_analysis/20,Task definitions .,28,0,3
dataset/preprocessed/training-data/sentiment_analysis/20,"In Subtask A , given a message we must classify whether the message expresses positive , negative , or neutral sentiment ( 3 - point scale ) .",29,0,28
dataset/preprocessed/training-data/sentiment_analysis/20,"In Subtasks B , C ( topic - based sentiment polarity classification ) we are given a message and a topic and must classify the message on 2 - poi nt scale ( Subtask B ) and a 5 - point scale ( Subtask C ) .",30,0,47
dataset/preprocessed/training-data/sentiment_analysis/20,"In Subtasks D , E ( quantification ) we are given a set of messages about a set of topics and must estimate the distribution of the tweets across 2 - point scale ( Subtask D ) and a 5 - point scale ( Subtask E ) .",31,0,48
dataset/preprocessed/training-data/sentiment_analysis/20,Unlabeled Dataset .,32,0,3
dataset/preprocessed/training-data/sentiment_analysis/20,"We collected a big dataset of 330M English Twitter messages , gathered from 12/2012 to 07/2016 , which is used ( 1 ) for calculating words statistics needed by our text processor and ( 2 ) for training our word embeddings .",33,0,42
dataset/preprocessed/training-data/sentiment_analysis/20,Pre-trained Word Embeddings .,34,0,4
dataset/preprocessed/training-data/sentiment_analysis/20,"Word embeddings are dense vector representations of words , capturing their semantic and syntactic information .",35,0,16
dataset/preprocessed/training-data/sentiment_analysis/20,"We leverage our big collection of Twitter messages to generate word embeddings , with vocabulary size of 660 K words , using Glo Ve .",36,0,25
dataset/preprocessed/training-data/sentiment_analysis/20,The pre-trained word embeddings are used for initializing the first layer ( embedding layer ) of our neural networks .,37,0,20
dataset/preprocessed/training-data/sentiment_analysis/20,"We developed our own text processing tool , in order to utilize most of the information in text , performing sentiment - aware tokenization , spell correction , word normalization , word segmentation ( for splitting hashtags ) and word annotation .",39,0,42
dataset/preprocessed/training-data/sentiment_analysis/20,The difficulty in tokenization is to avoid splitting expressions or words that should be kept intact ( as one token ) .,41,0,22
dataset/preprocessed/training-data/sentiment_analysis/20,Although there are some tokenizers geared towards,42,0,7
dataset/preprocessed/training-data/sentiment_analysis/20,"Twitter that recognize the Twitter markup and some basic sentiment expressions or simple emoticons , our tokenizer is able to identify most emoticons , emojis , expressions such as dates ( e.g. 07/11/2011 , April 23rd ) , times ( e.g. 4:30 pm , 11:00 a m ) , currencies ( e.g. $ 10 , 25 mil , 50 e ) , acronyms , censored words ( e.g. s**t ) , words with emphasis ( e.g. * very * ) and more .",43,0,83
dataset/preprocessed/training-data/sentiment_analysis/20,Text Postprocessing .,44,0,3
dataset/preprocessed/training-data/sentiment_analysis/20,"After the tokenization we add an extra postprocessing step , performing modifications on the extracted tokens .",45,0,17
dataset/preprocessed/training-data/sentiment_analysis/20,"This is where we perform spell correction , word normalization and segmentation and decide which tokens to omit , normalize or annotate ( surround or replace with special tags ) .",46,0,31
dataset/preprocessed/training-data/sentiment_analysis/20,"For the tasks of spell correction and word segmentation ) we used the Viterbi algorithm , utilizing word statistics ( unigrams and bigrams ) from our unlabeled dataset , to obtain word probabilities .",47,0,34
dataset/preprocessed/training-data/sentiment_analysis/20,"Moreover , we lowercase all words , and normalize URLs , emails and user handles ( @user ) .",48,0,19
dataset/preprocessed/training-data/sentiment_analysis/20,"After performing the aforementioned steps we decrease the vocabulary size , while keeping information that is usually lost during the tokenization phase .",49,0,23
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"FastSpeech : Fast , Robust and Controllable Text to Speech",2,1,10
dataset/preprocessed/training-data/text-to-speech_synthesis/1,Neural network based end - to - end text to speech ( TTS ) has significantly improved the quality of synthesized speech .,4,1,23
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Prominent methods ( e.g. , Tacotron 2 ) usually first generate mel-spectrogram from text , and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet .",5,0,28
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Compared with traditional concatenative and statistical parametric approaches , neural network based endto - end models suffer from slow inference speed , and the synthesized speech is usually not robust ( i.e. , some words are skipped or repeated ) and lack of controllability ( voice speed or prosody control ) .",6,0,52
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"In this work , we propose a novel feed - forward network based on Transformer to generate mel-spectrogram in parallel for TTS .",7,0,23
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Specifically , we extract attention alignments from an encoder - decoder based teacher model for phoneme duration prediction , which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation .",8,0,46
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality , nearly eliminates the problem of word skipping and repeating in particularly hard cases , and can adjust voice speed smoothly .",9,0,40
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Most importantly , compared with autoregressive Transformer TTS , our model speeds up mel-spectrogram generation by 270x and the end - to - end speech synthesis by 38 x .",10,0,30
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Therefore , we call our model FastSpeech .",11,0,8
dataset/preprocessed/training-data/text-to-speech_synthesis/1,3 * Equal contribution .,12,0,5
dataset/preprocessed/training-data/text-to-speech_synthesis/1,Corresponding author 3 Synthesized speech samples can be found in https://speechresearch.github.io/fastspeech/.,13,0,11
dataset/preprocessed/training-data/text-to-speech_synthesis/1,Text to speech ( TTS ) has attracted a lot of attention in recent years due to the advance in deep learning .,15,1,23
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Deep neural network based systems have become more and more popular for TTS , such as Tacotron , Tacotron 2 , Deep Voice 3 , and the fully end - to - end ClariNet .",16,0,35
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Those models usually first generate mel-spectrogram autoregressively from text input and then synthesize speech from the mel-spectrogram using vocoder such as Griffin - Lim , WaveNet , Parallel WaveNet , or WaveGlow .",17,0,33
dataset/preprocessed/training-data/text-to-speech_synthesis/1,Neural network based TTS has outperformed conventional concatenative and statistical parametric approaches in terms of speech quality .,18,1,18
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"In current neural network based TTS systems , mel- spectrogram is generated autoregressively .",19,0,14
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Due to the long sequence of the mel-spectrogram and the autoregressive nature , those systems face several challenges :",20,0,19
dataset/preprocessed/training-data/text-to-speech_synthesis/1,Slow inference speed for mel-spectrogram generation .,21,0,7
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Although CNN and Transformer based TTS can speedup the training over RNN - based models , all models generate a mel-spectrogram conditioned on the previously generated mel-spectrograms and suffer from slow inference speed , given the mel-spectrogram sequence is usually with a length of hundreds or thousands .",22,0,48
dataset/preprocessed/training-data/text-to-speech_synthesis/1,Synthesized speech is usually not robust .,23,0,7
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Due to error propagation and the wrong attention alignments between text and speech in the autoregressive generation , the generated mel-spectrogram is usually deficient with the problem of words skipping and repeating .",24,0,33
dataset/preprocessed/training-data/text-to-speech_synthesis/1,Synthesized speech is lack of controllability .,25,0,7
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Previous autoregressive models generate mel-spectrograms one by one automatically , without explicitly leveraging the alignments between text and speech .",26,0,20
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"As a consequence , it is usually hard to directly control the voice speed and prosody in the autoregressive generation .",27,0,21
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Considering the monotonous alignment between text and speech , to speedup mel- spectrogram generation , in this work , we propose a novel model , FastSpeech , which takes a text ( phoneme ) sequence as input and generates mel-spectrograms non-autoregressively .",28,0,42
dataset/preprocessed/training-data/text-to-speech_synthesis/1,It adopts a feed - forward network based on the self - attention in Transformer and 1D convolution .,29,0,19
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a length regulator that up - samples the phoneme sequence according to the phoneme duration ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) to match the length of the mel-spectrogram sequence .",30,0,67
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"The regulator is built on a phoneme duration predictor , which predicts the duration of each phoneme .",31,0,18
dataset/preprocessed/training-data/text-to-speech_synthesis/1,Our proposed FastSpeech can address the above - mentioned three challenges as follows :,32,0,14
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Through parallel mel-spectrogram generation , FastSpeech greatly speeds up the synthesis process .",33,0,13
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Phoneme duration predictor ensures hard alignments between a phoneme and its melspectrograms , which is very different from soft and automatic attention alignments in the autoregressive models .",34,0,28
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Thus , FastSpeech avoids the issues of error propagation and wrong attention alignments , consequently reducing the ratio of the skipped words and repeated words .",35,0,26
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"The length regulator can easily adjust voice speed by lengthening or shortening the phoneme duration to determine the length of the generated mel-spectrograms , and can also control part of the prosody by adding breaks between adjacent phonemes .",36,0,39
dataset/preprocessed/training-data/text-to-speech_synthesis/1,We conduct experiments on the LJSpeech dataset to test FastSpeech .,37,0,11
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"The results show that in terms of speech quality , FastSpeech nearly matches the autoregressive Transformer model .",38,0,18
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Furthermore , FastSpeech achieves 270x speedup on mel-spectrogram generation and 38x speedup on final speech synthesis compared with the autoregressive Transformer TTS model , almost eliminates the problem of word skipping and repeating , and can adjust voice speed smoothly .",39,0,41
dataset/preprocessed/training-data/text-to-speech_synthesis/1,We attach some audio files generated by our method in the supplementary materials .,40,0,14
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"In this section , we briefly overview the background of this work , including text to speech , sequence to sequence learning , and non-autoregressive sequence generation .",42,0,28
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Text to Speech TTS , which aims to synthesize natural and intelligible speech given text , has long been a hot research topic in the field of artificial intelligence .",43,0,30
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"The research on TTS has shifted from early concatenative synthesis , statistical parametric synthesis to neural network based parametric synthesis and end - to - end models , and the quality of the synthesized speech by end - to - end models is close to human parity .",44,0,48
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"Neural network based end - to - end TTS models usually first convert the text to acoustic features ( e.g. , mel-spectrograms ) and then transform mel-spectrograms into audio samples .",45,0,31
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"However , most neural TTS systems generate mel-spectrograms autoregressively , which suffers from slow inference speed , and synthesized speech usually lacks of robustness ( word skipping and repeating ) and controllability ( voice speed or prosody control ) .",46,0,40
dataset/preprocessed/training-data/text-to-speech_synthesis/1,"In this work , we propose FastSpeech to generate mel-spectrograms non-autoregressively , which sufficiently handles the above problems .",47,0,19
dataset/preprocessed/training-data/text-to-speech_synthesis/1,Sequence to Sequence Learning Sequence to sequence learning is usually built on the encoder - decoder framework :,48,0,18
dataset/preprocessed/training-data/text-to-speech_synthesis/1,The encoder takes the source sequence as input and generates a set of representations .,49,0,15
dataset/preprocessed/training-data/text-to-speech_synthesis/0,Token - Level Ensemble Distillation for Grapheme - to - Phoneme Conversion,2,1,12
dataset/preprocessed/training-data/text-to-speech_synthesis/0,Grapheme - to - phoneme ( G2P ) conversion is an important task in automatic speech recognition and text - to - speech systems .,4,1,25
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"Recently , G2P conversion is viewed as a sequence to sequence task and modeled by RNN or CNN based encoderdecoder framework .",5,0,22
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"However , previous works do not consider the practical issues when deploying G2P model in the production system , such as how to leverage additional unlabeled data to boost the accuracy , as well as reduce model size for online deployment .",6,0,42
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"In this work , we propose token - level ensemble distillation for G2P conversion , which can ( 1 ) boost the accuracy by distilling the knowledge from additional unlabeled data , and ( 2 ) reduce the model size but maintain the high accuracy , both of which are very practical and helpful in the online production system .",7,0,60
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"We use token - level knowledge distillation , which results in better accuracy than the sequence - level counterpart .",8,0,20
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"What is more , we adopt the Transformer instead of RNN or CNN based models to further boost the accuracy of G2P conversion .",9,0,24
dataset/preprocessed/training-data/text-to-speech_synthesis/0,Experiments on the publicly available CMU - Dict dataset and an internal English dataset demonstrate the effectiveness of our proposed method .,10,0,22
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"Particularly , our method achieves 19.88 % WER on CMUDict dataset , outperforming the previous works by more than 4.22 % WER , and setting the new state - of - the - art results .",11,0,36
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"Grapheme - to - phoneme ( G2P ) conversion aims to generate a sequence of pronunciation symbols ( phonemes ) given a sequence of letters ( graphemes ) , which is an important component in automatic speech recognition and text - to - speech systems to provide accurate pronunciations for the words not covered by the lexicon .",13,0,58
dataset/preprocessed/training-data/text-to-speech_synthesis/0,G2P conversion can be viewed as a sequence to sequence task and modeled by the encoder - decoder framework .,14,0,20
dataset/preprocessed/training-data/text-to-speech_synthesis/0,adopt LSTM for G2P conversion and achieve improvements than the previous joint n-gram model .,15,0,15
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"use convolutional sequence to sequence model and non-sequential decoding , and attain the previous best results on the public CMUDict dataset .",16,0,22
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"While previous works introduced the neural sequence to sequence models into G2P conversion and indeed achieved improvements over conventional methods , they did not take into account several practical issues of G2P conversion in the production system .",17,0,38
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"First , considering training data is always costly through human labeling , how to further leverage the unlimited amount of unlabeled data is critical to improve the performance",18,0,28
dataset/preprocessed/training-data/text-to-speech_synthesis/0,This work was done while the first author was an intern at Microsoft. of G2P conversion .,19,0,17
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"Second , large or ensemble models are too costly to serve when deploying in the online systems .",20,0,18
dataset/preprocessed/training-data/text-to-speech_synthesis/0,How to reduce the model size but maintain high accuracy is essential .,21,0,13
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"Inspired by the knowledge distillation in computer vision and natural language processing , in this work , we propose the token - level ensemble distillation for G2P conversion , to address the practical problems mentioned above .",22,0,37
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"First , we use knowledge distillation to leverage the large amount of unlabeled words .",23,0,15
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"Specifically , we train a teacher model to generate the phoneme sequence as well as its probability distribution given unlabeled grapheme sequence , and regard the unlabeled grapheme sequence and the generated phoneme sequence as pseudo labeled data , and add them into the original training data .",24,0,48
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"Second , we train a variety of models ( CNN , RNN and Transformer ) for ensemble to get higher accuracy , and transfer the knowledge of the ensemble models to a light - weight model that is suitable for online deployment , again by knowledge distillation .",25,0,48
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"Besides , we adopt Transformer instead of RNN or CNN as the basic encoder - decoder model structure , since it demonstrates advantages in a variety of sequence to sequence tasks , such as neural machine translation , text summarization , automatic speech recognition .",26,0,45
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"We conduct experiments on CMUDict 0.7 b and our internal dataset , and also leverage additional unlabeled words crawled from the web .",27,0,23
dataset/preprocessed/training-data/text-to-speech_synthesis/0,Our proposed method significantly boosts the accuracy of G2P conversion by 4.22 % WER compared with the previous works .,28,0,20
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"Specifically , Transformer model achieves higher accuracy than RNN and CNN based models , and tokenlevel distillation outperforms sequence - level distillation .",29,0,23
dataset/preprocessed/training-data/text-to-speech_synthesis/0,Our contributions are listed as follows :,30,0,7
dataset/preprocessed/training-data/text-to-speech_synthesis/0,( 1 ) We propose token - level ensemble distillation for grapheme - to - phoneme conversion .,31,0,18
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"We are the first to use unlabeled words to boost the accuracy of grapheme - to - phoneme conversion , and also the first to introduce Transformer into this task and achieve better performance .",32,0,35
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"( 3 ) Our method achieves the state - of - the - art accuracy on CMUdict dataset , outperforming the previous best result by 4.22 % WER .",33,0,29
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"In this section , we briefly review the background of graphemeto - phoneme conversion , Transformer model , as well as knowledge distillation .",35,0,24
dataset/preprocessed/training-data/text-to-speech_synthesis/0,Grapheme - to - Phoneme conversion,36,0,6
dataset/preprocessed/training-data/text-to-speech_synthesis/0,The G2P conversion is the process that generating the phoneme sequence ( pronunciation ) according to the grapheme sequence ( word ) .,37,0,23
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"G2P conversion is necessary and important as lexicon can not cover all words , due to many words are long - tailed and a lot of new words and compound words appear .",38,0,33
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"The spelling and pronunciation are not exactly corresponding for some languages , e.g. English .",39,0,15
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"What is more , the alignments between graphemes and phonemes are complex .",40,0,13
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"A grapheme may correspond to no phoneme , a single phoneme or many phonemes , as shown in , which makes G2P a hard task. for G2P conversion .",41,0,29
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"Recently , sequence to sequence models have achieved great success in machine translation task , and are soon applied on G2P conversion .",42,0,23
dataset/preprocessed/training-data/text-to-speech_synthesis/0,demonstrated that sequence to sequence models outperform joint sequence n-gram models .,43,0,12
dataset/preprocessed/training-data/text-to-speech_synthesis/0,combined joint n-gram models with Bi - LSTM models and achieved good performance in G2P conversion .,44,0,17
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"adopted convolutional sequence to sequence model and proposed the non-sequential decoding for G2P conversion , which achieved the previous state - of - theart result on the public CMUDict 0.7 b dataset .",45,0,33
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"While these sequence to sequence models achieve good performance on G2P conversion , there is still a gap when deploying online .",46,0,22
dataset/preprocessed/training-data/text-to-speech_synthesis/0,"In this work , we propose token - level ensemble distillation based on Transformer model , which can not only boost the accuracy of the G2P conversion with unlabeled words , but also reduce the model size for online deployment .",47,0,41
dataset/preprocessed/training-data/text-to-speech_synthesis/0,Transformer has achieved the state - of - the - art performance in many NLP tasks .,49,0,17
dataset/preprocessed/training-data/text-to-speech_synthesis/2,Transfer Learning from Speaker Verification to Multispeaker Text - To - Speech Synthesis,2,1,13
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"We describe a neural network - based system for text - to - speech ( TTS ) synthesis that is able to generate speech audio in the voice of different speakers , including those unseen during training .",4,1,38
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"Our system consists of three independently trained components : ( 1 ) a speaker encoder network , trained on a speaker verification task using an independent dataset of noisy speech without transcripts from thousands of speakers , to generate a fixed - dimensional embedding vector from only seconds of reference speech from a target speaker ; ( 2 ) a sequence - to - sequence synthesis network based on Tacotron 2 that generates a mel spectrogram from text , conditioned on the speaker embedding ; ( 3 ) an auto - regressive WaveNet - based vocoder network that converts the mel spectrogram into time domain waveform samples .",5,0,108
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively - trained speaker encoder to the multispeaker TTS task , and is able to synthesize natural speech from speakers unseen during training .",6,0,42
dataset/preprocessed/training-data/text-to-speech_synthesis/2,We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance .,7,0,25
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"Finally , we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training , indicating that the model has learned a high quality speaker representation .",8,0,40
dataset/preprocessed/training-data/text-to-speech_synthesis/2,The goal of this work is to build a TTS system which can generate natural speech for a variety of speakers in a data efficient manner .,10,1,27
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"We specifically address a zero - shot learning setting , where a few seconds of untranscribed reference audio from a target speaker is used to synthesize new speech in that speaker 's voice , without updating any model parameters .",11,0,40
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"Such systems have accessibility applications , such as restoring the ability to communicate naturally to users who have lost their voice and are therefore unable to provide many new training examples .",12,0,32
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"They could also enable new applications , such as transferring a voice across languages for more natural speech - to - speech translation , or generating realistic speech from text in low resource settings .",13,0,35
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"However , it is also important to note the potential for misuse of this technology , for example impersonating someone 's voice without their consent .",14,0,26
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"In order to address safety concerns consistent with principles such as , we verify that voices generated by the proposed model can easily be distinguished from real voices .",15,0,29
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"Synthesizing natural speech requires training on a large number of high quality speech - transcript pairs , and supporting many speakers usually uses tens of minutes of training data per speaker .",16,0,32
dataset/preprocessed/training-data/text-to-speech_synthesis/2,Recording a large amount of high quality data for many speakers is impractical .,17,0,14
dataset/preprocessed/training-data/text-to-speech_synthesis/2,Our approach is to decouple speaker modeling from speech synthesis by independently training a speaker - discriminative embedding network that captures the space of speaker characteristics and training a high quality TTS model on a smaller dataset conditioned on the representation learned by the first network .,18,0,47
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"Decoupling the networks enables them to be trained on independent data , which reduces the need to obtain high quality multispeaker training data .",19,0,24
dataset/preprocessed/training-data/text-to-speech_synthesis/2,We train the speaker embedding network on a speaker verification task to determine if two different utterances were spoken by the same speaker .,20,0,24
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"In contrast to the subsequent TTS model , this network is trained on untranscribed speech containing reverberation and background noise from a large number of speakers .",21,0,27
dataset/preprocessed/training-data/text-to-speech_synthesis/2,We demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and disjoint sets of speakers and still generalize well .,22,0,24
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"We train the synthesis network on 1.2K speakers and show that training the encoder on a much larger set of 18K speakers improves adaptation quality , and further enables synthesis of completely novel speakers by sampling from the embedding prior .",23,0,41
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"There has been significant interest in end - to - end training of TTS models , which are trained directly from text - audio pairs , without depending on hand crafted intermediate representations .",24,0,34
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"Tacotron 2 used WaveNet as a vocoder to invert spectrograms generated by an encoderdecoder architecture with attention , obtaining naturalness approaching that of human speech by combining Tacotron 's prosody with WaveNet 's audio quality .",25,0,36
dataset/preprocessed/training-data/text-to-speech_synthesis/2,It only supported a single speaker .,26,0,7
dataset/preprocessed/training-data/text-to-speech_synthesis/2,Gibiansky et al. introduced a multispeaker variation of Tacotron which learned low - dimensional speaker embedding for each training speaker .,27,0,21
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"Deep Voice 3 proposed a fully convolutional encoder - decoder architecture which scaled up to support over 2,400 speakers from LibriSpeech .",28,0,22
dataset/preprocessed/training-data/text-to-speech_synthesis/2,These systems learn a fixed set of speaker embeddings and therefore only support synthesis of voices seen during training .,29,0,20
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"In contrast , VoiceLoop proposed a novel architecture based on a fixed size memory buffer which can generate speech from voices unseen during training .",30,0,25
dataset/preprocessed/training-data/text-to-speech_synthesis/2,Obtaining good results required tens of minutes of enrollment speech and transcripts for a new speaker .,31,0,17
dataset/preprocessed/training-data/text-to-speech_synthesis/2,Recent extensions have enabled few - shot speaker adaptation where only a few seconds of speech per speaker ( without transcripts ) can be used to generate new speech in that speaker 's voice .,32,0,35
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"extends Deep Voice 3 , comparing a speaker adaptation method similar to where the model parameters ( including speaker embedding ) are fine - tuned on a small amount of adaptation data to a speaker encoding method which uses a neural network to predict speaker embedding directly from a spectrogram .",33,0,51
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"The latter approach is significantly more data efficient , obtaining higher naturalness using small amounts of adaptation data , in as few as one or two utterances .",34,0,28
dataset/preprocessed/training-data/text-to-speech_synthesis/2,It is also significantly more computationally efficient since it does not require hundreds of backpropagation iterations .,35,0,17
dataset/preprocessed/training-data/text-to-speech_synthesis/2,Nachmani et al. similarly extended VoiceLoop to utilize a target speaker encoding network to predict a speaker embedding .,36,0,19
dataset/preprocessed/training-data/text-to-speech_synthesis/2,This network is trained jointly with the synthesis network using a contrastive triplet loss to ensure that embeddings predicted from utterances by the same speaker are closer than embeddings computed from different speakers .,37,0,34
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"In addition , a cycle - consistency loss is used to ensure that the synthesized speech encodes to a similar embedding as the adaptation utterance .",38,0,26
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"A similar spectrogram encoder network , trained without a triplet loss , was shown to work for transferring target prosody to synthesized speech .",39,0,24
dataset/preprocessed/training-data/text-to-speech_synthesis/2,In this paper we demonstrate that training a similar encoder to discriminate between speakers leads to reliable transfer of speaker characteristics .,40,0,22
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"Our work is most similar to the speaker encoding models in , except that we utilize a network independently - trained for a speaker verification task on a large dataset of untranscribed audio from tens of thousands of speakers , using a state - of - the - art generalized end - to - end loss .",41,0,57
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"incorporated a similar speaker - discriminative representation into their model , however all components were trained jointly .",42,0,18
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"In contrast , we explore transfer learning from a pre-trained speaker verification model .",43,0,14
dataset/preprocessed/training-data/text-to-speech_synthesis/2,used a similar transfer learning configuration where a speaker embedding computed from a pre-trained speaker classifier was used to condition a TTS system .,44,0,24
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"In this paper we utilize an end - to - end synthesis network which does not rely on intermediate linguistic features , and a substantially different speaker embedding network which is not limited to a closed set of speakers .",45,0,40
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"Furthermore , we analyze how quality varies with the number of speakers in the training set , and find that zero - shot transfer requires training on thousands of speakers , many more than were used in .",46,0,38
dataset/preprocessed/training-data/text-to-speech_synthesis/2,Multispeaker speech synthesis model,47,0,4
dataset/preprocessed/training-data/text-to-speech_synthesis/2,"Our system is composed of three independently trained neural networks , illustrated in : ( 1 ) a recurrent speaker encoder , based on , which computes a fixed dimensional vector from a speech signal , ( 2 ) a sequence - to - sequence synthesizer , based on , which predicts a mel spectrogram from a sequence of grapheme or phoneme inputs , conditioned on the speaker embedding vector , and ( 3 ) an autoregressive WaveNet vocoder , which converts the spectrogram into time domain waveforms .",48,0,89
dataset/preprocessed/training-data/phrase_grounding/0,Multi - level Multimodal Common Semantic Space for Image - Phrase Grounding,2,1,12
dataset/preprocessed/training-data/phrase_grounding/0,We address the problem of phrase grounding by learning a multi - level common semantic space shared by the textual and visual modalities .,4,1,24
dataset/preprocessed/training-data/phrase_grounding/0,"We exploit multiple levels of feature maps of a Deep Convolutional Neural Network , as well as contextualized word and sentence embeddings extracted from a character - based language model .",5,0,31
dataset/preprocessed/training-data/phrase_grounding/0,"Following dedicated non-linear mappings for visual features at each level , word , and sentence embeddings , we obtain multiple instantiations of our common semantic space in which comparisons between any target text and the visual content is performed with cosine similarity .",6,0,43
dataset/preprocessed/training-data/phrase_grounding/0,We guide the model by a multi - level multimodal attention mechanism which outputs attended visual features at each level .,7,0,21
dataset/preprocessed/training-data/phrase_grounding/0,The best level is chosen to be compared with text content for maximizing the pertinence scores of image - sentence pairs of the ground truth .,8,0,26
dataset/preprocessed/training-data/phrase_grounding/0,Experiments conducted on three publicly available datasets show significant performance gains ( 20 % - 60 % relative ) over the state - of - the - art in phrase localization and set a new performance record on those datasets .,9,0,41
dataset/preprocessed/training-data/phrase_grounding/0,We provide a detailed ablation study to show the contribution of each element of our approach and release our code on GitHub 1 .,10,0,24
dataset/preprocessed/training-data/phrase_grounding/0,"Phrase grounding is the task of localizing within an image a given natural language input phrase , as illustrated in .",12,0,21
dataset/preprocessed/training-data/phrase_grounding/0,"This ability to link text and image content is a key component of many visual semantic tasks such as image captioning , visual question answering , text - based image retrieval , and robotic navigation .",13,0,36
dataset/preprocessed/training-data/phrase_grounding/0,It is especially challenging as it requires a good representation of both the visual and textual domain and an effective way of linking them .,14,0,25
dataset/preprocessed/training-data/phrase_grounding/0,"On the visual side , most of the works exploit Deep Convolutional Neural Networks but often rely on bounding box 1 https://github.com/hassanhub/MultiGrounding",15,0,22
dataset/preprocessed/training-data/phrase_grounding/0,A crowd of onlookers on a tractor ride watch a farmer hard at work in the field .,16,0,18
dataset/preprocessed/training-data/phrase_grounding/0,The phrase grounding task in the pointing game setting .,17,0,10
dataset/preprocessed/training-data/phrase_grounding/0,"Given the sentence on top and the image on the left , the goal is to point ( illustrated by the stars here ) to the correct location of each natural language query ( colored text ) .",18,0,38
dataset/preprocessed/training-data/phrase_grounding/0,Actual example of our method results on Flickr30 k .,19,0,10
dataset/preprocessed/training-data/phrase_grounding/0,"proposals or use a global feature of the image , limiting the localization ability and freedom of the method .",20,0,20
dataset/preprocessed/training-data/phrase_grounding/0,"On the textual side , methods rely on a closed vocabulary or try to train their own language model using small image - caption pairs datasets .",21,0,27
dataset/preprocessed/training-data/phrase_grounding/0,"Finally , the mapping between the two modalities is often performed with a weak linear strategy .",22,0,17
dataset/preprocessed/training-data/phrase_grounding/0,"We argue that approaches in the literature have not fully leveraged the potential of the more powerful visual and textual model developed recently , and there is room for developing more sophisticated representations and mapping approaches .",23,0,37
dataset/preprocessed/training-data/phrase_grounding/0,"In this work , we propose to explicitly learn a non-linear mapping of the visual and textual modalities into a common space , and do so at different granularity for each domain .",24,0,33
dataset/preprocessed/training-data/phrase_grounding/0,"Indeed , different layers of a deep network encode each region of the image with gradually increasing levels of discriminativeness and context awareness , similarly single words and whole sentences contain increasing levels of semantic meaning and context .",25,0,39
dataset/preprocessed/training-data/phrase_grounding/0,"This common space mapping is trained with weak supervision and exploited at test - time with a multi - level multimodal attention mechanism , where a natural formalism for computing attention heatmaps at each level , attended features and pertinence scoring , enables us to solve the phrase grounding task elegantly and effectively .",26,0,54
dataset/preprocessed/training-data/phrase_grounding/0,We evaluate our model on three commonly used datasets in the literature of textual grounding and show that it sets a new state - of - the - art performance by a large margin .,27,0,35
dataset/preprocessed/training-data/phrase_grounding/0,Our contributions in this paper are as follows :,28,0,9
dataset/preprocessed/training-data/phrase_grounding/0,"We learn , with weak - supervision , a non-linear mapping of visual and textual features to a common regionword - sentence semantic space , where comparison between any two semantic representations can be performed with a simple cosine similarity ;",29,0,41
dataset/preprocessed/training-data/phrase_grounding/0,"We propose a multi - level multimodal attention mechanism , producing either word - level or sentence - level attention maps at different semantic levels , enabling us to choose the most representative attended visual feature among different semantic levels ;",30,0,41
dataset/preprocessed/training-data/phrase_grounding/0,"We set new state - of - the - art performance on three commonly used datasets , and give detailed ablation results showing how each part of our method contributes to the final performance .",31,0,35
dataset/preprocessed/training-data/phrase_grounding/0,"In this section , we give an overview of related works in the literature and discuss how our method differs from them .",33,0,23
dataset/preprocessed/training-data/phrase_grounding/0,Grounding natural language in images,34,0,5
dataset/preprocessed/training-data/phrase_grounding/0,"The earliest works on solving textual grounding tried to tackle the problem by finding the right bounding box out of a set of proposals , usually obtained from prespecified models .",35,0,31
dataset/preprocessed/training-data/phrase_grounding/0,"The ranking of these proposals , for each text query , can be performed using scores estimated from a reconstruction or sentence generation procedure , or using distances in a common space .",36,0,33
dataset/preprocessed/training-data/phrase_grounding/0,"However , relying on a fixed set of pre-defined concepts and proposals may not be optimal and the quality of the bounding boxes defines an upper bound of the performance that can be achieved .",37,0,35
dataset/preprocessed/training-data/phrase_grounding/0,"Therefore , several methods have integrated the proposal step in their framework to improve the bounding box quality .",38,0,19
dataset/preprocessed/training-data/phrase_grounding/0,"These works often operate in a fully supervised setting , where the mapping between sentences and bounding boxes has to be provided at training time which is not always available and is costly to gather .",39,0,36
dataset/preprocessed/training-data/phrase_grounding/0,"Furthermore , methods based on bounding boxes often extract features separately for each bounding box , inducing a high computational cost .",40,0,22
dataset/preprocessed/training-data/phrase_grounding/0,"Therefore , some works choose not to rely on bounding boxes and propose to formalize the localization problem as finding a spatial heatmap for the referring expression .",41,0,28
dataset/preprocessed/training-data/phrase_grounding/0,"This setting is mostly weakly - supervised , whereat training time only the image and the text ( describing either the whole image or some parts of it ) are provided but not the corresponding bounding box or segmentation mask for each description .",42,0,44
dataset/preprocessed/training-data/phrase_grounding/0,This is the more general setting we are addressing in this paper .,43,0,13
dataset/preprocessed/training-data/phrase_grounding/0,The top - down approaches and the attention - based approach learn to produce a heatmap for each word of a vocabulary .,44,0,23
dataset/preprocessed/training-data/phrase_grounding/0,"At test time , all these methods produce the final heatmap by averaging the heatmaps of all the words in the query that exist in the vocabulary .",45,0,28
dataset/preprocessed/training-data/phrase_grounding/0,"Several grounding works have also explored the use of additional knowledge , such as image and linguistic structures , phrase context and exploiting pre-trained visual models predictions .",46,0,28
dataset/preprocessed/training-data/phrase_grounding/0,"In contrast to many works in the literature , we do n't use a pre-defined set of image concepts or words in our method .",47,0,25
dataset/preprocessed/training-data/phrase_grounding/0,We instead rely on visual feature maps and a character - based language model with contextualized embeddings which could handle any unseen word considering the context in the sentence .,48,0,30
dataset/preprocessed/training-data/phrase_grounding/0,Mapping to common space,49,0,4
dataset/preprocessed/training-data/natural_language_inference/56,Recurrent Relational Networks,2,1,3
dataset/preprocessed/training-data/natural_language_inference/56,"This paper is concerned with learning to solve tasks that require a chain of interdependent steps of relational inference , like answering complex questions about the relationships between objects , or solving puzzles where the smaller elements of a solution mutually constrain each other .",4,0,45
dataset/preprocessed/training-data/natural_language_inference/56,"We introduce the recurrent relational network , a general purpose module that operates on a graph representation of objects .",5,1,20
dataset/preprocessed/training-data/natural_language_inference/56,As a generalization of Santoro et al .,6,0,8
dataset/preprocessed/training-data/natural_language_inference/56,"[ 2017 ] 's relational network , it can augment any neural network model with the capacity to do many - step relational reasoning .",7,0,25
dataset/preprocessed/training-data/natural_language_inference/56,"We achieve state of the art results on the bAbI textual question - answering dataset with the recurrent relational network , consistently solving 20 / 20 tasks .",8,0,28
dataset/preprocessed/training-data/natural_language_inference/56,"As bAbI is not particularly challenging from a relational reasoning point of view , we introduce Pretty - CLEVR , a new diagnostic dataset for relational reasoning .",9,0,28
dataset/preprocessed/training-data/natural_language_inference/56,"In the Pretty - CLEVR set - up , we can vary the question to control for the number of relational reasoning steps thatare required to obtain the answer .",10,0,30
dataset/preprocessed/training-data/natural_language_inference/56,"Using Pretty - CLEVR , we probe the limitations of multi -layer perceptrons , relational and recurrent relational networks .",11,0,20
dataset/preprocessed/training-data/natural_language_inference/56,"Finally , we show how recurrent relational networks can learn to solve Sudoku puzzles from supervised training data , a challenging task requiring upwards of 64 steps of relational reasoning .",12,0,31
dataset/preprocessed/training-data/natural_language_inference/56,We achieve state - of - the - art results amongst comparable methods by solving 96.6 % of the hardest Sudoku puzzles .,13,0,23
dataset/preprocessed/training-data/natural_language_inference/56,A central component of human intelligence is the ability to abstractly reason about objects and their interactions .,15,0,18
dataset/preprocessed/training-data/natural_language_inference/56,"As an illustrative example , consider solving a Sudoku .",16,0,10
dataset/preprocessed/training-data/natural_language_inference/56,"A Sudoku consists of 81 cells thatare arranged in a 9 - by - 9 grid , which must be filled with digits 1 to 9 so that each digit appears exactly once in each row , column and 3 - by - 3 non -overlapping box , with a number of digits given 1 .",17,0,56
dataset/preprocessed/training-data/natural_language_inference/56,"To solve a Sudoku , one methodically reasons about the puzzle in terms of its cells and their interactions over many steps .",18,0,23
dataset/preprocessed/training-data/natural_language_inference/56,"One tries placing digits in cells and see how that affects other cells , iteratively working toward a solution .",19,0,20
dataset/preprocessed/training-data/natural_language_inference/56,"Contrast this with the canonical deep learning approach to solving problems , the multilayer perceptron ( MLP ) , or multilayer convolutional neural net ( CNN ) .",20,0,28
dataset/preprocessed/training-data/natural_language_inference/56,"These architectures take the entire Sudoku as an input and output the entire solution in a single forward pass , ignoring the inductive bias that objects exists in the world , and that they affect each other in a consistent manner .",21,0,42
dataset/preprocessed/training-data/natural_language_inference/56,Not surprisingly these models fall short when faced with problems that require even basic relational reasoning .,22,0,17
dataset/preprocessed/training-data/natural_language_inference/56,"The relational network of is an important first step towards a simple module for reasoning about objects and their interactions but it is limited to performing a single relational operation , and was evaluated on datasets that require a maximum of three steps of reasoning ( which , surprisingly , can be solved by a single relational reasoning step as we show ) .",23,0,64
dataset/preprocessed/training-data/natural_language_inference/56,"Looking beyond relational networks , there is a rich literature on logic and reasoning in artificial intelligence and machine learning , which we discuss in section 5 .",24,0,28
dataset/preprocessed/training-data/natural_language_inference/56,"Toward generally realizing the ability to methodically reason about objects and their interactions over many steps , this paper introduces a composite function , the recurrent relational network .",25,0,29
dataset/preprocessed/training-data/natural_language_inference/56,It serves as a modular component for many - step relational reasoning in end - to - end differentiable learning systems .,26,0,22
dataset/preprocessed/training-data/natural_language_inference/56,"It encodes the inductive biases that 1 ) objects exists in the world 2 ) they can be sufficiently described by properties 3 ) properties can changeover time 4 ) objects can affect each other and 5 ) given the properties , the effects object have on each other is invariant to time .",27,0,54
dataset/preprocessed/training-data/natural_language_inference/56,"An important insight from the work of is to decompose a function for relational reasoning into two components or "" modules "" :",28,0,23
dataset/preprocessed/training-data/natural_language_inference/56,"a perceptual front - end , which is tasked to recognize objects in the raw input and represent them as vectors , and a relational reasoning module , which uses the representation to reason about the objects and their interactions .",29,0,41
dataset/preprocessed/training-data/natural_language_inference/56,Both modules are trained jointly end - to - end .,30,0,11
dataset/preprocessed/training-data/natural_language_inference/56,"In computer science parlance , the relational reasoning module implements an interface : it operates on a graph of nodes and directed edges , where the nodes are represented by real valued vectors , and is differentiable .",31,0,38
dataset/preprocessed/training-data/natural_language_inference/56,This paper chiefly develops the relational reasoning side of that interface .,32,0,12
dataset/preprocessed/training-data/natural_language_inference/56,Some of the tasks we evaluate on can be efficiently and perfectly solved by hand - crafted algorithms that operate on the symbolic level .,33,0,25
dataset/preprocessed/training-data/natural_language_inference/56,"For example , 9 - by - 9 Sudokus can be solved in a fraction of a second with constraint propagation and search or with dancing links .",34,0,28
dataset/preprocessed/training-data/natural_language_inference/56,"These symbolic algorithms are superior in every respect but one : they do n't comply with the interface , as they are not differentiable and do n't work with real - valued vector descriptions .",35,0,35
dataset/preprocessed/training-data/natural_language_inference/56,They therefore can not be used in a combined model with a deep learning perceptual front - end and learned end - to - end .,36,0,26
dataset/preprocessed/training-data/natural_language_inference/56,"Following , we use the term "" relational reasoning "" liberally for an object - and interaction - centric approach to problem solving .",37,0,24
dataset/preprocessed/training-data/natural_language_inference/56,"Although the term "" relational reasoning "" is similar to terms in other branches of science , like relational logic or first order logic , no direct parallel is intended .",38,0,31
dataset/preprocessed/training-data/natural_language_inference/56,"This paper considers many - step relational reasoning , a challenging task for deep learning architectures .",39,0,17
dataset/preprocessed/training-data/natural_language_inference/56,"We develop a recurrent relational reasoning module , which constitutes our main contribution .",40,0,14
dataset/preprocessed/training-data/natural_language_inference/56,"We show that it is a powerful architecture for many - step relational reasoning on three varied datasets , achieving state - of - the - art results on bAbI and Sudoku .",41,0,33
dataset/preprocessed/training-data/natural_language_inference/56,Recurrent Relational Networks,42,1,3
dataset/preprocessed/training-data/natural_language_inference/56,"We ground the discussion of a recurrent relational network in something familiar , solving a Sudoku puzzle .",43,0,18
dataset/preprocessed/training-data/natural_language_inference/56,"A simple strategy works by noting that if a certain Sudoku cell is given as a "" 7 "" , one can safely remove "" 7 "" as an option from other cells in the same row , column and box .",44,0,42
dataset/preprocessed/training-data/natural_language_inference/56,"In a message passing framework , that cell needs to send a message to each other cell in the same row , column , and box , broadcasting it 's value as "" 7 "" , and informing those cells not to take the value "" 7 "" .",45,0,49
dataset/preprocessed/training-data/natural_language_inference/56,"In an iteration t , these messages are sent simultaneously , in parallel , between all cells .",46,0,18
dataset/preprocessed/training-data/natural_language_inference/56,"Each cell i should then consider all incoming messages , and update it s internal state ht i to h t+1 i .",47,0,23
dataset/preprocessed/training-data/natural_language_inference/56,"With the updated state each cell should send out new messages , and the process repeats .",48,0,17
dataset/preprocessed/training-data/natural_language_inference/56,Message passing on a graph .,49,0,6
dataset/preprocessed/training-data/natural_language_inference/54,Structural Embedding of Syntactic Trees for Machine Comprehension,2,1,8
dataset/preprocessed/training-data/natural_language_inference/54,Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees .,4,0,29
dataset/preprocessed/training-data/natural_language_inference/54,"In this paper , we propose structural embedding of syntactic trees ( SEST ) , an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension .",5,0,40
dataset/preprocessed/training-data/natural_language_inference/54,We evaluate our approach using a state - of - the - art neural attention model on the SQuAD dataset .,6,0,21
dataset/preprocessed/training-data/natural_language_inference/54,Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers thatare syntactically coherent over the baseline methods .,7,0,26
dataset/preprocessed/training-data/natural_language_inference/54,"Reading comprehension such as SQuAD or News QA requires identifying a span from a given context , which is an extension to the traditional question answering task , aiming at responding questions posed by human with natural language .",9,1,39
dataset/preprocessed/training-data/natural_language_inference/54,"Many works have been proposed to leverage deep neural networks for such question answering tasks , most of which involve learning the query - aware context representations .",10,0,28
dataset/preprocessed/training-data/natural_language_inference/54,"Although deep learning based methods demonstrated great potentials for question answering , none them take syntactic information of the sentences such as con - * Authors contributed equally to this work .",11,0,32
dataset/preprocessed/training-data/natural_language_inference/54,stituency tree and dependency tree into consideration .,12,0,8
dataset/preprocessed/training-data/natural_language_inference/54,Such techniques have been proven to be useful in many natural language understanding tasks in the past and illustrated noticeable improvements such as the work by .,13,0,27
dataset/preprocessed/training-data/natural_language_inference/54,"In this paper , we adopt similar ideas but apply them to a neural attention model for question answering .",14,0,20
dataset/preprocessed/training-data/natural_language_inference/54,The constituency tree ) of a sentence defines internal nodes and terminal nodes to represent phrase structure grammars and the actual words .,15,0,23
dataset/preprocessed/training-data/natural_language_inference/54,"illustrates the constituency tree of the sentence "" the architect or engineer acts as the project coordinator "" .",16,0,19
dataset/preprocessed/training-data/natural_language_inference/54,"Here , "" the architect or engineer "" and "" the project coordinator "" are labeled as noun phrases ( "" NP "" ) , which is critical for answering the question below .",17,0,34
dataset/preprocessed/training-data/natural_language_inference/54,"Here , the question asks for the name of certain occupation that can be best answered using an noun phrase .",18,0,21
dataset/preprocessed/training-data/natural_language_inference/54,"Utilizing the knowledge of a constituency relations , we can reduce the size of the candidate space and help the algorithm to identify the correct answer .",19,0,27
dataset/preprocessed/training-data/natural_language_inference/54,"Whose role is to design the works , prepare the specifications and produce construction drawings , administer the contract , tender the works , and manage the works from inception to completion ?",20,0,33
dataset/preprocessed/training-data/natural_language_inference/54,"On the other hand , a dependency tree is constructed based on the dependency structure of a sentence .",21,0,19
dataset/preprocessed/training-data/natural_language_inference/54,displays the dependency tree for sentence,22,0,6
dataset/preprocessed/training-data/natural_language_inference/54,"The Annual Conference , roughly the equivalent of a diocese in the Anglican Communion and the Roman Catholic Church or a synod in some Lutheran denominations such as the Evangelical Lutheran Church in America , is the basic unit of organization within the UMC .",23,0,45
dataset/preprocessed/training-data/natural_language_inference/54,""" The Annual Conference "" being the subject of "" the basic unit of organization within the UMC "" provides a critical clue for the model to skip over a large chunk of the text when answering the question "" What is the basic unit of organization within the UMC "" .",24,0,52
dataset/preprocessed/training-data/natural_language_inference/54,"As we show in the analysis section , adding dependency information dramatically helps identify dependency structures within the sentence , which is otherwise difficult to learn .",25,0,27
dataset/preprocessed/training-data/natural_language_inference/54,"In this paper , we propose Structural Embedding of Syntactic Trees ( SEST ) that encode syntactic information structured by constituency tree and dependency tree into neural attention models for the question answering task .",26,0,35
dataset/preprocessed/training-data/natural_language_inference/54,"Experimental results on SQuAD dataset illustrates that the syntactic information helps the model to choose the answers thatare both succinct and grammatically coherent , which boosted the performance on both qualitative studies and numerical results .",27,0,36
dataset/preprocessed/training-data/natural_language_inference/54,"Our focus is to show adding structural embedding can improve baseline models , rather than directly compare to published SQuAD results .",28,0,22
dataset/preprocessed/training-data/natural_language_inference/54,"Although the methods proposed in the paper are demonstrated using syntactic trees , we note that similar approaches can be used to encode other types of tree structured information such as knowledge graphs and ontology relations .",29,0,37
dataset/preprocessed/training-data/natural_language_inference/54,The general framework of our model is illustrated in .,31,0,10
dataset/preprocessed/training-data/natural_language_inference/54,Here the input of the model is the embedding of the context and question while the output is two indices begin and end which indicate the begin and end indices of the answer in the context space .,32,0,38
dataset/preprocessed/training-data/natural_language_inference/54,The input of the model contains two parts : the word / character model and the syntactic model .,33,0,19
dataset/preprocessed/training-data/natural_language_inference/54,The shaded portion of our model in represents the encoded syntactic information of both context and question thatare fed into the model .,34,0,23
dataset/preprocessed/training-data/natural_language_inference/54,"To gain an insight of how the encoding works , consider a sentence which syntactic tree consists of four nodes ( o1 , o 2 , o3 , o 4 ) .",35,0,32
dataset/preprocessed/training-data/natural_language_inference/54,A specific word is represented to be a sequence of nodes from it s leave all the way to the root .,36,0,22
dataset/preprocessed/training-data/natural_language_inference/54,We cover how this process work in detail in Section 3.1.1 and 3.1.2 .,37,0,14
dataset/preprocessed/training-data/natural_language_inference/54,Another input that will be fed into deep learning model is the embedding information for words and characters respectively .,38,0,20
dataset/preprocessed/training-data/natural_language_inference/54,There are many ways to convert words in a sentence into a highdimensional embedding .,39,0,15
dataset/preprocessed/training-data/natural_language_inference/54,We choose Glo Ve to obtain a pre-trained and fixed vector for each word .,40,0,15
dataset/preprocessed/training-data/natural_language_inference/54,"Instead of using a fixed embedding , we use Convolutional Neural Networks ( CNN ) to model character level embedding , which values can be changed during training .",41,0,29
dataset/preprocessed/training-data/natural_language_inference/54,"To integrate both embeddings into the deep neural model , we feed the concatenation of them for the question and the context to be the input of the model .",42,0,30
dataset/preprocessed/training-data/natural_language_inference/54,The inputs are processed in the embedding layer to form more abstract representations .,43,0,14
dataset/preprocessed/training-data/natural_language_inference/54,Here we choose a multi - layer bi-directional Long Short Term Memory ( LSTM ) to obtain more abstract representations for words in the contexts and questions .,44,0,28
dataset/preprocessed/training-data/natural_language_inference/54,"After that , we employ an attention layer to fuse information from both the contexts and the questions .",45,0,19
dataset/preprocessed/training-data/natural_language_inference/54,Various matching mechanisms using attentions have been extensively studied for machine comprehension tasks .,46,0,14
dataset/preprocessed/training-data/natural_language_inference/54,We use the Bi-directional Attention flow model which performs contextto - question and question - to - context attentions in both directions .,47,0,23
dataset/preprocessed/training-data/natural_language_inference/54,The context - to - question attention signifies which question words are most relevant to each context word .,48,0,19
dataset/preprocessed/training-data/natural_language_inference/54,"For each context word , the attention weight is first computed by a softmax function with question words , and the attention vector of each context word is then computed by a weighted sum of the question words ' embeddings obtained from the embedding layer .",49,0,46
dataset/preprocessed/training-data/natural_language_inference/82,Dynamic Entity Representation with Max - pooling Improves Machine Reading,2,1,10
dataset/preprocessed/training-data/natural_language_inference/82,"We propose a novel neural network model for machine reading , DER Network , which explicitly implements a reader building dynamic meaning representations for entities by gathering and accumulating information around the entities as it reads a document .",4,0,39
dataset/preprocessed/training-data/natural_language_inference/82,"Evaluated on a recent large scale dataset ( Hermann et al. , 2015 ) , our model exhibits better results than previous research , and we find that max - pooling is suited for modeling the accumulation of information on entities .",5,0,42
dataset/preprocessed/training-data/natural_language_inference/82,Further analysis suggests that our model can put together multiple pieces of information encoded in different sentences to answer complicated questions .,6,0,22
dataset/preprocessed/training-data/natural_language_inference/82,Our code for the model is available at https://github.com/soskek/der-network,7,0,9
dataset/preprocessed/training-data/natural_language_inference/82,"Machine reading systems can be tested on their ability to answer queries about contents of documents that they read , thus a central problem is how the information of documents should be organized in the system and retrieved by the queries .",9,0,42
dataset/preprocessed/training-data/natural_language_inference/82,"Recently , large scale datasets of document - queryanswer triples have been constructed from online newspaper articles and their summaries , by replacing named entities in the summaries with placeholders to form Cloze ) style questions ) .",10,0,38
dataset/preprocessed/training-data/natural_language_inference/82,These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers .,11,0,17
dataset/preprocessed/training-data/natural_language_inference/82,"( @entity1 ) @entity0 maybe @entity 2 in the popular @entity 4 superhero films , but he recently dealt in some advanced bionic technology himself .",12,0,26
dataset/preprocessed/training-data/natural_language_inference/82,"@entity0 recently presented a robotic arm to young @entity 7 , a @entity 8 boy who is missing his right arm from just above his elbow .",13,0,27
dataset/preprocessed/training-data/natural_language_inference/82,"the arm was made by @entity12 , a ! "" [ X ] "" star @entity0 presents a young child with a bionic arm ! :",14,0,26
dataset/preprocessed/training-data/natural_language_inference/82,A document - query - answer triple constructed from a news article and its bullet point summary .,15,0,18
dataset/preprocessed/training-data/natural_language_inference/82,An entity in the summary ( Robert Downey Jr. ) is replaced by the placeholder [ X ] to form a query .,16,0,23
dataset/preprocessed/training-data/natural_language_inference/82,All entities are anonymized to exclude world knowledge and focus on reading comprehension .,17,0,14
dataset/preprocessed/training-data/natural_language_inference/82,"In this paper , we hypothesize that a reader without world knowledge can only understand a named entity by dynamically constructing its meaning from the contexts .",18,0,27
dataset/preprocessed/training-data/natural_language_inference/82,"For example , in , a reader reading the sentence "" Robert Downey Jr. maybe Iron Man . . . "" can only understand "" Robert Downey Jr. "" as something that "" may be Iron Man "" at this stage , given that it does not know Robert Downey Jr. a priori .",19,0,54
dataset/preprocessed/training-data/natural_language_inference/82,"Information about this entity can only be accumulated by its subsequent occurrence , such as "" Downey recently presented a robotic arm . . . "" .",20,0,27
dataset/preprocessed/training-data/natural_language_inference/82,"Thus , named entities basically serve as anchors to link multiple pieces of information encoded in different sentences .",21,0,19
dataset/preprocessed/training-data/natural_language_inference/82,"This insight has been reflected by the anonymization process in construction of the dataset , in which coreferent entities ( e.g. "" Robert Downey Jr. "" and "" Downey "" ) are replaced by randomly permuted abstract entity markers ( e.g. "" @en - tity0 "" ) , in order to prevent additional world knowledge from being attached to the surface form of the entities .",22,0,66
dataset/preprocessed/training-data/natural_language_inference/82,"We , however , take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity , by gathering and accumulating information on that entity as it reads a document ( Section 2 ) .",23,0,41
dataset/preprocessed/training-data/natural_language_inference/82,"Evaluation of our model , DER Network , exhibits better results than previous research ( Section 3 ) .",24,0,19
dataset/preprocessed/training-data/natural_language_inference/82,"In particular , we find that max - pooling of entity representations , which is intended to model the accumulation of information on entities , can drastically improve performance .",25,0,30
dataset/preprocessed/training-data/natural_language_inference/82,Further analysis suggests that max - pooling can help our model draw multiple pieces of information from different sentences .,26,0,20
dataset/preprocessed/training-data/natural_language_inference/82,"Following Hermann et al. , our model estimates the conditional probability p ( e | D , q ) , where q is a query and Dis a document .",28,0,30
dataset/preprocessed/training-data/natural_language_inference/82,"A candidate answer for the query is denoted bye , which in this paper is any named entity .",29,0,19
dataset/preprocessed/training-data/natural_language_inference/82,Our model can be factorized as :,30,0,7
dataset/preprocessed/training-data/natural_language_inference/82,"in which u ( q ) is the learned meaning for the query and v( e ; D , q ) the dynamically constructed meaning for an entity , depending on the document D and the query q.",31,0,38
dataset/preprocessed/training-data/natural_language_inference/82,We note that ( 1 ) is in contrast to the factorization used by Hermann et al. :,32,0,18
dataset/preprocessed/training-data/natural_language_inference/82,"in which a vector u ( D , q ) is learned to represent the status of a reader after reading a document and a query , and this vector is used to retrieve an answer by coupling with the answer vector v ( a ) .",33,0,47
dataset/preprocessed/training-data/natural_language_inference/82,1 Factorization ( 2 ) relies on the hypothesis that there exists a fixed vector for each candidate answer representing its meaning .,34,0,23
dataset/preprocessed/training-data/natural_language_inference/82,"However , as we argued in Section 1 , an entity surface does not possess meaning ; rather , it serves as an anchor to link pieces of information about it .",35,0,32
dataset/preprocessed/training-data/natural_language_inference/82,"Therefore , we hypothesize that the meaning representation v( e ; D , q ) of an entity e should be dynamically constructed from its surrounding contexts , and the meanings are "" accumulated "" through the reader reading the document D .",36,0,43
dataset/preprocessed/training-data/natural_language_inference/82,"We explain the construction of v (e ; D , q ) in Section 2.1 , and propose a max - pooling process for modeling information accumulation in Section 2.2 .",37,0,31
dataset/preprocessed/training-data/natural_language_inference/82,Dynamic Entity Representation,38,0,3
dataset/preprocessed/training-data/natural_language_inference/82,"For any entity e , we take it s context c as any sentence that includes a token of e .",39,0,21
dataset/preprocessed/training-data/natural_language_inference/82,"Then , we use bidirectional single - layer LSTMs to encode c into vectors .",40,0,15
dataset/preprocessed/training-data/natural_language_inference/82,"LSTM is a neural cell that outputs a vector h c , t for each token tin the sentence c ; taking the word vector x c , t of the token as input , each h c, t is calculated recurrently from its precedent vector h c ,t ?",41,0,50
dataset/preprocessed/training-data/natural_language_inference/82,"1 or h c , t + 1 , depending on the direction of the encoding .",42,0,17
dataset/preprocessed/training-data/natural_language_inference/82,"Formally , we write forward and backward LSTMs as :",43,0,10
dataset/preprocessed/training-data/natural_language_inference/82,"Then , denoting the length of the sentence c as T and the index of the entity e token as ? , we define the dynamic entity representation d e , c as the concatenation of the vectors [ h c , T , h c , 1 , h c , ? , h c, ? ] encoded by a feed - forward layer :",44,0,66
dataset/preprocessed/training-data/natural_language_inference/82,"in which s e , c ( q ) is calculated by the attention mechanism , modeling the degree to which our reader should attend to a particular occurrence of an entity , given the query q.",45,0,37
dataset/preprocessed/training-data/natural_language_inference/82,"More precisely , s e , c ( q ) is defined as the following :",46,0,16
dataset/preprocessed/training-data/natural_language_inference/82,"where s e , c ( q ) is calculated by taking the softmax of s e , c ( q ) , which is calculated from the dynamic entity representation d e , c and the query vector q .",47,0,41
dataset/preprocessed/training-data/natural_language_inference/82,"The vector m , matrix W dm , and the bias b sin are learned parameters in the attention mechanism .",48,0,21
dataset/preprocessed/training-data/natural_language_inference/82,Vector m is used hereto map a vector value to a scalar .,49,0,13
dataset/preprocessed/training-data/natural_language_inference/28,ROTATIONAL UNIT OF MEMORY,2,0,4
dataset/preprocessed/training-data/natural_language_inference/28,The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks ( RNN ) to state - of - the - art performance in a variety of sequential tasks .,4,1,36
dataset/preprocessed/training-data/natural_language_inference/28,"However , RNN still have a limited capacity to manipulate long - term memory .",5,1,15
dataset/preprocessed/training-data/natural_language_inference/28,To bypass this weakness the most successful applications of RNN use external techniques such as attention mechanisms .,6,0,18
dataset/preprocessed/training-data/natural_language_inference/28,In this paper we propose a novel RNN model that unifies the state - of - the - art approaches : Rotational Unit of Memory ( RUM ) .,7,0,29
dataset/preprocessed/training-data/natural_language_inference/28,"The core of RUM is its rotational operation , which is , naturally , a unitary matrix , providing architectures with the power to learn long - term dependencies by overcoming the vanishing and exploding gradients problem .",8,0,38
dataset/preprocessed/training-data/natural_language_inference/28,"Moreover , the rotational unit also serves as associative memory .",9,0,11
dataset/preprocessed/training-data/natural_language_inference/28,"We evaluate our model on synthetic memorization , question answering and language modeling tasks .",10,0,15
dataset/preprocessed/training-data/natural_language_inference/28,RUM learns the Copying Memory task completely and improves the state - of - the - art result in the Recall task .,11,0,23
dataset/preprocessed/training-data/natural_language_inference/28,RUM 's performance in the bAbI Question Answering task is comparable to that of models with attention mechanism .,12,0,19
dataset/preprocessed/training-data/natural_language_inference/28,"We also improve the state - of - the - art result to 1.189 bits - per- character ( BPC ) loss in the Character Level Penn Treebank ( PTB ) task , which is to signify the applications of RUM to real - world sequential data .",13,0,48
dataset/preprocessed/training-data/natural_language_inference/28,"The universality of our construction , at the core of RNN , establishes RUM as a promising approach to language modeling , speech recognition and machine translation .",14,0,28
dataset/preprocessed/training-data/natural_language_inference/28,* equal contribution 1 ar Xiv:1710.09537v1 [ cs.LG ],15,0,9
dataset/preprocessed/training-data/natural_language_inference/28,"Recurrent neural networks are widely used in a variety of machine learning applications such as language modeling ) , machine translation ) and speech recognition ) .",17,0,27
dataset/preprocessed/training-data/natural_language_inference/28,Their flexibility of taking inputs of dynamic length makes RNN particularly useful for these tasks .,18,0,16
dataset/preprocessed/training-data/natural_language_inference/28,"However , the traditional RNN models such as Long Short - Term Memory ( LSTM , ) and Gated Recurrent Unit ( GRU , ) exhibit some weaknesses that prevent them from achieving human level performance :",19,0,37
dataset/preprocessed/training-data/natural_language_inference/28,"1 ) limited memory - they can only remember a hidden state , which usually occupies a small part of a model ; 2 ) gradient vanishing / explosion ) during training - trained with backpropagation through time the models fail to learn long - term dependencies .",20,0,48
dataset/preprocessed/training-data/natural_language_inference/28,Several ways to address those problems are known .,21,0,9
dataset/preprocessed/training-data/natural_language_inference/28,"One solution is to use soft and local attention mechanisms ) , which is crucial for most modern applications of RNN .",22,0,22
dataset/preprocessed/training-data/natural_language_inference/28,"Nevertheless , researchers are still interested in improving basic RNN cell models to process sequential data better .",23,0,18
dataset/preprocessed/training-data/natural_language_inference/28,Numerous works ; ) use associative memory to span a large memory space .,24,0,14
dataset/preprocessed/training-data/natural_language_inference/28,"For example , a practical way to implement associative memory is to set weight matrices as trainable structures that change according to input instances for training .",25,0,27
dataset/preprocessed/training-data/natural_language_inference/28,"Furthermore , the recent concept of unitary or orthogonal evolution matrices ; ) also provides a theoretical and empirical solution to the problem of memorizing long - term dependencies .",26,0,30
dataset/preprocessed/training-data/natural_language_inference/28,"Here , we propose a novel RNN cell that resolves simultaneously those weaknesses of basic RNN .",27,0,17
dataset/preprocessed/training-data/natural_language_inference/28,The Rotational Unit of Memory is a modified gated model whose rotational operation acts as associative memory and is strictly an orthogonal matrix .,28,0,24
dataset/preprocessed/training-data/natural_language_inference/28,We tested our model on several benchmarks .,29,0,8
dataset/preprocessed/training-data/natural_language_inference/28,RUM is able to solve the synthetic Copying Memory task while traditional LSTM and GRU fail .,30,0,17
dataset/preprocessed/training-data/natural_language_inference/28,"For synthetic Recall task , RUM exhibits a stronger ability to remember sequences , hence outperforming state - of - the - art RNN models such as Fastweight RNN ) and WeiNet ) .",31,0,34
dataset/preprocessed/training-data/natural_language_inference/28,By using RUM we achieve the state - of - the - art result in the real - world Character Level Penn Treebank task .,32,0,25
dataset/preprocessed/training-data/natural_language_inference/28,RUM also outperforms all basic RNN models in the bAbI question answering task .,33,0,14
dataset/preprocessed/training-data/natural_language_inference/28,"This performance is competitive with that of memory networks , which take advantage of attention mechanisms .",34,0,17
dataset/preprocessed/training-data/natural_language_inference/28,Our contributions are as follows :,35,0,6
dataset/preprocessed/training-data/natural_language_inference/28,We develop the concept of the Rotational Unit that combines the memorization advantage of unitary / orthogonal matrices with the dynamic structure of associative memory ;,37,0,26
dataset/preprocessed/training-data/natural_language_inference/28,We implement the rotational operation into a novel RNN model - RUM - which outperforms significantly the current frontier of models on a variety of sequential tasks .,39,0,28
dataset/preprocessed/training-data/natural_language_inference/28,MOTIVATION AND RELATED WORK,40,0,4
dataset/preprocessed/training-data/natural_language_inference/28,The problem of the gradient vanishing and exploding problem is well - known to obstruct the learning of long - term dependencies ) .,42,0,24
dataset/preprocessed/training-data/natural_language_inference/28,We will give a brief mathematical motivation of the problem .,43,0,11
dataset/preprocessed/training-data/natural_language_inference/28,Let 's assume the cost function is C.,44,0,8
dataset/preprocessed/training-data/natural_language_inference/28,In order to evaluate ?C /?,45,0,6
dataset/preprocessed/training-data/natural_language_inference/28,"W ij , one computes the derivative gradient using the chain rule :",46,0,13
dataset/preprocessed/training-data/natural_language_inference/28,where D ( k ) = diag{ ? ( Wx ( k ) + Ah ( k?1 ) + b ) } is the Jacobian matrix of the point - wise nonlinearity .,47,0,33
dataset/preprocessed/training-data/natural_language_inference/28,"As long as the eigenvalues of D ( k ) are of order unity , then if W has eigenvalues ?",48,0,21
dataset/preprocessed/training-data/natural_language_inference/28,"i 1 , they will cause gradient explosion ?",49,0,9
dataset/preprocessed/training-data/natural_language_inference/7,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,2,1,8
dataset/preprocessed/training-data/natural_language_inference/7,"The reading comprehension task , that asks questions about a given evidence document , is a central problem in natural language understanding .",4,0,23
dataset/preprocessed/training-data/natural_language_inference/7,Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline .,5,0,28
dataset/preprocessed/training-data/natural_language_inference/7,"However , Rajpurkar et al . ( 2016 ) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text .",6,0,27
dataset/preprocessed/training-data/natural_language_inference/7,"In this paper , we focus on this answer extraction task , presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network .",7,1,35
dataset/preprocessed/training-data/natural_language_inference/7,We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers .,8,0,28
dataset/preprocessed/training-data/natural_language_inference/7,Our approach improves upon the best published results of Wang & Jiang ( 2016 ) by 5 % and decreases the error of Rajpurkar et al. 's baseline by > 50 %.,9,0,32
dataset/preprocessed/training-data/natural_language_inference/7,"Recently , Rajpurkar et al. ( 2016 ) released the less restricted SQUAD dataset 1 that does not place any constraints on the set of allowed answers , other than that they should be drawn from the evidence document .",10,0,40
dataset/preprocessed/training-data/natural_language_inference/7,Rajpurkar et al. proposed a baseline system that chooses answers from the constituents identified by an existing syntactic parser .,11,0,20
dataset/preprocessed/training-data/natural_language_inference/7,"This allows them to prune the O ( N 2 ) answer candidates in each document of length N , but it also effectively renders 20.7 % of all questions unanswerable .",12,0,32
dataset/preprocessed/training-data/natural_language_inference/7,"Subsequent work by Wang & Jiang ( 2016 ) significantly improve upon this baseline by using an endto - end neural network architecture to identify answer spans by labeling either individual words , or the start and end of the answer span .",13,0,43
dataset/preprocessed/training-data/natural_language_inference/7,"Both of these methods do not make independence assumptions about substructures , but they are susceptible to search errors due to greedy training and decoding .",14,0,26
dataset/preprocessed/training-data/natural_language_inference/7,A primary goal of natural language processing is to develop systems that can answer questions about the contents of documents .,17,0,21
dataset/preprocessed/training-data/natural_language_inference/7,"The reading comprehension task is of practical interest - we want computers to be able to read the world 's text and then answer our questions - and , since we believe it requires deep language understanding , it has also become a flagship task in NLP research .",18,0,49
dataset/preprocessed/training-data/natural_language_inference/7,A number of reading comprehension datasets have been developed that focus on answer selection from a small set of alternatives defined by annotators or existing NLP pipelines that can not be trained end - to - end .,19,0,38
dataset/preprocessed/training-data/natural_language_inference/7,"Subsequently , the models proposed for this task have tended to make use of the limited set of candidates , basing their predictions on mention - level attention weights , or centering classifiers , or network memories on candidate locations .",20,0,41
dataset/preprocessed/training-data/natural_language_inference/7,"In contrast , here we argue that it is beneficial to simplify the decoding procedure by enumerating all possible answer spans .",21,0,22
dataset/preprocessed/training-data/natural_language_inference/7,"By explicitly representing each answer span , our model can be globally normalized during training and decoded exactly during evaluation .",22,0,21
dataset/preprocessed/training-data/natural_language_inference/7,"A naive approach to building the O ( N 2 ) spans of up to length N would require a network that is cubic in size with respect to the passage length , and such a network would be untrainable .",23,0,41
dataset/preprocessed/training-data/natural_language_inference/7,"To overcome this , we present a novel neural architecture called RASOR that builds fixed - length span representations , reusing recurrent computations for shared substructures .",24,0,27
dataset/preprocessed/training-data/natural_language_inference/7,"We demonstrate that directly classifying each of the competing spans , and training with global normalization over all possible spans , leads to a significant increase in performance .",25,0,29
dataset/preprocessed/training-data/natural_language_inference/7,"In our experiments , we show an increase in performance over of 5 % in terms of exact match to a reference answer , and 3.6 % in terms of predicted answer F1 with respect to the reference .",26,0,39
dataset/preprocessed/training-data/natural_language_inference/7,"On both of these metrics , we close the gap between Rajpurkar et al. 's baseline and the human - performance upper-bound by > 50 %.",27,0,26
dataset/preprocessed/training-data/natural_language_inference/7,EXTRACTIVE QUESTION ANSWERING,28,0,3
dataset/preprocessed/training-data/natural_language_inference/7,"Extractive question answering systems take as input a question q = {q 0 , . . . , q n } and a passage of text p = {p 0 , . . . , pm } from which they predict a single answer span a = a start , a end , represented as a pair of indices into p.",30,0,61
dataset/preprocessed/training-data/natural_language_inference/7,"Machine learned extractive question answering systems , such as the one presented here , learn a predictor function f ( q , p ) ?",31,0,25
dataset/preprocessed/training-data/natural_language_inference/7,"a from a training dataset of q , p , a triples .",32,0,13
dataset/preprocessed/training-data/natural_language_inference/7,"For the SQUAD dataset , the original paper from implemented a linear model with sparse features based on n-grams and part - of - speech tags present in the question and the candidate answer .",34,0,35
dataset/preprocessed/training-data/natural_language_inference/7,"Other than lexical features , they also used syntactic information in the form of dependency paths to extract more general features .",35,0,22
dataset/preprocessed/training-data/natural_language_inference/7,"They set a strong baseline for following work and also presented an in depth analysis , showing that lexical and syntactic features contribute most strongly to their model 's performance .",36,0,31
dataset/preprocessed/training-data/natural_language_inference/7,"Subsequent work by use an end - to - end neural network method that uses a Match - LSTM to model the question and the passage , and uses pointer networks to extract the answer span from the passage .",37,0,40
dataset/preprocessed/training-data/natural_language_inference/7,This model resorts to greedy decoding and falls short in terms of performance compared to our model ( see Section 5 for more detail ) .,38,0,26
dataset/preprocessed/training-data/natural_language_inference/7,"While we only compare to published baselines , there are other unpublished competitive systems on the SQUAD leaderboard , as listed in footnote",39,0,23
dataset/preprocessed/training-data/natural_language_inference/7,"A task that is closely related to extractive question answering is the Cloze task , in which the goal is to predict a concealed span from a declarative sentence given a passage of supporting text .",41,0,36
dataset/preprocessed/training-data/natural_language_inference/7,presented a Cloze dataset in which the task is to predict the correct entity in an incomplete sentence given an abstractive summary of a news article .,42,0,27
dataset/preprocessed/training-data/natural_language_inference/7,Hermann et al .,43,0,4
dataset/preprocessed/training-data/natural_language_inference/7,also present various neural architectures to solve the problem .,44,0,10
dataset/preprocessed/training-data/natural_language_inference/7,"Although this dataset is large and varied in domain , recent analysis by shows that simple models can achieve close to the human upper bound .",45,0,26
dataset/preprocessed/training-data/natural_language_inference/7,"As noted by the authors of the SQUAD paper , the annotated answers in the SQUAD dataset are often spans that include non-entities and can be longer phrases , unlike the Cloze datasets , thus making the task more challenging .",46,0,41
dataset/preprocessed/training-data/natural_language_inference/7,"Another , more traditional line of work has focused on extractive question answering on sentences , where the task is to extract a sentence from a document , given a question .",47,0,32
dataset/preprocessed/training-data/natural_language_inference/7,"Relevant datasets include datasets from the annual TREC evaluations and WikiQA , where the latter dataset specifically focused on Wikipedia passages .",48,0,22
dataset/preprocessed/training-data/natural_language_inference/7,"There has been a line of interesting recent publications using neural architectures , focused on this variety of extractive question answering .",49,0,22
dataset/preprocessed/training-data/natural_language_inference/60,Dynamic Meta - Embeddings for Improved Sentence Representations,2,1,8
dataset/preprocessed/training-data/natural_language_inference/60,"While one of the first steps in many NLP systems is selecting what pre-trained word embeddings to use , we argue that such a step is better left for neural networks to figure out by themselves .",4,0,37
dataset/preprocessed/training-data/natural_language_inference/60,"To that end , we introduce dynamic meta-embeddings , a simple yet effective method for the supervised learning of embedding ensembles , which leads to stateof - the - art performance within the same model class on a variety of tasks .",5,0,42
dataset/preprocessed/training-data/natural_language_inference/60,We subsequently show how the technique can be used to shed new light on the usage of word embeddings in NLP systems .,6,0,23
dataset/preprocessed/training-data/natural_language_inference/60,Multi-domain Standard word embeddings,7,0,4
dataset/preprocessed/training-data/natural_language_inference/60,It is no exaggeration to say that word embeddings have revolutionized NLP .,9,0,13
dataset/preprocessed/training-data/natural_language_inference/60,"From early distributional semantic models to deep learningbased word embeddings , word - level meaning representations have found applications in a wide variety of core NLP tasks , to the extent that they are now ubiquitous in the field .",10,0,40
dataset/preprocessed/training-data/natural_language_inference/60,A sprawling literature has emerged about what types of embeddings are most useful for which tasks .,11,0,17
dataset/preprocessed/training-data/natural_language_inference/60,"For instance , there has been extensive work on understanding what word embeddings learn , evaluating their performance , specializing them for certain tasks , learning sub - word level representations , et cetera .",12,0,35
dataset/preprocessed/training-data/natural_language_inference/60,"One of the first steps in designing many NLP systems is selecting what kinds of word embed - dings to use , with people often resorting to freely available pre-trained embeddings .",13,0,32
dataset/preprocessed/training-data/natural_language_inference/60,"While this is often a sensible thing to do , the usefulness of word embeddings for downstream tasks tends to be hard to predict , as downstream tasks can be poorly correlated with word - level benchmarks .",14,0,38
dataset/preprocessed/training-data/natural_language_inference/60,An alternative is to try to combine the strengths of different word embeddings .,15,0,14
dataset/preprocessed/training-data/natural_language_inference/60,"Recent work in socalled "" meta- embeddings "" , which ensembles embedding sets , has been gaining traction .",16,0,19
dataset/preprocessed/training-data/natural_language_inference/60,"Metaembeddings are usually created in a separate preprocessing step , rather than in a process that is dynamically adapted to the task .",17,0,23
dataset/preprocessed/training-data/natural_language_inference/60,"In this work , we explore the supervised learning of task - specific , dynamic meta-embeddings , and apply the technique to sentence representations .",18,1,25
dataset/preprocessed/training-data/natural_language_inference/60,"The proposed approach turns out to be highly effective , leading to state - of - the - art performance within the same model class on a variety of tasks , opening up new are as for exploration and yielding insights into the usage of word embeddings .",19,0,48
dataset/preprocessed/training-data/natural_language_inference/60,Why Is This a Good Idea ?,20,0,7
dataset/preprocessed/training-data/natural_language_inference/60,Our technique brings several important benefits to NLP applications .,21,0,10
dataset/preprocessed/training-data/natural_language_inference/60,"First , it is embedding - agnostic , meaning that one of the main ( and perhaps most important ) hyperparameters in NLP pipelines is made obsolete .",22,0,28
dataset/preprocessed/training-data/natural_language_inference/60,"Second , as we will show , it leads to improved performance on a variety of tasks .",23,0,18
dataset/preprocessed/training-data/natural_language_inference/60,"Third , and perhaps most importantly , it allows us to overcome common pitfalls with current systems :",24,0,18
dataset/preprocessed/training-data/natural_language_inference/60,One of the main problems with NLP systems is dealing with out - ofvocabulary words : our method increases lexical coverage by allowing systems to take the union over different embeddings .,26,0,32
dataset/preprocessed/training-data/natural_language_inference/60,"are often trained on a single domain , such as Wikipedia or newswire .",27,0,14
dataset/preprocessed/training-data/natural_language_inference/60,"With our method , embeddings from different domains can be combined , optionally while taking into account contextual information .",28,0,20
dataset/preprocessed/training-data/natural_language_inference/60,"Multi-modality Multi-modal information has proven useful in many tasks , yet the question of multi-modal fusion remains an open problem .",29,0,21
dataset/preprocessed/training-data/natural_language_inference/60,Our method offers a straightforward solution for combining information from different modalities .,30,0,13
dataset/preprocessed/training-data/natural_language_inference/60,"While it is often unclear how to evaluate word embedding performance , our method allows for inspecting the weights that networks assign to different embeddings , providing a direct , task - specific , evaluation method for word embeddings .",32,0,40
dataset/preprocessed/training-data/natural_language_inference/60,Interpretability and Linguistic Analysis Different word embeddings work well on different tasks .,33,0,13
dataset/preprocessed/training-data/natural_language_inference/60,"This is well - known in the field , but knowing why this happens is less wellunderstood .",34,0,18
dataset/preprocessed/training-data/natural_language_inference/60,"Our method sheds light on which embeddings are preferred in which linguistic contexts , for different tasks , and allows us to speculate as to why that is the case .",35,0,31
dataset/preprocessed/training-data/natural_language_inference/60,"In what follows , we explore dynamic meta-embeddings and show that this method outperforms the naive concatenation of various word embeddings , while being more efficient .",37,0,27
dataset/preprocessed/training-data/natural_language_inference/60,"We apply the technique in a BiLSTM - max sentence encoder and evaluate it on wellknown tasks in the field : natural language inference ( SNLI and MultiNLI ; 4 ) , sentiment analysis ( SST ; 5 ) , and image - caption retrieval ( Flickr30k ; 6 ) .",38,0,51
dataset/preprocessed/training-data/natural_language_inference/60,In each case we show state - of - the - art performance within the class of single sentence encoder models .,39,0,22
dataset/preprocessed/training-data/natural_language_inference/60,"Furthermore , we include an extensive analysis ( 7 ) to highlight the general usefulness of our technique and to illustrate how it can lead to new insights .",40,0,29
dataset/preprocessed/training-data/natural_language_inference/60,"Thanks to their widespread popularity in NLP , a sprawling literature has emerged about learning and applying word embeddings - much too large to fully cover here , so we focus on previous work that combines multiple embeddings for downstream tasks .",42,0,42
dataset/preprocessed/training-data/natural_language_inference/60,combine unsupervised embeddings with supervised ones for sentiment classification .,43,0,10
dataset/preprocessed/training-data/natural_language_inference/60,and learn to combine word - level and character - level embeddings .,44,0,13
dataset/preprocessed/training-data/natural_language_inference/60,"Contextual representations have been used in neural machine translation as well , e.g. for learning contextual word vectors and applying them in other tasks or for learning context - dependent representations to solve dis ambiguation problems in machine translation .",45,0,40
dataset/preprocessed/training-data/natural_language_inference/60,"Neural tensor skip - gram models learn to combine word , topic and context embeddings ; context2vec learns a more sophisticated context representation separately from target embeddings ; and learn word representations with distributed word representation with multi-contextual mixed embedding .",46,0,41
dataset/preprocessed/training-data/natural_language_inference/60,"Recent work in "" meta-embeddings "" , which ensembles embedding sets , has been gaining traction ) - here , we show that the idea can be applied in context , and to sentence representations .",47,0,36
dataset/preprocessed/training-data/natural_language_inference/60,"Furthermore , these works obtain metaembeddings as a preprocessing step , rather than learning them dynamically in a supervised setting , as we do here .",48,0,26
dataset/preprocessed/training-data/natural_language_inference/60,"Similarly to , who proposed deep contextualized word representations derived from language models and which led to impressive performance on a variety of tasks , our method allows for contextualization , in this case of embedding set weights .",49,0,39
dataset/preprocessed/training-data/natural_language_inference/88,Long Short - Term Memory - Networks for Machine Reading,2,1,10
dataset/preprocessed/training-data/natural_language_inference/88,In this paper we address the question of how to render sequence - level networks better at handling structured input .,4,0,21
dataset/preprocessed/training-data/natural_language_inference/88,We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention .,5,0,23
dataset/preprocessed/training-data/natural_language_inference/88,The reader extends the Long Short - Term Memory architecture with a memory network in place of a single memory cell .,6,0,22
dataset/preprocessed/training-data/natural_language_inference/88,"This enables adaptive memory usage during recurrence with neural attention , offering away to weakly induce relations among tokens .",7,0,20
dataset/preprocessed/training-data/natural_language_inference/88,The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder - decoder architecture .,8,0,25
dataset/preprocessed/training-data/natural_language_inference/88,"Experiments on language modeling , sentiment analysis , and natural language inference show that our model matches or outperforms the state of the art .",9,0,25
dataset/preprocessed/training-data/natural_language_inference/88,The FBI is chasing a criminal on the run .,10,0,10
dataset/preprocessed/training-data/natural_language_inference/88,The The FBI is chasing a criminal on the run .,11,0,11
dataset/preprocessed/training-data/natural_language_inference/88,The The FBI FBI is chasing a criminal on the run .,12,0,12
dataset/preprocessed/training-data/natural_language_inference/88,The The FBI FBI is is chasing a criminal on the run .,13,0,13
dataset/preprocessed/training-data/natural_language_inference/88,The The FBI FBI is is chasing chasing a criminal on the run .,14,0,14
dataset/preprocessed/training-data/natural_language_inference/88,The The FBI FBI is is chasing chasing a a criminal on the run .,15,0,15
dataset/preprocessed/training-data/natural_language_inference/88,The The FBI FBI is is chasing chasing a a criminal criminal on the run .,16,0,16
dataset/preprocessed/training-data/natural_language_inference/88,The The FBI FBI is is chasing chasing a a criminal criminal on on the run .,17,0,17
dataset/preprocessed/training-data/natural_language_inference/88,The The FBI FBI is is chasing chasing a a criminal criminal on on the the run .,18,0,18
dataset/preprocessed/training-data/natural_language_inference/88,The The FBI FBI is is chasing chasing a a criminal criminal on on the the run run .,19,0,19
dataset/preprocessed/training-data/natural_language_inference/88,How can a sequence - level network induce relations which are presumed latent during text processing ?,21,0,17
dataset/preprocessed/training-data/natural_language_inference/88,How can a recurrent network attentively memorize longer sequences in a way that humans do ?,22,0,16
dataset/preprocessed/training-data/natural_language_inference/88,In this paper we design a machine reader that automatically learns to understand text .,23,0,15
dataset/preprocessed/training-data/natural_language_inference/88,"The term machine reading is related to a wide range of tasks from answering reading comprehension questions , to fact and relation extraction , ontology learning , and textual entailment .",24,0,31
dataset/preprocessed/training-data/natural_language_inference/88,"Rather than focusing on a specific task , we develop a general - purpose reading simula - tor , drawing inspiration from human language processing and the fact language comprehension is incremental with readers continuously extracting the meaning of utterances on a word - by - word basis .",25,0,49
dataset/preprocessed/training-data/natural_language_inference/88,"In order to understand texts , our machine reader should provide facilities for extracting and representing meaning from natural language text , storing meanings internally , and working with stored meanings to derive further consequences .",26,0,36
dataset/preprocessed/training-data/natural_language_inference/88,"Ideally , such a system should be robust , open - domain , and degrade gracefully in the presence of semantic representations which maybe incomplete , inaccurate , or incomprehensible .",27,0,31
dataset/preprocessed/training-data/natural_language_inference/88,"It would also be desirable to simulate the behavior of English speakers who process text sequentially , from left to right , fixating nearly every word while they read and creating partial representations for sentence prefixes .",28,0,37
dataset/preprocessed/training-data/natural_language_inference/88,Language modeling tools such as recurrent neural networks ( RNN ) bode well with human reading behavior .,29,0,18
dataset/preprocessed/training-data/natural_language_inference/88,"RNNs treat each sentence as a sequence of words and recursively compose each word with its previous memory , until the meaning of the whole sentence has been derived .",30,0,30
dataset/preprocessed/training-data/natural_language_inference/88,"In practice , however , sequence - level networks are met with at least three challenges .",31,0,17
dataset/preprocessed/training-data/natural_language_inference/88,"The first one concerns model training problems associated with vanishing and exploding gradients , which can be partially ameliorated with gated activation functions , such as the Long Short - Term Memory ( LSTM ) , and gradient clipping .",32,0,40
dataset/preprocessed/training-data/natural_language_inference/88,The second issue relates to memory compression problems .,33,0,9
dataset/preprocessed/training-data/natural_language_inference/88,"As the input sequence gets compressed and blended into a single dense vector , suf - Color red represents the current word being fixated , blue represents memories .",34,0,29
dataset/preprocessed/training-data/natural_language_inference/88,Shading indicates the degree of memory activation .,35,0,8
dataset/preprocessed/training-data/natural_language_inference/88,ficiently large memory capacity is required to store past information .,36,0,11
dataset/preprocessed/training-data/natural_language_inference/88,"As a result , the network generalizes poorly to long sequences while wasting memory on shorter ones .",37,0,18
dataset/preprocessed/training-data/natural_language_inference/88,"Finally , it should be acknowledged that sequence - level networks lack a mechanism for handling the structure of the input .",38,0,22
dataset/preprocessed/training-data/natural_language_inference/88,This imposes an inductive bias which is at odds with the fact that language has inherent structure .,39,0,18
dataset/preprocessed/training-data/natural_language_inference/88,"In this paper , we develop a text processing system which addresses these limitations while maintaining the incremental , generative property of a recurrent language model .",40,0,27
dataset/preprocessed/training-data/natural_language_inference/88,Recent attempts to render neural networks more structure aware have seen the incorporation of external memories in the context of recurrent neural networks .,41,0,24
dataset/preprocessed/training-data/natural_language_inference/88,The idea is to use multiple memory slots outside the recurrence to piece - wise store representations of the input ; read and write operations for each slot can be modeled as an attention mechanism with a recurrent controller .,42,0,40
dataset/preprocessed/training-data/natural_language_inference/88,We also leverage memory and attention to empower a recurrent network with stronger memorization capability and more importantly the ability to discover relations among tokens .,43,0,26
dataset/preprocessed/training-data/natural_language_inference/88,This is realized by inserting a memory network module in the update of a recurrent network together with attention for memory addressing .,44,0,23
dataset/preprocessed/training-data/natural_language_inference/88,"The attention acts as a weak inductive module discovering relations between input tokens , and is trained without direct supervision .",45,0,21
dataset/preprocessed/training-data/natural_language_inference/88,"As a point of departure from previous work , the memory network we employ is internal to the recurrence , thus strengthening the interaction of the two and leading to a representation learner which is able to rea-son over shallow structures .",46,0,42
dataset/preprocessed/training-data/natural_language_inference/88,"The resulting model , which we term Long Short - Term Memory - Network ( LSTMN ) , is a reading simulator that can be used for sequence processing tasks .",47,0,31
dataset/preprocessed/training-data/natural_language_inference/88,illustrates the reading behavior of the LSTMN .,48,0,8
dataset/preprocessed/training-data/natural_language_inference/88,The model processes text incrementally while learning which past tokens in the memory and to what extent they relate to the current token being processed .,49,0,26
dataset/preprocessed/training-data/natural_language_inference/62,Explicit Utilization of General Knowledge in Machine Reading Comprehension,2,1,9
dataset/preprocessed/training-data/natural_language_inference/62,"To bridge the gap between Machine Reading Comprehension ( MRC ) models and human beings , which is mainly reflected in the hunger for data and the robustness to noise , in this paper , we explore how to integrate the neural networks of MRC models with the general knowledge of human beings .",4,1,54
dataset/preprocessed/training-data/natural_language_inference/62,"On the one hand , we propose a data enrichment method , which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage - question pair .",5,0,31
dataset/preprocessed/training-data/natural_language_inference/62,"On the other hand , we propose an end - to - end MRC model named as Knowledge Aided Reader ( KAR ) , which explicitly uses the above extracted general knowledge to assist its attention mechanisms .",6,0,38
dataset/preprocessed/training-data/natural_language_inference/62,"Based on the data enrichment method , KAR is comparable in performance with the state - of - the - art MRC models , and significantly more robust to noise than them .",7,0,33
dataset/preprocessed/training-data/natural_language_inference/62,"When only a subset ( 20 % - 80 % ) of the training examples are available , KAR outperforms the state - of the - art MRC models by a large margin , and is still reasonably robust to noise .",8,0,42
dataset/preprocessed/training-data/natural_language_inference/62,"Machine Reading Comprehension ( MRC ) , as the name suggests , requires a machine to read a passage and answer its relevant questions .",10,0,25
dataset/preprocessed/training-data/natural_language_inference/62,"Since the answer to each question is supposed to stem from the corresponding passage , a common MRC solution is to develop a neural - network - based MRC model that predicts an answer span ( i.e. the answer start position and the answer end position ) from the passage of each given passage - question pair .",11,0,58
dataset/preprocessed/training-data/natural_language_inference/62,"To facilitate the explorations and innovations in this are a , many MRC datasets have been established , such as SQuAD , MS MARCO , and Trivi - a QA .",12,0,31
dataset/preprocessed/training-data/natural_language_inference/62,"Consequently , many pioneering MRC models have been proposed , such as BiDAF , R - NET , and QANet .",13,0,21
dataset/preprocessed/training-data/natural_language_inference/62,"According to the leader board of SQuAD , the state - of - the - art MRC models have achieved the same performance as human beings .",14,0,27
dataset/preprocessed/training-data/natural_language_inference/62,"However , does this imply that they have possessed the same reading comprehension ability as human beings ?",15,0,18
dataset/preprocessed/training-data/natural_language_inference/62,OF COURSE NOT .,16,0,4
dataset/preprocessed/training-data/natural_language_inference/62,"There is a huge gap between MRC models and human beings , which is mainly reflected in the hunger for data and the robustness to noise .",17,0,27
dataset/preprocessed/training-data/natural_language_inference/62,"On the one hand , developing MRC models requires a large amount of training examples ( i.e. the passage - question pairs labeled with answer spans ) , while human beings can achieve good performance on evaluation examples ( i.e. the passage - question pairs to address ) without training examples .",18,0,52
dataset/preprocessed/training-data/natural_language_inference/62,"On the other hand , revealed that intentionally injected noise ( e.g. misleading sentences ) in evaluation examples causes the performance of MRC models to drop significantly , while human beings are far less likely to suffer from this .",19,0,40
dataset/preprocessed/training-data/natural_language_inference/62,"The reason for these phenomena , we believe , is that MRC models can only utilize the knowledge contained in each given passagequestion pair , but in addition to this , human beings can also utilize general knowledge .",20,0,39
dataset/preprocessed/training-data/natural_language_inference/62,A typical category of general knowledge is inter-word semantic connections .,21,0,11
dataset/preprocessed/training-data/natural_language_inference/62,"As shown in , such general knowledge is essential to the reading comprehension ability of human beings .",22,0,18
dataset/preprocessed/training-data/natural_language_inference/62,A promising strategy to bridge the gap mentioned above is to integrate the neural networks of MRC models with the general knowledge of human beings .,23,0,26
dataset/preprocessed/training-data/natural_language_inference/62,"To this end , it is necessary to solve two problems : extracting general knowledge from passagequestion pairs and utilizing the extracted general knowledge in the prediction of answer spans .",24,0,31
dataset/preprocessed/training-data/natural_language_inference/62,"The first problem can be solved with knowledge bases , which store general knowledge in structured forms .",25,0,18
dataset/preprocessed/training-data/natural_language_inference/62,"A broad variety of knowledge bases are available , such as WordNet storing semantic knowledge , ConceptNet storing commonsense knowledge , and",26,0,22
dataset/preprocessed/training-data/natural_language_inference/62,"Answer Teachers may use a lesson plan to facilitate student learning , providing a course of study which is called the curriculum .",28,0,23
dataset/preprocessed/training-data/natural_language_inference/62,What can a teacher use to help students learn ?,29,0,10
dataset/preprocessed/training-data/natural_language_inference/62,"Manufacturing accounts for a significant but declining share of employment , although the city 's garment industry is showing a resurgence in Brooklyn .",31,0,24
dataset/preprocessed/training-data/natural_language_inference/62,In what borough is the garment business prominent ?,32,0,9
dataset/preprocessed/training-data/natural_language_inference/62,"Two examples about the importance of inter-word semantic connections to the reading comprehension ability of human beings : in the first one , we can find the answer because we know "" facilitate "" is a synonym of "" help "" ; in the second one , we can find the answer because we know "" Brooklyn "" is a hyponym of "" borough "" .",34,0,66
dataset/preprocessed/training-data/natural_language_inference/62,Freebase storing factoid knowledge .,35,0,5
dataset/preprocessed/training-data/natural_language_inference/62,"In this paper , we limit the scope of general knowledge to inter-word semantic connections , and thus use WordNet as our knowledge base .",36,0,25
dataset/preprocessed/training-data/natural_language_inference/62,The existing way to solve the second problem is to encode general knowledge in vector space so that the encoding results can be used to enhance the lexical or contextual representations of words .,37,0,34
dataset/preprocessed/training-data/natural_language_inference/62,"However , this is an implicit way to utilize general knowledge , since in this way we can neither understand nor control the functioning of general knowledge .",38,0,28
dataset/preprocessed/training-data/natural_language_inference/62,"In this paper , we discard the existing implicit way and instead explore an explicit ( i.e. understandable and controllable ) way to utilize general knowledge .",39,0,27
dataset/preprocessed/training-data/natural_language_inference/62,The contribution of this paper is two - fold .,40,0,10
dataset/preprocessed/training-data/natural_language_inference/62,"On the one hand , we propose a data enrichment method , which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage - question pair .",41,0,31
dataset/preprocessed/training-data/natural_language_inference/62,"On the other hand , we propose an end - to - end MRC model named as Knowledge Aided Reader ( KAR ) , which explicitly uses the above extracted general knowledge to assist its attention mechanisms .",42,0,38
dataset/preprocessed/training-data/natural_language_inference/62,"Based on the data enrichment method , KAR is comparable in performance with the state - of - the - art MRC models , and significantly more robust to noise than them .",43,0,33
dataset/preprocessed/training-data/natural_language_inference/62,"When only a subset ( 20 % - 80 % ) of the training examples are available , KAR outperforms the stateof - the - art MRC models by a large margin , and is still reasonably robust to noise .",44,0,41
dataset/preprocessed/training-data/natural_language_inference/62,"In this section , we elaborate a WordNet - based data enrichment method , which is aimed at extracting inter-word semantic connections from each passage - question pair in our MRC dataset .",47,0,33
dataset/preprocessed/training-data/natural_language_inference/62,"The extraction is performed in a controllable manner , and the extracted results are provided as general knowledge to our MRC model .",48,0,23
dataset/preprocessed/training-data/natural_language_inference/62,Semantic Relation Chain,49,0,3
dataset/preprocessed/training-data/natural_language_inference/49,Published as a conference paper at ICLR 2018 NATURAL LANGUAGE INFERENCE OVER INTERACTION SPACE,2,1,14
dataset/preprocessed/training-data/natural_language_inference/49,Natural Language Inference ( NLI ) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis .,4,0,26
dataset/preprocessed/training-data/natural_language_inference/49,"We introduce Interactive Inference Network ( IIN ) , a novel class of neural network architectures that is able to achieve high - level understanding of the sentence pair by hierarchically extracting semantic features from interaction space .",5,0,38
dataset/preprocessed/training-data/natural_language_inference/49,"We show that an interaction tensor ( attention weight ) contains semantic information to solve natural language inference , and a denser interaction tensor contains richer semantic information .",6,0,29
dataset/preprocessed/training-data/natural_language_inference/49,"One instance of such architecture , Densely Interactive Inference Network ( DIIN ) , demonstrates the state - of - the - art performance on large scale NLI copora and large - scale NLI alike corpus .",7,0,37
dataset/preprocessed/training-data/natural_language_inference/49,It 's noteworthy that DIIN achieve a greater than 20 % error reduction on the challenging Multi - Genre NLI ( MultiNLI ; Williams et al. 2017 ) dataset with respect to the strongest published system .,8,0,37
dataset/preprocessed/training-data/natural_language_inference/49,"Natural Language Inference ( NLI also known as recognizing textual entiailment , or RTE ) task requires one to determine whether the logical relationship between two sentences is among entailment ( if the premise is true , then the hypothesis must be true ) , contradiction ( if the premise is true , then the hypothesis must be false ) and neutral ( neither entailment nor contradiction ) .",10,1,69
dataset/preprocessed/training-data/natural_language_inference/49,"NLI is known as a fundamental and yet challenging task for natural language understanding , not only because it requires one to identify the language pattern , but also to understand certain commonsense knowledge .",11,0,35
dataset/preprocessed/training-data/natural_language_inference/49,"In , three samples from MultiNLI corpus show solving the task requires one to handle the full complexity of lexical and compositional semantics .",12,0,24
dataset/preprocessed/training-data/natural_language_inference/49,The previous work on NLI ( or RTE ) has extensively researched on conventional approaches ) .,13,0,17
dataset/preprocessed/training-data/natural_language_inference/49,Recent progress on NLI is enabled by the availability of 570 k human annotated dataset and the advancement of representation learning technique .,14,0,23
dataset/preprocessed/training-data/natural_language_inference/49,"Among the core representation learning techniques , attention mechanism is broadly applied in many NLU tasks since its introduction : machine translation , abstractive summarization , Reading Comprehension , dialog system , etc .",15,0,34
dataset/preprocessed/training-data/natural_language_inference/49,"As described by , "" An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .",16,0,41
dataset/preprocessed/training-data/natural_language_inference/49,"The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key "" .",17,0,34
dataset/preprocessed/training-data/natural_language_inference/49,"Attention mechanism is known for its alignment between representations , focusing one part of representation over another , and modeling the dependency regardless of sequence length .",18,0,27
dataset/preprocessed/training-data/natural_language_inference/49,"Observing attention 's powerful capability , we hypothesize that the attention weight can assist with machine to understanding the text .",19,0,21
dataset/preprocessed/training-data/natural_language_inference/49,"A regular attention weight , the core component of the attention mechanism , encodes the crosssentence word relationship into a alignment matrix .",20,0,23
dataset/preprocessed/training-data/natural_language_inference/49,"However , a multi-head attention weight Vaswani et al.",21,0,9
dataset/preprocessed/training-data/natural_language_inference/49,"can encode such interaction into multiple alignment matrices , which shows a more powerful alignment .",22,0,16
dataset/preprocessed/training-data/natural_language_inference/49,"In this work , we push the multi-head attention to a extreme by building a word - by - word dimension - wise alignment tensor which we call interaction tensor .",23,0,31
dataset/preprocessed/training-data/natural_language_inference/49,The interaction tensor encodes the high - order alignment relationship between sentences pair .,24,0,14
dataset/preprocessed/training-data/natural_language_inference/49,"Our experiments demonstrate that by capturing the rich semantic features in the interaction tensor , we are able to solve natural language inference task well , especially in cases with paraphrase , antonyms and overlapping words .",25,0,37
dataset/preprocessed/training-data/natural_language_inference/49,We dub the general framework as Interactive Inference Network ( IIN ) .,26,0,13
dataset/preprocessed/training-data/natural_language_inference/49,"To the best of our knowledge , it is the first attempt to solve natural language inference task in the interaction space .",27,0,23
dataset/preprocessed/training-data/natural_language_inference/49,"We further explore one instance of Interactive Inference Network , Densely Interactive Inference Network ( DIIN ) , which achieves new state - of - the - art performance on both SNLI and MultiNLI copora .",28,0,36
dataset/preprocessed/training-data/natural_language_inference/49,"To test the generality of the architecture , we interpret the paraphrase identification task as natural language inference task where matching as entailment , not - matching as neutral .",29,0,30
dataset/preprocessed/training-data/natural_language_inference/49,"We test the model on Quora Question Pair dataset , which contains over 400 k real world question pair , and achieves new state - of - the - art performance .",30,0,32
dataset/preprocessed/training-data/natural_language_inference/49,"We introduce the related work in Section 2 , and discuss the general framework of IIN along with a specific instance that enjoys state - of - the - art performance on multiple datasets in Section 3 .",31,0,38
dataset/preprocessed/training-data/natural_language_inference/49,We describe experiments and analysis in Section 4 .,32,0,9
dataset/preprocessed/training-data/natural_language_inference/49,"Finally , we conclude and discuss future work in Section 5 .",33,0,12
dataset/preprocessed/training-data/natural_language_inference/49,The early exploration on NLI mainly rely on conventional methods and small scale datasets .,35,0,15
dataset/preprocessed/training-data/natural_language_inference/49,The availability of SNLI dataset with 570 k human annotated sentence pairs has enabled a good deal of progress on natural language understanding .,36,0,24
dataset/preprocessed/training-data/natural_language_inference/49,"The essential representation learning techniques for NLU such as attention , memory and the use of parse structure are studied on the SNLI which serves as an important benchmark for sentence understanding .",37,0,33
dataset/preprocessed/training-data/natural_language_inference/49,The models trained on NLI task can be divided into two categories : ( i ) sentence encoding - based model which aims to find vector representation for each sentence and classifies the relation by using the concatenation of two vector representation along with their absolute element - wise difference and element - wise product .,38,0,56
dataset/preprocessed/training-data/natural_language_inference/49,( ii ) Joint feature models which use the cross sentence feature or attention from one sentence to another .,39,0,20
dataset/preprocessed/training-data/natural_language_inference/49,"After neural attention mechanism is successfully applied on the machine translation task , such technique has became widely used in both natural language process and computer vision domains .",40,0,29
dataset/preprocessed/training-data/natural_language_inference/49,"Many variants of attention technique such as hard - attention , self - attention , multi-hop attention , bidirectional attention and multi-head attention are also introduced to tackle more complicated tasks .",41,0,32
dataset/preprocessed/training-data/natural_language_inference/49,"Before this work , neural attention mechanism is mainly used to make alignment , focusing on specific part of the representation .",42,0,22
dataset/preprocessed/training-data/natural_language_inference/49,"In this work , we want to show that attention weight contains rich semantic information required for understanding the logical relationship between sentence pair .",43,0,25
dataset/preprocessed/training-data/natural_language_inference/49,"Though RNN or LSTM are very good for variable length sequence modeling , using Convolutional neural network in NLU tasks is very desirable because of its parallelism in computation .",44,0,30
dataset/preprocessed/training-data/natural_language_inference/49,"Convolutional structure has been successfully applied in various domain such as machine translation , sentence classification , text matching and sentiment analysis , etc .",45,0,25
dataset/preprocessed/training-data/natural_language_inference/49,"The convolution structure is also applied on different level of granularity such as byte , character , word and sentences levels .",46,0,22
dataset/preprocessed/training-data/natural_language_inference/49,INTERACTIVE INFERENCE NETWORK,48,0,3
dataset/preprocessed/training-data/natural_language_inference/49,The Interactive Inference Network ( IIN ) is a hierarchical multi-stage process and consists of five components .,49,0,18
dataset/preprocessed/training-data/natural_language_inference/71,Gated Orthogonal Recurrent Units : On Learning to Forget,2,0,9
dataset/preprocessed/training-data/natural_language_inference/71,We present a novel recurrent neural network ( RNN ) based model that combines the remembering ability of unitary RNNs with the ability of gated RNNs to effectively forget redundant / irrelevant information in its memory .,4,1,37
dataset/preprocessed/training-data/natural_language_inference/71,We achieve this by extending unitary RNNs with a gating mechanism .,5,0,12
dataset/preprocessed/training-data/natural_language_inference/71,"Our model is able to outperform LSTMs , GRUs and Unitary RNNs on several long - term dependency benchmark tasks .",6,0,21
dataset/preprocessed/training-data/natural_language_inference/71,We empirically both show the orthogonal / unitary RNNs lack the ability to forget and also the ability of GORU to simultaneously remember long term dependencies while forgetting irrelevant information .,7,0,31
dataset/preprocessed/training-data/natural_language_inference/71,This plays an important role in recurrent neural networks .,8,0,10
dataset/preprocessed/training-data/natural_language_inference/71,"We provide competitive results along with an analysis of our model on many natural sequential tasks including the bAbI Question Answering , TIMIT speech spectrum prediction , Penn TreeBank , and synthetic tasks that involve long - term dependencies such as algorithmic , parenthesis , denoising and copying tasks .",9,0,50
dataset/preprocessed/training-data/natural_language_inference/71,Recurrent Neural Networks with gating units - such as Long Short Term Memory ( LSTMs ) and Gated Recurrent Units ( GRUs ) ),11,1,24
dataset/preprocessed/training-data/natural_language_inference/71,"- have led to rapid progress in different are as of machine learning such as language modeling , neural machine translation , and speech recognition .",12,0,26
dataset/preprocessed/training-data/natural_language_inference/71,These works have proven the importance of gating units for Recurrent Neural Networks .,13,1,14
dataset/preprocessed/training-data/natural_language_inference/71,The main advantage of using these gated units in RNNs is primarily due to the ease of optimization of the models using them and to reduce the learning degeneracies such as vanishing gradients that can cripple conventional RNNs .,14,1,39
dataset/preprocessed/training-data/natural_language_inference/71,"Most importantly , by designing special gates , it is easier to impose a particular behavior on the model , such as creating shortcut connections through time by using input and forget gates in LSTMs and resetting the memory via the reset gate of a GRU .",15,0,47
dataset/preprocessed/training-data/natural_language_inference/71,This feature also brings modularity to the neural network design that seems to make training of those models easier .,16,0,20
dataset/preprocessed/training-data/natural_language_inference/71,Gated RNNs are also empirically shown to achieve better results for a wide variety of real - world tasks .,17,0,20
dataset/preprocessed/training-data/natural_language_inference/71,"Recently , using unitary and orthogonal matrices ( instead of general matrices ) in RNNs have attracted an increasing amount of attention in the machine learning community .",18,0,28
dataset/preprocessed/training-data/natural_language_inference/71,This trend was following the demonstration that these matrices can be effective in solving tasks involving long - term dependencies and gradients vanishing / exploding problem .,19,0,27
dataset/preprocessed/training-data/natural_language_inference/71,Thus a unitary / orthogonal RNN can capture long term dependencies more effectively in sequential data than a conventional RNN or LSTM .,20,0,23
dataset/preprocessed/training-data/natural_language_inference/71,"As a result , this type of model has been shown to perform well on tasks that would require rote memorization and simple reasoning , such as the copy task ( Hochreiter and Schmidhuber 1997 ) and the sequential MNIST .",21,0,41
dataset/preprocessed/training-data/natural_language_inference/71,Those models can just be viewed as an extension to vanilla RNNs ) that replaces the transition matrices with either unitary or orthogonal matrices .,22,0,25
dataset/preprocessed/training-data/natural_language_inference/71,"In this paper , we refer the ability of a model to omit parts of the input sequence that contain redundant information and to filter out the noise input in general as the means of a forgetting mechanism .",23,0,39
dataset/preprocessed/training-data/natural_language_inference/71,"Previously have shown the importance of the forgetting mechanism for LSTM networks and with very similar motivations , we discuss the utilization of a forgetting mechanism for RNNs with orthogonal transitions .",24,0,32
dataset/preprocessed/training-data/natural_language_inference/71,"The importance of forgetting for those networks is mainly due to that unitary / orthogonal RNNs can backpropagate the gradients without vanishing through time , and it is very easy for them to just have an output that depends on equal amounts of all the elements of the whole input sequence .",25,0,52
dataset/preprocessed/training-data/natural_language_inference/71,"From this perspective , learning to forget can be difficult with unitary / orthogonal RNNs and they can clog up the memory with useless information .",26,0,26
dataset/preprocessed/training-data/natural_language_inference/71,"However , most real - world applications and natural tasks require the model to filter out irrelevant or redundant information from the input sequence .",27,0,25
dataset/preprocessed/training-data/natural_language_inference/71,"We argue that difficulties of forgetting can cause unitary and orthogonal RNNs to perform badly on many realistic tasks , and demonstrate this empirically with a toy task .",28,0,29
dataset/preprocessed/training-data/natural_language_inference/71,"We propose a new architecture , the Gated Orthogonal Recurrent Unit ( GORU ) , which combines the advantages of the above two frameworks , namely ( i ) the ability to capture long term dependencies by using orthogonal matrices and ( ii ) the ability to "" forget "" by using a GRU structure .",29,0,56
dataset/preprocessed/training-data/natural_language_inference/71,"We demonstrate that GORU is able to learn long term dependencies effectively , even in complicated datasets which require a forgetting ability .",30,0,23
dataset/preprocessed/training-data/natural_language_inference/71,"In this work , we focus on implementation of orthogonal transition matrices which is just a subset of the unitary matrices .",31,0,22
dataset/preprocessed/training-data/natural_language_inference/71,"GORU outperforms a recent variation of unitary RNN called EURNN ) on language modeling , denoising , parenthesis and the question answering tasks .",32,0,24
dataset/preprocessed/training-data/natural_language_inference/71,We show that the unitary RNN fails catastrophically on a denoising task which requires the model to forget .,33,0,19
dataset/preprocessed/training-data/natural_language_inference/71,"On question answering , speech spectrum prediction , algorithmic , parenthesis and the denoising tasks , GORU achieves better accuracy on the test set over all other models that we compare against .",34,0,33
dataset/preprocessed/training-data/natural_language_inference/71,"We have attempted to use gates on the unitary matrices with complex numbers , but we encountered some training challenges of training gating mechanisms , thus we have decided to just to focus on orthogonal matrices for this paper .",35,0,40
dataset/preprocessed/training-data/natural_language_inference/71,Given an input sequence x t ?,37,0,7
dataset/preprocessed/training-data/natural_language_inference/71,"R dx , t ? { 1 , 2 , , T } , a vanilla RNN defines a sequence of hidden states ht ?",38,0,25
dataset/preprocessed/training-data/natural_language_inference/71,Rd h updated at each time step according to the rule,39,0,11
dataset/preprocessed/training-data/natural_language_inference/71,where W h ?,40,0,4
dataset/preprocessed/training-data/natural_language_inference/71,"Rd h d h , W x ?",41,0,8
dataset/preprocessed/training-data/natural_language_inference/71,R dxd hand b ?,42,0,5
dataset/preprocessed/training-data/natural_language_inference/71,Rd hare model parameters and ?,43,0,6
dataset/preprocessed/training-data/natural_language_inference/71,is a nonlinear activation function .,44,0,6
dataset/preprocessed/training-data/natural_language_inference/71,RNNs have proven to be effective for solving sequential tasks due to their flexibility to .,45,0,16
dataset/preprocessed/training-data/natural_language_inference/71,"However , a well - known problem called gradient vanishing and gradient explosion has prevented RNNs from efficiently learning long - term dependencies .",46,0,24
dataset/preprocessed/training-data/natural_language_inference/71,"Several approaches have been developed to solve this problem , with LSTMs and GRUs being the most successful and widely used .",47,0,22
dataset/preprocessed/training-data/natural_language_inference/71,Gated Recurrent Unit,48,0,3
dataset/preprocessed/training-data/natural_language_inference/71,"A big step forward from LSTM is the Gated Recurrent Unit ( GRU ) , proposed by Cho et al , , which removed the extra memory state in LSTM .",49,0,31
dataset/preprocessed/training-data/natural_language_inference/3,Modelling Interaction of Sentence Pair with Coupled- LSTMs,2,1,8
dataset/preprocessed/training-data/natural_language_inference/3,"Recently , there is rising interest in modelling the interactions of two sentences with deep neural networks .",4,1,18
dataset/preprocessed/training-data/natural_language_inference/3,"However , most of the existing methods encode two sequences with separate encoders , in which a sentence is encoded with little or no information from the other sentence .",5,0,30
dataset/preprocessed/training-data/natural_language_inference/3,"In this paper , we propose a deep architecture to model the strong interaction of sentence pair with two coupled - LSTMs .",6,0,23
dataset/preprocessed/training-data/natural_language_inference/3,"Specifically , we introduce two coupled ways to model the interdependences of two LSTMs , coupling the local contextualized interactions of two sentences .",7,0,24
dataset/preprocessed/training-data/natural_language_inference/3,We then aggregate these interactions and use a dynamic pooling to select the most informative features .,8,0,17
dataset/preprocessed/training-data/natural_language_inference/3,Experiments on two very large datasets demonstrate the efficacy of our proposed architecture and its superiority to state - of the - art methods .,9,0,25
dataset/preprocessed/training-data/natural_language_inference/3,"Distributed representations of words or sentences have been widely used in many natural language processing ( NLP ) tasks , such as text classification , question answering and machine translation and soon .",11,0,33
dataset/preprocessed/training-data/natural_language_inference/3,"Among these tasks , a common problem is modelling the relevance / similarity of the sentence pair , which is also called text semantic matching .",12,1,26
dataset/preprocessed/training-data/natural_language_inference/3,"Recently , deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses .",13,0,22
dataset/preprocessed/training-data/natural_language_inference/3,"According to the phases of interaction between two sentences , previous models can be classified into three categories .",14,0,19
dataset/preprocessed/training-data/natural_language_inference/3,"Models Some early works focus on sentence level interactions , such as ARC - I , and soon .",16,0,19
dataset/preprocessed/training-data/natural_language_inference/3,"These models first encode two sequences with some basic ( Neural Bagof - words , BOW ) or advanced ( RNN , CNN ) components of neural networks separately , and then compute the matching * Corresponding author .",17,0,39
dataset/preprocessed/training-data/natural_language_inference/3,score based on the distributed vectors of two sentences .,18,0,10
dataset/preprocessed/training-data/natural_language_inference/3,"In this paradigm , two sentences have no interaction until arriving final phase .",19,0,14
dataset/preprocessed/training-data/natural_language_inference/3,"Some improved methods focus on utilizing multi-granularity representation ( word , phrase and sentence level ) , such as MultiGranCNN and Multi - Perspective .",21,0,25
dataset/preprocessed/training-data/natural_language_inference/3,"Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence , such as ABCNN , Attention LSTM .",22,0,30
dataset/preprocessed/training-data/natural_language_inference/3,"These models can alleviate the weak interaction problem , but are still insufficient to model the contextualized interaction on the word as well as phrase level .",23,0,27
dataset/preprocessed/training-data/natural_language_inference/3,Strong Interaction Models,24,0,3
dataset/preprocessed/training-data/natural_language_inference/3,These models directly build an interaction space between two sentences and model the interaction at different positions .,25,0,18
dataset/preprocessed/training-data/natural_language_inference/3,ARC - II and MV - LSTM .,26,0,8
dataset/preprocessed/training-data/natural_language_inference/3,These models enable the model to easily capture the difference between semantic capacity of two sentences .,27,0,17
dataset/preprocessed/training-data/natural_language_inference/3,"In this paper , we propose a new deep neural network architecture to model the strong interactions of two sentences .",28,0,21
dataset/preprocessed/training-data/natural_language_inference/3,"Different with modelling two sentences with separated LSTMs , we utilize two interdependent LSTMs , called coupled - LSTMs , to fully affect each other at different time steps .",29,0,30
dataset/preprocessed/training-data/natural_language_inference/3,The output of coupled - LSTMs at each step depends on both sentences .,30,0,14
dataset/preprocessed/training-data/natural_language_inference/3,"Specifically , we propose two interdependent ways for the coupled - LSTMs : loosely coupled model ( LC - LSTMs ) and tightly coupled model ( TC - LSTMs ) .",31,0,31
dataset/preprocessed/training-data/natural_language_inference/3,"Similar to bidirectional LSTM for single sentence , there are four directions can be used in coupled - LSTMs .",32,0,20
dataset/preprocessed/training-data/natural_language_inference/3,"To utilize all the information of four directions of coupled - LSTMs , we aggregate them and adopt a dynamic pooling strategy to automatically select the most informative interaction signals .",33,0,31
dataset/preprocessed/training-data/natural_language_inference/3,"Finally , we feed them into a fully connected layer , followed by an output layer to compute the matching score .",34,0,22
dataset/preprocessed/training-data/natural_language_inference/3,The contributions of this paper can be summarized as follows .,35,0,11
dataset/preprocessed/training-data/natural_language_inference/3,"Different with the architectures of using similarity matrix , our proposed architecture directly model the strong interactions of two sentences with coupled - LSTMs , which can capture the useful local semantic relevances of two sentences .",37,0,37
dataset/preprocessed/training-data/natural_language_inference/3,Our architecture can also capture the multiple granular interactions by several stacked coupled - LSTMs layers .,38,0,17
dataset/preprocessed/training-data/natural_language_inference/3,"Compared to the previous works on text matching , we perform extensive empirical studies on two very large datasets .",40,0,20
dataset/preprocessed/training-data/natural_language_inference/3,The massive scale of the datasets allows us to train a very deep neural networks .,41,0,16
dataset/preprocessed/training-data/natural_language_inference/3,Experiment results demonstrate that our proposed architecture is more effective than state - of - the - art methods .,42,0,20
dataset/preprocessed/training-data/natural_language_inference/3,Sentence Modelling with LSTM,43,0,4
dataset/preprocessed/training-data/natural_language_inference/3,"Long short - term memory network ( LSTM ) ] is a type of recurrent neural network ( RNN ) , and specifically addresses the issue of learning long - term dependencies .",44,0,33
dataset/preprocessed/training-data/natural_language_inference/3,LSTM maintains a memory cell that updates and exposes its content only when deemed necessary .,45,0,16
dataset/preprocessed/training-data/natural_language_inference/3,"While there are numerous LSTM variants , here we use the LSTM architecture used by , which is similar to the architecture of but without peep - hole connections .",46,0,30
dataset/preprocessed/training-data/natural_language_inference/3,"We define the LSTM units at each time step t to be a collection of vectors in Rd : an input gate it , a forget gate ft , an output gate o t , a memory cell ct and a hidden state ht .",47,0,45
dataset/preprocessed/training-data/natural_language_inference/3,d is the number of the LSTM units .,48,0,9
dataset/preprocessed/training-data/natural_language_inference/3,"The elements of the gating vectors it , ft and o tare in [ 0 , 1 ] .",49,0,19
dataset/preprocessed/training-data/natural_language_inference/25,Contextualized Word Representations for Reading Comprehension,2,1,6
dataset/preprocessed/training-data/natural_language_inference/25,Reading a document and extracting an answer to a question about its content has attracted substantial attention recently .,4,0,19
dataset/preprocessed/training-data/natural_language_inference/25,"While most work has focused on the interaction between the question and the document , in this work we evaluate the importance of context when the question and document are processed independently .",5,0,33
dataset/preprocessed/training-data/natural_language_inference/25,"We take a standard neural architecture for this task , and show that by providing rich contextualized word representations from a large pre-trained language model as well as allowing the model to choose between contextdependent and context - independent word representations , we can obtain dramatic improvements and reach performance comparable to state - of - the - art on the competitive SQUAD dataset .",6,0,65
dataset/preprocessed/training-data/natural_language_inference/25,Reading comprehension ( RC ) is a high - level task in natural language understanding that requires reading a document and answering questions about its content .,8,1,27
dataset/preprocessed/training-data/natural_language_inference/25,"RC has attracted substantial attention over the last few years with the advent of large annotated datasets , computing resources , and neural network models and optimization procedures .",9,1,29
dataset/preprocessed/training-data/natural_language_inference/25,"Reading comprehension models must invariably represent word tokens contextually , as a function of their encompassing sequence ( document or question ) .",10,0,23
dataset/preprocessed/training-data/natural_language_inference/25,"The vast majority of RC systems encode contextualized representations of words in both the document and question as hidden states of bidirectional RNNs , and focus model design and capacity around question - document interaction , carry - ing out calculations where information from both is available .",11,0,48
dataset/preprocessed/training-data/natural_language_inference/25,"Analysis of current RC models has shown that models tend to react to simple word - matching between the question and document , as well as benefit from explicitly providing matching information in model inputs .",12,0,36
dataset/preprocessed/training-data/natural_language_inference/25,"In this work , we hypothesize that the stillrelatively - small size of RC datasets drives this behavior , which leads to models that make limited use of context when representing word tokens .",13,0,34
dataset/preprocessed/training-data/natural_language_inference/25,"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",14,0,45
dataset/preprocessed/training-data/natural_language_inference/25,"This simple addition already places the model 's performance on par with recent work , and allows us to demonstrate the importance of context .",15,0,25
dataset/preprocessed/training-data/natural_language_inference/25,"Motivated by these findings , we turn to a semisupervised setting in which we leverage a language model , pre-trained on large amounts of data , as a sequence encoder which forcibly facilitates context utilization .",16,0,36
dataset/preprocessed/training-data/natural_language_inference/25,"We find that model performance substantially improves , reaching accuracy comparable to state - of - the - art on the competitive SQuAD dataset , showing that contextual word representations captured by the language model are beneficial for reading comprehension .",17,0,41
dataset/preprocessed/training-data/natural_language_inference/25,Contextualized Word Representations,19,0,3
dataset/preprocessed/training-data/natural_language_inference/25,"We consider the task of extractive reading comprehension : given a paragraph of text p = ( p 1 , . . . , p n ) and a question q = ( q 1 , . . . , q m ) , an answer span ( p l , . . . , pr ) is to be extracted , i.e. , a pair of indices",21,0,68
dataset/preprocessed/training-data/natural_language_inference/25,1 ? l ? r ?,22,0,6
dataset/preprocessed/training-data/natural_language_inference/25,n into pare to be predicted .,23,0,7
dataset/preprocessed/training-data/natural_language_inference/25,"When encoding a word token in its encompassing sequence ( question or passage ) , we are interested in allowing extra computation over the sequence and evaluating the extent to which context is utilized in the resultant representation .",24,0,39
dataset/preprocessed/training-data/natural_language_inference/25,"To that end , we employ a re-embedding component in which a contextual and a non-contextual representation are explicitly combined per token .",25,0,23
dataset/preprocessed/training-data/natural_language_inference/25,"Specifically , for a sequence of word - embeddings w 1 , . . . , wk with wt ?",26,0,20
dataset/preprocessed/training-data/natural_language_inference/25,"R dw , the re-embedding of the t- th token wt is the result of a Highway layer and is defined as :",27,0,23
dataset/preprocessed/training-data/natural_language_inference/25,"where x t is a function strictly of the word - type of the t- th token , u t is a function of the enclosing sequence , W g , W z , U g , U z are parameter matrices , and the element - wise product operator .",28,0,51
dataset/preprocessed/training-data/natural_language_inference/25,where the latter is a character - based representation of the token 's word - type produced via a CNN over character embeddings .,30,0,24
dataset/preprocessed/training-data/natural_language_inference/25,"We note that word - embeddings wt are pre-trained and are kept fixed during training , as is commonly done in order to reduce model capacity and mitigate overfitting .",31,0,30
dataset/preprocessed/training-data/natural_language_inference/25,We next describe different formulations for the contextual term u t .,32,0,12
dataset/preprocessed/training-data/natural_language_inference/25,"RNN - based token re-embedding ( TR ) Here we set {u 1 , . . . , u k } = BiLSTM ( x 1 , . . . , x k ) as the hidden states of the top layer in a stacked BiLSTM of multiple layers , each uni-directional LSTM in each layer having d h cells and u k ?",33,0,64
dataset/preprocessed/training-data/natural_language_inference/25,R 2 d h .,34,0,5
dataset/preprocessed/training-data/natural_language_inference/25,LM - augmented token re-embedding ( TR + LM ),35,0,10
dataset/preprocessed/training-data/natural_language_inference/25,"The simple module specified above allows better exploitation of the context that a token appears in , if such exploitation is needed and is not learned by the rest of the network , which operates over w 1 , . . . , wk .",36,0,45
dataset/preprocessed/training-data/natural_language_inference/25,Our findings in Section 4 indicate that context is crucial but that in our setting it maybe utilized to a limited extent .,37,0,23
dataset/preprocessed/training-data/natural_language_inference/25,"We hypothesize that the main determining factor in this behavior is the relatively small size of the data and its distribution , which does not require using long - range context in most examples .",38,0,35
dataset/preprocessed/training-data/natural_language_inference/25,"Therefore , we leverage a strong language model that was pre-trained on large corpora as a fixed encoder which supplies additional contextualized token representations .",39,0,25
dataset/preprocessed/training-data/natural_language_inference/25,"We denote these representations as {o 1 , . . . , o k } and set u t = [u t ; o t ] for {u 1 , . . . , u k } = BiLSTM ( x 1 , . . . , x k ) .",40,0,51
dataset/preprocessed/training-data/natural_language_inference/25,"The LM we use is from , 2 trained on the One Billion Words Benchmark dataset .",41,0,17
dataset/preprocessed/training-data/natural_language_inference/25,"It consists of an initial layer which produces character - based word representations , followed by two stacked LSTM layers and a softmax prediction layer .",42,0,26
dataset/preprocessed/training-data/natural_language_inference/25,The hidden state outputs of each LSTM layer are projected down to a lower dimension via a bottleneck layer .,43,0,20
dataset/preprocessed/training-data/natural_language_inference/25,"We set {o 1 , . . . , o k } to either the projections of the first layer , referred to as TR + LM ( L1 ) , or those of the second one , referred to as TR + LM ( L2 ) .",44,0,48
dataset/preprocessed/training-data/natural_language_inference/25,"With both re-embedding schemes , we use the resulting representations w 1 , . . . , wk as a drop - in replacement for the word - embedding inputs fed to a standard model , described next .",45,0,39
dataset/preprocessed/training-data/natural_language_inference/25,"We build upon , who proposed the RaSoR model .",47,0,10
dataset/preprocessed/training-data/natural_language_inference/25,"For word - embedding inputs q 1 , . . . , q m and p 1 , . . . , p n of dimension d w , RaSoR consists of the following components :",48,0,36
dataset/preprocessed/training-data/natural_language_inference/25,Passage - independent question representation,49,0,5
dataset/preprocessed/training-data/natural_language_inference/40,Linguistic Knowledge as Memory for Recurrent Neural Networks,2,1,8
dataset/preprocessed/training-data/natural_language_inference/40,Training recurrent neural networks to model long term dependencies is difficult .,4,1,12
dataset/preprocessed/training-data/natural_language_inference/40,"Hence , we propose to use external linguistic knowledge as an explicit signal to inform the model which memories it should utilize .",5,0,23
dataset/preprocessed/training-data/natural_language_inference/40,"Specifically , external knowledge is used to augment a sequence with typed edges between arbitrarily distant elements , and the resulting graph is decomposed into directed acyclic subgraphs .",6,0,29
dataset/preprocessed/training-data/natural_language_inference/40,"We introduce a model that encodes such graphs as explicit memory in recurrent neural networks , and use it to model coreference relations in text .",7,0,26
dataset/preprocessed/training-data/natural_language_inference/40,"We apply our model to several text comprehension tasks and achieve new state - of - the - art results on all considered benchmarks , including CNN , bAbi , and LAMBADA .",8,0,33
dataset/preprocessed/training-data/natural_language_inference/40,"On the bAbi QA tasks , our model solves 15 out of the 20 tasks with only 1000 training examples per task .",9,0,23
dataset/preprocessed/training-data/natural_language_inference/40,Analysis of the learned representations further demonstrates the ability of our model to encode fine - grained entity information across a document .,10,0,23
dataset/preprocessed/training-data/natural_language_inference/40,"Sequential data appears in many real world applications involving natural language , videos , speech and financial markets .",12,0,19
dataset/preprocessed/training-data/natural_language_inference/40,Predictions involving such data require accurate modeling of dependencies between elements of the sequence which maybe arbitrarily far apart .,13,0,20
dataset/preprocessed/training-data/natural_language_inference/40,"Deep learning offers the promise of extracting these dependencies in a purely data driven manner , with Recurrent Neural Networks ( RNNs ) being the architecture of choice .",14,0,29
dataset/preprocessed/training-data/natural_language_inference/40,"RNNs show excellent performance when the dependencies of interest range short spans of the sequence , however they can be notoriously hard to train to discover longer range dependencies .",15,0,30
dataset/preprocessed/training-data/natural_language_inference/40,introduced Long Short Term Memory ( LSTM ) networks which use a special unit called the Constant Error Carousel ( CEC ) to alleviate this problem .,16,0,27
dataset/preprocessed/training-data/natural_language_inference/40,The CEC has a memory cell with a constant linear connection to itself which allows gradients to flow overlong distances .,17,0,21
dataset/preprocessed/training-data/natural_language_inference/40,"introduced a simplified version of LSTMs called Gated Recurrent Units ( GRU ) with has one less gate , and consequently fewer parameters .",18,0,24
dataset/preprocessed/training-data/natural_language_inference/40,Both LSTMs and GRUs have been hugely popular for modeling sequence data .,19,0,13
dataset/preprocessed/training-data/natural_language_inference/40,"Despite these extensions , empirical studies have shown that it is still difficult to train RNNs with long - range dependencies ( see for example , ) .",20,0,28
dataset/preprocessed/training-data/natural_language_inference/40,"One suggested explanation for this is that the network must propagate all the information in a single fixed - size vector , which maybe infeasible .",21,0,26
dataset/preprocessed/training-data/natural_language_inference/40,This led to the introduction of the attention mechanism which adapts the sequence model with a more explicit form of long term memory .,22,0,24
dataset/preprocessed/training-data/natural_language_inference/40,"At each time step t , the model can perform a "" soft "" lookup over all previous outputs through a weighted average t?1 i=1 ?",23,0,26
dataset/preprocessed/training-data/natural_language_inference/40,i hi .,24,0,3
dataset/preprocessed/training-data/natural_language_inference/40,The weights ?,25,0,3
dataset/preprocessed/training-data/natural_language_inference/40,i are the outputs of another network whose parameters are learned from data .,26,0,14
dataset/preprocessed/training-data/natural_language_inference/40,Augmenting sequence models with attention has lead to significant improvements in various language modeling domains .,27,0,16
dataset/preprocessed/training-data/natural_language_inference/40,"Other architectures , such as Memory Networks , further build on this idea by introducing a memory module for the soft - lookup operation , and a number of models allow the RNN to hold differentiable "" memories "" of past elements to discover long range correlations .",28,0,48
dataset/preprocessed/training-data/natural_language_inference/40,"However , showed that even memory - augmented neural models do not look beyond the immediately preceding time steps .",29,0,20
dataset/preprocessed/training-data/natural_language_inference/40,"Clearly , training RNNs to discover long range dependencies without an explicit signal is challenging .",30,0,16
dataset/preprocessed/training-data/natural_language_inference/40,In this paper we do not attempt to solve this problem .,31,0,12
dataset/preprocessed/training-data/natural_language_inference/40,"Instead we argue that in many applications , information about long - term dependencies maybe readily available in the form of symbolic knowledge .",32,0,24
dataset/preprocessed/training-data/natural_language_inference/40,"As an example , consider the sequence of text shown in .",33,0,12
dataset/preprocessed/training-data/natural_language_inference/40,"Standard preprocessing tools can be used to extract relations such as coreference and hypernymy between pairs of tokens , which can be added as extra edges in addition to the sequential links between elements .",34,0,35
dataset/preprocessed/training-data/natural_language_inference/40,We argue that these extra signals can be used to provide the RNN model with locations of an explicit memory of distant elements when computing the representation of the current element .,35,0,32
dataset/preprocessed/training-data/natural_language_inference/40,"The content of a memory is the representation of the linked element , and edge labels can distinguish different types of memories .",36,0,23
dataset/preprocessed/training-data/natural_language_inference/40,In this manner symbolic knowledge can guide the propagation of information through a recurrent network .,37,0,16
dataset/preprocessed/training-data/natural_language_inference/40,"Technically , incorporating these "" skip connections "" into the sequence converts it into a graph with cycles .",38,0,19
dataset/preprocessed/training-data/natural_language_inference/40,"Graph based neural networks can be used to handle such data , but they are computationally expensive when the number of nodes in the graph is large .",39,0,28
dataset/preprocessed/training-data/natural_language_inference/40,"Instead , we utilize the order inherent in the the unaugmented sequence to decompose the graph into two Directed Acyclic Graphs ( DAGs ) with a topological ordering .",40,0,29
dataset/preprocessed/training-data/natural_language_inference/40,"We introduce the Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework to compute the representation of such graphs while touching every node only once , and implement a GRU version of it called MAGE - GRU .",41,0,41
dataset/preprocessed/training-data/natural_language_inference/40,"MAGE - RNN learns separate representations for propagation along each edge type , which leads to superior performance empirically .",42,0,20
dataset/preprocessed/training-data/natural_language_inference/40,"In cases where there is at most a single incoming edge of a particular type at anode , it reduces to a memory augmented regular RNN whose memory access is determined by a symbolic signal .",43,0,36
dataset/preprocessed/training-data/natural_language_inference/40,"We use MAGE - RNN to model coreference relations for text comprehension tasks , where answers to a query have to be extracted from a context document .",44,0,28
dataset/preprocessed/training-data/natural_language_inference/40,Tokens in a document are connected by a coreference relation if they refer to the same underlying entity .,45,0,19
dataset/preprocessed/training-data/natural_language_inference/40,"Identifying such relations is important for developing an understanding of the document , and hence we augment RNN architectures for text comprehension with an explicit memory of coreferent mentions .",46,0,30
dataset/preprocessed/training-data/natural_language_inference/40,"MAGE - GRU leads to a consistent improvement over the vanilla GRU , as well as a baseline where the coreference information is added as input features to the model .",47,0,31
dataset/preprocessed/training-data/natural_language_inference/40,"By further replacing GRU units in existing reading comprehension models with MAGE - GRUs we achieve stateof - the - art performance on three well studied benchmarks the b Abi QA tasks , the LAMBADA dataset , and the CNN dataset .",48,0,42
dataset/preprocessed/training-data/natural_language_inference/40,An analysis of the learned representations by the model also show its effectiveness in encoding fine - grained information about the entities in a document .,49,0,26
dataset/preprocessed/training-data/natural_language_inference/39,Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement,2,1,12
dataset/preprocessed/training-data/natural_language_inference/39,"Textual similarity measurement is a challenging problem , as it requires understanding the semantics of input sentences .",4,1,18
dataset/preprocessed/training-data/natural_language_inference/39,"Most previous neural network models use coarse - grained sentence modeling , which has difficulty capturing fine - grained word - level information for semantic comparisons .",5,0,27
dataset/preprocessed/training-data/natural_language_inference/39,"As an alternative , we propose to explicitly model pairwise word interactions and present a novel similarity focus mechanism to identify important correspondences for better similarity measurement .",6,0,28
dataset/preprocessed/training-data/natural_language_inference/39,Our ideas are implemented in a novel neural network architecture that demonstrates state - of the - art accuracy on three SemEval tasks and two answer selection tasks .,7,0,29
dataset/preprocessed/training-data/natural_language_inference/39,"Given two pieces of text , measuring their semantic textual similarity ( STS ) remains a fundamental problem in language research and lies at the core of many language processing tasks , including question answering , query ranking , and paraphrase generation .",9,1,43
dataset/preprocessed/training-data/natural_language_inference/39,"Traditional NLP approaches , e.g. , developing hand - crafted features , suffer from sparsity because of language ambiguity and the limited amount of annotated data available .",10,0,28
dataset/preprocessed/training-data/natural_language_inference/39,"Neural networks and distributed representations can alleviate such sparsity , thus neural network - based models are widely used by recent systems for the STS problem .",11,0,27
dataset/preprocessed/training-data/natural_language_inference/39,"However , most previous neural network approaches are based on sentence modeling , which first maps each input sentence into a fixed - length vector and then performs comparisons on these representations .",12,0,33
dataset/preprocessed/training-data/natural_language_inference/39,"Despite its conceptual simplicity , researchers have raised concerns about this approach :",13,0,13
dataset/preprocessed/training-data/natural_language_inference/39,"Will fine - grained wordlevel information , which is crucial for similarity measurement , get lost in the coarse - grained sentence representations ?",14,0,24
dataset/preprocessed/training-data/natural_language_inference/39,"Is it really effective to "" cram "" whole sentence meanings into fixed - length vectors ?",15,0,17
dataset/preprocessed/training-data/natural_language_inference/39,"In contrast , we focus on capturing fine - grained word - level information directly .",16,0,16
dataset/preprocessed/training-data/natural_language_inference/39,Our contribution is twofold :,17,0,5
dataset/preprocessed/training-data/natural_language_inference/39,"First , instead of using sentence modeling , we propose pairwise word interaction modeling that encourages explicit word context interactions across sentences .",18,0,23
dataset/preprocessed/training-data/natural_language_inference/39,"This is inspired by our own intuitions of how people recognize textual similarity : given two sentences sent 1 and sent 2 , a careful reader might look for corresponding semantic units , which we operationalize in our pairwise word interaction modeling technique ( Sec. 5 ) .",19,0,48
dataset/preprocessed/training-data/natural_language_inference/39,"Second , based on the pairwise word interactions , we describe a novel similarity focus layer which helps the model selectively identify important word interactions depending on their importance for similarity measurement .",20,0,33
dataset/preprocessed/training-data/natural_language_inference/39,"Since not all words are created equal , important words that can make more contributions deserve extra "" focus "" from the model ( Sec. 6 ) .",21,0,28
dataset/preprocessed/training-data/natural_language_inference/39,We conducted thorough evaluations on ten test sets from three SemEval STS competitions and two answer selection tasks .,22,0,19
dataset/preprocessed/training-data/natural_language_inference/39,We outperform the recent multiperspective convolutional neural networks of and demonstrate state - of - the - art accuracy on all five tasks .,23,0,24
dataset/preprocessed/training-data/natural_language_inference/39,"In addition , we conducted ablation studies and visualized our models to show the clear benefits of modeling pairwise word interactions for similarity measurement .",24,0,25
dataset/preprocessed/training-data/natural_language_inference/39,Feature engineering was the dominant approach in most previous work ; different types of sparse features were explored and found useful .,26,0,22
dataset/preprocessed/training-data/natural_language_inference/39,"For example , n- gram overlap features at the word and character levels , syntax features , knowledge - based features using Word - Net and word - alignment features .",27,0,31
dataset/preprocessed/training-data/natural_language_inference/39,The recent shift from sparse feature engineering to neural network model engineering has significantly improved accuracy on STS datasets .,28,0,20
dataset/preprocessed/training-data/natural_language_inference/39,"Most previous work use sentence modeling with a "" Siamese "" structure .",29,0,13
dataset/preprocessed/training-data/natural_language_inference/39,"For example , used convolutional neural networks that combine hierarchical structures with layer - by - layer composition and pooling .",30,0,21
dataset/preprocessed/training-data/natural_language_inference/39,"and concurrently proposed tree - structured long short - term memory networks , which recursively construct sentence representations following their syntactic trees .",31,0,23
dataset/preprocessed/training-data/natural_language_inference/39,There are many other examples of neural network - based sentence modeling approaches for the STS problem .,32,0,18
dataset/preprocessed/training-data/natural_language_inference/39,Sentence modeling is coarse - grained by nature .,33,0,9
dataset/preprocessed/training-data/natural_language_inference/39,"Most recently , despite still using a sentence modeling approach , moved toward finegrained representations by exploiting multiple perspectives of input sentences with different types of convolution filters and pooling , generating a "" matrix "" representation where rows and columns capture different aspects of the sentence ; comparisons over local regions of the representation are then performed .",34,0,59
dataset/preprocessed/training-data/natural_language_inference/39,"achieves highly competitive accuracy , suggesting the usefulness of fine - grained information .",35,0,14
dataset/preprocessed/training-data/natural_language_inference/39,"However , these multiple perspectives are obtained at the cost of increased model complexity , resulting in slow model training .",36,0,21
dataset/preprocessed/training-data/natural_language_inference/39,"In this work , we take a different approach by focusing directly on pairwise word interaction modeling .",37,0,18
dataset/preprocessed/training-data/natural_language_inference/39,3 Model Overview shows our end - to - end model with four major components :,38,0,16
dataset/preprocessed/training-data/natural_language_inference/39,"Bidirectional Long Short - Term Memory Networks ( Bi - LSTMs ) are used for context modeling of input sentences , which serves as the basis for all following components ( Sec. 4 ) .",40,0,35
dataset/preprocessed/training-data/natural_language_inference/39,2 . A novel pairwise word interaction modeling technique encourages direct comparisons between word contexts across sentences ( Sec. 5 ) .,41,0,22
dataset/preprocessed/training-data/natural_language_inference/39,3 . A novel similarity focus layer helps the model identify important pairwise word interactions across sentences ( Sec. 6 ) .,42,0,22
dataset/preprocessed/training-data/natural_language_inference/39,A 19 - layer deep convolutional neural network ( ConvNet ) converts the similarity measurement problem into a pattern recognition problem for final classification ( Sec. 7 ) .,44,0,29
dataset/preprocessed/training-data/natural_language_inference/39,"To our best knowledge this is the first neural network model , a novel hybrid architecture combining Bi - LSTMs and a deep ConvNet , that uses a similarity focus mechanism with selective attention to important pairwise word interactions for the STS problem .",45,0,44
dataset/preprocessed/training-data/natural_language_inference/39,"Our approach only uses pretrained word embeddings , and unlike several previous neural network models , we do not use sparse features , unsupervised model pretraining , syntactic parsers , or external resources like Word Net .",46,0,37
dataset/preprocessed/training-data/natural_language_inference/39,We describe details of each component in the following sections .,47,0,11
dataset/preprocessed/training-data/natural_language_inference/39,Different words occurring in similar semantic contexts of respective sentences have a higher chance to contribute to the similarity measurement .,49,0,21
dataset/preprocessed/training-data/natural_language_inference/73,Reinforced Self - Attention Network : a Hybrid of Hard and Soft Attention for Sequence Modeling,2,0,16
dataset/preprocessed/training-data/natural_language_inference/73,Many natural language processing tasks solely rely on sparse dependencies between a few tokens in a sentence .,4,0,18
dataset/preprocessed/training-data/natural_language_inference/73,"Soft attention mechanisms show promising performance in modeling local / global dependencies by soft probabilities between every two tokens , but they are not effective and efficient when applied to long sentences .",5,0,33
dataset/preprocessed/training-data/natural_language_inference/73,"By contrast , hard attention mechanisms directly select a subset of tokens but are difficult and inefficient to train due to their combinatorial nature .",6,0,25
dataset/preprocessed/training-data/natural_language_inference/73,"In this paper , we integrate both soft and hard attention into one context fusion model , "" reinforced self - attention ( ReSA ) "" , for the mutual benefit of each other .",7,0,35
dataset/preprocessed/training-data/natural_language_inference/73,"In ReSA , a hard attention trims a sequence for a soft self - attention to process , while the soft attention feeds reward signals back to facilitate the training of the hard one .",8,0,35
dataset/preprocessed/training-data/natural_language_inference/73,"For this purpose , we develop a novel hard attention called "" reinforced sequence sampling ( RSS ) "" , selecting tokens in parallel and trained via policy gradient .",9,0,30
dataset/preprocessed/training-data/natural_language_inference/73,"Using two RSS modules , ReSA efficiently extracts the sparse dependencies between each pair of selected tokens .",10,0,18
dataset/preprocessed/training-data/natural_language_inference/73,"We finally propose an RNN / CNN - free sentence - encoding model , "" reinforced self - attention network ( ReSAN ) "" , solely based on ReSA .",11,0,30
dataset/preprocessed/training-data/natural_language_inference/73,It achieves state - of - the - art performance on both Stanford Natural Language Inference ( SNLI ) and Sentences Involving Compositional Knowledge ( SICK ) datasets .,12,0,29
dataset/preprocessed/training-data/natural_language_inference/73,Equipping deep neural networks ( DNN ) with attention mechanisms provides an effective and parallelizable approach for context fusion and sequence compression .,14,1,23
dataset/preprocessed/training-data/natural_language_inference/73,"It achieves compelling time efficiency and state - of - the - art performance in a broad range of natural language processing ( NLP ) tasks , such as neural machine translation , dialogue generation , machine reading / comprehension , natural language inference , sentiment classification , etc .",15,0,50
dataset/preprocessed/training-data/natural_language_inference/73,"Recently , some neural nets based solely on attention , especially self - attention , outperform traditional recurrent or convolutional neural networks on NLP tasks , such as machine translation and sentence embedding , which further demonstrates the power of attention mechanisms in capturing contextual dependencies .",16,0,47
dataset/preprocessed/training-data/natural_language_inference/73,Soft and hard attention are the two main types of attention mechanisms .,17,0,13
dataset/preprocessed/training-data/natural_language_inference/73,"In soft attention , a categorical distribution is calculated over a sequence of elements .",18,0,15
dataset/preprocessed/training-data/natural_language_inference/73,The resulting probabilities reflect the importance of each element and are used as weights to produce a context - aware encoding that is the weighted sum of all elements .,19,0,30
dataset/preprocessed/training-data/natural_language_inference/73,"Hence , soft attention only requires a small number of parameters and less computation time .",20,0,16
dataset/preprocessed/training-data/natural_language_inference/73,"Moreover , soft attention mechanism is fully differentiable and thus can be easily trained by endto - end back - propagation when attached to any existing neural net .",21,0,29
dataset/preprocessed/training-data/natural_language_inference/73,"However , the softmax function usually assigns small but non-zero probabilities to trivial elements , which will weaken the attention given to the few truly significant elements .",22,0,28
dataset/preprocessed/training-data/natural_language_inference/73,"Unlike the widely - studied soft attention , in hard attention , a subset of elements is selected from an input sequence .",23,0,23
dataset/preprocessed/training-data/natural_language_inference/73,"Hard attention mechanism forces a model to concentrate solely on the important elements , entirely discarding the others .",24,0,19
dataset/preprocessed/training-data/natural_language_inference/73,"In fact , various NLP tasks solely rely on very sparse tokens from along text input .",25,0,17
dataset/preprocessed/training-data/natural_language_inference/73,"Hard attention is well suited to these tasks , because it overcomes the weaknesses associated with soft attention in long sequences .",26,0,22
dataset/preprocessed/training-data/natural_language_inference/73,"However , hard attention mechanism is time - inefficient with sequential sampling and non-differentiable by virtue of their combinatorial nature .",27,0,21
dataset/preprocessed/training-data/natural_language_inference/73,"Thus , it can not be optimized through back - propagation and more typically rely on policy gradient , e.g. , REINFORCE .",28,0,23
dataset/preprocessed/training-data/natural_language_inference/73,"As a result , training a hard attention model is usually an inefficient process - some even find convergence difficult - and combining them with other neural nets in an end - to - end manner is problematic .",29,0,39
dataset/preprocessed/training-data/natural_language_inference/73,"However , soft and hard attention mechanisms might be integrated into a single model to benefit each other in overcoming their inherent dis advantages , and this notion motivates our study .",30,0,32
dataset/preprocessed/training-data/natural_language_inference/73,"Specifically , a hard attention mechanism is used to encode rich structural information about the contextual dependencies and trims along sequence into a much shorter one for a soft attention mechanism to process .",31,0,34
dataset/preprocessed/training-data/natural_language_inference/73,"Conversely , the soft one is used to provide a stable environment and strong reward signals to help in training the hard one .",32,0,24
dataset/preprocessed/training-data/natural_language_inference/73,"Such method would improve both the prediction quality of the soft attention mechanism and the trainability of the hard attention mechanism , while boosting the ability to model contextual dependencies .",33,0,31
dataset/preprocessed/training-data/natural_language_inference/73,"To the best of our knowledge , the idea of combining hard and soft attention within a model has not yet been studied .",34,0,24
dataset/preprocessed/training-data/natural_language_inference/73,Existing works focus on only one of the two types .,35,0,11
dataset/preprocessed/training-data/natural_language_inference/73,"In this paper , we first propose a novel hard attention mechanism called "" reinforced sequence sampling ( RSS ) "" , which selects tokens from an input sequence in parallel , and differs from existing ones in that it is highly parallelizable without any recurrent structure .",36,0,48
dataset/preprocessed/training-data/natural_language_inference/73,"We then develop a model , "" reinforced self - attention ( ReSA ) "" , which naturally combines the RSS with a soft self - attention .",37,0,28
dataset/preprocessed/training-data/natural_language_inference/73,"In ReSA , two parameter - untied RSS are respectively applied to two copies of the input sequence , where the tokens from one and another are called dependent and head tokens , respectively .",38,0,35
dataset/preprocessed/training-data/natural_language_inference/73,Re SA only models the sparse dependencies between the head and dependent tokens selected by the two RSS modules .,39,0,20
dataset/preprocessed/training-data/natural_language_inference/73,"Finally , we build an sentence - encoding model , "" reinforced self - attention network ( ReSAN ) "" , based on ReSA without any CNN / RNN structure .",40,0,31
dataset/preprocessed/training-data/natural_language_inference/73,We test ReSAN on natural language inference and semantic relatedness tasks .,41,0,12
dataset/preprocessed/training-data/natural_language_inference/73,"The results show that ReSAN achieves the best test accuracy among all sentence - encoding models on the official leaderboard of the Stanford Natural Language Inference ( SNLI ) dataset , and state - of - the - art performance on the Sentences Involving Compositional Knowledge ( SICK ) dataset .",42,0,51
dataset/preprocessed/training-data/natural_language_inference/73,"Compared to the commonly - used models , ReSAN is more efficient and has better prediction quality than existing recurrent / convolutional neural networks , selfattention networks , and even well - designed models ( e.g. , semantic tree or external memory based models ) .",43,0,46
dataset/preprocessed/training-data/natural_language_inference/73,All the experiments codes are released at https://github.com/ taoshen58/DiSAN /tree/master/ReSAN .,44,0,11
dataset/preprocessed/training-data/natural_language_inference/73,1 ) lowercase denotes a vector ; 2 ) bold lowercase denotes a sequence of vectors ( stored as a matrix ) ; and 3 ) uppercase denotes a matrix or a tensor .,46,0,34
dataset/preprocessed/training-data/natural_language_inference/73,"Given an input sequence x = [ x 1 , . . . , x n ] ?",49,0,18
dataset/preprocessed/training-data/natural_language_inference/74,Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,2,1,11
dataset/preprocessed/training-data/natural_language_inference/74,"Natural Language Inference ( NLI ) , also known as Recognizing Textual Entailment ( RTE ) , is one of the most important problems in natural language processing .",4,1,29
dataset/preprocessed/training-data/natural_language_inference/74,It requires to infer the logical relationship between two given sentences .,5,0,12
dataset/preprocessed/training-data/natural_language_inference/74,"While current approaches mostly focus on the interaction architectures of the sentences , in this paper , we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model .",6,1,36
dataset/preprocessed/training-data/natural_language_inference/74,"We observe that people usually use some discourse markers such as "" so "" or "" but "" to represent the logical relationship between two sentences .",7,0,27
dataset/preprocessed/training-data/natural_language_inference/74,"These words potentially have deep connections with the meanings of the sentences , thus can be utilized to help improve the representations of them .",8,0,25
dataset/preprocessed/training-data/natural_language_inference/74,"Moreover , we use reinforcement learning to optimize a new objective function with a reward defined by the property of the NLI datasets to make full use of the labels information .",9,0,32
dataset/preprocessed/training-data/natural_language_inference/74,Experiments show that our method achieves the state - of - the - art performance on several large - scale datasets .,10,0,22
dataset/preprocessed/training-data/natural_language_inference/74,Here sentences mean either the whole sentences or the main clauses of a compound sentence .,12,0,16
dataset/preprocessed/training-data/natural_language_inference/74,"In this paper , we focus on the task of Natural Language Inference ( NLI ) , which is known as a significant yet challenging task for natural language understanding .",14,0,31
dataset/preprocessed/training-data/natural_language_inference/74,"In this task , we are given two sentences which are respectively called premise and hypothesis .",15,0,17
dataset/preprocessed/training-data/natural_language_inference/74,"The goal is to determine whether the logical relationship between them is entailment , neutral , or contradiction .",16,0,19
dataset/preprocessed/training-data/natural_language_inference/74,"Recently , performance on NLI ) * corresponding author Premise : A soccer game with multiple males playing .",17,0,19
dataset/preprocessed/training-data/natural_language_inference/74,Hypothesis : Some men are playing a sport .,18,0,9
dataset/preprocessed/training-data/natural_language_inference/74,Label : Entailment Premise :,19,0,5
dataset/preprocessed/training-data/natural_language_inference/74,An older and younger man smiling .,20,0,7
dataset/preprocessed/training-data/natural_language_inference/74,Hypothesis : Two men are smiling and laughing at the cats playing on the floor .,21,0,16
dataset/preprocessed/training-data/natural_language_inference/74,Label : Neutral Premise : A black race car starts up in front of a crowd of people Hypothesis :,22,0,20
dataset/preprocessed/training-data/natural_language_inference/74,A man is driving down a lonely road .,23,0,9
dataset/preprocessed/training-data/natural_language_inference/74,Contradiction has been significantly boosted since the release of some high quality large - scale benchmark datasets such as SNLI and MultiNLI. shows some examples in SNLI .,25,0,28
dataset/preprocessed/training-data/natural_language_inference/74,"Most state - of - the - art works focus on the interaction architectures between the premise and the hypothesis , while they rarely concerned the discourse relations of the sentences , which is a core issue in natural language understanding .",26,0,42
dataset/preprocessed/training-data/natural_language_inference/74,People usually use some certain set of words to express the discourse relation between two sentences,27,0,16
dataset/preprocessed/training-data/natural_language_inference/74,"These words , such as "" but "" or "" and "" , are denoted as discourse markers .",29,0,19
dataset/preprocessed/training-data/natural_language_inference/74,"These discourse markers have deep connections with the intrinsic relations of two sentences and intuitively correspond to the intent of NLI , such as "" but "" to "" contradiction "" , "" so "" to "" entailment "" , etc .",30,0,42
dataset/preprocessed/training-data/natural_language_inference/74,Very few NLI works utilize this information revealed by discourse markers .,31,0,12
dataset/preprocessed/training-data/natural_language_inference/74,proposed to use discourse markers to help rep-resent the meanings of the sentences .,32,0,14
dataset/preprocessed/training-data/natural_language_inference/74,"However , they represent each sentence by a single vector and directly concatenate them to predict the answer , which is too simple and not ideal for the largescale datasets .",33,0,31
dataset/preprocessed/training-data/natural_language_inference/74,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",34,0,40
dataset/preprocessed/training-data/natural_language_inference/74,We first propose a sentence encoder model that learns the representations of the sentences from the DMP task and then inject the encoder to the NLI network .,35,0,28
dataset/preprocessed/training-data/natural_language_inference/74,"Moreover , because our NLI datasets are manually annotated , each example from the datasets might get several different labels from the annotators although they will finally come to a consensus and also provide a certain label .",36,0,38
dataset/preprocessed/training-data/natural_language_inference/74,"In consideration of that different confidence level of the final labels should be discriminated , we employ reinforcement learning with a reward defined by the uniformity extent of the original labels to train the model .",37,0,36
dataset/preprocessed/training-data/natural_language_inference/74,The contributions of this paper can be summarized as follows .,38,0,11
dataset/preprocessed/training-data/natural_language_inference/74,"Unlike previous studies , we solve the task of the natural language inference via transferring knowledge from another supervised task .",39,0,21
dataset/preprocessed/training-data/natural_language_inference/74,We propose the Discourse Marker Augmented Network to combine the learned encoder of the sentences with the integrated NLI model .,40,0,21
dataset/preprocessed/training-data/natural_language_inference/74,"According to the property of the datasets , we incorporate reinforcement learning to optimize a new objective function to make full use of the labels ' information .",41,0,28
dataset/preprocessed/training-data/natural_language_inference/74,We conduct extensive experiments on two large - scale datasets to show that our method achieves better performance than other stateof - the - art solutions to the problem .,42,0,30
dataset/preprocessed/training-data/natural_language_inference/74,2 Task Description,43,0,3
dataset/preprocessed/training-data/natural_language_inference/74,2.1 Natural Language Inference ( NLI ),44,0,7
dataset/preprocessed/training-data/natural_language_inference/74,"In the natural language inference tasks , we are given a pair of sentences ( P , H ) , which respectively means the premise and hypothesis .",45,0,28
dataset/preprocessed/training-data/natural_language_inference/74,"Our goal is to judge whether their logical relationship between their meanings by picking a label from a small set : entailment ( The hypothesis is definitely a true description of the premise ) , neutral ( The hypothesis might be a true description of the premise ) , and contradiction ( The hypothesis is definitely a false description of the premise ) .",46,0,64
dataset/preprocessed/training-data/natural_language_inference/74,Discourse Marker Prediction ( DMP ),47,0,6
dataset/preprocessed/training-data/natural_language_inference/74,"For DMP , we are given a pair of sentences ( S 1 , S 2 ) , which is originally the first half and second half of a complete sentence .",48,0,32
dataset/preprocessed/training-data/natural_language_inference/74,The model must predict which discourse marker was used by the author to link the two ideas from a set of candidates .,49,0,23
dataset/preprocessed/training-data/natural_language_inference/43,Evaluating Semantic Parsing against a Simple Web - based Question Answering Model,2,1,12
dataset/preprocessed/training-data/natural_language_inference/43,Semantic parsing shines at analyzing complex natural language that involves composition and computation over multiple pieces of evidence .,4,0,19
dataset/preprocessed/training-data/natural_language_inference/43,"However , datasets for semantic parsing contain many factoid questions that can be answered from a single web document .",5,0,20
dataset/preprocessed/training-data/natural_language_inference/43,"In this paper , we propose to evaluate semantic parsing - based question answering models by comparing them to a question answering baseline that queries the web and extracts the answer only from web snippets , without access to the target knowledge - base .",6,0,45
dataset/preprocessed/training-data/natural_language_inference/43,"We investigate this approach on COMPLEXQUESTIONS , a dataset designed to focus on compositional language , and find that our model obtains reasonable performance ( ? 35 F 1 compared to 41 F 1 of state - of - the - art ) .",7,0,44
dataset/preprocessed/training-data/natural_language_inference/43,"We find in our analysis that our model performs well on complex questions involving conjunctions , but struggles on questions that involve relation composition and superlatives .",8,0,27
dataset/preprocessed/training-data/natural_language_inference/43,"Question answering ( QA ) has witnessed a surge of interest in recent years , as it is one of the prominent tests for natural language understanding .",10,0,28
dataset/preprocessed/training-data/natural_language_inference/43,"QA can be coarsely divided into semantic parsingbased QA , where a question is translated into a logical form that is executed against a knowledgebase , and unstructured QA , where a question is answered directly from some relevant text .",11,0,41
dataset/preprocessed/training-data/natural_language_inference/43,"In semantic parsing , background knowledge has already been compiled into a knowledge - base ( KB ) , and thus the challenge is in interpreting the question , which may contain compositional constructions ( "" What is the second - highest mountain in Europe ? "" ) or computations ( "" What is the difference in population between France and Germany ? "" ) .",12,0,66
dataset/preprocessed/training-data/natural_language_inference/43,"In unstructured QA , the model needs to also interpret the language of a document , and thus most datasets focus on matching the question against the document and extracting the answer from some local context , such as a sentence or a paragraph .",13,0,45
dataset/preprocessed/training-data/natural_language_inference/43,"Since semantic parsing models excel at handling complex linguistic constructions and reasoning over multiple facts , a natural way to examine whether a benchmark indeed requires modeling these properties , is to train an unstructured QA model , and check if it under-performs compared to semantic parsing models .",14,0,49
dataset/preprocessed/training-data/natural_language_inference/43,"If questions can be answered by examining local contexts only , then the use of a knowledge - base is perhaps unnecessary .",15,0,23
dataset/preprocessed/training-data/natural_language_inference/43,"However , to the best of our knowledge , only models that utilize the KB have been evaluated on common semantic parsing benchmarks .",16,0,24
dataset/preprocessed/training-data/natural_language_inference/43,The goal of this paper is to bridge this evaluation gap .,17,0,12
dataset/preprocessed/training-data/natural_language_inference/43,"We develop a simple log - linear model , in the spirit of traditional web - based QA systems , that answers questions by querying the web and extracting the answer from returned web snippets .",18,0,36
dataset/preprocessed/training-data/natural_language_inference/43,"Thus , our evaluation scheme is suitable for semantic parsing benchmarks in which the knowledge required for answering questions is covered by the web ( in contrast with virtual assitants for which the knowledge is specific to an application ) .",19,0,41
dataset/preprocessed/training-data/natural_language_inference/43,"We test this model on COMPLEXQUESTIONS , a dataset designed to require more compositionality compared to earlier datasets , such as WEBQUESTIONS and SIMPLEQUESTIONS .",20,0,25
dataset/preprocessed/training-data/natural_language_inference/43,"We find that a simple QA model , despite having no access to the target KB , performs reasonably well on this dataset ( ? 35 F 1 compared to the state - of - the - art of 41 F 1 ) .",21,0,44
dataset/preprocessed/training-data/natural_language_inference/43,"Moreover , for the subset of questions for which the right answer can be found in one of the web snippets , we outperform the semantic parser ( 51.9 F 1 vs. 48.5 F 1 ) .",22,0,37
dataset/preprocessed/training-data/natural_language_inference/43,"We analyze results for different types of compositionality and find that superlatives and relation composition constructions are challenging for a webbased QA system , while conjunctions and events with multiple arguments are easier .",23,0,34
dataset/preprocessed/training-data/natural_language_inference/43,An important insight is that semantic parsers must overcome the mismatch between natural language and formal language .,24,0,18
dataset/preprocessed/training-data/natural_language_inference/43,"Consequently , language that can be easily matched against the web may become challenging to express in logical form .",25,0,20
dataset/preprocessed/training-data/natural_language_inference/43,"For example , the word "" wife "" is anatomic binary relation in natural language , but expressed with a complex binary ?x.?y. Spouse ( x , y ) ? Gender ( x , Female ) in knowledge - bases .",26,0,41
dataset/preprocessed/training-data/natural_language_inference/43,"Thus , some of the complexity of understanding natural language is removed when working with a natural language representation .",27,0,20
dataset/preprocessed/training-data/natural_language_inference/43,"To conclude , we propose to evaluate the extent to which semantic parsing - based QA benchmarks require compositionality by comparing semantic parsing models to a baseline that extracts the answer from short web snippets .",28,0,36
dataset/preprocessed/training-data/natural_language_inference/43,"We obtain reasonable performance on COMPLEXQUESTIONS , and analyze the types of compositionality thatare challenging for a web - based QA model .",29,0,23
dataset/preprocessed/training-data/natural_language_inference/43,"To ensure reproducibility , we release our dataset , which attaches to each example from COMPLEXQUES - TIONS the top - 100 retrieved web snippets .",30,0,26
dataset/preprocessed/training-data/natural_language_inference/43,Problem Setting and Dataset,32,0,4
dataset/preprocessed/training-data/natural_language_inference/43,"Given a training set of triples {q ( i ) , R ( i ) , a ( i ) } N i = 1 , where q ( i ) is a question , R ( i ) is a web result set , and a ( i ) is the answer , our goal is to learn a model that produces an answer a for a new questionresult set pair ( q , R ) .",33,0,78
dataset/preprocessed/training-data/natural_language_inference/43,"A web result set R consists of K (= 100 ) web snippets , where each snippet s i 1 Data can be downloaded from https : //worksheets.codalab.org/worksheets/",34,0,28
dataset/preprocessed/training-data/natural_language_inference/43,"0x91d77db37e0a4 bbbaeb37b8972f4784 f / R : s1 : Billy Batts ( Character ) - Biography - IMDb Billy Batts ( Character ) on IMDb : Movies , TV , Celebs , and more ...",35,0,34
dataset/preprocessed/training-data/natural_language_inference/43,... Devino is portrayed by Frank Vincent in the film Goodfellas .,36,0,12
dataset/preprocessed/training-data/natural_language_inference/43,Page last updated by !!! de leted !!!,37,0,8
dataset/preprocessed/training-data/natural_language_inference/43,s 2 : Frank Vincent,38,0,5
dataset/preprocessed/training-data/natural_language_inference/43,"He appeared in Scorsese 's 1990 film Goodfellas , where he played Billy Batts , a made man in the Gambino crime family .",40,0,24
dataset/preprocessed/training-data/natural_language_inference/43,He also played a role in Scorsese 's ... . . . s 100 : Voice - over in Goodfellas,41,0,20
dataset/preprocessed/training-data/natural_language_inference/43,"In the summer when they played cards all night , nobody ever called the cops .",42,0,16
dataset/preprocessed/training-data/natural_language_inference/43,But we had a problem with Billy Batts .,44,0,9
dataset/preprocessed/training-data/natural_language_inference/43,This was a touchy thing .,45,0,6
dataset/preprocessed/training-data/natural_language_inference/43,Tommy had killed a made man .,46,0,7
dataset/preprocessed/training-data/natural_language_inference/43,Billy was apart of the Bambino crew and untouchable .,47,0,10
dataset/preprocessed/training-data/natural_language_inference/43,"Before you ... q : "" who played the part of billy batts in goodfellas ? "" a : "" Frank Vincent "" has a title and a text fragment .",48,0,31
dataset/preprocessed/training-data/natural_language_inference/43,An example for a training example is provided in .,49,0,10
dataset/preprocessed/training-data/natural_language_inference/16,Bilateral Multi - Perspective Matching for Natural Language Sentences,2,0,9
dataset/preprocessed/training-data/natural_language_inference/16,Natural language sentence matching is a fundamental technology for a variety of tasks .,4,1,14
dataset/preprocessed/training-data/natural_language_inference/16,Previous approaches either match sentences from a single direction or only apply single granular ( wordby - word or sentence - by- sentence ) matching .,5,0,26
dataset/preprocessed/training-data/natural_language_inference/16,"In this work , we propose a bilateral multi-perspective matching ( BiMPM ) model .",6,0,15
dataset/preprocessed/training-data/natural_language_inference/16,"Given two sentences P and Q , our model first encodes them with a BiL - STM encoder .",7,0,19
dataset/preprocessed/training-data/natural_language_inference/16,"Next , we match the two encoded sentences in two directions P against Q and Q against P .",8,0,19
dataset/preprocessed/training-data/natural_language_inference/16,"In each matching direction , each time step of one sentence is matched against all timesteps of the other sentence from multiple perspectives .",9,0,24
dataset/preprocessed/training-data/natural_language_inference/16,"Then , another BiLSTM layer is utilized to aggregate the matching results into a fixed - length matching vector .",10,0,20
dataset/preprocessed/training-data/natural_language_inference/16,"Finally , based on the matching vector , a decision is made through a fully connected layer .",11,0,18
dataset/preprocessed/training-data/natural_language_inference/16,"We evaluate our model on three tasks : paraphrase identification , natural language inference and answer sentence selection .",12,0,19
dataset/preprocessed/training-data/natural_language_inference/16,Experimental results on standard benchmark datasets show that our model achieves the state - of - the - art performance on all tasks .,13,0,24
dataset/preprocessed/training-data/natural_language_inference/16,Natural language sentence matching ( NLSM ) is the task of comparing two sentences and identifying the relationship between them .,15,1,21
dataset/preprocessed/training-data/natural_language_inference/16,It is a fundamental technology for a variety of tasks .,16,0,11
dataset/preprocessed/training-data/natural_language_inference/16,"For example , in a paraphrase identification task , NLSM is used to determine whether two sentences are paraphrase or not .",17,1,22
dataset/preprocessed/training-data/natural_language_inference/16,"For a natural language inference task , NLSM is utilized to judge whether a hypothesis sentence can be inferred from a premise sentence .",18,0,24
dataset/preprocessed/training-data/natural_language_inference/16,"For question answering and information retrieval tasks , NLSM is employed to assess the relevance between query - answer pairs and rank all the candidate answers .",19,0,27
dataset/preprocessed/training-data/natural_language_inference/16,"For machine comprehension tasks , NLSM is used for matching a passage with a question and pointing out the correct answer span .",20,0,23
dataset/preprocessed/training-data/natural_language_inference/16,"With the renaissance of neural network models , two types of deep learning frameworks were proposed for NLSM .",21,0,19
dataset/preprocessed/training-data/natural_language_inference/16,"The first framework is based on the "" Siamese "" architecture .",22,0,12
dataset/preprocessed/training-data/natural_language_inference/16,"In this framework , the same neural network encoder ( e.g. , a CNN or a RNN ) is applied to two input sentences individually , so that both of the two sentences are encoded into sentence vectors in the same embedding space .",23,0,44
dataset/preprocessed/training-data/natural_language_inference/16,"Then , a matching decision is made solely based on the two sentence vectors .",24,0,15
dataset/preprocessed/training-data/natural_language_inference/16,"The advantage of this framework is that sharing parameters makes the model smaller and easier to train , and the sentence vectors can be used for visualization , sentence clustering and many other purposes .",25,0,35
dataset/preprocessed/training-data/natural_language_inference/16,"However , a dis advantage is that there is no explicit interaction between the two sentences during the encoding procedure , which may lose some important information .",26,0,28
dataset/preprocessed/training-data/natural_language_inference/16,"To deal with this problem , a second framework "" matchingaggregation "" has been proposed .",27,0,16
dataset/preprocessed/training-data/natural_language_inference/16,"Under this framework , smaller units ( such as words or contextual vectors ) of the two sentences are firstly matched , and then the matching results are aggregated ( by a CNN or a LSTM ) into a vector to make the final decision .",28,0,46
dataset/preprocessed/training-data/natural_language_inference/16,"The new framework captures more interactive features between the two sentences , therefore it acquires significant improvements .",29,0,18
dataset/preprocessed/training-data/natural_language_inference/16,"However , the previous "" matchingaggregation "" approaches still have some limitations .",30,0,13
dataset/preprocessed/training-data/natural_language_inference/16,"First , some of the approaches only explored the word - by - word matching , but ignored other granular matchings ( e.g. , phrase - by - sentence ) ; Second , the matching is only performed in a single direction ( e.g. , matching P against Q ) , but neglected the reverse direction ( e.g. , matching Q against P ) .",31,0,65
dataset/preprocessed/training-data/natural_language_inference/16,"In this paper , to tackle these limitations , we propose a bilateral multi-perspective matching ( BiMPM ) model for NLSM tasks .",32,0,23
dataset/preprocessed/training-data/natural_language_inference/16,"Our model essentially belongs to the "" matching aggregation "" framework .",33,0,12
dataset/preprocessed/training-data/natural_language_inference/16,"Given two sentences P and Q , our model first encodes them with a bidirectional Long Short - Term Memory Network ( BiLSTM ) .",34,0,25
dataset/preprocessed/training-data/natural_language_inference/16,"Next , we match the two encoded sentences in two directions P ? Q and P ?",35,0,17
dataset/preprocessed/training-data/natural_language_inference/16,"In each matching direction , let 's say P ?",37,0,10
dataset/preprocessed/training-data/natural_language_inference/16,"Q , each time step of Q is matched against all time - steps of P from multiple perspectives .",38,0,20
dataset/preprocessed/training-data/natural_language_inference/16,"Then , another BiLSTM layer is utilized to aggregate the matching results into a fixed - length matching vector .",39,0,20
dataset/preprocessed/training-data/natural_language_inference/16,"Finally , based on the matching vector , a decision is made through a fully connected layer .",40,0,18
dataset/preprocessed/training-data/natural_language_inference/16,"We evaluate our model on three NLSM tasks : paraphrase identification , natural lan - guage inference and answer sentence selection .",41,0,22
dataset/preprocessed/training-data/natural_language_inference/16,Experimental results on standard benchmark datasets show that our model achieves the state - of - the - art performance on all tasks .,42,0,24
dataset/preprocessed/training-data/natural_language_inference/16,"In following parts , we start with a brief definition of the NLSM task ( Section 2 ) , followed by the details of our model ( Section 3 ) .",43,0,31
dataset/preprocessed/training-data/natural_language_inference/16,Then we evaluate our model on standard benchmark datasets ( Section 4 ) .,44,0,14
dataset/preprocessed/training-data/natural_language_inference/16,"We talk about related work in Section 5 , and conclude this work in Section 6 .",45,0,17
dataset/preprocessed/training-data/natural_language_inference/16,"Formally , we can represent each example of the NLSM task as a triple ( P , Q , y ) , where P = ( p 1 , ... , p j , ... , p M ) is a sentence with a length M , Q = ( q 1 , ... , q i , ... , q N ) is the second sentence with a length N , y ?",47,0,74
dataset/preprocessed/training-data/natural_language_inference/16,"Y is the label representing the relationship between P and Q , and Y is a set of taskspecific labels .",48,0,21
dataset/preprocessed/training-data/natural_language_inference/16,"The NLSM task can be represented as estimating a conditional probability Pr ( y|P , Q ) based on the training set , and predicting the relationship for testing examples by y * = arg max y?Y Pr ( y|P , Q ) .",49,0,44
dataset/preprocessed/training-data/natural_language_inference/10,Learning to Compose Task - Specific Tree Structures,2,1,8
dataset/preprocessed/training-data/natural_language_inference/10,"For years , recursive neural networks ( Rv NNs ) have been shown to be suitable for representing text into fixed - length vectors and achieved good performance on several natural language processing tasks .",4,0,35
dataset/preprocessed/training-data/natural_language_inference/10,"However , the main drawback of RvNNs is that they require structured input , which makes data preparation and model implementation hard .",5,0,23
dataset/preprocessed/training-data/natural_language_inference/10,"In this paper , we propose Gumbel Tree - LSTM , a novel tree - structured long short - term memory architecture that learns how to compose task - specific tree structures only from plain text data efficiently .",6,1,39
dataset/preprocessed/training-data/natural_language_inference/10,Our model uses Straight - Through Gumbel - Softmax estimator to decide the parent node among candidates dynamically and to calculate gradients of the discrete decision .,7,0,27
dataset/preprocessed/training-data/natural_language_inference/10,"We evaluate the proposed model on natural language inference and sentiment analysis , and show that our model outperforms or is at least comparable to previous models .",8,0,28
dataset/preprocessed/training-data/natural_language_inference/10,We also find that our model converges significantly faster than other models .,9,0,13
dataset/preprocessed/training-data/natural_language_inference/10,"Techniques for mapping natural language into vector space have received a lot of attention , due to their capability of representing ambiguous semantics of natural language using dense vectors .",11,0,30
dataset/preprocessed/training-data/natural_language_inference/10,"Among them , methods of learning representations of words , e.g. word2vec or GloVe , are relatively well - studied empirically and theoretically , and some of them became typical choices to consider when initializing word representations for better performance at downstream tasks .",12,0,44
dataset/preprocessed/training-data/natural_language_inference/10,"Meanwhile , research on sentence representation is still in active progress , and accordingly various architecturesdesigned with different intuition and tailored for different tasks - are being proposed .",13,0,29
dataset/preprocessed/training-data/natural_language_inference/10,"In the midst of them , three architectures are most frequently used in obtaining sentence representation from words .",14,0,19
dataset/preprocessed/training-data/natural_language_inference/10,"Convolutional neural networks ( CNNs ) utilize local distribution of words to encode sentences , similar to n-gram models .",15,0,20
dataset/preprocessed/training-data/natural_language_inference/10,Recurrent neural networks ( RNNs ) encode sentences by reading words in sequential order .,16,0,15
dataset/preprocessed/training-data/natural_language_inference/10,"Recursive neural networks Copyright c 2018 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",17,0,18
dataset/preprocessed/training-data/natural_language_inference/10,All rights reserved .,18,0,4
dataset/preprocessed/training-data/natural_language_inference/10,"( RvNNs 1 ) ) , on which this paper focuses , rely on structured input ( e.g. parse tree ) to encode sentences , based on the intuition that there is significant semantics in the hierarchical structure of words .",19,0,41
dataset/preprocessed/training-data/natural_language_inference/10,"It is also notable that RvNNs are generalization of RNNs , as linear chain structures on which RNNs operate are equivalent to left - or right - skewed trees .",20,0,30
dataset/preprocessed/training-data/natural_language_inference/10,"Although there is significant benefit in processing a sentence in a tree - structured recursive manner , data annotated with parse trees could be expensive to prepare and hard to be computed in batches ) .",21,0,36
dataset/preprocessed/training-data/natural_language_inference/10,"Furthermore , the optimal hierarchical composition of words might differ depending on the properties of a task .",22,0,18
dataset/preprocessed/training-data/natural_language_inference/10,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require structured data and learns to compose task - specific tree structures without explicit guidance .",23,0,36
dataset/preprocessed/training-data/natural_language_inference/10,"Our Gumbel Tree - LSTM model is based on tree - structured long short - term memory ( Tree - LSTM ) architecture , which is one of the most renowned variants of RvNN .",24,0,35
dataset/preprocessed/training-data/natural_language_inference/10,"To learn how to compose task - specific tree structures without depending on structured input , our model introduces composition query vector that measures validity of a composition .",25,0,29
dataset/preprocessed/training-data/natural_language_inference/10,"Using validity scores computed by the composition query vector , our model recursively selects compositions until only a single representation remains .",26,0,22
dataset/preprocessed/training-data/natural_language_inference/10,We use Straight - Through ( ST ) Gumbel - Softmax estimator to sample compositions in the training phase .,27,0,20
dataset/preprocessed/training-data/natural_language_inference/10,"ST Gumbel - Softmax estimator relaxes the discrete sampling operation to be continuous in the backward pass , thus our model can be trained via the standard backpropagation .",28,0,29
dataset/preprocessed/training-data/natural_language_inference/10,"Also , since the computation is performed layer - wise , our model is easy to implement and naturally supports batched computation .",29,0,23
dataset/preprocessed/training-data/natural_language_inference/10,"From experiments on natural language inference and sentiment analysis tasks , we find that our proposed model outperforms or is at least comparable to previous sentence encoder models and converges significantly faster than them .",30,0,35
dataset/preprocessed/training-data/natural_language_inference/10,The contributions of our work are as follows :,31,0,9
dataset/preprocessed/training-data/natural_language_inference/10,We designed a novel sentence encoder architecture that learns to compose task - specific trees from plain text data .,32,0,20
dataset/preprocessed/training-data/natural_language_inference/10,We showed from experiments that the proposed architecture outperforms or is competitive to state - of - the - art models .,33,0,22
dataset/preprocessed/training-data/natural_language_inference/10,We also observed that our model converges faster than others .,34,0,11
dataset/preprocessed/training-data/natural_language_inference/10,"Specifically , we saw that our model significantly outperforms previous RvNN works trained on parse trees in all conducted experiments , from which we hypothesize that syntactic parse tree may not be the best structure for every task and the optimal structure could differ per task .",35,0,47
dataset/preprocessed/training-data/natural_language_inference/10,"In the next section , we briefly introduce previous works which have similar objectives to that of our work .",36,0,20
dataset/preprocessed/training-data/natural_language_inference/10,Then we describe the proposed model in detail and present findings from experiments .,37,0,14
dataset/preprocessed/training-data/natural_language_inference/10,Lastly we summarize the over all content and discuss future work .,38,0,12
dataset/preprocessed/training-data/natural_language_inference/10,There have been several works that aim to learn hierarchical latent structure of text by recursively composing words into sentence representation .,40,0,22
dataset/preprocessed/training-data/natural_language_inference/10,Some of them carry unsupervised learning on structures by making composition operations soft .,41,0,14
dataset/preprocessed/training-data/natural_language_inference/10,"To the best of our knowledge , gated recursive convolutional neural network ( grConv ) ) is the first model of its kind and used as an encoder for neural machine translation .",42,0,33
dataset/preprocessed/training-data/natural_language_inference/10,The grConv architecture uses gating mechanism to control the information flow from children to parent .,43,0,16
dataset/preprocessed/training-data/natural_language_inference/10,gr Conv and its variants are also applied to sentence classification tasks .,44,0,13
dataset/preprocessed/training-data/natural_language_inference/10,Neural tree indexer ( NTI ) utilizes soft hierarchical structures by using Tree - LSTM instead of grConv.,45,0,18
dataset/preprocessed/training-data/natural_language_inference/10,"Although models that operate with soft structures are naturally capable of being trained via backpropagation , the structures predicted by them are ambiguous and thus it is hard to interpret them .",46,0,32
dataset/preprocessed/training-data/natural_language_inference/10,CYK Tree -LSTM resolves this ambiguity while maintaining the soft property by introducing the concept of CYK parsing algorithm ) .,47,0,21
dataset/preprocessed/training-data/natural_language_inference/10,"Though their model reduces the ambiguity by explicitly representing anode as a weighted sum of all candidate compositions , it is memory intensive since the number of candidates linearly increases by depth .",48,0,33
dataset/preprocessed/training-data/natural_language_inference/10,"On the other hand , there exist some previous works that maintain the discreteness of tree composition processes , instead of relying on the soft hierarchical structure .",49,0,28
dataset/preprocessed/training-data/natural_language_inference/86,Multi - Perspective Context Matching for Machine Comprehension,2,1,8
dataset/preprocessed/training-data/natural_language_inference/86,"Previous machine comprehension ( MC ) datasets are either too small to train endto - end deep learning models , or not difficult enough to evaluate the ability of current MC techniques .",4,1,33
dataset/preprocessed/training-data/natural_language_inference/86,"The newly released SQuAD dataset alleviates these limitations , and gives us a chance to develop more realistic MC models .",5,0,21
dataset/preprocessed/training-data/natural_language_inference/86,"Based on this dataset , we propose a Multi - Perspective Context Matching ( MPCM ) model , which is an end - to - end system that directly predicts the answer beginning and ending points in a passage .",6,0,40
dataset/preprocessed/training-data/natural_language_inference/86,Our model first adjusts each word - embedding vector in the passage by multiplying a relevancy weight computed against the question .,7,0,22
dataset/preprocessed/training-data/natural_language_inference/86,"Then , we encode the question and weighted passage by using bi-directional LSTMs .",8,0,14
dataset/preprocessed/training-data/natural_language_inference/86,"For each point in the passage , our model matches the context of this point against the encoded question from multiple perspectives and produces a matching vector .",9,0,28
dataset/preprocessed/training-data/natural_language_inference/86,"Given those matched vectors , we employ another bi-directional LSTM to aggregate all the information and predict the beginning and ending points .",10,0,23
dataset/preprocessed/training-data/natural_language_inference/86,Experimental result on the test set of SQuAD shows that our model achieves a competitive result on the leaderboard .,11,0,20
dataset/preprocessed/training-data/natural_language_inference/86,"To address the weakness of the previous MC datasets , Rajpurkar et al. ( 2016 ) developed the Stanford Question Answering dataset ( SQuAD ) .",12,0,26
dataset/preprocessed/training-data/natural_language_inference/86,"Comparing with other datasets , SQuAD is more",13,0,8
dataset/preprocessed/training-data/natural_language_inference/86,Machine Comprehension ( MC ) is a compelling yet challenging task in both natural language processing and artificial intelligent research .,15,0,21
dataset/preprocessed/training-data/natural_language_inference/86,It s task is to enable machine to understand a given passage and then answer questions related to the passage .,16,0,21
dataset/preprocessed/training-data/natural_language_inference/86,"In recent years , several benchmark datasets have been developed to measure and accelerate the progress of MC technologies .",17,0,20
dataset/preprocessed/training-data/natural_language_inference/86,RCTest is one of the representative datasets .,18,0,8
dataset/preprocessed/training-data/natural_language_inference/86,"It consists of 500 fictional stories and 4 multiple choice questions per story ( 2,000 questions in total ) .",19,0,20
dataset/preprocessed/training-data/natural_language_inference/86,A variety of MC methods were proposed based on this dataset .,20,0,12
dataset/preprocessed/training-data/natural_language_inference/86,"However , the limited size of this dataset prevents researchers from building end - to - end deep neural network models , and the state - of - the - art performances are still dominated by the methods highly relying on hand - crafted features or employing additional knowledge .",21,0,50
dataset/preprocessed/training-data/natural_language_inference/86,"To deal with the scarcity of large scale supervised data , proposed to create millions of Cloze style MC examples automatically from news articles on the CNN and Daily Mail websites .",22,0,32
dataset/preprocessed/training-data/natural_language_inference/86,"They observed that each news article has a number of bullet points , which summarise aspects of the information in the article .",23,0,23
dataset/preprocessed/training-data/natural_language_inference/86,"Therefore , they constructed a corpus of ( passage , question , answer ) triples by replacing one entity in these bullet points at a time with a placeholder .",24,0,30
dataset/preprocessed/training-data/natural_language_inference/86,"Then , the MC task is converted into filling the placeholder in the question with an entity within the corresponding passage .",25,0,22
dataset/preprocessed/training-data/natural_language_inference/86,"Based on this large - scale corpus , several end - to - end deep neural network models are proposed successfully realistic and challenging for several reasons : ( 1 ) it is almost two orders of magnitude larger than previous manually labeled datasets ; ( 2 ) all the questions are human - written , instead of the automatically generated Cloze style questions ; ( 3 ) the answer can bean arbitrary span within the passage , rather than a limited set of multiple choices or entities ; ( 4 ) different forms of reasoning is required for answering these questions .",26,0,103
dataset/preprocessed/training-data/natural_language_inference/86,"In this work , we focus on the SQuAD dataset and propose an end - to - end deep neural network model for machine comprehension .",27,0,26
dataset/preprocessed/training-data/natural_language_inference/86,Our basic assumption is that a span in a passage is more likely to be the correct answer if the context of this span is very similar to the question .,28,0,31
dataset/preprocessed/training-data/natural_language_inference/86,"Based on this assumption , we design a Multi - Perspective Context Matching ( MPCM ) model to identify the answer span by matching the context of each point in the passage with the question from multiple perspectives .",29,0,39
dataset/preprocessed/training-data/natural_language_inference/86,"Instead of enumerating all the possible spans explicitly and ranking them , our model identifies the answer span by predicting the beginning and ending points individually with globally normalized probability distributions across the whole passage .",30,0,36
dataset/preprocessed/training-data/natural_language_inference/86,Ablation studies show that all components in our MPCM model are crucial .,31,0,13
dataset/preprocessed/training-data/natural_language_inference/86,Experimental result on the test set of SQuAD shows that our model achieves a competitive result on the leaderboard .,32,0,20
dataset/preprocessed/training-data/natural_language_inference/86,"In following parts , we start with a brief definition of the MC task ( Section 2 ) , followed by the details of our MPCM model ( Section 3 ) .",33,0,32
dataset/preprocessed/training-data/natural_language_inference/86,Then we evaluate our model on the SQuAD dataset ( Section 4 ) .,34,0,14
dataset/preprocessed/training-data/natural_language_inference/86,"Generally , a MC instance involves a question , a passage containing the answer , and the correct answer span within the passage .",36,0,24
dataset/preprocessed/training-data/natural_language_inference/86,"To do well on this task , a MC model need to comprehend the question , reason among the passage , and then identify the answer span .",37,0,28
dataset/preprocessed/training-data/natural_language_inference/86,demonstrates three examples from SQuAD .,38,0,6
dataset/preprocessed/training-data/natural_language_inference/86,"Formally , we can represent the SQuAD dataset as a set of tuples ( Q , P , A ) , where Q = ( q 1 , ... , q i , ... , q M ) is the question with a length M , P = ( p 1 , ... , p j , ... , p N ) is the passage with a length N , and A = ( a b , a e ) is the answer span , ab and a e are the beginning and ending points and 1 ? ab ? a e ?",39,0,103
dataset/preprocessed/training-data/natural_language_inference/86,"The MC task can be represented as estimating the conditional probability Pr ( A|Q , P ) based on the training set , and predicting answers for testing instances by",41,0,30
dataset/preprocessed/training-data/natural_language_inference/86,where A ( P ) is a set of answer candidates from P .,42,0,14
dataset/preprocessed/training-data/natural_language_inference/86,"As the size of A ( P ) is in the order of O ( N 2 ) , we make a simple independent assumption of predicting the beginning and endding points , and simplify the model as",43,0,38
dataset/preprocessed/training-data/natural_language_inference/86,"where Pr ( a b | Q , P ) ( or Pr ( a e | Q , P ) ) is the probability of the ab - th ( or a e - th ) position ( point ) of P to be the beginning ( or ending ) point of the answer span .",44,0,57
dataset/preprocessed/training-data/natural_language_inference/86,Multi - Perspective Context Matching Model,45,0,6
dataset/preprocessed/training-data/natural_language_inference/86,"In this section , we propose a Multi - Perspective Context Matching ( MPCM ) model to estimate probability distributions Pr ( a b | Q , P ) and Pr ( a e | Q , P ) .",46,0,40
dataset/preprocessed/training-data/natural_language_inference/86,shows the architecture of our MPCM model .,47,0,8
dataset/preprocessed/training-data/natural_language_inference/86,"The predictions of Pr ( a b | Q , P ) and Pr ( a e | Q , P ) only differentiate at the last prediction layer .",48,0,30
dataset/preprocessed/training-data/natural_language_inference/86,And all other layers below the prediction layer are shared .,49,0,11
dataset/preprocessed/training-data/natural_language_inference/46,The NarrativeQA Reading Comprehension Challenge,2,1,5
dataset/preprocessed/training-data/natural_language_inference/46,"Reading comprehension ( RC ) - in contrast to information retrieval - requires integrating information and reasoning about events , entities , and their relations across a full document .",4,1,30
dataset/preprocessed/training-data/natural_language_inference/46,"Question answering is conventionally used to assess RC ability , in both artificial agents and children learning to read .",5,1,20
dataset/preprocessed/training-data/natural_language_inference/46,"However , existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information ( e.g. , local context similarity or global term frequency ) ; they thus fail to test for the essential integrative aspect of RC .",6,0,46
dataset/preprocessed/training-data/natural_language_inference/46,"To encourage progress on deeper comprehension of language , we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts .",7,0,35
dataset/preprocessed/training-data/natural_language_inference/46,These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience .,8,0,25
dataset/preprocessed/training-data/natural_language_inference/46,"We show that although humans solve the tasks easily , standard RC models struggle on the tasks presented here .",9,0,20
dataset/preprocessed/training-data/natural_language_inference/46,We provide an analysis of the dataset and the challenges it presents .,10,0,13
dataset/preprocessed/training-data/natural_language_inference/46,Natural language understanding seeks to create models that read and comprehend text .,12,0,13
dataset/preprocessed/training-data/natural_language_inference/46,"A common strategy for assessing the language understanding capabilities of comprehension models is to demonstrate that they can answer questions about documents they read , akin to how reading comprehension is tested in children when they are learning to read .",13,0,41
dataset/preprocessed/training-data/natural_language_inference/46,"After reading a document , a reader usually can not reproduce Title : Ghostbusters II Question : How is Oscar related to Dana ?",14,0,24
dataset/preprocessed/training-data/natural_language_inference/46,Answer : her son Summary snippet : . . .,15,0,10
dataset/preprocessed/training-data/natural_language_inference/46,"Peter 's former girlfriend Dana Barrett has had a son , Oscar . . .",16,0,15
dataset/preprocessed/training-data/natural_language_inference/46,Story snippet :,17,0,3
dataset/preprocessed/training-data/natural_language_inference/46,DANA ( setting the wheel brakes on the buggy ),18,0,10
dataset/preprocessed/training-data/natural_language_inference/46,"Thank you ,",19,0,3
dataset/preprocessed/training-data/natural_language_inference/46,I 'll get the hang of this eventually .,21,0,9
dataset/preprocessed/training-data/natural_language_inference/46,"She continues digging in her purse while Frank leans over the buggy and makes funny faces at the baby , OSCAR , a very cute nine - month old boy .",22,0,31
dataset/preprocessed/training-data/natural_language_inference/46,FRANK ( to the baby ),23,0,6
dataset/preprocessed/training-data/natural_language_inference/46,"Hiya , Oscar .",24,0,4
dataset/preprocessed/training-data/natural_language_inference/46,"What do you say , slugger ?",25,0,7
dataset/preprocessed/training-data/natural_language_inference/46,FRANK ( to Dana ),26,0,5
dataset/preprocessed/training-data/natural_language_inference/46,"That 's a good - looking kid you got there , Ms. Barrett .",27,0,14
dataset/preprocessed/training-data/natural_language_inference/46,Figure 1 :,28,0,3
dataset/preprocessed/training-data/natural_language_inference/46,Example question - answer pair .,29,0,6
dataset/preprocessed/training-data/natural_language_inference/46,"The snippets here were extracted by humans from summaries and the full text of movie scripts or books , respectively , and are not provided to the model as supervision or at test time .",30,0,35
dataset/preprocessed/training-data/natural_language_inference/46,"Instead , the model will need to read the full text and locate salient snippets based solely on the question and its reading of the document in order to generate the answer .",31,0,33
dataset/preprocessed/training-data/natural_language_inference/46,"the entire text from memory , but often can answer questions about underlying narrative elements of the document : the salient entities , events , places , and the relations between them .",32,0,33
dataset/preprocessed/training-data/natural_language_inference/46,"Thus , testing understanding requires creation of questions that examine high - level abstractions instead of just facts occurring in one sentence at a time .",33,0,26
dataset/preprocessed/training-data/natural_language_inference/46,"Unfortunately , superficial questions about a document may often be answered successfully ( by both humans and machines ) using a shallow pattern match - ing strategies or guessing based on global salience .",34,0,34
dataset/preprocessed/training-data/natural_language_inference/46,"In the following section , we survey existing QA datasets , showing that they are either too small or answerable by shallow heuristics ( Section 2 ) .",35,0,28
dataset/preprocessed/training-data/natural_language_inference/46,"On the other hand , questions which are not about the surface form of the text , but rather about the underlying narrative , require the formation of more abstract representations about the events and relations expressed in the course of the document .",36,0,44
dataset/preprocessed/training-data/natural_language_inference/46,"Answering such questions requires that readers integrate information which maybe distributed across several statements throughout the document , and generate a cogent answer on the basis of this integrated information .",37,0,31
dataset/preprocessed/training-data/natural_language_inference/46,"That is , they test that the reader comprehends language , not just that it can pattern match .",38,0,19
dataset/preprocessed/training-data/natural_language_inference/46,"We present a new task and dataset , which we call NarrativeQA , which will test and reward artificial agents approaching this level of competence ( Section 3 ) .",39,0,30
dataset/preprocessed/training-data/natural_language_inference/46,"The dataset consists of stories , which are books and movie scripts , with human written questions and answers based solely on human - generated abstractive summaries .",40,0,28
dataset/preprocessed/training-data/natural_language_inference/46,"For the RC tasks , questions maybe answered using just the summaries or the full story text .",41,0,18
dataset/preprocessed/training-data/natural_language_inference/46,We give a short example of a sample movie script from this dataset in .,42,0,15
dataset/preprocessed/training-data/natural_language_inference/46,Fictional stories have a number of advantages as a domain .,43,0,11
dataset/preprocessed/training-data/natural_language_inference/46,"First , they are largely self - contained : beyond the basic fundamental vocabulary of English , all the information about salient entities and concepts required to understand the narrative is present in the document , with the expectation that a reasonably competent language user would be able to understand it .",44,0,52
dataset/preprocessed/training-data/natural_language_inference/46,"1 Second , story summaries are abstractive and generally written by independent authors who know the work only as a reader .",45,0,22
dataset/preprocessed/training-data/natural_language_inference/46,We make the dataset available online .,46,0,7
dataset/preprocessed/training-data/natural_language_inference/46,Review of Reading Comprehension Datasets and Models,48,0,7
dataset/preprocessed/training-data/natural_language_inference/46,"There are a large number of datasets and associated tasks available for the training and evaluation of read - 1 For example , new names and words maybe coined by the author ( e.g. "" muggle "" in Harry Potter novels ) but the reader need only appeal to the book itself to understand the meaning of these concepts , and their place in the narrative .",49,0,67
dataset/preprocessed/training-data/natural_language_inference/26,Semantics - aware BERT for Language Understanding,2,0,7
dataset/preprocessed/training-data/natural_language_inference/26,"The latest work on language representations carefully integrates contextualized features into language model training , which enables a series of success especially in various machine reading comprehension and natural language inference tasks .",4,1,33
dataset/preprocessed/training-data/natural_language_inference/26,"However , the existing language representation models including ELMo , GPT and BERT only exploit plain context - sensitive features such as character or word embeddings .",5,0,27
dataset/preprocessed/training-data/natural_language_inference/26,They rarely consider incorporating structured semantic information which can provide rich semantics for language representation .,6,0,16
dataset/preprocessed/training-data/natural_language_inference/26,"To promote natural language understanding , we propose to incorporate explicit contextual semantics from pre-trained semantic role labeling , and introduce an improved language representation model , Semanticsaware BERT ( Sem BERT ) , which is capable of explicitly absorbing contextual semantics over a BERT backbone .",7,0,47
dataset/preprocessed/training-data/natural_language_inference/26,Sem - BERT keeps the convenient usability of its BERT precursor in alight fine - tuning way without substantial task - specific modifications .,8,0,24
dataset/preprocessed/training-data/natural_language_inference/26,"Compared with BERT , semantics - aware BERT is as simple in concept but more powerful .",9,0,17
dataset/preprocessed/training-data/natural_language_inference/26,It obtains new state - of the - art or substantially improves results on ten reading comprehension and language inference tasks .,10,0,22
dataset/preprocessed/training-data/natural_language_inference/26,"Recently , deep contextual language model ( LM ) has been shown effective for learning universal language representations , achieving state - of - the - art results in a series of flagship natural language understanding ( NLU ) tasks .",12,1,41
dataset/preprocessed/training-data/natural_language_inference/26,"Some prominent examples are Embedding from Language models ( ELMo ) , Generative Pre-trained Transformer ( OpenAI GPT ) , Bidirectional Encoder Representations from Transformers ( BERT ) and Generalized Autoregressive Pretraining ( XLNet ) .",13,0,36
dataset/preprocessed/training-data/natural_language_inference/26,"Providing fine - grained contextual embedding , these pre-trained models could be either easily applied to downstream models as the encoder or used for fine - tuning .",14,0,28
dataset/preprocessed/training-data/natural_language_inference/26,"Despite the success of those well pre-trained language models , we argue that current techniques which only focus on language modeling restrict the power of the pretrained representations .",15,0,29
dataset/preprocessed/training-data/natural_language_inference/26,"The major limitation of existing language models lies in only taking plain contextual features for both representation and training objective , rarely considering explicit contextual semantic clues .",16,0,28
dataset/preprocessed/training-data/natural_language_inference/26,"Even though well pre-trained language models can implicitly represent contextual semantics more or less , they can be further enhanced by incorporating external knowledge .",17,0,25
dataset/preprocessed/training-data/natural_language_inference/26,"To this end , there is a recent trend of incorporating extra knowledge to pre-trained language models .",18,0,18
dataset/preprocessed/training-data/natural_language_inference/26,A number of studies have found deep learning models might not really understand the natural language texts ) and vulnerably suffer from adversarial attacks .,19,0,25
dataset/preprocessed/training-data/natural_language_inference/26,"Through their observation , deep learning models pay great attention to non-significant words and ignore important ones .",20,0,18
dataset/preprocessed/training-data/natural_language_inference/26,"For attractive question answering challenge ) , we observe a number of answers produced by previous models are semantically incomplete ( As shown in Section 6.3 ) , which suggests that the current NLU models suffer from insufficient contextual semantic representation and learning .",21,0,44
dataset/preprocessed/training-data/natural_language_inference/26,"Actually , NLU tasks share the similar task purpose as sentence contextual semantic analysis .",22,0,15
dataset/preprocessed/training-data/natural_language_inference/26,"Briefly , semantic role labeling ( SRL ) over a sentence is to discover who did what to whom , when and why with respect to the central meaning of the sentence , which naturally matches the task target of NLU .",23,0,42
dataset/preprocessed/training-data/natural_language_inference/26,"For example , in question answering tasks , questions are usually formed with who , what , how , when and why , which can be conveniently formulized into the predicateargument relationship in terms of contextual semantics .",24,0,38
dataset/preprocessed/training-data/natural_language_inference/26,"In human language , a sentence usually involves various predicate - argument structures , while neural models encode sentence into embedding representation , with little consideration of the modeling of multiple semantic structures .",25,0,34
dataset/preprocessed/training-data/natural_language_inference/26,"Thus we are motivated to enrich the sentence contextual semantics in multiple predicate - specific argument sequences by presenting SemBERT : Semantics - aware BERT , which is a fine - tuned BERT with explicit contextual semantic clues .",26,0,39
dataset/preprocessed/training-data/natural_language_inference/26,The proposed SemBERT learns the representation in a fine - grained manner and takes both strengths of BERT on plain context representation and explicit semantics for deeper meaning representation .,27,0,30
dataset/preprocessed/training-data/natural_language_inference/26,Our model consists of three components :,28,0,7
dataset/preprocessed/training-data/natural_language_inference/26,1 ) an out - ofshelf semantic role labeler to annotate the input sentences with a variety of semantic role labels ; 2 ) an sequence encoder where a pre-trained language model is used to build representation for input raw texts and the semantic role labels are mapped to embedding in parallel ; 3 ) a semantic integration component to integrate the text representation with the contextual explicit semantic embedding to obtain the joint representation for downstream tasks .,29,0,79
dataset/preprocessed/training-data/natural_language_inference/26,The proposed SemBERT will be directly applied to typical NLU tasks .,30,0,12
dataset/preprocessed/training-data/natural_language_inference/26,"Our model is evaluated on 11 benchmark datasets involving natural language inference , question answering , semantic similarity and text classification .",31,0,22
dataset/preprocessed/training-data/natural_language_inference/26,Sem - BERT obtains new state - of - the - art on SNLI and also obtains significant gains on the GLUE benchmark and SQuAD 2.0 .,32,0,27
dataset/preprocessed/training-data/natural_language_inference/26,Ablation studies and analysis verify that our introduced explicit semantics is essential to the further performance improvement and SemBERT essentially and effectively works as a unified semantics - enriched language representation model 1 .,33,0,34
dataset/preprocessed/training-data/natural_language_inference/26,Background and Related Work,34,0,4
dataset/preprocessed/training-data/natural_language_inference/26,Language Modeling for NLU,35,0,4
dataset/preprocessed/training-data/natural_language_inference/26,Natural language understanding tasks require a comprehensive understanding of natural languages and the ability to do further inference and reasoning .,36,0,21
dataset/preprocessed/training-data/natural_language_inference/26,"A common trend among NLU studies is that models are becoming more and more sophisticated with stacked attention mechanisms or large amount of corpus , resulting in explosive growth of computational cost .",37,0,33
dataset/preprocessed/training-data/natural_language_inference/26,"Notably , well pre-trained contextual language models such as ELMo , GPT ) and BERT ) have been shown powerful to boost NLU tasks to reach new high performance .",38,0,30
dataset/preprocessed/training-data/natural_language_inference/26,Distributed representations have been widely used as a standard part of NLP models due to the ability to capture the local co-occurence of words from large scale unlabeled text ) .,39,0,31
dataset/preprocessed/training-data/natural_language_inference/26,"However , these approaches for learning word vectors only involve a single , context independent representation for each word with litter consideration of contextual encoding in sentence level .",40,0,29
dataset/preprocessed/training-data/natural_language_inference/26,"Thus recently introduced contextual language models including ELMo , GPT , BERT and XLNet fill the gap by strengthening the contextual sentence modeling for better representation , among which BERT uses a different pre-training objective , masked language model , which allows capturing both sides of context , left and right .",41,0,52
dataset/preprocessed/training-data/natural_language_inference/26,"Besides , BERT also introduces a next sentence prediction task that jointly pre-trains text - pair representations .",42,0,18
dataset/preprocessed/training-data/natural_language_inference/26,The latest evaluation shows that BERT is powerful and convenient for downstream NLU tasks .,43,0,15
dataset/preprocessed/training-data/natural_language_inference/26,The major technical improvement over traditional embeddings of these newly proposed language models is that they focus on extracting context - sensitive features from language models .,44,0,27
dataset/preprocessed/training-data/natural_language_inference/26,"When integrating these contextual word embeddings with existing task - specific architectures , ELMo helps boost several major NLP benchmarks including question answering on SQuAD , sentiment analysis , and named entity recognition , while BERT especially shows effective on language understanding tasks on GLUE , MultiNLI and SQ uAD .",45,0,51
dataset/preprocessed/training-data/natural_language_inference/26,"In this work , we follow this line of extracting context - sensitive features and take pre-trained BERT as our backbone encoder for jointly learning explicit context semantics .",46,0,29
dataset/preprocessed/training-data/natural_language_inference/26,Explicit Contextual Semantics,47,0,3
dataset/preprocessed/training-data/natural_language_inference/26,"Although distributed representations including the latest advanced pre-trained contextual language models have already been strengthened by semantics to some extent from linguistic sense , we argue such implicit semantics may not be enough to support a powerful contextual representation for NLU , according to our observation on the semantically incomplete answer span generated by BERT on SQuAD , which motivates us to directly introduce explicit semantics .",48,0,67
dataset/preprocessed/training-data/natural_language_inference/26,"There are a few formal semantic frames , including FrameNet and PropBank , in which the latter is more popularly implemented in computational linguistics .",49,0,25
dataset/preprocessed/training-data/natural_language_inference/100,aNMM : Ranking Short Answer Texts with Attention - Based Neural Matching Model,2,0,13
dataset/preprocessed/training-data/natural_language_inference/100,"As an alternative to question answering methods based on feature engineering , deep learning approaches such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) have recently been proposed for semantic matching of questions and answers .",4,1,45
dataset/preprocessed/training-data/natural_language_inference/100,"To achieve good results , however , these models have been combined with additional features such as word overlap or BM25 scores .",5,0,23
dataset/preprocessed/training-data/natural_language_inference/100,"Without this combination , these models perform significantly worse than methods based on linguistic feature engineering .",6,0,17
dataset/preprocessed/training-data/natural_language_inference/100,"In this paper , we propose an attention based neural matching model for ranking short answer text .",7,0,18
dataset/preprocessed/training-data/natural_language_inference/100,We adopt value - shared weighting scheme instead of position - shared weighting scheme for combining different matching signals and incorporate question term importance learning using question attention network .,8,0,30
dataset/preprocessed/training-data/natural_language_inference/100,"Using the popular benchmark TREC QA data , we show that the relatively simple a NMM model can significantly outperform other neural network models that have been used for the question answering task , and is competitive with models thatare combined with additional features .",9,0,45
dataset/preprocessed/training-data/natural_language_inference/100,"When a NMM is combined with additional features , it outperforms all baselines .",10,0,14
dataset/preprocessed/training-data/natural_language_inference/100,"Question answering ( QA ) , which returns exact answers as either short facts or long passages to natural language questions issued by users , is a challenging task and plays a central role in the next generation of advanced web search .",12,1,43
dataset/preprocessed/training-data/natural_language_inference/100,"Many of current QA systems use a learning to rank approach that encodes question / answer pairs with complex linguistic features including lexical , syntactic and semantic features .",13,1,29
dataset/preprocessed/training-data/natural_language_inference/100,"For instance , Surdeanu et al .",14,0,7
dataset/preprocessed/training-data/natural_language_inference/100,Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page .,15,0,51
dataset/preprocessed/training-data/natural_language_inference/100,Copyrights for components of this work owned by others than ACM must be honored .,16,0,15
dataset/preprocessed/training-data/natural_language_inference/100,Abstracting with credit is permitted .,17,0,6
dataset/preprocessed/training-data/natural_language_inference/100,"To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and / or a fee .",18,0,27
dataset/preprocessed/training-data/natural_language_inference/100,"Request permissions from permissions@acm.org . 23 ] investigated a wide range of feature types including similarity features , translation features , density / frequency features and web correlation features for learning to rank answers and show improvements in accuracy .",19,0,40
dataset/preprocessed/training-data/natural_language_inference/100,"However , such methods rely on manual feature engineering , which is often time - consuming and requires domain dependent expertise and experience .",20,0,24
dataset/preprocessed/training-data/natural_language_inference/100,"Moreover , they may need additional NLP parsers or external knowledge sources that may not be available for some languages .",21,0,21
dataset/preprocessed/training-data/natural_language_inference/100,"Recently , researchers have been studying deep learning approaches to automatically learn semantic match between questions and answers .",22,0,19
dataset/preprocessed/training-data/natural_language_inference/100,Such methods are built on the top of neural network models such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) .,23,0,30
dataset/preprocessed/training-data/natural_language_inference/100,The proposed models have the benefit of not requiring hand - crafted linguistic features and external resources .,24,0,18
dataset/preprocessed/training-data/natural_language_inference/100,Some of them achieve state - of the art performance for the answer sentence selection task benchmarked by the TREC QA track .,25,0,23
dataset/preprocessed/training-data/natural_language_inference/100,"However , the weakness of the existing studies is that the proposed deep models , either based on CNNs or LSTMs , need to be combined with additional features such as word overlap features and BM25 to perform well .",26,0,40
dataset/preprocessed/training-data/natural_language_inference/100,"Without combining these additional features , their performance is significantly worse than the results obtained by the state - of - the - art methods based on linguistic feature engineering .",27,0,31
dataset/preprocessed/training-data/natural_language_inference/100,This led us to propose the following research questions :,28,0,10
dataset/preprocessed/training-data/natural_language_inference/100,"Without combining additional features , could we build deep learning models that can achieve comparable or even better performance than methods using feature engineering ?",30,0,25
dataset/preprocessed/training-data/natural_language_inference/100,"By combining additional features , could our model outperform state - of - the - art models for question answering ?",32,0,21
dataset/preprocessed/training-data/natural_language_inference/100,"To address these research questions , we analyze the existing current deep learning architectures for answer ranking and make the following two key observations :",33,0,25
dataset/preprocessed/training-data/natural_language_inference/100,Architectures not specifically designed for question / answer matching :,35,0,10
dataset/preprocessed/training-data/natural_language_inference/100,Some methods employ CNNs for question / answer matching .,36,0,10
dataset/preprocessed/training-data/natural_language_inference/100,"However , CNNs are originally designed for computer vision ( CV ) , which uses position - shared weights with local perceptive filters , to learn spatial regularities in many CV tasks .",37,0,33
dataset/preprocessed/training-data/natural_language_inference/100,"However , such spatial regularities may not exist in semantic matching between questions and answers , since important similarity signals between question and answer terms could appear in any position due to the complex linguistic property of natural languages .",38,0,40
dataset/preprocessed/training-data/natural_language_inference/100,"Meanwhile , models based on LSTMs view the question / answer matching problem in a sequential way .",39,0,18
dataset/preprocessed/training-data/natural_language_inference/100,"Without direct interactions between question and answer terms , the model may not be able to capture sufficiently detailed matching signals between them .",40,0,24
dataset/preprocessed/training-data/natural_language_inference/100,Lack of modeling question focus :,42,0,6
dataset/preprocessed/training-data/natural_language_inference/100,"Understanding the focus of questions , e.g. , important terms in a question , is helpful for ranking the answers correctly .",43,0,22
dataset/preprocessed/training-data/natural_language_inference/100,"For example , given a question like "" Where was the first burger king restaurant opened ? "" , it is critical for the answer to talk about "" burger "" , "" king "" , "" open "" , etc .",44,0,42
dataset/preprocessed/training-data/natural_language_inference/100,Most existing text matching models do not explicitly model question focus .,45,0,12
dataset/preprocessed/training-data/natural_language_inference/100,"For example , models based on CNNs treat all the question terms as equally important when matching to answer terms .",46,0,21
dataset/preprocessed/training-data/natural_language_inference/100,Models based on LSTMs usually model question terms closer to the end to be more important .,47,0,17
dataset/preprocessed/training-data/natural_language_inference/100,"To handle these issues in the existing deep learning architectures for ranking answers , we propose an attention based neural matching model ( a NMM ) .",48,0,27
dataset/preprocessed/training-data/natural_language_inference/100,The novel properties of the proposed model and our contributions can be summarized as follows :,49,0,16
dataset/preprocessed/training-data/natural_language_inference/21,DiSAN : Directional Self - Attention Network for RNN / CNN - Free Language Understanding,2,1,15
dataset/preprocessed/training-data/natural_language_inference/21,"Recurrent neural nets ( RNN ) and convolutional neural nets ( CNN ) are widely used on NLP tasks to capture the long - term and local dependencies , respectively .",4,0,31
dataset/preprocessed/training-data/natural_language_inference/21,"Attention mechanisms have recently attracted enormous interest due to their highly parallelizable computation , significantly less training time , and flexibility in modeling dependencies .",5,0,25
dataset/preprocessed/training-data/natural_language_inference/21,"We propose a novel attention mechanism in which the attention between elements from input sequence ( s ) is directional and multi-dimensional ( i.e. , feature - wise ) .",6,0,30
dataset/preprocessed/training-data/natural_language_inference/21,"A light - weight neural net , "" Directional Self - Attention Network ( DiSAN ) "" , is then proposed to learn sentence embedding , based solely on the proposed attention without any RNN / CNN structure .",7,0,39
dataset/preprocessed/training-data/natural_language_inference/21,"DiSAN is only composed of a directional self - attention with temporal order encoded , followed by a multi-dimensional attention that compresses the sequence into a vector representation .",8,0,29
dataset/preprocessed/training-data/natural_language_inference/21,"Despite it s simple form , DiSAN outperforms complicated RNN models on both prediction quality and time efficiency .",9,0,19
dataset/preprocessed/training-data/natural_language_inference/21,"It achieves the best test accuracy among all sentence encoding methods and improves the most recent best result by 1.02 % on the Stanford Natural Language Inference ( SNLI ) dataset , and shows stateof - the - art test accuracy on the Stanford Sentiment Treebank ( SST ) , Multi - Genre natural language inference ( MultiNLI ) , Sentences Involving Compositional Knowledge ( SICK ) , Customer Review , MPQA , TREC question - type classification and Subjectivity ( SUBJ ) datasets .",10,0,85
dataset/preprocessed/training-data/natural_language_inference/21,"performance on a large number of NLP tasks , including neural machine translation , natural language inference , conversation generation , question answering , machine reading comprehension , and sentiment analysis .",12,0,32
dataset/preprocessed/training-data/natural_language_inference/21,The attention uses a hidden layer to compute a categorical distribution over elements from the input sequence to reflect their importance weights .,13,0,23
dataset/preprocessed/training-data/natural_language_inference/21,"It allows RNN / CNN to maintain a variable - length memory , so that elements from the input sequence can be selected by their importance / relevance and merged into the output .",14,0,34
dataset/preprocessed/training-data/natural_language_inference/21,"In contrast to RNN and CNN , the attention mechanism is trained to capture the dependencies that make significant contributions to the task , regardless of the distance between the elements in the sequence .",15,0,35
dataset/preprocessed/training-data/natural_language_inference/21,It can thus provide complementary information to the distance - aware dependencies modeled by RNN / CNN .,16,0,18
dataset/preprocessed/training-data/natural_language_inference/21,"In addition , computing attention only requires matrix multiplication , which is highly parallelizable compared to the sequential computation of RNN .",17,0,22
dataset/preprocessed/training-data/natural_language_inference/21,"In a very recent work , an attention mechanism is solely used to construct a sequence to sequence ( seq2seq ) model that achieves a state - of - the - art quality score on the neural machine translation ( NMT ) task .",18,0,44
dataset/preprocessed/training-data/natural_language_inference/21,"The seq2seq model , "" Transformer "" , has an encoder - decoder structure that is only composed of stacked attention networks , without using either recurrence or convolution .",19,0,30
dataset/preprocessed/training-data/natural_language_inference/21,"The proposed attention , "" multi- head attention "" , projects the input sequence to multiple subspaces , then applies scaled dotproduct attention to its representation in each subspace , and lastly concatenates their output .",20,0,36
dataset/preprocessed/training-data/natural_language_inference/21,"By doing this , it can combine different attentions from multiple subspaces .",21,0,13
dataset/preprocessed/training-data/natural_language_inference/21,This mechanism is used in Transformer to compute both the context - aware features inside the encoder / decoder and the bottleneck features between them .,22,0,26
dataset/preprocessed/training-data/natural_language_inference/21,"The attention mechanism has more flexibility in sequence length than RNN / CNN , and is more task / data - driven when modeling dependencies .",23,0,26
dataset/preprocessed/training-data/natural_language_inference/21,"Unlike sequential models , its computation can be easily and significantly accelerated by existing distributed / parallel computing schemes .",24,0,20
dataset/preprocessed/training-data/natural_language_inference/21,"However , to the best of our knowledge , a neural net entirely based on attention has not been designed for other NLP tasks except NMT , especially those that can not be cast into a seq2seq problem .",25,0,39
dataset/preprocessed/training-data/natural_language_inference/21,"Compared to RNN , a dis advantage of most attention mechanisms is that the temporal order information is lost , which however might be important to the task .",26,0,29
dataset/preprocessed/training-data/natural_language_inference/21,This explains why positional encoding is applied to the sequence before being processed by the attention in Transformer .,27,0,19
dataset/preprocessed/training-data/natural_language_inference/21,How to model order information within an attention is still an open problem .,28,0,14
dataset/preprocessed/training-data/natural_language_inference/21,"The goal of this paper is to develop a unified and RNN / CNN - free attention network that can be generally utilized to learn the sentence encoding model for different NLP tasks , such as natural language inference , sentiment analysis , sentence classification and semantic relatedness .",29,0,49
dataset/preprocessed/training-data/natural_language_inference/21,We focus on the sentence encoding model because it is a basic module of most DNNs used in the NLP literature .,30,0,22
dataset/preprocessed/training-data/natural_language_inference/21,"We propose a novel attention mechanism that differs from previous ones in that it is 1 ) multi-dimensional : the attention w.r.t. each pair of elements from the source ( s ) is a vector , where each entry is the attention computed on each feature ; and 2 ) directional : it uses one or multiple positional masks to model the asymmetric attention between two elements .",31,0,68
dataset/preprocessed/training-data/natural_language_inference/21,"We compute feature - wise attention since each element in a sequence is usually represented by a vector , e.g. , word / character embedding , and attention on different features can contain different information about dependency , thus to handle the variation of contexts around the same word .",32,0,50
dataset/preprocessed/training-data/natural_language_inference/21,We apply positional masks to attention distribution since they can easily encode prior structure knowledge such as temporal order and dependency parsing .,33,0,23
dataset/preprocessed/training-data/natural_language_inference/21,"This design mitigates the weakness of attention in modeling order information , and takes full advantage of parallel computing .",34,0,20
dataset/preprocessed/training-data/natural_language_inference/21,"We then build a light - weight and RNN / CNN - free neural network , "" Directional Self - Attention Network ( DiSAN ) "" , for sentence encoding .",35,0,31
dataset/preprocessed/training-data/natural_language_inference/21,This network relies entirely on the proposed attentions and does not use any RNN / CNN structure .,36,0,18
dataset/preprocessed/training-data/natural_language_inference/21,"In DiSAN , the input sequence is processed by directional ( forward and backward ) self - attentions to model context dependency and produce context - aware representations for all tokens .",37,0,32
dataset/preprocessed/training-data/natural_language_inference/21,"Then , a multi-dimensional attention computes a vector representation of the entire sequence , which can be passed into a classification / regression module to compute the final prediction for a particular task .",38,0,34
dataset/preprocessed/training-data/natural_language_inference/21,"Unlike Transformer , neither stacking of attention blocks nor an encoderdecoder structure is required .",39,0,15
dataset/preprocessed/training-data/natural_language_inference/21,"The simple architecture of DiSAN leads to fewer parameters , less computation and easier parallelization .",40,0,16
dataset/preprocessed/training-data/natural_language_inference/21,"In experiments 1 , we compare DiSAN with the currently popular methods on various NLP tasks , e.g. , natural language inference , sentiment analysis , sentence classification , etc .",41,0,31
dataset/preprocessed/training-data/natural_language_inference/21,DiSAN achieves the highest test accuracy on the Stanford Natural Language Inference ( SNLI ) dataset among sentence - encoding models and improves the currently best result by 1.02 % .,42,0,31
dataset/preprocessed/training-data/natural_language_inference/21,"It also shows the state - of - the - art performance on the Stanford Sentiment Treebank ( SST ) , Multi - Genre natural language inference ( MultiNLI ) , SICK , Customer Review , MPQA , SUBJ and TREC question - type classification datasets .",43,0,47
dataset/preprocessed/training-data/natural_language_inference/21,"Meanwhile , it has fewer parameters and exhibits much higher computation efficiency than the mod-els it outperforms , e.g. , LSTM and tree - based models .",44,0,27
dataset/preprocessed/training-data/natural_language_inference/21,1 ) Lowercase denotes a vector ; 2 ) bold lowercase denotes a sequence of vectors ( stored as a matrix ) ; and 3 ) uppercase denotes a matrix or a tensor .,46,0,34
dataset/preprocessed/training-data/natural_language_inference/21,"In the pipeline of NLP tasks , a sentence is denoted by a sequence of discrete tokens ( e.g. , words or characters ) v = [ v 1 , v 2 , . . . , v n ] , where vi could be a one - hot vector whose dimension length equals the number of distinct tokens N .",49,0,61
dataset/preprocessed/training-data/natural_language_inference/35,Multi - Style Generative Reading Comprehension,2,1,6
dataset/preprocessed/training-data/natural_language_inference/35,"This study tackles generative reading comprehension ( RC ) , which consists of answering questions based on textual evidence and natural language generation ( NLG ) .",4,1,27
dataset/preprocessed/training-data/natural_language_inference/35,"We propose a multi-style abstractive summarization model for question answering , called Masque .",5,0,14
dataset/preprocessed/training-data/natural_language_inference/35,The proposed model has two key characteristics .,6,0,8
dataset/preprocessed/training-data/natural_language_inference/35,"First , unlike most studies on RC that have focused on extracting an answer span from the provided passages , our model instead focuses on generating a summary from the question and multiple passages .",7,0,35
dataset/preprocessed/training-data/natural_language_inference/35,This serves to cover various answer styles required for real - world applications .,8,0,14
dataset/preprocessed/training-data/natural_language_inference/35,"Second , whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model , our approach learns multi-style answers within a model to improve the NLG capability for all styles involved .",9,0,41
dataset/preprocessed/training-data/natural_language_inference/35,This also enables our model to give an answer in the target style .,10,0,14
dataset/preprocessed/training-data/natural_language_inference/35,Experiments show that our model achieves state - of - the - art performance on the Q&A task and the Q & A + NLG task of MS MARCO 2.1 and the summary task of Nar-rative QA .,11,0,38
dataset/preprocessed/training-data/natural_language_inference/35,We observe that the transfer of the style - independent NLG capability to the target style is the key to its success .,12,0,23
dataset/preprocessed/training-data/natural_language_inference/35,Question answering has been a long - standing research problem .,14,0,11
dataset/preprocessed/training-data/natural_language_inference/35,"Recently , reading comprehension ( RC ) , a challenge to answer a question given textual evidence provided in a document set , has received much attention .",15,1,28
dataset/preprocessed/training-data/natural_language_inference/35,"Current mainstream studies have treated RC as a process of extracting an answer span from one passage or multiple passages , which is usually done by predicting the start and end positions of the answer .",16,1,36
dataset/preprocessed/training-data/natural_language_inference/35,* Work done during an internship at NTT .,17,0,9
dataset/preprocessed/training-data/natural_language_inference/35,"The demand for answering questions in natural language is increasing rapidly , and this has led to the development of smart devices such as Alexa .",18,0,26
dataset/preprocessed/training-data/natural_language_inference/35,"In comparison with answer span extraction , however , the natural language generation ( NLG ) capability for RC has been less studied .",19,0,24
dataset/preprocessed/training-data/natural_language_inference/35,"While datasets such as MS MARCO and Nar-rative QA have been proposed for providing abstractive answers , the stateof - the - art methods for these datasets are based on answer span extraction .",20,0,34
dataset/preprocessed/training-data/natural_language_inference/35,Generative models suffer from a dearth of training data to cover open - domain questions .,21,0,16
dataset/preprocessed/training-data/natural_language_inference/35,"Moreover , to satisfy various information needs , intelligent agents should be capable of answering one question in multiple styles , such as wellformed sentences , which make sense even without the context of the question and passages , and concise phrases .",22,0,43
dataset/preprocessed/training-data/natural_language_inference/35,"These capabilities complement each other , but previous studies can not use and control different styles within a model .",23,0,20
dataset/preprocessed/training-data/natural_language_inference/35,"In this study , we propose Masque , a generative model for multi-passage RC .",24,1,15
dataset/preprocessed/training-data/natural_language_inference/35,It achieves stateof - the - art performance on the Q&A task and the Q & A + NLG task of MS MARCO 2.1 and the summary task of Narrative QA .,25,0,32
dataset/preprocessed/training-data/natural_language_inference/35,The main contri-butions of this study are as follows .,26,0,10
dataset/preprocessed/training-data/natural_language_inference/35,Multi - source abstractive summarization .,27,0,6
dataset/preprocessed/training-data/natural_language_inference/35,"We introduce the pointer - generator mechanism for generating an abstractive answer from the question and multiple passages , which covers various answer styles .",28,0,25
dataset/preprocessed/training-data/natural_language_inference/35,We extend the mechanism to a Transformer based one that allows words to be generated from a vocabulary and to be copied from the question and passages .,29,0,28
dataset/preprocessed/training-data/natural_language_inference/35,Multi- style learning for style control and transfer .,30,0,9
dataset/preprocessed/training-data/natural_language_inference/35,We introduce multi-style learning that enables our model to control answer styles and improves RC for all styles involved .,31,0,20
dataset/preprocessed/training-data/natural_language_inference/35,"We also extend the pointer - generator to a conditional decoder by introducing an artificial token corresponding to each style , as in .",32,0,24
dataset/preprocessed/training-data/natural_language_inference/35,"For each decoding step , it controls the mixture weights over three distributions with the given style ( ) .",33,0,20
dataset/preprocessed/training-data/natural_language_inference/35,This paper considers the following task :,35,0,7
dataset/preprocessed/training-data/natural_language_inference/35,", and an answer style label s , an RC model outputs an answer y = {y 1 , . . . , y T } conditioned on the style .",36,0,31
dataset/preprocessed/training-data/natural_language_inference/35,"In short , given a 3 - tuple ( x q , {x pk } , s ) , the system predicts P ( y ) .",37,0,27
dataset/preprocessed/training-data/natural_language_inference/35,"The training data is a set of 6 - tuples : ( x q , {x pk } , s , y , a , {r pk } ) , where a and {r pk } are optional .",38,0,39
dataset/preprocessed/training-data/natural_language_inference/35,"Here , a is 1 if the question is answerable with the provided passages and 0 otherwise , and r pk is 1 if the k - th passage is required to formulate the answer and 0 otherwise .",39,0,39
dataset/preprocessed/training-data/natural_language_inference/35,"We propose a Multi-style Abstractive Summarization model for QUEstion answering , called Masque .",41,0,14
dataset/preprocessed/training-data/natural_language_inference/35,"Masque directly models the conditional probability p ( y | x q , {x pk } , s ) .",42,0,20
dataset/preprocessed/training-data/natural_language_inference/35,As shown in 4 .,43,0,5
dataset/preprocessed/training-data/natural_language_inference/35,The answer sentence decoder ( 3.4 ) outputs an answer sentence conditioned on the target style .,44,0,17
dataset/preprocessed/training-data/natural_language_inference/35,Our model is based on multi-source abstractive summarization : the answer that it generates can be viewed as a summary from the question and passages .,45,0,26
dataset/preprocessed/training-data/natural_language_inference/35,The model also learns multi-style answers together .,46,0,8
dataset/preprocessed/training-data/natural_language_inference/35,"With these two characteristics , we aim to acquire the style - independent NLG ability and transfer it to the target style .",47,0,23
dataset/preprocessed/training-data/natural_language_inference/35,"In addition , to improve natural language understanding in the reader module , our model considers RC , passage ranking , and answer possibility classification together as multi-task learning .",48,0,30
dataset/preprocessed/training-data/natural_language_inference/35,Question - Passages Reader,49,0,4
dataset/preprocessed/training-data/natural_language_inference/97,A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data,2,1,11
dataset/preprocessed/training-data/natural_language_inference/97,Understanding unstructured text is a major goal within natural language processing .,4,1,12
dataset/preprocessed/training-data/natural_language_inference/97,Comprehension tests pose questions based on short text passages to evaluate such understanding .,5,0,14
dataset/preprocessed/training-data/natural_language_inference/97,"In this work , we investigate machine comprehension on the challenging MCTest benchmark .",6,0,14
dataset/preprocessed/training-data/natural_language_inference/97,"Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features .",7,0,19
dataset/preprocessed/training-data/natural_language_inference/97,"We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .",8,0,19
dataset/preprocessed/training-data/natural_language_inference/97,"The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set .",9,0,34
dataset/preprocessed/training-data/natural_language_inference/97,Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word - embedding representations of text .,10,0,26
dataset/preprocessed/training-data/natural_language_inference/97,"When trained with a methodology designed to help cope with limited training data , our Parallel - Hierarchical model sets a new state of the art for MCTest , outperforming previous feature - engineered approaches slightly and previous neural approaches by a significant margin ( over 15 % absolute ) .",11,0,51
dataset/preprocessed/training-data/natural_language_inference/97,"Humans learn in a variety of ways - by communication with each other , and by study , the reading of text .",13,0,23
dataset/preprocessed/training-data/natural_language_inference/97,"Comprehension of unstructured text by machines , at a near- human level , is a major goal for natural language processing .",14,0,22
dataset/preprocessed/training-data/natural_language_inference/97,It has garnered significant attention from the machine learning research community in recent years .,15,0,15
dataset/preprocessed/training-data/natural_language_inference/97,Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,16,1,31
dataset/preprocessed/training-data/natural_language_inference/97,"Such tests are objectively gradable and can be used to assess a range of abilities , from basic understanding to causal reasoning to inference .",17,0,25
dataset/preprocessed/training-data/natural_language_inference/97,"Given a text passage and a question about its content , a system is tested on its ability to determine the correct answer .",18,0,24
dataset/preprocessed/training-data/natural_language_inference/97,"In this work , we focus on MCTest , a complex but data - limited comprehension benchmark , whose multiple - choice questions require not only extraction but also inference and limited reasoning .",19,0,34
dataset/preprocessed/training-data/natural_language_inference/97,"Inference and reasoning are important human skills that apply broadly , beyond language .",20,0,14
dataset/preprocessed/training-data/natural_language_inference/97,We present a parallel - hierarchical approach to machine comprehension designed to work well in a data - limited setting .,21,0,21
dataset/preprocessed/training-data/natural_language_inference/97,"There are many use-cases in which comprehension over limited data would be handy : for example , user manuals , internal documentation , legal contracts , and soon .",22,0,29
dataset/preprocessed/training-data/natural_language_inference/97,"Moreover , work towards more efficient learning from any quantity of data is important in its own right , for bringing machines more inline with the way humans learn .",23,0,30
dataset/preprocessed/training-data/natural_language_inference/97,"Typically , artificial neural networks require numerous parameters to capture complex patterns , and the more parameters , the more training data is required to tune them .",24,0,28
dataset/preprocessed/training-data/natural_language_inference/97,"Likewise , deep models learn to extract their own features , but this is a data - intensive process .",25,0,20
dataset/preprocessed/training-data/natural_language_inference/97,Our model learns to comprehend at a high level even when data is sparse .,26,0,15
dataset/preprocessed/training-data/natural_language_inference/97,The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .,27,0,22
dataset/preprocessed/training-data/natural_language_inference/97,We refer to a question combined with one of its answer candidates as a hypothesis ( to be detailed below ) .,28,0,22
dataset/preprocessed/training-data/natural_language_inference/97,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",29,0,37
dataset/preprocessed/training-data/natural_language_inference/97,"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .",30,0,23
dataset/preprocessed/training-data/natural_language_inference/97,"As in the semantic perspective , we consider matches over complete sentences .",31,0,13
dataset/preprocessed/training-data/natural_language_inference/97,"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .",32,0,29
dataset/preprocessed/training-data/natural_language_inference/97,"Finally , this word - level sliding window operates on two different views of text sentences : the sequential view , where words appear in their natural order , and the dependency view , where words are reordered based on a linearization of the sentence 's dependency graph .",33,0,49
dataset/preprocessed/training-data/natural_language_inference/97,Words are represented throughout by embedding vectors .,34,0,8
dataset/preprocessed/training-data/natural_language_inference/97,These distinct perspectives naturally form a hierarchy that we depict in .,35,0,12
dataset/preprocessed/training-data/natural_language_inference/97,"Language is hierarchical , so it makes sense that comprehension relies on hierarchical levels of understanding .",36,0,17
dataset/preprocessed/training-data/natural_language_inference/97,The perspectives of our model can be considered a type of feature .,37,0,13
dataset/preprocessed/training-data/natural_language_inference/97,"However , they are implemented by parametric differentiable functions .",38,0,10
dataset/preprocessed/training-data/natural_language_inference/97,"This is in contrast to most previous efforts on MCTest , whose numerous hand - engineered features can not be trained .",39,0,22
dataset/preprocessed/training-data/natural_language_inference/97,"Our model , significantly , can be trained end - to - end with backpropagation .",40,0,16
dataset/preprocessed/training-data/natural_language_inference/97,"To facilitate learning with limited data , we also develop a unique training scheme .",41,0,15
dataset/preprocessed/training-data/natural_language_inference/97,We initialize the model 's neural networks to perform specific heuristic functions that yield decent ( thought not impressive ) performance on the dataset .,42,0,25
dataset/preprocessed/training-data/natural_language_inference/97,"Thus , the training scheme gives the model a safe , reasonable baseline from which to start learning .",43,0,19
dataset/preprocessed/training-data/natural_language_inference/97,We call this technique training wheels .,44,0,7
dataset/preprocessed/training-data/natural_language_inference/97,Computational models that comprehend ( insofar as they perform well on MC datasets ) have developed contemporaneously in several research groups .,45,0,22
dataset/preprocessed/training-data/natural_language_inference/97,"Models designed specifically for MCTest include those of , and more recently , , and .",46,0,16
dataset/preprocessed/training-data/natural_language_inference/97,"In experiments , our Parallel - Hierarchical model achieves state - of - the - art accuracy on MCTest , outperforming these existing methods .",47,0,25
dataset/preprocessed/training-data/natural_language_inference/97,"Below we describe related work , the mathematical details of our model , and our experiments , then analyze our results .",48,0,22
dataset/preprocessed/training-data/natural_language_inference/30,Open Question Answering with Weakly Supervised Embedding Models,2,1,8
dataset/preprocessed/training-data/natural_language_inference/30,Building computers able to answer questions on any subject is along standing goal of artificial intelligence .,4,0,17
dataset/preprocessed/training-data/natural_language_inference/30,Promising progress has recently been achieved by methods that learn to map questions to logical forms or data base queries .,5,0,21
dataset/preprocessed/training-data/natural_language_inference/30,Such approaches can be effective but at the cost of either large amounts of human - labeled data or by defining lexicons and grammars tailored by practitioners .,6,0,28
dataset/preprocessed/training-data/natural_language_inference/30,"In this paper , we instead take the radical approach of learning to map questions to vectorial feature representations .",7,0,20
dataset/preprocessed/training-data/natural_language_inference/30,"By mapping answers into the same space one can query any knowledge base independent of its schema , without requiring any grammar or lexicon .",8,0,25
dataset/preprocessed/training-data/natural_language_inference/30,Our method is trained with a new optimization procedure combining stochastic gradient descent followed by a fine - tuning step using the weak supervision provided by blending automatically and collaboratively generated resources .,9,0,33
dataset/preprocessed/training-data/natural_language_inference/30,"We empirically demonstrate that our model can capture meaningful signals from its noisy supervision leading to major improvements over paralex , the only existing method able to be trained on similar weakly labeled data .",10,0,35
dataset/preprocessed/training-data/natural_language_inference/30,"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .",12,1,26
dataset/preprocessed/training-data/natural_language_inference/30,Any advance on this difficult topic would bring a huge leap forward in building new ways of accessing knowledge .,13,0,20
dataset/preprocessed/training-data/natural_language_inference/30,"An important development in this are a has been the creation of large - scale Knowledge Bases ( KBs ) , such as Freebase and DBpedia which store huge amounts of general - purpose information .",14,0,36
dataset/preprocessed/training-data/natural_language_inference/30,"They are organized as data bases of triples connecting pairs of entities by various relationships and of the form ( left entity , relationship , right entity ) .",15,0,29
dataset/preprocessed/training-data/natural_language_inference/30,Question answering is then defined as the task of retrieving the correct entity or set of entities from a KB given a query expressed as a question in natural language .,16,1,31
dataset/preprocessed/training-data/natural_language_inference/30,The use of KBs simplifies the problem by separating the issue of collecting and organizing information ( i.e. information extraction ) from the one of searching through it ( i.e. question answering or natural language interfacing ) .,17,0,38
dataset/preprocessed/training-data/natural_language_inference/30,"However , open question answering remains challenging because of the scale of these KBs ( billions of triples , millions of entities and relationships ) and of the difficulty for machines to interpret natural language .",18,0,36
dataset/preprocessed/training-data/natural_language_inference/30,Recent progress has been made by tackling this problem with semantic parsers .,19,0,13
dataset/preprocessed/training-data/natural_language_inference/30,These methods convert questions into logical forms or data base queries ( e.g. in SPARQL ) which are then subsequently used to query KBs for answers .,20,0,27
dataset/preprocessed/training-data/natural_language_inference/30,"Even if such systems have shown the ability to handle large - scale KBs , they require practitioners to hand - craft lexicons , grammars , and KB schema for the parsing to be effective .",21,0,36
dataset/preprocessed/training-data/natural_language_inference/30,"This nonnegligible human intervention might not be generic enough to conveniently scale up to new data bases with other schema , broader vocabularies or other languages than English .",22,0,29
dataset/preprocessed/training-data/natural_language_inference/30,"In this paper , we instead take the approach of converting questions to ( uninterpretable ) vectorial representations which require no pre-defined grammars or lexicons and can query any KB independent of its schema .",23,0,35
dataset/preprocessed/training-data/natural_language_inference/30,"Following , we focus on answering simple factual questions on a broad range of topics , more specifically , those for which single KB triples stand for both the question and an answer ( of which there maybe many ) .",24,0,41
dataset/preprocessed/training-data/natural_language_inference/30,"For example , ( parrotfish.e , live - in.r , southern - water .e ) stands for",25,0,17
dataset/preprocessed/training-data/natural_language_inference/30,What is parrotfish 's habitat ?,26,0,6
dataset/preprocessed/training-data/natural_language_inference/30,"and southern - water.e and ( cantonese.e , be-major - language - in.r , hong - kong.e ) for",27,0,19
dataset/preprocessed/training-data/natural_language_inference/30,What is the main language of Hong - Kong ?,28,0,10
dataset/preprocessed/training-data/natural_language_inference/30,"In this task , the main difficulties come from lexical variability rather than from complex syntax , having multiple answers per question , and the absence of a supervised training signal .",30,0,32
dataset/preprocessed/training-data/natural_language_inference/30,Our approach is based on learning low - dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space .,31,0,34
dataset/preprocessed/training-data/natural_language_inference/30,"Unfortunately , we do not have access to any human labeled ( query , answer ) supervision for this task .",32,0,21
dataset/preprocessed/training-data/natural_language_inference/30,"In order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data , we make use of weak supervision .",33,0,27
dataset/preprocessed/training-data/natural_language_inference/30,We show empirically that our model is able to take advantage of noisy and indirect supervision by ( i ) automatically generating questions from KB triples and treating this as training data ; and ( ii ) supplementing this with a data set of questions collaboratively marked as paraphrases but with no associated answers .,34,0,55
dataset/preprocessed/training-data/natural_language_inference/30,We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2.4 M entities and 600 k relationships .,35,0,33
dataset/preprocessed/training-data/natural_language_inference/30,Our method strongly outperforms previous results on the WikiAnswers + ReVerb evaluation data set introduced by .,36,0,17
dataset/preprocessed/training-data/natural_language_inference/30,"Even if the embeddings obtained after training are of good quality , the scale of the optimization problem makes it hard to control and to lead to convergence .",37,0,29
dataset/preprocessed/training-data/natural_language_inference/30,"Thus , we propose a method to fine - tune embedding - based models by carefully optimizing a matrix parameterizing the similarity used in the embedding space , leading to a consistent improvement in performance .",38,0,36
dataset/preprocessed/training-data/natural_language_inference/30,The rest of the paper is organized as follows .,39,0,10
dataset/preprocessed/training-data/natural_language_inference/30,Section 2 discusses some previous work and Section 3 introduces the problem of open question answering .,40,0,17
dataset/preprocessed/training-data/natural_language_inference/30,"Then , Section 4 presents our model and Section 5 our experimental results .",41,0,14
dataset/preprocessed/training-data/natural_language_inference/30,"Large - scale question answering has along history , mostly initiated via the TREC tracks .",43,0,16
dataset/preprocessed/training-data/natural_language_inference/30,"The first successful systems transformed the questions into queries which were fed to web search engines , the answer being subsequently extracted from top returned pages or snippets .",44,0,29
dataset/preprocessed/training-data/natural_language_inference/30,Such approaches require significant engineering to hand - craft queries and then parse and search over results .,45,0,18
dataset/preprocessed/training-data/natural_language_inference/30,"The emergence of large - scale KBs , such as Freebase or DBpedia , changed the setting by transforming open question answering into a problem of querying a KB using natural language .",46,0,33
dataset/preprocessed/training-data/natural_language_inference/30,"This is a challenging problem , which would require huge amount of labeled data to be tackled properly by purely supervised machine learning methods because of the great variability of language and of the large scale of KBs .",47,0,39
dataset/preprocessed/training-data/natural_language_inference/30,"The earliest methods for open question - answering with KBs , based on hand - written templates , were not robust enough to such variability over possibly evolving KBs ( addition / deletion of triples and entities ) .",48,0,39
dataset/preprocessed/training-data/natural_language_inference/30,The solution to gain more expressiveness via machine learning comes from distant or indirect supervision to circumvent the issue of labeled data .,49,0,23
dataset/preprocessed/training-data/natural_language_inference/52,Tell Me Why : Using Question Answering as Distant Supervision for Answer Justification,2,1,13
dataset/preprocessed/training-data/natural_language_inference/52,"For many applications of question answering ( QA ) , being able to explain why a given model chose an answer is critical .",4,0,24
dataset/preprocessed/training-data/natural_language_inference/52,"However , the lack of labeled data for answer justifications makes learning this difficult and expensive .",5,0,17
dataset/preprocessed/training-data/natural_language_inference/52,"Here we propose an approach that uses answer ranking as distant supervision for learning how to select informative justifications , where justifications serve as inferential connections between the question and the correct answer while often containing little lexical overlap with either .",6,0,42
dataset/preprocessed/training-data/natural_language_inference/52,We propose a neural network architecture for QA that reranks answer justifications as an intermediate ( and human - interpretable ) step in answer selection .,7,0,26
dataset/preprocessed/training-data/natural_language_inference/52,"Our approach is informed by a set of features designed to combine both learned representations and explicit features to capture the connection between questions , answers , and answer justifications .",8,0,31
dataset/preprocessed/training-data/natural_language_inference/52,We show that with this end - to - end approach we are able to significantly improve upon a strong IR baseline in both justification ranking ( + 9 % rated highly relevant ) and answer selection ( + 6 % P@1 ) .,9,0,44
dataset/preprocessed/training-data/natural_language_inference/52,"Developing interpretable machine learning ( ML ) models , that is , models where a human user can understand what the model is learning , is considered by many to be crucial for ensuring usability and accelerating progress .",11,1,39
dataset/preprocessed/training-data/natural_language_inference/52,"For many applications of question answering ( QA ) , i.e. , finding short answers to natural language questions , simply providing an answer is not sufficient .",12,0,28
dataset/preprocessed/training-data/natural_language_inference/52,A complete Question :,13,0,4
dataset/preprocessed/training-data/natural_language_inference/52,Which of these is a response to an internal stimulus ?,14,0,11
dataset/preprocessed/training-data/natural_language_inference/52,( A ) A sunflower turns to face the rising sun .,15,0,12
dataset/preprocessed/training-data/natural_language_inference/52,( B ) A cucumber tendril wraps around a wire .,16,0,11
dataset/preprocessed/training-data/natural_language_inference/52,( C ) A pine tree knocked sideways in a landslide grows upward in a bend .,17,0,17
dataset/preprocessed/training-data/natural_language_inference/52,( D ) Guard cells of a tomato plant leaf close when there is little water in the roots .,18,0,20
dataset/preprocessed/training-data/natural_language_inference/52,Plants rely on hormones to send signals within the plant in order to respond to internal stimuli such as alack of water or nutrients . :,20,0,26
dataset/preprocessed/training-data/natural_language_inference/52,Example of an 8th grade science question with a justification for the correct answer .,21,0,15
dataset/preprocessed/training-data/natural_language_inference/52,"Note the lack of direct lexical overlap present between the justification and the correct answer , demonstrating the difficulty of the task of finding justifications using traditional distant supervision methods .",22,0,31
dataset/preprocessed/training-data/natural_language_inference/52,"approach must be interpretable , i.e. , able to explain why an answer is correct .",23,0,16
dataset/preprocessed/training-data/natural_language_inference/52,"For example , in the medical domain , a QA approach that answers treatment questions would not be trusted if the treatment recommendation is not explained in terms that can be understood by the human user .",24,0,37
dataset/preprocessed/training-data/natural_language_inference/52,One approach to interpreting complex models is to make use of human- interpretable information generated by the model to gain insight into what the model is learning .,25,0,28
dataset/preprocessed/training-data/natural_language_inference/52,"We follow the intuition of , whose two - component network first generates text spans from an input document , and then uses these text spans to make predictions .",26,0,30
dataset/preprocessed/training-data/natural_language_inference/52,these intermediate text spans to infer the model 's preferences .,27,0,11
dataset/preprocessed/training-data/natural_language_inference/52,"By learning these human - readable intermediate representations endto - end with a downstream task , the representations are optimized to correlate with what the model learns is discriminatory for the task , and they can be evaluated against what a human would consider to be important .",28,0,48
dataset/preprocessed/training-data/natural_language_inference/52,Here we apply this general framework for model interpretability to QA .,29,0,12
dataset/preprocessed/training-data/natural_language_inference/52,"In this work , we focus on answering multiplechoice science exam questions ; see example in ) .",30,0,18
dataset/preprocessed/training-data/natural_language_inference/52,"This domain is challenging as : ( a ) approximately 70 % of science exam ques- tion shave been shown to require complex forms of inference to solve , and ( b ) there are few structured knowledge bases to support this inference .",31,0,44
dataset/preprocessed/training-data/natural_language_inference/52,"Within this domain , we propose an approach that learns to both select and explain answers , when the only supervision available is for which answer is correct ( but not how to explain it ) .",32,0,37
dataset/preprocessed/training-data/natural_language_inference/52,"Intuitively , our approach chooses the justifications that provide the most help towards ranking the correct answers higher than incorrect ones .",33,0,22
dataset/preprocessed/training-data/natural_language_inference/52,"More formally , our neural network approach alternates between using the current model with max - pooling to choose the highest scoring justifications for correct answers , and optimizing the answer ranking model given these justifications .",34,0,37
dataset/preprocessed/training-data/natural_language_inference/52,"Crucially , these reranked texts serve as our human - readable answer justifications , and by examining them , we gain insight into what the model learned was useful for the QA task .",35,0,34
dataset/preprocessed/training-data/natural_language_inference/52,The specific contributions of this work are :,36,0,8
dataset/preprocessed/training-data/natural_language_inference/52,We propose an end - to - end neural method for learning to answer questions and select a high - quality justification for those answers .,38,0,26
dataset/preprocessed/training-data/natural_language_inference/52,Our approach re-ranks free - text answer justifications without the need for structured knowledge bases .,39,0,16
dataset/preprocessed/training-data/natural_language_inference/52,"With supervision only for the correct answers , we learn this re-ranking through a form of distant supervision - i.e. , the answer ranking supervises the justification re-ranking .",40,0,29
dataset/preprocessed/training-data/natural_language_inference/52,"We investigate two distinct categories of features in this "" little data "" domain : explicit features , and learned representations .",42,0,22
dataset/preprocessed/training-data/natural_language_inference/52,"We show that , with limited training , explicit features perform far better despite their simplicity .",43,0,17
dataset/preprocessed/training-data/natural_language_inference/52,"We demonstrate a large ( + 9 % ) improvement in generating high - quality justifications over a strong information retrieval ( IR ) baseline , while maintaining near state - of - the - art performance on the multiple - choice scienceexam QA task , demonstrating the success of the end - to - end strategy .",45,0,58
dataset/preprocessed/training-data/natural_language_inference/52,"In many ways , deep learning has become the canonical example of the "" black box "" of machine learning and many of the approaches to explaining it can be loosely categorized into two types : approaches that try to interpret the parameters themselves ( e.g. , with visualizations and heat maps , and approaches that generate humaninterpretable information that is ideally correlated with what is being learned inside the model ( e.g. , ) .",47,0,76
dataset/preprocessed/training-data/natural_language_inference/52,Our approach falls into the latter type - we use our model 's reranking of humanreadable justifications to give us insight into what the model considers informative for answering questions .,48,0,31
dataset/preprocessed/training-data/natural_language_inference/52,"This allows us to see where we do well ( Section 6.2 ) , and where we can improve ( Section 6.3 ) .",49,0,24
dataset/preprocessed/training-data/natural_language_inference/68,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,2,1,9
dataset/preprocessed/training-data/natural_language_inference/68,Machine comprehension of text is an important problem in natural language processing .,4,1,13
dataset/preprocessed/training-data/natural_language_inference/68,"A recently released dataset , the Stanford Question Answering Dataset ( SQuAD ) , offers a large number of real questions and their answers created by humans through crowdsourcing .",5,0,30
dataset/preprocessed/training-data/natural_language_inference/68,"SQuAD provides a challenging testbed for evaluating machine comprehension algorithms , partly because compared with previous datasets , in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths .",6,0,38
dataset/preprocessed/training-data/natural_language_inference/68,We propose an end - to - end neural architecture for the task .,7,0,14
dataset/preprocessed/training-data/natural_language_inference/68,"The architecture is based on match - LSTM , a model we proposed previously for textual entailment , and Pointer Net , a sequence - to - sequence model proposed by Vinyals et al. ( 2015 ) to constrain the output tokens to be from the input sequences .",8,0,49
dataset/preprocessed/training-data/natural_language_inference/68,We propose two ways of using Pointer Net for our task .,9,0,12
dataset/preprocessed/training-data/natural_language_inference/68,Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. ( 2016 ) using logistic regression and manually crafted features .,10,0,30
dataset/preprocessed/training-data/natural_language_inference/68,Machine comprehension of text is one of the ultimate goals of natural language processing .,12,0,15
dataset/preprocessed/training-data/natural_language_inference/68,"While the ability of a machine to understand text can be assessed in many different ways , in recent years , several benchmark datasets have been created to focus on answering questions as away to evaluate machine comprehension .",13,0,39
dataset/preprocessed/training-data/natural_language_inference/68,"In this setup , typically the machine is first presented with apiece of text such as a news article or a story .",14,0,23
dataset/preprocessed/training-data/natural_language_inference/68,The machine is then expected to answer one or multiple questions related to the text .,15,0,16
dataset/preprocessed/training-data/natural_language_inference/68,"In most of the benchmark datasets , a question can be treated as a multiple choice question , whose correct answer is to be chosen from a set of provided candidate answers .",16,0,33
dataset/preprocessed/training-data/natural_language_inference/68,"Presumably , questions with more given candidate answers are more challenging .",17,0,12
dataset/preprocessed/training-data/natural_language_inference/68,The Stanford Question Answering Dataset ( SQuAD ) introduced recently by contains such more challenging questions whose correct answers can be any sequence of tokens from the given text .,18,0,30
dataset/preprocessed/training-data/natural_language_inference/68,"Moreover , unlike some other datasets whose questions and answers were created automatically in Cloze style , the questions and answers in SQu AD were created by humans through crowdsourcing , which makes the dataset more realistic .",19,0,38
dataset/preprocessed/training-data/natural_language_inference/68,"Given these advantages of the SQuAD dataset , in this paper , we focus on this new dataset to study machine comprehension of text .",20,0,25
dataset/preprocessed/training-data/natural_language_inference/68,A sample piece of text and three of its associated questions are shown in .,21,0,15
dataset/preprocessed/training-data/natural_language_inference/68,"Traditional solutions to this kind of question answering tasks rely on NLP pipelines that involve multiple steps of linguistic analyses and feature engineering , including syntactic parsing , named entity recognition , question classification , semantic parsing , etc .",22,0,40
dataset/preprocessed/training-data/natural_language_inference/68,"Recently , with the advances of applying neural network models in NLP , there has been much interest in building end - to - end neural architectures for various NLP tasks , including several pieces of work on machine comprehension .",23,0,41
dataset/preprocessed/training-data/natural_language_inference/68,"However , given the properties of previous machine comprehension datasets , existing end - to - end neural architectures for the task either rely on the candidate answers or assume that the In 1870 , Tesla moved to Karlovac , to attend school at the Higher Real Gymnasium , where he was profoundly influenced by a math teacher Martin Sekuli ?.",24,0,61
dataset/preprocessed/training-data/natural_language_inference/68,"The classes were held in German , as it was a school within the Austro-Hungarian Military Frontier .",25,0,18
dataset/preprocessed/training-data/natural_language_inference/68,"Tesla was able to perform integral calculus in his head , which prompted his teachers to believe that he was cheating .",26,0,22
dataset/preprocessed/training-data/natural_language_inference/68,"He finished a four - year term in three years , graduating in 1873 .",27,0,15
dataset/preprocessed/training-data/natural_language_inference/68,In what language were the classes given ?,29,0,8
dataset/preprocessed/training-data/natural_language_inference/68,2 . Who was Tesla 's main influence in Karlovac ?,31,0,11
dataset/preprocessed/training-data/natural_language_inference/68,Martin Sekuli ?,32,0,3
dataset/preprocessed/training-data/natural_language_inference/68,3 . Why did Tesla go to Karlovac ?,33,0,9
dataset/preprocessed/training-data/natural_language_inference/68,attend school at the Higher Real Gymnasium :,34,0,8
dataset/preprocessed/training-data/natural_language_inference/68,"A paragraph from Wikipedia and three associated questions together with their answers , taken from the SQuAD dataset .",35,0,19
dataset/preprocessed/training-data/natural_language_inference/68,The tokens in bold in the paragraph are our predicted answers while the texts next to the questions are the ground truth answers .,36,0,24
dataset/preprocessed/training-data/natural_language_inference/68,"answer is a single token , which make these methods unsuitable for the SQuAD dataset .",37,0,16
dataset/preprocessed/training-data/natural_language_inference/68,"In this paper , we propose a new end - to - end neural architecture to address the machine comprehension problem as defined in the SQuAD dataset .",38,0,28
dataset/preprocessed/training-data/natural_language_inference/68,"Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .",39,0,34
dataset/preprocessed/training-data/natural_language_inference/68,"We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .",40,0,50
dataset/preprocessed/training-data/natural_language_inference/68,We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .,41,0,23
dataset/preprocessed/training-data/natural_language_inference/68,We also further extend the boundary model with a search mechanism .,42,0,12
dataset/preprocessed/training-data/natural_language_inference/68,Experiments on the SQuAD dataset show that our two models both outperform the best performance reported by .,43,0,18
dataset/preprocessed/training-data/natural_language_inference/68,"Moreover , using an ensemble of several of our models , we can achieve very competitive performance on SQuAD .",44,0,20
dataset/preprocessed/training-data/natural_language_inference/68,Our contributions can be summarized as follows :,45,0,8
dataset/preprocessed/training-data/natural_language_inference/68,"( 1 ) We propose two new end - to - end neural network models for machine comprehension , which combine match - LSTM and Ptr- Net to handle the special properties of the SQuAD dataset .",46,0,37
dataset/preprocessed/training-data/natural_language_inference/68,"( 2 ) We have achieved the performance of an exact match score of 67.9 % and an F1 score of 77.0 % on the unseen test dataset , which is much better than the featureengineered solution .",47,0,38
dataset/preprocessed/training-data/natural_language_inference/68,"Our performance is also close to the state of the art on SQuAD , which is 71.6 % in terms of exact match and 80.4 % in terms of F1 from Salesforce Research .",48,0,34
dataset/preprocessed/training-data/natural_language_inference/68,( 3 ) Our further analyses of the models reveal some useful insights for further improving the method .,49,0,19
dataset/preprocessed/training-data/natural_language_inference/69,Constructing Datasets for Multi-hop Reading Comprehension Across Documents,2,1,8
dataset/preprocessed/training-data/natural_language_inference/69,"Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence , paragraph , or document .",4,1,22
dataset/preprocessed/training-data/natural_language_inference/69,"Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods , but currently no resources exist to train and test this capability .",5,0,30
dataset/preprocessed/training-data/natural_language_inference/69,We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods .,6,0,26
dataset/preprocessed/training-data/natural_language_inference/69,"In our task , a model learns to seek and combine evidence - effectively performing multihop , alias multi-step , inference .",7,0,22
dataset/preprocessed/training-data/natural_language_inference/69,"We devise a methodology to produce datasets for this task , given a collection of query - answer pairs and thematically linked documents .",8,0,24
dataset/preprocessed/training-data/natural_language_inference/69,"Two datasets from different domains are induced , 1 and we identify potential pitfalls and devise circumvention strategies .",9,0,19
dataset/preprocessed/training-data/natural_language_inference/69,We evaluate two previously proposed competitive models and find that one can integrate information across documents .,10,0,17
dataset/preprocessed/training-data/natural_language_inference/69,"However , both models struggle to select relevant information ; and providing documents guaranteed to be relevant greatly improves their performance .",11,0,22
dataset/preprocessed/training-data/natural_language_inference/69,"While the models outperform several strong baselines , their best accuracy reaches 54.5 % on an annotated test set , compared to human performance at 85.0 % , leaving ample room for improvement .",12,0,34
dataset/preprocessed/training-data/natural_language_inference/69,Devising computer systems capable of answering questions about knowledge described using text has 1 Available at http://qangaroo.cs.ucl.ac.uk,14,0,17
dataset/preprocessed/training-data/natural_language_inference/69,"The Arabian Sea is a region of the northern Indian Ocean bounded on the north by Pakistan and Iran , on the west by northeastern Somalia and the Arabian Peninsula , and on the east by India : A sample from the WIKIHOP dataset where it is necessary to combine information spread across multiple documents to infer the correct answer .",15,0,61
dataset/preprocessed/training-data/natural_language_inference/69,been a longstanding challenge in Natural Language Processing ( NLP ) .,16,0,12
dataset/preprocessed/training-data/natural_language_inference/69,Contemporary end - to - end Reading Comprehension ( RC ) methods can learn to extract the correct answer span within a given text and approach human - level performance .,17,1,31
dataset/preprocessed/training-data/natural_language_inference/69,"However , for existing datasets , relevant information is often concentrated locally within a single sentence , emphasizing the role of locating , matching , and aligning information between query and support text .",18,0,34
dataset/preprocessed/training-data/natural_language_inference/69,"For example , observed that a simple binary word - in - query indicator feature boosted the relative accuracy of a baseline model by 27.9 % .",19,0,27
dataset/preprocessed/training-data/natural_language_inference/69,"We argue that , in order to further the ability of machine comprehension methods to extract knowledge from text , we must move beyond a scenario where relevant information is coherently and explicitly stated within a single document .",20,0,39
dataset/preprocessed/training-data/natural_language_inference/69,"Methods with this capability would aid Information Extraction ( IE ) applications , such as discovering drug - drug interac- tions by connecting protein interactions reported across different publications .",21,0,30
dataset/preprocessed/training-data/natural_language_inference/69,They would also benefit search and Question Answering ( QA ) applications where the required information can not be found in a single location .,22,0,25
dataset/preprocessed/training-data/natural_language_inference/69,"shows an example from WIKIPEDIA , where the goal is to identify the country property of the Hanging Gardens of Mumbai .",23,0,22
dataset/preprocessed/training-data/natural_language_inference/69,"This can not be inferred solely from the article about them without additional background knowledge , as the answer is not stated explicitly .",24,0,24
dataset/preprocessed/training-data/natural_language_inference/69,"However , several of the linked articles mention the correct answer India ( and other countries ) , but cover different topics ( e.g. Mumbai , Arabian .",25,0,28
dataset/preprocessed/training-data/natural_language_inference/69,"Finding the answer requires multi-hop reasoning : figuring out that the Hanging Gardens are located in Mumbai , and then , from a second document , that Mumbai is a city in India .",26,0,34
dataset/preprocessed/training-data/natural_language_inference/69,We define a novel RC task in which a model should learn to answer queries by combining evidence stated across documents .,27,0,22
dataset/preprocessed/training-data/natural_language_inference/69,We introduce a methodology to induce datasets for this task and derive two datasets .,28,0,15
dataset/preprocessed/training-data/natural_language_inference/69,"The first , WIKIHOP , uses sets of WIKIPEDIA articles where answers to queries about specific properties of an entity can not be located in the entity 's article .",29,0,30
dataset/preprocessed/training-data/natural_language_inference/69,"In the second dataset , MEDHOP , the goal is to establish drug - drug interactions based on scientific findings about drugs and proteins and their interactions , found across multiple MEDLINE abstracts .",30,0,34
dataset/preprocessed/training-data/natural_language_inference/69,"For both datasets we draw upon existing Knowledge Bases ( KBs ) , WIKIDATA and DRUG - BANK , as ground truth , utilizing distant supervision ) to induce the data - similar to and .",31,0,36
dataset/preprocessed/training-data/natural_language_inference/69,"We establish that for 74.1 % and 68.0 % of the samples , the answer can be inferred from the given documents by a human annotator .",32,0,27
dataset/preprocessed/training-data/natural_language_inference/69,"Still , constructing multi-document datasets is challenging ; we encounter and prescribe remedies for several pitfalls associated with their assembly - for example , spurious co-locations of answers and specific documents .",33,0,32
dataset/preprocessed/training-data/natural_language_inference/69,For both datasets we then establish several strong baselines and evaluate the performance of two previously proposed competitive RC models .,34,0,21
dataset/preprocessed/training-data/natural_language_inference/69,"We find that one can integrate information across documents , but neither excels at selecting relevant information from a larger documents set , as their accuracy increases sig - nificantly when given only documents guaranteed to be relevant .",35,0,39
dataset/preprocessed/training-data/natural_language_inference/69,"The best model reaches 54.5 % on an annotated test set , compared to human performance at 85.0 % , indicating ample room for improvement .",36,0,26
dataset/preprocessed/training-data/natural_language_inference/69,"In summary , our key contributions are as follows :",37,0,10
dataset/preprocessed/training-data/natural_language_inference/69,"Firstly , proposing a cross - document multi-step RC task , as well as a general dataset induction strategy .",38,0,20
dataset/preprocessed/training-data/natural_language_inference/69,"Secondly , assembling two datasets from different domains and identifying dataset construction pitfalls and remedies .",39,0,16
dataset/preprocessed/training-data/natural_language_inference/69,"Thirdly , establishing multiple baselines , including two recently proposed RC models , as well as analysing model behaviour in detail through ablation studies .",40,0,25
dataset/preprocessed/training-data/natural_language_inference/69,Task and Dataset Construction Method,41,0,5
dataset/preprocessed/training-data/natural_language_inference/69,"We will now formally define the multi -hop RC task , and a generic methodology to construct multi - hop RC datasets .",42,0,23
dataset/preprocessed/training-data/natural_language_inference/69,"Later , in Sections 3 and 4 we will demonstrate how this method is applied in practice by creating datasets for two different domains .",43,0,25
dataset/preprocessed/training-data/natural_language_inference/69,"A model is given a query q , a set of supporting documents Sq , and a set of candidate answers C q - all of which are mentioned in Sq .",45,0,32
dataset/preprocessed/training-data/natural_language_inference/69,The goal is to identify the correct answer a * ?,46,0,11
dataset/preprocessed/training-data/natural_language_inference/69,C q by drawing on the support documents,47,0,8
dataset/preprocessed/training-data/natural_language_inference/69,"Sq . Queries could potentially have several true answers when not constrained to rely on a specific set of support documents - e.g. , queries about the parent of a certain individual .",48,0,33
dataset/preprocessed/training-data/natural_language_inference/69,"However , in our setup each sample has only one true answer among C q and Sq . Note that even though we will utilize background information during dataset assembly , such information will not be available to a model : the document set will be provided in random order and without any metadata .",49,0,55
dataset/preprocessed/training-data/natural_language_inference/99,Smarnet : Teaching Machines to Read and Comprehend Like Human,2,0,10
dataset/preprocessed/training-data/natural_language_inference/99,"Machine Comprehension ( MC ) is a challenging task in Natural Language Processing field , which aims to guide the machine to comprehend a passage and answer the given question .",4,1,31
dataset/preprocessed/training-data/natural_language_inference/99,"Many existing approaches on MC task are suffering the inefficiency in some bottlenecks , such as insufficient lexical understanding , complex question - passage interaction , incorrect answer extraction and soon .",5,1,32
dataset/preprocessed/training-data/natural_language_inference/99,"In this paper , we address these problems from the viewpoint of how humans deal with reading tests in a scientific way .",6,0,23
dataset/preprocessed/training-data/natural_language_inference/99,"Specifically , we first propose a novel lexical gating mechanism to dynamically combine the words and characters representations .",7,0,19
dataset/preprocessed/training-data/natural_language_inference/99,We then guide the machines to read in an interactive way with attention mechanism and memory network .,8,0,18
dataset/preprocessed/training-data/natural_language_inference/99,Finally we add a checking layer to refine the answer for insurance .,9,0,13
dataset/preprocessed/training-data/natural_language_inference/99,The extensive experiments on two popular datasets SQuAD and Trivia QA show that our method exceeds considerable performance than most stateof - the - art solutions at the time of submission .,10,0,32
dataset/preprocessed/training-data/natural_language_inference/99,Teaching machines to learn reading comprehension is one of the core tasks in NLP field .,12,0,16
dataset/preprocessed/training-data/natural_language_inference/99,Recently machine comprehension task accumulates much concern among NLP researchers .,13,1,11
dataset/preprocessed/training-data/natural_language_inference/99,"We have witnessed significant progress since the release of large - scale datasets like SQuAD , MS - MARCO , , ) and Children 's Book Test .",14,0,28
dataset/preprocessed/training-data/natural_language_inference/99,The essential problem of machine comprehension is to predict the correct answer referring to a given passage with relevant question .,15,0,21
dataset/preprocessed/training-data/natural_language_inference/99,"If a machine can obtain a good score from predicting the right answer , we can say the machine is capable of understanding the given context .",16,0,27
dataset/preprocessed/training-data/natural_language_inference/99,Many previous approaches ) ( Gong and Bowman 2017 ) ) adopt attention mechanisms along with deep neural network tactics and pointer network to establish interactions between the question and the passage .,17,0,33
dataset/preprocessed/training-data/natural_language_inference/99,The superiority of these frameworks are to enable questions focus on more relevant targeted are as within passages .,18,0,19
dataset/preprocessed/training-data/natural_language_inference/99,"Although these works have achieved promising performance for MC task , most of them still suffer from the The majority of the work was done while the first author was interning at the Eigen Technologies .",19,0,36
dataset/preprocessed/training-data/natural_language_inference/99,"For all the time through , we consider a philosophy question :",20,0,12
dataset/preprocessed/training-data/natural_language_inference/99,What will people do when they are having a reading comprehension test ?,21,0,13
dataset/preprocessed/training-data/natural_language_inference/99,Recall how our teacher taught us may shed some light .,22,0,11
dataset/preprocessed/training-data/natural_language_inference/99,"As a student , we recite words with relevant properties such as part - of - speech tag , the synonyms , entity type and soon .",23,0,27
dataset/preprocessed/training-data/natural_language_inference/99,"In order to promote answer 's accuracy , we iteratively and interactively read the question and the passage to locate the answer 's boundary .",24,0,25
dataset/preprocessed/training-data/natural_language_inference/99,Sometimes we will check the answer to ensure the refining accuracy .,25,0,12
dataset/preprocessed/training-data/natural_language_inference/99,Here we draw a flow path to depict what on earth the scientific reading skills are in the .,26,0,19
dataset/preprocessed/training-data/natural_language_inference/99,"As we see , basic word understanding , iterative reading interaction and attentive checking are crucial in order to guarantee the answer accuracy .",27,0,24
dataset/preprocessed/training-data/natural_language_inference/99,"In this paper , we propose the novel framework named Smarnet with the hope that it can become as smart as humans .",28,0,23
dataset/preprocessed/training-data/natural_language_inference/99,We design the structure in the viewpoint of imitating how humans take the reading comprehension test .,29,0,17
dataset/preprocessed/training-data/natural_language_inference/99,"Specifically , we first introduce the Smarnet framework that exploits fine - grained word understanding with various attribution discriminations , like humans recite words with corresponding properties .",30,0,28
dataset/preprocessed/training-data/natural_language_inference/99,We then develop the interactive attention with memory network to mimic human reading procedure .,31,0,15
dataset/preprocessed/training-data/natural_language_inference/99,We also add a checking layer on the answer refining in order to ensure the accuracy .,32,0,17
dataset/preprocessed/training-data/natural_language_inference/99,The main contributions of this paper are as follows : :,33,0,11
dataset/preprocessed/training-data/natural_language_inference/99,Fine - grained gating on lexical attributions of words and characters .,34,0,12
dataset/preprocessed/training-data/natural_language_inference/99,""" POS , NER , TF , EM , Surpris al , QType "" refer to part - of - speech tags , named entity tags , term frequency , exact match , surpris al extent , question type .",35,0,40
dataset/preprocessed/training-data/natural_language_inference/99,We enrich the word representation with detailed lexical properties .,36,0,10
dataset/preprocessed/training-data/natural_language_inference/99,We adopt a fine - grained gating mechanism to dynamically control the amount of word level and character level representations based on the properties of words .,37,0,27
dataset/preprocessed/training-data/natural_language_inference/99,We guide the machine to read by imitating human 's behavior in the reading procedure .,38,0,16
dataset/preprocessed/training-data/natural_language_inference/99,We apply the Interactive attention to comprehensively model the question and passage representation .,39,0,14
dataset/preprocessed/training-data/natural_language_inference/99,We adopt memory network to enhance the comprehending capacity and accuracy .,40,0,12
dataset/preprocessed/training-data/natural_language_inference/99,We utilize a checking mechanism with passage self alignment on the revised pointer network .,41,0,15
dataset/preprocessed/training-data/natural_language_inference/99,This helps to locate the answer boundary and promote the prediction accuracy for insurance .,42,0,15
dataset/preprocessed/training-data/natural_language_inference/99,The goal of open - domain MC task is to infer the proper answer from the given text .,44,0,19
dataset/preprocessed/training-data/natural_language_inference/99,"For notation , given a passage P = {p 1 , p 2 , ... , pm } and a question Q = {q 1 , q 2 , ... , q n } , where m and n are the length of the passage and the question .",45,0,49
dataset/preprocessed/training-data/natural_language_inference/99,"Each token is denoted as ( w i , Ci ) , where w i is the word embedding extracts from pre-trained word embedding lookups , Ci is the char - level matrix representing one - hot encoding of characters .",46,0,41
dataset/preprocessed/training-data/natural_language_inference/99,"The model should read and comprehend the interactions between P and Q , and predict an answer A based on a continuous sub-span of P .",47,0,26
dataset/preprocessed/training-data/natural_language_inference/99,The general framework of MC task can be coarsely summarized as a three - layer hierarchical process :,49,0,18
dataset/preprocessed/training-data/natural_language_inference/23,FUSIONNET : FUSING VIA FULLY - AWARE ATTENTION WITH APPLICATION TO MACHINE COMPREHENSION,2,1,13
dataset/preprocessed/training-data/natural_language_inference/23,"This paper introduces a new neural structure called FusionNet , which extends existing attention approaches from three perspectives .",4,0,19
dataset/preprocessed/training-data/natural_language_inference/23,"First , it puts forward a novel concept of "" history of word "" to characterize attention information from the lowest word - level embedding up to the highest semantic - level representation .",5,0,34
dataset/preprocessed/training-data/natural_language_inference/23,"Second , it identifies an attention scoring function that better utilizes the "" history of word "" concept .",6,0,19
dataset/preprocessed/training-data/natural_language_inference/23,"Third , it proposes a fully - aware multi-level attention mechanism to capture the complete information in one text ( such as a question ) and exploit it in its counterpart ( such as context or passage ) layer by layer .",7,0,42
dataset/preprocessed/training-data/natural_language_inference/23,"We apply FusionNet to the Stanford Question Answering Dataset ( SQuAD ) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing ( Oct. 4th , 2017 ) .",8,0,41
dataset/preprocessed/training-data/natural_language_inference/23,"Meanwhile , we verify the generalization of Fusion - Net with two adversarial SQuAD datasets and it sets up the new state - of - the - art on both datasets : on AddSent , FusionNet increases the best F1 metric from 46.6 % to 51.4 % ; on AddOneSent , FusionNet boosts the best F1 metric from 56.0 % to 60.7 % .",9,0,64
dataset/preprocessed/training-data/natural_language_inference/23,A broad - coverage challenge corpus for sentence understanding through inference .,10,0,12
dataset/preprocessed/training-data/natural_language_inference/23,"arXiv preprint ar Xiv :1704.05426 , 2017 .",11,0,8
dataset/preprocessed/training-data/natural_language_inference/23,"In this appendix , we compare with published state - of - the - art architectures on the SQuAD dev set .",12,0,22
dataset/preprocessed/training-data/natural_language_inference/23,The comparison is shown in Figure 5 and 6 for EM and F1 score respectively .,13,0,16
dataset/preprocessed/training-data/natural_language_inference/23,The performance of FusionNet is shown under different training epochs .,14,0,11
dataset/preprocessed/training-data/natural_language_inference/23,Each epoch loops through all the examples in the training set once .,15,0,13
dataset/preprocessed/training-data/natural_language_inference/23,"On a single NVIDIA GeForce GTX Titan X GPU , each epoch took roughly 20 minutes when batch size 32 is used .",16,0,23
dataset/preprocessed/training-data/natural_language_inference/23,"Context : The Alpine Rhine is part of the Rhine , a famous European river .",18,0,16
dataset/preprocessed/training-data/natural_language_inference/23,"The Alpine Rhine begins in the most western part of the Swiss canton of Graubnden , and later forms the border between Switzerland to the West and Liechtenstein and later Austria to the East .",19,0,35
dataset/preprocessed/training-data/natural_language_inference/23,"On the other hand , the Danube separates Romania and Bulgaria .",20,0,12
dataset/preprocessed/training-data/natural_language_inference/23,What is the other country the Rhine separates Switzerland to ?,22,0,11
dataset/preprocessed/training-data/natural_language_inference/23,Answer : Liechtenstein : Question - answer pair for a passage discussing Alpine Rhine .,23,0,15
dataset/preprocessed/training-data/natural_language_inference/23,"Teaching machines to read , process and comprehend text and then answer questions is one of key problems in artificial intelligence .",24,1,22
dataset/preprocessed/training-data/natural_language_inference/23,gives an example of the machine reading comprehension task .,25,0,10
dataset/preprocessed/training-data/natural_language_inference/23,It feeds a machine with apiece of context and a question and teaches it to find a correct answer to the question .,26,0,23
dataset/preprocessed/training-data/natural_language_inference/23,"This requires the machine to possess high capabilities in comprehension , inference and reasoning .",27,0,15
dataset/preprocessed/training-data/natural_language_inference/23,This is considered a challenging task in artificial intelligence and has already attracted numerous research efforts from the neural network and natural language processing communities .,28,0,26
dataset/preprocessed/training-data/natural_language_inference/23,Many neural network models have been proposed for this challenge and they generally frame this problem as a machine reading comprehension ( MRC ) task .,29,1,26
dataset/preprocessed/training-data/natural_language_inference/23,"The key innovation in recent models lies in how to ingest information in the question and characterize it in the context , in order to provide an accurate answer to the question .",30,0,33
dataset/preprocessed/training-data/natural_language_inference/23,"This is often modeled as attention in the neural network community , which is a mechanism to attend the question into the context so as to find the answer related to the question .",31,0,34
dataset/preprocessed/training-data/natural_language_inference/23,"Some attend the word - level embedding from the question to context , while some attend the high - level representation in the question to augment the context .",32,0,29
dataset/preprocessed/training-data/natural_language_inference/23,"However we observed that none of the existing approaches has captured the full information in the context or the question , which could be vital for complete information comprehension .",33,0,30
dataset/preprocessed/training-data/natural_language_inference/23,"Taking image recognition as an example , information in various levels of representations can capture different aspects of details in an image : pixel , stroke and shape .",34,0,29
dataset/preprocessed/training-data/natural_language_inference/23,We argue that this hypothesis also holds in language understanding and MRC .,35,1,13
dataset/preprocessed/training-data/natural_language_inference/23,"In other words , an approach that utilizes all the information from the word embedding level up to the highest level representation would be substantially beneficial for understanding both the question and the context , hence yielding more accurate answers .",36,0,41
dataset/preprocessed/training-data/natural_language_inference/23,"However , the ability to consider all layers of representation is often limited by the difficulty to make the neural model learn well , as model complexity will surge beyond capacity .",37,0,32
dataset/preprocessed/training-data/natural_language_inference/23,We conjectured this is why previous literature tailored their models to only consider partial information .,38,0,16
dataset/preprocessed/training-data/natural_language_inference/23,"To alleviate this challenge , we identify an attention scoring function utilizing all layers of representation with less training burden .",39,0,21
dataset/preprocessed/training-data/natural_language_inference/23,This leads to an attention that thoroughly captures the complete information between the question and the context .,40,0,18
dataset/preprocessed/training-data/natural_language_inference/23,"With this fully - aware attention , we put forward a multi -level attention mechanism to understand the information in the question , and exploit it layer by layer on the context side .",41,0,34
dataset/preprocessed/training-data/natural_language_inference/23,"All of these innovations are integrated into a new end - to - end structure called FusionNet in , with details described in Section 3 .",42,0,26
dataset/preprocessed/training-data/natural_language_inference/23,"We submitted FusionNet to , a machine reading comprehension dataset .",43,0,11
dataset/preprocessed/training-data/natural_language_inference/23,"At the time of writing , our model ranked in the first place in both single model and ensemble model categories .",44,0,22
dataset/preprocessed/training-data/natural_language_inference/23,The ensemble model achieves an exact match ( EM ) score of 78.8 % and F1 score of 85.9 % .,45,0,21
dataset/preprocessed/training-data/natural_language_inference/23,"Furthermore , we have tested FusionNet against adversarial SQuAD datasets .",46,0,11
dataset/preprocessed/training-data/natural_language_inference/23,"Results show that FusionNet outperforms existing state - of - the - art architectures in both datasets : on AddSent , FusionNet increases the best F1 metric from 46.6 % to 51.4 % ; on AddOneSent , FusionNet boosts the best F1 metric from 56.0 % to 60.7 % .",47,0,50
dataset/preprocessed/training-data/natural_language_inference/23,"In Appendix D , we also applied to natural language inference task and shown decent improvement .",48,0,17
dataset/preprocessed/training-data/natural_language_inference/23,This demonstrated the exceptional performance of FusionNet .,49,0,8
dataset/preprocessed/training-data/natural_language_inference/58,ReasoNet : Learning to Stop Reading in Machine Comprehension,2,1,9
dataset/preprocessed/training-data/natural_language_inference/58,Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem .,4,0,20
dataset/preprocessed/training-data/natural_language_inference/58,"In this paper , we describe a novel neural network architecture called the Reasoning Network ( ReasoNet ) for machine comprehension tasks .",5,0,23
dataset/preprocessed/training-data/natural_language_inference/58,"ReasoNets make use of multiple turns toe ectively exploit and then reason over the relation among queries , documents , and answers .",6,0,23
dataset/preprocessed/training-data/natural_language_inference/58,"Di erent from previous approaches using a xed number of turns during inference , ReasoNets introduce a termination state to relax this constraint on the reasoning depth .",7,0,28
dataset/preprocessed/training-data/natural_language_inference/58,"With the use of reinforcement learning , ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results , or to terminate reading when it concludes that existing information is adequate to produce an answer .",8,0,39
dataset/preprocessed/training-data/natural_language_inference/58,"ReasoNets achieve superior performance in machine comprehension datasets , including unstructured CNN and Daily Mail datasets , the Stanford SQuAD dataset , and a structured Graph Reachability dataset .",9,0,29
dataset/preprocessed/training-data/natural_language_inference/58,"Teaching machines to read , process , and comprehend natural language documents is a coveted goal for arti cial intelligence .",11,0,21
dataset/preprocessed/training-data/natural_language_inference/58,"Genuine reading comprehension is extremely challenging , since e ective comprehension involves thorough understanding of documents and sophisticated inference .",12,0,20
dataset/preprocessed/training-data/natural_language_inference/58,"Toward solving this machine reading comprehension problem , in recent years , several works have collected various datasets , in the form of question , passage , and answer , to test machine on answering a question based on the provided passage .",13,0,43
dataset/preprocessed/training-data/natural_language_inference/58,Some large - scale cloze - style datasets have gained signi cant attention along with powerful deep learning models .,14,0,20
dataset/preprocessed/training-data/natural_language_inference/58,Recent approaches on cloze - style datasets can be separated into two categories : single - turn and multi-turn reasoning .,15,0,21
dataset/preprocessed/training-data/natural_language_inference/58,Single turn reasoning models utilize attention mechanisms to emphasize speci c parts of the document which are relevant to the query .,16,0,22
dataset/preprocessed/training-data/natural_language_inference/58,Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro tor commercial advantage and that copies bear this notice and the full citation on the rst page .,17,0,51
dataset/preprocessed/training-data/natural_language_inference/58,Copyrights for components of this work owned by others than ACM must be honored .,18,0,15
dataset/preprocessed/training-data/natural_language_inference/58,Abstracting with credit is permitted .,19,0,6
dataset/preprocessed/training-data/natural_language_inference/58,"To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior speci c permission and / or a fee .",20,0,28
dataset/preprocessed/training-data/natural_language_inference/58,Request permissions from permissions@acm.org .,21,0,5
dataset/preprocessed/training-data/natural_language_inference/58,"KDD '17 , August 13 - 17 , 2017 , Halifax , NS , Canada 2017 ACM .",22,0,18
dataset/preprocessed/training-data/natural_language_inference/58,978-1-4503-4887-4/17/08 . . . $ 15.00,23,0,6
dataset/preprocessed/training-data/natural_language_inference/58,DOI : These attention models subsequently calculate the relevance between a query and the corresponding weighted representations of document subunits ( e.g. sentences or words ) to score target candidates .,24,0,31
dataset/preprocessed/training-data/natural_language_inference/58,"However , considering the sophistication of the problem , after a single - turn comprehension , readers often revisit some speci c passage or the question to grasp a better understanding of the problem .",25,0,35
dataset/preprocessed/training-data/natural_language_inference/58,"With this motivation , recent advances in reading comprehension have made use of multiple turns to infer the relation between query , document and answer .",26,0,26
dataset/preprocessed/training-data/natural_language_inference/58,"By repeatedly processing the document and the question after digesting intermediate information , multi-turn reasoning can generally produce a better answer and these existing works have demonstrated its superior performance consistently .",27,0,32
dataset/preprocessed/training-data/natural_language_inference/58,Existing multi-turn models have a pre-de ned number of hops or iterations in their inference without regard to the complexity of each individual query or document .,28,0,27
dataset/preprocessed/training-data/natural_language_inference/58,"However , when human read a document with a question in mind , we often decide whether we want to stop reading if we believe the observed information is adequate already to answer the question , or continue reading after digesting intermediate information until we can answer the question with con dence .",29,0,53
dataset/preprocessed/training-data/natural_language_inference/58,This behavior generally varies from document to document or question to question because it is related to the sophistication of the document or the di culty of the question .,30,0,30
dataset/preprocessed/training-data/natural_language_inference/58,"Meanwhile , the analysis in also illustrates the huge variations in the di culty level with respect to questions in the CNN / Daily Mail datasets .",31,0,27
dataset/preprocessed/training-data/natural_language_inference/58,"For a signi ca nt part of the datasets , this analysis shows that the problem can not be solved without appropriate reasoning on both its query and document .",32,0,30
dataset/preprocessed/training-data/natural_language_inference/58,"With this motivation , we propose a novel neural network architecture called Reasoning Network ( ReasoNet ) .",33,0,18
dataset/preprocessed/training-data/natural_language_inference/58,which tries to mimic the inference process of human readers .,34,0,11
dataset/preprocessed/training-data/natural_language_inference/58,"With a question in mind , ReasoNets read a document repeatedly , each time focusing on di erent parts of the document until a satisfying answer is found or formed .",35,0,31
dataset/preprocessed/training-data/natural_language_inference/58,"This reminds us of a Chinese proverb : "" The meaning of a book will become clear if you read it hundreds of times . "" .",36,0,27
dataset/preprocessed/training-data/natural_language_inference/58,"Moreover , unlike previous approaches using xed number of hops or iterations , ReasoNets introduce a termination state in the inference .",37,0,22
dataset/preprocessed/training-data/natural_language_inference/58,"This state can decide whether to continue the inference to the next turn after digesting intermediate information , or to terminate the whole inference when it concludes that existing information is sufcient to yield an answer .",38,0,37
dataset/preprocessed/training-data/natural_language_inference/58,"The number of turns in the inference is dynamically modeled by both the document and the query , and can be learned automatically according to the di culty of the problem .",39,0,32
dataset/preprocessed/training-data/natural_language_inference/58,"One of the signi cant challenges ReasoNets face is how to design an e cient training method , since the termination state is discrete and not connected to the nal output .",40,0,32
dataset/preprocessed/training-data/natural_language_inference/58,This prohibits canonical back - propagation method being directly applied to train ReasoNets .,41,0,14
dataset/preprocessed/training-data/natural_language_inference/58,"Motivated by , we tackle this challenge by proposing a reinforcement learning approach , which utilizes an instance - dependent reward baseline , to successfully train ReasoNets .",42,0,28
dataset/preprocessed/training-data/natural_language_inference/58,"Finally , by accounting for a dynamic termination state during inference and applying proposed deep reinforcement learning optimization method , ReasoNets achieve the state - of - the - art results in machine comprehension datasets , including unstructured CNN and Daily Mail datasets , and the proposed structured Graph Reachability dataset , when the paper is rst publicly available on arXiv .",43,0,62
dataset/preprocessed/training-data/natural_language_inference/58,"At the time of the paper submission , we apply ReasoNet to the competitive Stanford Question Answering Dataset ( SQuAD ) , ReasoNets outperform all existing published approaches and rank at second place on the test set leaderboard .",44,0,39
dataset/preprocessed/training-data/natural_language_inference/58,This paper is organized as follows .,45,0,7
dataset/preprocessed/training-data/natural_language_inference/58,"In Section 2 , we review and compare recent work on machine reading comprehension tasks .",46,0,16
dataset/preprocessed/training-data/natural_language_inference/58,"In Section 3 , we introduce our proposed ReasoNet model architecture and training objectives .",47,0,15
dataset/preprocessed/training-data/natural_language_inference/58,Section 4 presents the experimental setting and results on unstructured and structured machine reading comprehension tasks .,48,0,17
dataset/preprocessed/training-data/natural_language_inference/91,A Fast Unified Model for Parsing and Sentence Understanding,2,1,9
dataset/preprocessed/training-data/natural_language_inference/91,Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences .,4,0,16
dataset/preprocessed/training-data/natural_language_inference/91,"However , they suffer from two key technical problems that make them slow and unwieldy for large - scale NLP tasks : they usually operate on parsed sentences and they do not directly support batched computation .",5,0,37
dataset/preprocessed/training-data/natural_language_inference/91,"We address these issues by introducing the Stackaugmented Parser - Interpreter Neural Network ( SPINN ) , which combines parsing and interpretation within a single treesequence hybrid model by integrating treestructured sentence interpretation into the linear sequential structure of a shift - reduce parser .",6,0,45
dataset/preprocessed/training-data/natural_language_inference/91,"Our model supports batched computation for a speedup of up to 25 over other tree - structured models , and its integrated parser can operate on unparsed data with little loss in accuracy .",7,0,34
dataset/preprocessed/training-data/natural_language_inference/91,We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence - encoding models .,8,0,21
dataset/preprocessed/training-data/natural_language_inference/91,"A wide range of current models in NLP are built around a neural network component that produces vector representations of sentence meaning ( e.g. , .",10,0,26
dataset/preprocessed/training-data/natural_language_inference/91,"This component , the sentence encoder , is generally formulated as a learned parametric function from a sequence of word vectors to a sentence vector , and this function can take a range of different forms .",11,0,37
dataset/preprocessed/training-data/natural_language_inference/91,Common sentence encoders include sequence - based recurrent neural network * The first two authors contributed equally . :,12,0,19
dataset/preprocessed/training-data/natural_language_inference/91,An illustration of two standard designs for sentence encoders .,13,0,10
dataset/preprocessed/training-data/natural_language_inference/91,"The TreeRNN , unlike the sequence - based RNN , requires a substantially different connection structure for each sentence , making batched computation impractical .",14,0,25
dataset/preprocessed/training-data/natural_language_inference/91,"models ( RNNs , see ) with Long Short - Term Memory ( LSTM , , which accumulate information over the sentence sequentially ; convolutional neural networks , which accumulate information using filters over short local sequences of words or characters ; and tree - structured recursive neural networks ( TreeRNNs , ) , which propagate information up a binary parse tree .",15,0,63
dataset/preprocessed/training-data/natural_language_inference/91,"Of these , the TreeRNN appears to be the principled choice , since meaning in natural language sentences is known to be constructed recursively according to a tree structure .",16,0,30
dataset/preprocessed/training-data/natural_language_inference/91,"TreeRNNs have shown promise , but have largely been overlooked in favor of sequencebased RNNs because of their incompatibility with batched computation and their reliance on external parsers .",17,0,29
dataset/preprocessed/training-data/natural_language_inference/91,"Batched computation - performing synchronized computation across many examples at once - yields order - of - magnitude improvements in model run time , and is crucial in enabling neural networks to be trained efficiently on large datasets .",18,0,39
dataset/preprocessed/training-data/natural_language_inference/91,"Because TreeRNNs use a different model structure for each sentence , as in , efficient batching is impossible in standard implementations .",19,0,22
dataset/preprocessed/training-data/natural_language_inference/91,"Partly to address efficiency problems , standard TreeRNN models commonly only operate on sentences that have already been processed by a syntactic parser , which slows and complicates the use of these models at test time for most applications .",20,0,40
dataset/preprocessed/training-data/natural_language_inference/91,"This paper introduces a new model to address both these issues : the Stack - augmented Parser - Interpreter Neural Network , or SPINN , shown in .",21,0,28
dataset/preprocessed/training-data/natural_language_inference/91,"SPINN executes the computations of a tree - structured model in a linearized sequence , and can incorporate a neural network parser that produces the required parse structure on the fly .",22,0,32
dataset/preprocessed/training-data/natural_language_inference/91,This design improves upon the TreeRNN architecture in three ways :,23,0,11
dataset/preprocessed/training-data/natural_language_inference/91,"At test time , it can simultaneously parse and interpret unparsed sentences , removing the dependence on an external parser at nearly no additional computational cost .",24,0,27
dataset/preprocessed/training-data/natural_language_inference/91,"Secondly , it supports batched computation for both parsed and unparsed sentences , yielding dramatic speedups over standard TreeRNNs .",25,0,20
dataset/preprocessed/training-data/natural_language_inference/91,"Finally , it supports a novel tree - sequence hybrid architecture for handling local linear context in sentence interpretation .",26,0,20
dataset/preprocessed/training-data/natural_language_inference/91,This model is a basically plausible model of human sentence processing and yields substantial accuracy gains over pure sequence - or tree - based models .,27,0,26
dataset/preprocessed/training-data/natural_language_inference/91,"We evaluate SPINN on the Stanford Natural Language Inference entailment task , and find that it significantly outperforms other sentence - encoding - based models , even with a relatively simple and underpowered implementation of the built - in parser .",28,0,41
dataset/preprocessed/training-data/natural_language_inference/91,We also find that SPINN yields speed increases of up to 25 over a standard TreeRNN implementation .,29,0,18
dataset/preprocessed/training-data/natural_language_inference/91,"There is a fairly long history of work on building neural network - based parsers that use the core operations and data structures from transition - based parsing , of which shift - reduce parsing is a variant .",31,0,39
dataset/preprocessed/training-data/natural_language_inference/91,"In addition , there has been recent work proposing models de-signed primarily for generative language modeling tasks that use this architecture as well .",32,0,24
dataset/preprocessed/training-data/natural_language_inference/91,"To our knowledge , SPINN is the first model to use this architecture for the purpose of sentence interpretation , rather than parsing or generation .",33,0,26
dataset/preprocessed/training-data/natural_language_inference/91,present versions of the TreeRNN model which are capable of operating over unparsed inputs .,34,0,15
dataset/preprocessed/training-data/natural_language_inference/91,"However , these methods require an expensive search process at test time .",35,0,13
dataset/preprocessed/training-data/natural_language_inference/91,Our model presents a much faster alternative approach .,36,0,9
dataset/preprocessed/training-data/natural_language_inference/91,"Our model : SPINN 3.1 Background : Shift - reduce parsing SPINN is inspired by shift - reduce parsing , which builds a tree structure over a sequence ( e.g. , a natural language sentence ) by a single left - to - right scan over its tokens .",38,0,49
dataset/preprocessed/training-data/natural_language_inference/91,"The formalism is widely used in natural language parsing ( e.g. , .",39,0,13
dataset/preprocessed/training-data/natural_language_inference/91,"A shift - reduce parser accepts a sequence of input tokens x = ( x 0 , . . . , x N?1 ) and consumes transitions a = ( a 0 , . . . , a T ?1 ) , where each at ?",40,0,46
dataset/preprocessed/training-data/natural_language_inference/91,"{ shift , reduce } specifies one step of the parsing process .",41,0,13
dataset/preprocessed/training-data/natural_language_inference/91,In general a parser may also generate these transitions on the fly as it reads the tokens .,42,0,18
dataset/preprocessed/training-data/natural_language_inference/91,"It proceeds left - to - right through a transition sequence , combining the input tokens x incrementally into a tree structure .",43,0,23
dataset/preprocessed/training-data/natural_language_inference/91,"For any binarybranching tree structure over N words , this requires T = 2N ?",44,0,15
dataset/preprocessed/training-data/natural_language_inference/91,1 transitions through a total of T + 1 states .,45,0,11
dataset/preprocessed/training-data/natural_language_inference/91,The parser uses two auxiliary data structures : a stack S of partially completed subtrees and a buffer B of tokens yet to be parsed .,46,0,26
dataset/preprocessed/training-data/natural_language_inference/91,The parser is initialized with the stack empty and the buffer containing the tokens x of the sentence in order .,47,0,21
dataset/preprocessed/training-data/natural_language_inference/91,"Let S , B = ? , x denote this starting state .",48,0,13
dataset/preprocessed/training-data/natural_language_inference/91,"It next proceeds through the transition sequence , where each transition at selects one of the two following operations .",49,0,20
dataset/preprocessed/training-data/natural_language_inference/98,Recurrent Neural Network - Based Sentence Encoder with Gated Attention for Natural Language Inference,2,1,14
dataset/preprocessed/training-data/natural_language_inference/98,The RepEval 2017,4,0,3
dataset/preprocessed/training-data/natural_language_inference/98,"Task aims to evaluate natural language understanding models for sentence representation , in which a sentence is represented as a fixedlength vector with neural networks and the quality of the representation is tested with a natural language inference task .",6,0,40
dataset/preprocessed/training-data/natural_language_inference/98,"This paper describes our system ( alpha ) that is ranked among the top in the Shared Task , on both the in - domain test set ( obtaining a 74.9 % accuracy ) and on the crossdomain test set ( also attaining a 74.9 % accuracy ) , demonstrating that the model generalizes well to the cross -domain data .",7,0,61
dataset/preprocessed/training-data/natural_language_inference/98,Our model is equipped with intra-sentence gated - attention composition which helps achieve a better performance .,8,0,17
dataset/preprocessed/training-data/natural_language_inference/98,"In addition to submitting our model to the Shared Task , we have also tested it on the Stanford Natural Language Inference ( SNLI ) dataset .",9,0,27
dataset/preprocessed/training-data/natural_language_inference/98,"We obtain an accuracy of 85.5 % , which is the best reported result on SNLI when cross - sentence attention is not allowed , the same condition enforced in RepEval 2017 .",10,0,33
dataset/preprocessed/training-data/natural_language_inference/98,The RepEval 2017,12,0,3
dataset/preprocessed/training-data/natural_language_inference/98,"Task aims to evaluate language understanding models for sentence representation with natural language inference ( NLI ) tasks , where a sentence is represented as a fixedlength vector .",14,1,29
dataset/preprocessed/training-data/natural_language_inference/98,Modeling inference in human language is very challenging but is a basic problem in natural language understanding .,15,0,18
dataset/preprocessed/training-data/natural_language_inference/98,"Specifically , NLI is concerned with determining whether a hypothesis sentence h can be inferred from a premise sentence p.",16,1,20
dataset/preprocessed/training-data/natural_language_inference/98,"Most previous top - performing neural network models on NLI use attention models between a premise and its hypothesis , while how much information can be encoded in a fixed - length vector without such cross - sentence attention deserves some further understanding .",17,0,44
dataset/preprocessed/training-data/natural_language_inference/98,"In this paper , we describe the model we submitted to the RepEval 2017 Shared Task , which achieves the top performance on both the indomain and cross - domain test set .",18,0,33
dataset/preprocessed/training-data/natural_language_inference/98,"Natural language inference ( NLI ) , also named recognizing textual entailment ( RTE ) includes a large bulk of early work on rather small datasets with more conventional methods ) .",20,0,32
dataset/preprocessed/training-data/natural_language_inference/98,"More recently , the large datasets are available , which makes it possible to train natural language inference models based on neural networks .",21,0,24
dataset/preprocessed/training-data/natural_language_inference/98,"Natural language inference models based on neural networks are mainly separated into two kind of ways , sentence encoder - based models and cross - sentence attention - based models .",22,0,31
dataset/preprocessed/training-data/natural_language_inference/98,"Among them , Enhanced Sequential Inference Model ( ESIM ) with cross - sentence attention represents the state of the art .",23,0,22
dataset/preprocessed/training-data/natural_language_inference/98,"However , in this paper we principally concentrate on sentence encoder - based model .",24,0,15
dataset/preprocessed/training-data/natural_language_inference/98,Many researchers have studied sentence encoder - based model for natural language inference .,25,0,14
dataset/preprocessed/training-data/natural_language_inference/98,"It is , however , not very clear if the potential of the sentence encoderbased model has been well exploited .",26,0,21
dataset/preprocessed/training-data/natural_language_inference/98,"In this paper , we demonstrate that proposed models based on gated - attention can achieve a new state - of - theart performance for natural language inference .",27,0,29
dataset/preprocessed/training-data/natural_language_inference/98,"We present here the proposed natural language inference networks which are composed of the following major components : word embedding , sequence encoder , composition layer , and the toplayer classifier .",29,0,32
dataset/preprocessed/training-data/natural_language_inference/98,shows a view of the architecture of our neural language inference network .,30,0,13
dataset/preprocessed/training-data/natural_language_inference/98,"In our notation , a sentence ( premise or hypothesis ) is indicated as x = ( x 1 , . . . , x l ) , where l is the length of the sentence .",32,0,37
dataset/preprocessed/training-data/natural_language_inference/98,We concatenate embeddings learned at two different levels to represent each word in the sentence : the character composition and holistic word - level embedding .,33,0,26
dataset/preprocessed/training-data/natural_language_inference/98,"The character composition feeds all characters of each word into a convolutional neural network ( CNN ) with max - pooling to obtain representations c = ( c 1 , . . . , cl ) .",34,0,37
dataset/preprocessed/training-data/natural_language_inference/98,"In addition , we also use the pre-trained Glo Ve vectors for each word as holistic wordlevel embedding w = ( w 1 , . . . , w l ) .",35,0,32
dataset/preprocessed/training-data/natural_language_inference/98,"Therefore , each word is represented as a concatenation of the character - composition vector and word - level embedding e = ( [ c 1 ; w 1 ] , . . . , [ c l ; w l ] ) .",36,0,44
dataset/preprocessed/training-data/natural_language_inference/98,"This is performed on both the premise and hypothesis , resulting into two matrices : thee p ?",37,0,18
dataset/preprocessed/training-data/natural_language_inference/98,R ndw for a premise and thee h ?,38,0,9
dataset/preprocessed/training-data/natural_language_inference/98,"R mdw for a hypothesis , where n and mare the length of the premise and hypothesis respectively , and d w is the embedding dimension .",39,0,27
dataset/preprocessed/training-data/natural_language_inference/98,"To represent words and their context in a premise and hypothesis , sentence pairs are fed into sentence encoders to obtain hidden vectors ( h p and h h ) .",41,0,31
dataset/preprocessed/training-data/natural_language_inference/98,We use stacked bidirectional LSTMs ( BiL - STM ) as the encoders .,42,0,14
dataset/preprocessed/training-data/natural_language_inference/98,"Shortcut connections are applied , which concatenate word embeddings and input hidden states at each layer in the stacked BiLSTM except for the bottom layer .",43,0,26
dataset/preprocessed/training-data/natural_language_inference/98,( 1 ),44,0,3
dataset/preprocessed/training-data/natural_language_inference/98,where d is the dimension of hidden states of LSTMs .,45,0,11
dataset/preprocessed/training-data/natural_language_inference/98,A BiLSTM concatenate a forward and backward LSTM on a sequence,46,0,11
dataset/preprocessed/training-data/natural_language_inference/98,", starting from the left and the right end , respectively .",47,0,12
dataset/preprocessed/training-data/natural_language_inference/98,Hidden states of unidirectional LSTM (,48,0,6
dataset/preprocessed/training-data/natural_language_inference/34,Dynamically Fused Graph Network for Multi-hop Reasoning,2,1,7
dataset/preprocessed/training-data/natural_language_inference/34,Text - based question answering ( TBQA ) has been studied extensively in recent years .,4,1,16
dataset/preprocessed/training-data/natural_language_inference/34,Most existing approaches focus on finding the answer to a question within a single paragraph .,5,0,16
dataset/preprocessed/training-data/natural_language_inference/34,"However , many difficult questions require multiple supporting evidence from scattered text across two or more documents .",6,0,18
dataset/preprocessed/training-data/natural_language_inference/34,"In this paper , we propose the Dynamically Fused Graph Network ( DFGN ) , a novel method to answer those questions requiring multiple scattered evidence and reasoning over them .",7,0,31
dataset/preprocessed/training-data/natural_language_inference/34,"Inspired by human 's step - by - step reasoning behavior , DFGN includes a dynamic fusion layer that starts from the entities mentioned in the given query , explores along the entity graph dynamically built from the text , and gradually finds relevant supporting entities from the given documents .",8,0,51
dataset/preprocessed/training-data/natural_language_inference/34,"We evaluate DFGN on HotpotQA , a public TBQA dataset requiring multi-hop reasoning .",9,0,14
dataset/preprocessed/training-data/natural_language_inference/34,DFGN achieves competitive results on the public board .,10,0,9
dataset/preprocessed/training-data/natural_language_inference/34,"Furthermore , our analysis shows DFGN could produce interpretable reasoning chains .",11,0,12
dataset/preprocessed/training-data/natural_language_inference/34,Question answering ( QA ) has been a popular topic in natural language processing .,13,1,15
dataset/preprocessed/training-data/natural_language_inference/34,QA provides a quantifiable way to evaluate an NLP system 's capability on language understanding and reasoning .,14,1,18
dataset/preprocessed/training-data/natural_language_inference/34,Most previous work focus on finding evidence and answers from a single paragraph .,15,0,14
dataset/preprocessed/training-data/natural_language_inference/34,It rarely tests deep reasoning capabilities of the underlying model .,16,0,11
dataset/preprocessed/training-data/natural_language_inference/34,"In fact , observe that most questions in existing QA benchmarks can be answered by retrieving These authors contributed equally .",17,0,21
dataset/preprocessed/training-data/natural_language_inference/34,The order of authorship is decided through dice rolling .,18,0,10
dataset/preprocessed/training-data/natural_language_inference/34,Work done while Lin Qiu was a research intern in ByteDance AI Lab .,19,0,14
dataset/preprocessed/training-data/natural_language_inference/34,The Sum of All Fears is a best - selling thriller novel by Tom Clancy ...,20,0,16
dataset/preprocessed/training-data/natural_language_inference/34,It was the fourth of Clancy 's Jack Ryan books to be turned into a film ...,21,0,17
dataset/preprocessed/training-data/natural_language_inference/34,Dr. John Patrick,22,0,3
dataset/preprocessed/training-data/natural_language_inference/34,"Jack Ryan Sr. , KCVO ( Hon. ) , Ph.D. is a fictional character created by Tom Clancy who appears in many of his novels and their respective film adaptations ...",23,0,31
dataset/preprocessed/training-data/natural_language_inference/34,Net Force Explorers is a series of young adult novels created by Tom Clancy and Steve Pieczenik as a spin - off of the military fiction series ...,24,0,28
dataset/preprocessed/training-data/natural_language_inference/34,What fiction character created by Tom Clancy was turned into a film in 2002 ?,26,0,15
dataset/preprocessed/training-data/natural_language_inference/34,Answer : Jack Ryan Input Paragraphs :,27,0,7
dataset/preprocessed/training-data/natural_language_inference/34,Original Entity Graph,28,0,3
dataset/preprocessed/training-data/natural_language_inference/34,Second Mask Applied First Mask Applied : Example of multi-hop text - based QA .,29,0,15
dataset/preprocessed/training-data/natural_language_inference/34,One question and three document paragraphs are given .,30,0,9
dataset/preprocessed/training-data/natural_language_inference/34,"Our proposed DFGN conducts multi-step reasoning over the facts by constructing an entity graph from multiple paragraphs , predicting a dynamic mask to select a subgraph , propagating information along the graph , and finally transfer the information from the graph back to the text in order to localize the answer .",31,0,52
dataset/preprocessed/training-data/natural_language_inference/34,"Nodes are entity occurrences , with the color denoting the underlying entity .",32,0,13
dataset/preprocessed/training-data/natural_language_inference/34,Edges are constructed from co-occurrences .,33,0,6
dataset/preprocessed/training-data/natural_language_inference/34,The gray circles are selected by DFGN in each step .,34,0,11
dataset/preprocessed/training-data/natural_language_inference/34,a small set of sentences without reasoning .,35,0,8
dataset/preprocessed/training-data/natural_language_inference/34,"To address this issue , there are several recently proposed QA datasets particularly designed to evaluate a system 's multi-hop reasoning capabilities , including WikiHop , Complex We - bQuestions , and Hot - potQA .",36,0,36
dataset/preprocessed/training-data/natural_language_inference/34,"In this paper , we study the problem of multi-hop text - based QA , which requires multi-hop reasoning among evidence scattered around multiple raw documents .",37,0,27
dataset/preprocessed/training-data/natural_language_inference/34,"In particular , a query utterance and a set of accompanying documents are given , but not all of them are relevant .",38,0,23
dataset/preprocessed/training-data/natural_language_inference/34,The answer can only be obtained by selecting two or more evidence from the documents and inferring among them ( see 1 for an example ) .,39,0,27
dataset/preprocessed/training-data/natural_language_inference/34,This setup is versatile and does not rely on any additional predefined knowledge base .,40,0,15
dataset/preprocessed/training-data/natural_language_inference/34,Therefore the models are expected to generalize well and to answer questions in open domains .,41,0,16
dataset/preprocessed/training-data/natural_language_inference/34,There are two main challenges to answer questions of this kind .,42,0,12
dataset/preprocessed/training-data/natural_language_inference/34,"Firstly , since not every document contain relevant information , multi-hop textbased QA requires filtering out noises from multiple paragraphs and extracting useful information .",43,0,25
dataset/preprocessed/training-data/natural_language_inference/34,"To address this , recent studies propose to build entity graphs from input paragraphs and apply graph neural networks ( GNNs ) to aggregate the information through entity graphs .",44,0,30
dataset/preprocessed/training-data/natural_language_inference/34,"However , all of the existing work apply GNNs based on a static global entity graph of each QA pair , which can be considered as performing implicit reasoning .",45,0,30
dataset/preprocessed/training-data/natural_language_inference/34,"Instead of them , we argue that the queryguided multi-hop reasoning should be explicitly performed on a dynamic local entity graph tailored according to the query .",46,0,27
dataset/preprocessed/training-data/natural_language_inference/34,"Secondly , previous work on multi-hop QA ( e.g. WikiHop ) usually aggregates document information to an entity graph , and answers are then directly selected on entities of the entity graph .",47,0,33
dataset/preprocessed/training-data/natural_language_inference/34,"However , in a more realistic setting , the answers may even not reside in entities of the extracted entity graph .",48,0,22
dataset/preprocessed/training-data/natural_language_inference/34,"Thus , existing approaches can hardly be directly applied to open - domain multi - hop QA tasks like Hotpot QA .",49,0,22
dataset/preprocessed/training-data/natural_language_inference/32,FLOWQA : GRASPING FLOW IN HISTORY FOR CONVERSATIONAL MACHINE COMPREHENSION,2,1,10
dataset/preprocessed/training-data/natural_language_inference/32,"Conversational machine comprehension requires the understanding of the conversation history , such as previous question / answer pairs , the document context and the current question .",4,0,27
dataset/preprocessed/training-data/natural_language_inference/32,"To enable traditional , single - turn models to encode the history comprehensively , we introduce FLOW , a mechanism that can incorporate intermediate representations generated during the process of answering previous questions , through an alternating parallel processing structure .",5,0,41
dataset/preprocessed/training-data/natural_language_inference/32,"Compared to approaches that concatenate previous questions / answers as input , FLOW integrates the latent semantics of the conversation history more deeply .",6,0,24
dataset/preprocessed/training-data/natural_language_inference/32,"Our model , FLOWQA , shows superior performance on two recently proposed conversational challenges ( + 7.2 % F 1 on CoQA and + 4.0 % on QuAC ) .",7,0,30
dataset/preprocessed/training-data/natural_language_inference/32,The effectiveness of FLOW also shows in other tasks .,8,0,10
dataset/preprocessed/training-data/natural_language_inference/32,"By reducing sequential instruction understanding to conversational machine comprehension , FLOWQA outperforms the best models on all three domains in SCONE , with + 1.8 % to + 4.4 % improvement in accuracy .",9,0,34
dataset/preprocessed/training-data/natural_language_inference/32,Work done during internship at Allen Institute for Artificial Intelligence .,11,0,11
dataset/preprocessed/training-data/natural_language_inference/32,"1 We use "" reasoning "" to refer to the model process of finding the answer .",12,0,17
dataset/preprocessed/training-data/natural_language_inference/32,"We present FLOWQA , a model designed for conversational machine comprehension .",13,0,12
dataset/preprocessed/training-data/natural_language_inference/32,FLOWQA consists of two main components : a base neural model for single - turn MC and a FLOW mechanism that encodes the conversation history .,14,0,26
dataset/preprocessed/training-data/natural_language_inference/32,"Instead of using the shallow history , i.e. , previous questions and answers , we feed the model with the entire hidden representations generated during the process of answering previous questions .",15,0,32
dataset/preprocessed/training-data/natural_language_inference/32,"These hidden representations potentially capture related information , such as phrases and facts in the context , for answering the previous questions , and hence provide additional clues on what the current conversation is revolving around .",16,0,37
dataset/preprocessed/training-data/natural_language_inference/32,"This FLOW mechanism is also remarkably effective at tracking the world states for sequential instruction understanding ( Long et al. , 2016 ) : after mapping world states as context and instructions as questions , FLOWQA can interpret a sequence of inter-connected instructions and generate corresponding world state changes as answers .",17,0,52
dataset/preprocessed/training-data/natural_language_inference/32,"The FLOW mechanism can be viewed as stacking single - turn QA models along the dialog progression ( i.e. , the question turns ) and building information flow along the dialog .",18,0,32
dataset/preprocessed/training-data/natural_language_inference/32,"This information transfer happens for each context word , allowing rich information in the reasoning process to flow .",19,0,19
dataset/preprocessed/training-data/natural_language_inference/32,"The design is analogous to recurrent neural networks , where each single update unit is now an entire question answering process .",20,0,22
dataset/preprocessed/training-data/natural_language_inference/32,"Because there are two recurrent structures in our modeling , one in the context for each question and the other in the conversation progression , a naive implementation leads to a highly unparallelizable structure .",21,0,35
dataset/preprocessed/training-data/natural_language_inference/32,"To handle this issue , we propose an alternating parallel processing structure , which alternates between sequentially processing one dimension in parallel of the other dimension , and thus speeds up training significantly .",22,0,34
dataset/preprocessed/training-data/natural_language_inference/32,The young girl and her dog set out a trip into the woods one day .,24,0,16
dataset/preprocessed/training-data/natural_language_inference/32,Upon entering the woods the girl and her dog found that the woods were dark and cold .,25,0,18
dataset/preprocessed/training-data/natural_language_inference/32,"The girl was a little scared and was thinking of turning back , but yet they went on .",26,0,19
dataset/preprocessed/training-data/natural_language_inference/32,Context : : An illustration of conversational machine comprehension with an example from the Conversational Question Answering Challenge dataset ( CoQA ) .,27,0,23
dataset/preprocessed/training-data/natural_language_inference/32,"Humans seek information in a conversational manner , by asking follow - up questions for additional information based on what they have already learned .",28,0,25
dataset/preprocessed/training-data/natural_language_inference/32,Recently proposed conversational machine comprehension ( MC ) datasets aim to enable models to assist in such information seeking dialogs .,29,1,21
dataset/preprocessed/training-data/natural_language_inference/32,They consist of a sequence of question / answer pairs where questions can only be understood along with the conversation history .,30,0,22
dataset/preprocessed/training-data/natural_language_inference/32,illustrates this new challenge .,31,0,5
dataset/preprocessed/training-data/natural_language_inference/32,Existing approaches take a single - turn MC model and augment the current question and context with the previous questions and answers .,32,0,23
dataset/preprocessed/training-data/natural_language_inference/32,"However , this offers only a partial solution , ignoring previous reasoning 1 processes performed by the model .",33,0,19
dataset/preprocessed/training-data/natural_language_inference/32,"FLOWQA achieves strong empirical results on conversational machine comprehension tasks , and improves the state of the art on various datasets ( from 67.8 % to 75.0 % on CoQA and 60.1 % to 64.1 % on QuAC ) .",34,0,40
dataset/preprocessed/training-data/natural_language_inference/32,"While designed for conversational machine comprehension , FLOWQA also shows superior performance on a seemingly different task - understanding a sequence of natural language instructions ( framed previously as a sequential semantic parsing problem ) .",35,0,36
dataset/preprocessed/training-data/natural_language_inference/32,"When tested on SCONE , FLOWQA outperforms all existing systems in three domains , resulting in a range of accuracy improvement from + 1.8 % to + 4.4 % .",36,0,30
dataset/preprocessed/training-data/natural_language_inference/32,Our code can be found in https://github.com/momohuang/FlowQA.,37,0,7
dataset/preprocessed/training-data/natural_language_inference/32,BACKGROUND : MACHINE COMPREHENSION,38,0,4
dataset/preprocessed/training-data/natural_language_inference/32,"In this section , we introduce the task formulations of machine comprehension in both single - turn and conversational settings , and discuss the main ideas of state - of - the - art MC models .",39,0,37
dataset/preprocessed/training-data/natural_language_inference/32,"Given an evidence document ( context ) and a question , the task is to find the answer to the question based on the context .",41,0,26
dataset/preprocessed/training-data/natural_language_inference/32,"The context C = {c 1 , c 2 , . . . cm } is described as a sequence of m words and the question Q = {q 1 , q 2 . . . q n } a sequence of n words .",42,0,45
dataset/preprocessed/training-data/natural_language_inference/32,"In the extractive setting , the answer A must be a span in the context .",43,0,16
dataset/preprocessed/training-data/natural_language_inference/32,"Conversational machine comprehension is a generalization of the singleturn setting : the agent needs to answer multiple , potentially inter-dependent questions in sequence .",44,0,24
dataset/preprocessed/training-data/natural_language_inference/32,"The meaning of the current question may depend on the conversation history ( e.g. , in , the third question such as ' Where ? ' can not be answered in isolation ) .",45,0,34
dataset/preprocessed/training-data/natural_language_inference/32,"Thus , previous conversational history ( i.e. , question / answer pairs ) is provided as an input in addition to the context and the current question .",46,0,28
dataset/preprocessed/training-data/natural_language_inference/32,"For single - turn MC , many top - performing models share a similar architecture , consisting of four major components : ( 1 ) question encoding , ( 2 ) context encoding , ( 3 ) reasoning , and finally ( 4 ) answer prediction .",48,0,47
dataset/preprocessed/training-data/natural_language_inference/32,"Initially the word embeddings ( e.g. , of question tokens Q and context tokens C are taken as input and fed into contextual integration layers , such as LSTMs or self attentions , to encode the question and context .",49,0,40
dataset/preprocessed/training-data/natural_language_inference/48,Iterative Alternating Neural Attention for Machine Reading,2,1,7
dataset/preprocessed/training-data/natural_language_inference/48,"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document .",4,1,26
dataset/preprocessed/training-data/natural_language_inference/48,"Unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine - grained exploration of both the query and the document .",5,0,38
dataset/preprocessed/training-data/natural_language_inference/48,Our model outperforms state - of - the - art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children 's Book Test ( CBT ) dataset .,6,0,32
dataset/preprocessed/training-data/natural_language_inference/48,"Recently , the idea of training machine comprehension models that can read , understand , and answer questions about a text has come closer to reality principally through two factors .",8,0,31
dataset/preprocessed/training-data/natural_language_inference/48,"The first is the advent of deep learning techniques , which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data .",9,0,30
dataset/preprocessed/training-data/natural_language_inference/48,"The second factor is the formulation of standard machine comprehension benchmarks based on Cloze - style queries , which permit fast integration loops between model conception and experimental evaluation .",10,0,30
dataset/preprocessed/training-data/natural_language_inference/48,Cloze - style queries are created by deleting a particular word in a natural - language statement .,11,0,18
dataset/preprocessed/training-data/natural_language_inference/48,The task is to guess which word was deleted .,12,0,10
dataset/preprocessed/training-data/natural_language_inference/48,"In a pragmatic approach , recent work formed such questions by extracting a sentence from a larger document .",13,0,19
dataset/preprocessed/training-data/natural_language_inference/48,"In contrast to considering a stand - alone statement , the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word .",14,0,33
dataset/preprocessed/training-data/natural_language_inference/48,Such contextual dependencies may also be injected by removing a word from a short human - crafted summary of a larger body of text .,15,0,25
dataset/preprocessed/training-data/natural_language_inference/48,The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text .,16,0,20
dataset/preprocessed/training-data/natural_language_inference/48,"In both cases , the machine comprehension system is presented with an ablated query and the document to which the original query refers .",17,0,24
dataset/preprocessed/training-data/natural_language_inference/48,The missing word is assumed to appear in the document .,18,0,11
dataset/preprocessed/training-data/natural_language_inference/48,"Encouraged by the recent success of deep learning attention architectures , we propose a novel neural attention - based inference model designed to perform machine reading comprehension tasks .",19,0,29
dataset/preprocessed/training-data/natural_language_inference/48,The model first reads the document and the query using a recurrent neural network .,20,0,15
dataset/preprocessed/training-data/natural_language_inference/48,"Then , it deploys an iterative inference process to uncover the inferential links that exist between the missing query word , the query , and the document .",21,0,28
dataset/preprocessed/training-data/natural_language_inference/48,"This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .",22,0,30
dataset/preprocessed/training-data/natural_language_inference/48,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,23,0,21
dataset/preprocessed/training-data/natural_language_inference/48,"This permits our model to reason about different parts of the query in a sequential way , based on the information that has been gathered previously from the document .",24,0,30
dataset/preprocessed/training-data/natural_language_inference/48,"After a fixed number of iterations , the model uses a summary of its inference process to predict the answer .",25,0,21
dataset/preprocessed/training-data/natural_language_inference/48,This paper makes the following contributions .,26,0,7
dataset/preprocessed/training-data/natural_language_inference/48,"We present a novel iterative , alternating attention mechanism that , unlike existing models , does not compress the query to a single representation , but instead alternates its attention between the query and the document to obtain a fine - grained query representation within a fixed computation time .",27,0,50
dataset/preprocessed/training-data/natural_language_inference/48,Our architecture tightly integrates previous ideas related to bidirectional readers and iterative attention processes .,28,0,15
dataset/preprocessed/training-data/natural_language_inference/48,It obtains state - of - theart results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks .,29,0,28
dataset/preprocessed/training-data/natural_language_inference/48,One of the advantages of using Cloze - style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention .,31,0,32
dataset/preprocessed/training-data/natural_language_inference/48,The CBT and corpora are two such datasets .,32,0,9
dataset/preprocessed/training-data/natural_language_inference/48,The CBT 1 corpus was generated from well - known children 's books available through Project Gutenberg .,33,0,18
dataset/preprocessed/training-data/natural_language_inference/48,Documents consist of 20 - sentence excerpts from these books .,34,0,11
dataset/preprocessed/training-data/natural_language_inference/48,The related query is formed from an excerpt 's 21st sentence by replacing a single word with an anonymous placeholder token .,35,0,22
dataset/preprocessed/training-data/natural_language_inference/48,The dataset is divided into four subsets depending on the type of the word replaced .,36,0,16
dataset/preprocessed/training-data/natural_language_inference/48,"The subsets are named entity , common noun , verb , and preposition .",37,0,14
dataset/preprocessed/training-data/natural_language_inference/48,"We will focus our evaluation solely on the first two subsets , i.e. CBT - NE ( named entity ) and CBT - CN ( common nouns ) , since the latter two are relatively simple as demonstrated by .",38,0,40
dataset/preprocessed/training-data/natural_language_inference/48,The CNN 2 corpus was generated from news articles available through the CNN website .,39,0,15
dataset/preprocessed/training-data/natural_language_inference/48,"The documents are given by the full articles themselves , which are accompanied by short , bullet - point summary statements .",40,0,22
dataset/preprocessed/training-data/natural_language_inference/48,"Instead of extracting a query from the articles themselves , the authors replace a named entity within each article summary with an anonymous placeholder token .",41,0,26
dataset/preprocessed/training-data/natural_language_inference/48,"For both datasets , the training and evaluation data consist of tuples ( Q , D , A , a ) , where Q is the query ( represented as a sequence of words ) , Dis the document , A is the set of possible answers , and a ?",42,0,51
dataset/preprocessed/training-data/natural_language_inference/48,Alternating Iterative Attention,44,0,3
dataset/preprocessed/training-data/natural_language_inference/48,Our model is represented in .,45,0,6
dataset/preprocessed/training-data/natural_language_inference/48,It s workflow has three steps .,46,0,7
dataset/preprocessed/training-data/natural_language_inference/48,"First is the encoding phase , in which we compute a set of vector representations , acting as a memory of the content of the input document and query .",47,0,30
dataset/preprocessed/training-data/natural_language_inference/48,"Next , the inference phase aims to untangle the complex semantic relationships linking the document and the query in order to provide sufficiently strong evidence for the answer prediction to be successful .",48,0,33
dataset/preprocessed/training-data/natural_language_inference/48,"To accomplish this , we use an iterative process that , at each iteration , alternates attentive memory accesses to the query and the document .",49,0,26
dataset/preprocessed/training-data/natural_language_inference/85,Enhanced LSTM for Natural Language Inference,2,1,6
dataset/preprocessed/training-data/natural_language_inference/85,Reasoning and inference are central to human and artificial intelligence .,4,1,11
dataset/preprocessed/training-data/natural_language_inference/85,Modeling inference in human language is very challenging .,5,0,9
dataset/preprocessed/training-data/natural_language_inference/85,"With the availability of large annotated data ( Bowman et al. , 2015 ) , it has recently become feasible to train neural network based inference models , which have shown to be very effective .",6,0,36
dataset/preprocessed/training-data/natural_language_inference/85,"In this paper , we present a new state - of - the - art result , achieving the accuracy of 88.6 % on the Stanford Natural Language Inference Dataset .",7,0,31
dataset/preprocessed/training-data/natural_language_inference/85,"Unlike the previous top models that use very complicated network architectures , we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models .",8,0,31
dataset/preprocessed/training-data/natural_language_inference/85,"Based on this , we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition , we achieve additional improvement .",9,0,27
dataset/preprocessed/training-data/natural_language_inference/85,"Particularly , incorporating syntactic parsing information contributes to our best result - it further improves the performance even when added to the already very strong model .",10,0,27
dataset/preprocessed/training-data/natural_language_inference/85,Reasoning and inference are central to both human and artificial intelligence .,12,0,12
dataset/preprocessed/training-data/natural_language_inference/85,"Modeling inference in human language is notoriously challenging but is a basic problem towards true natural language understanding , as pointed out by , "" a necessary ( if not sufficient ) condition for true natural language understanding is a mastery of open - domain natural language inference . """,13,0,50
dataset/preprocessed/training-data/natural_language_inference/85,The previous work has included extensive research on recognizing textual entailment .,14,0,12
dataset/preprocessed/training-data/natural_language_inference/85,"Specifically , natural language inference ( NLI ) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a premise p , as depicted in the following example from MacCartney ( 2009 ) , where the hypothesis is regarded to be entailed from the premise .",15,1,49
dataset/preprocessed/training-data/natural_language_inference/85,"Several airlines polled saw costs grow more than expected , even after adjusting for inflation .",17,0,16
dataset/preprocessed/training-data/natural_language_inference/85,h : Some of the companies in the poll reported cost increases .,18,0,13
dataset/preprocessed/training-data/natural_language_inference/85,The most recent years have seen advances in modeling natural language inference .,19,0,13
dataset/preprocessed/training-data/natural_language_inference/85,"An important contribution is the creation of a much larger annotated dataset , the Stanford Natural Language Inference ( SNLI ) dataset .",20,0,23
dataset/preprocessed/training-data/natural_language_inference/85,"The corpus has 570,000 human-written English sentence pairs manually labeled by multiple human subjects .",21,0,15
dataset/preprocessed/training-data/natural_language_inference/85,This makes it feasible to train more complex inference models .,22,0,11
dataset/preprocessed/training-data/natural_language_inference/85,"Neural network models , which often need relatively large annotated data to estimate their parameters , have shown to achieve the state of the art on SNLI .",23,0,28
dataset/preprocessed/training-data/natural_language_inference/85,"While some previous top - performing models use rather complicated network architectures to achieve the state - of - the - art results , we demonstrate in this paper that enhancing sequential inference models based on chain models can outperform all previous results , suggesting that the potentials of such sequential inference approaches have not been fully exploited yet .",24,0,60
dataset/preprocessed/training-data/natural_language_inference/85,"More specifically , we show that our sequential inference model achieves an accuracy of 88.0 % on the SNLI benchmark .",25,0,21
dataset/preprocessed/training-data/natural_language_inference/85,Exploring syntax for NLI is very attractive to us .,26,1,10
dataset/preprocessed/training-data/natural_language_inference/85,"In many problems , syntax and semantics interact closely , including in semantic composition , among others .",27,0,18
dataset/preprocessed/training-data/natural_language_inference/85,"Complicated tasks such as natural language inference could well involve both , which has been discussed in the context of recognizing textual entailment ( RTE ) .",28,0,27
dataset/preprocessed/training-data/natural_language_inference/85,"In this paper , we are interested in exploring this within the neural network frameworks , with the presence of relatively large training data .",29,0,25
dataset/preprocessed/training-data/natural_language_inference/85,"We show that by explicitly encoding parsing information with recursive networks in both local inference modeling and inference composition and by incorporating it into our framework , we achieve additional improvement , increasing the performance to a new state of the art with an 88.6 % accuracy .",30,0,48
dataset/preprocessed/training-data/natural_language_inference/85,"Early work on natural language inference has been performed on rather small datasets with more conventional methods ( refer to MacCartney ( 2009 ) for a good literature survey ) , which includes a large bulk of work on recognizing textual entailment , such as , among others .",32,0,49
dataset/preprocessed/training-data/natural_language_inference/85,"More recently , made available the SNLI dataset with 570,000 human annotated sentence pairs .",33,0,15
dataset/preprocessed/training-data/natural_language_inference/85,They also experimented with simple classification models as well as simple neural networks that encode the premise and hypothesis independently .,34,0,21
dataset/preprocessed/training-data/natural_language_inference/85,"proposed neural attention - based models for NLI , which captured the attention information .",35,0,15
dataset/preprocessed/training-data/natural_language_inference/85,"In general , attention based models have been shown to be effective in a wide range of tasks , including machine translation , speech recognition , image caption , and text summarization , among others .",36,0,36
dataset/preprocessed/training-data/natural_language_inference/85,"For NLI , the idea allows neural models to pay attention to specific are as of the sentences .",37,0,19
dataset/preprocessed/training-data/natural_language_inference/85,A variety of more advanced networks have been developed since then .,38,0,12
dataset/preprocessed/training-data/natural_language_inference/85,"Among them , more relevant to ours are the approaches proposed by and , which are among the best performing models .",39,0,22
dataset/preprocessed/training-data/natural_language_inference/85,propose a relatively simple but very effective decomposable model .,40,0,10
dataset/preprocessed/training-data/natural_language_inference/85,The model decomposes the NLI problem into subproblems that can be solved separately .,41,0,14
dataset/preprocessed/training-data/natural_language_inference/85,"On the other hand , propose much more complicated networks that consider sequential LSTM - based encoding , recursive networks , and complicated combinations of attention models , which provide about 0.5 % gain over the results reported by .",42,0,40
dataset/preprocessed/training-data/natural_language_inference/85,"It is , however , not very clear if the potential of the sequential inference networks has been well exploited for NLI .",43,0,23
dataset/preprocessed/training-data/natural_language_inference/85,"In this paper , we first revisit this problem and show that enhancing sequential inference models based on chain networks can actually outperform all previous results .",44,0,27
dataset/preprocessed/training-data/natural_language_inference/85,We further show that explicitly considering recursive architectures to encode syntactic parsing information for NLI could further improve the performance .,45,0,21
dataset/preprocessed/training-data/natural_language_inference/85,Hybrid Neural Inference Models,46,0,4
dataset/preprocessed/training-data/natural_language_inference/85,"We present here our natural language inference networks which are composed of the following major components : input encoding , local inference modeling , and inference composition .",47,0,28
dataset/preprocessed/training-data/natural_language_inference/85,shows a high - level view of the architecture .,48,0,10
dataset/preprocessed/training-data/natural_language_inference/85,"Vertically , the figure depicts the three major components , and horizontally , the left side of the figure represents our sequential NLI model named ESIM , and the right side represents networks that incorporate syntactic parsing information in tree LSTMs .",49,0,42
dataset/preprocessed/training-data/natural_language_inference/87,SG - Net : Syntax - Guided Machine Reading Comprehension,2,1,10
dataset/preprocessed/training-data/natural_language_inference/87,"For machine reading comprehension , the capacity of effectively modeling the linguistic knowledge from the detailriddled and lengthy passages and getting ride of the noises is essential to improve its performance .",4,0,32
dataset/preprocessed/training-data/natural_language_inference/87,"Traditional attentive models attend to all words without explicit constraint , which results in inaccurate concentration on some dispensable words .",5,0,21
dataset/preprocessed/training-data/natural_language_inference/87,"In this work , we propose using syntax to guide the text modeling by incorporating explicit syntactic constraints into attention mechanism for better linguistically motivated word representations .",6,0,28
dataset/preprocessed/training-data/natural_language_inference/87,"In detail , for self - attention network ( SAN ) sponsored Transformer - based encoder , we introduce syntactic dependency of interest ( SDOI ) design into the SAN to form an SDOI - SAN with syntax - guided selfattention .",7,0,42
dataset/preprocessed/training-data/natural_language_inference/87,Syntax - guided network ( SG - Net ) is then composed of this extra SDOI - SAN and the SAN from the original Transformer encoder through a dual contextual architecture for better linguistics inspired representation .,8,0,37
dataset/preprocessed/training-data/natural_language_inference/87,"To verify its effectiveness , the proposed SG - Net is applied to typical pre-trained language model BERT which is right based on a Transformer encoder .",9,0,27
dataset/preprocessed/training-data/natural_language_inference/87,Extensive experiments on popular benchmarks including SQuAD 2.0 and RACE show that the proposed SG - Net design helps achieve substantial performance improvement over strong baselines .,10,0,27
dataset/preprocessed/training-data/natural_language_inference/87,"Recently , much progress has been made in general - purpose language modeling that can be used across a wide range of tasks .",12,0,24
dataset/preprocessed/training-data/natural_language_inference/87,"Understanding the meaning of a sentence is a prerequisite to solve many natural language understanding ( NLU ) problems , such as machine reading comprehension ( MRC ) based question answering .",13,1,32
dataset/preprocessed/training-data/natural_language_inference/87,"Obviously , it requires a good representation of the meaning of a sentence .",14,0,14
dataset/preprocessed/training-data/natural_language_inference/87,"A person reads most words superficially and pays more attention to the key ones during reading and understanding sentences ( Wang , .",15,0,23
dataset/preprocessed/training-data/natural_language_inference/87,"Although a variety of attentive models have been proposed to imitate human learning , most of them , especially global attention methods equally tackle each word and attend to all words in a sentence without explicit pruning and prior focus , which would result in inaccurate concentration on some dispensable words .",16,0,52
dataset/preprocessed/training-data/natural_language_inference/87,We observe that the accuracy of MRC models decreases when answering long questions ( shown in Section 5.1 ) .,17,1,20
dataset/preprocessed/training-data/natural_language_inference/87,"Generally , if the text is particularly lengthy and detailedriddled , it would be quite difficult for deep learning model to understand as it suffers from noise and pays vague attention on the text components , let alone accurately answering questions .",18,0,42
dataset/preprocessed/training-data/natural_language_inference/87,"In contrast , existing studies have verified that human reads sentences efficiently by taking a sequence of fixation and saccades after a quick first glance .",19,0,26
dataset/preprocessed/training-data/natural_language_inference/87,"Besides , for passage involved reading comprehension , a input sequence always consists of multiple sentences .",20,0,17
dataset/preprocessed/training-data/natural_language_inference/87,"Nearly all of the current attentive methods and language models regard the input sequence as a whole , e.g. , a passage , with no consideration of the inner linguistic structure inside each sentence .",21,0,35
dataset/preprocessed/training-data/natural_language_inference/87,This would result in process bias caused by much noise and lack of associated spans for each concerned word .,22,0,20
dataset/preprocessed/training-data/natural_language_inference/87,All these factors motivate us to seek for an informative method that can selectively pick out important words by only considering the related subset of words of syntactic importance inside each input sentence explicitly .,23,0,35
dataset/preprocessed/training-data/natural_language_inference/87,"With a guidance of syntactic structure clues , the syntax - guided method could give more accurate attentive signals and reduce the impact of the noise brought about by lengthy sentences .",24,0,32
dataset/preprocessed/training-data/natural_language_inference/87,"So far , we have two types of broadly adopted contextualized encoders for building sentence - level representation , .",25,0,20
dataset/preprocessed/training-data/natural_language_inference/87,The latter has shown its superiority which is empowered by a self - attention network ( SAN ) design .,26,0,20
dataset/preprocessed/training-data/natural_language_inference/87,"In this paper , we extend the self - attention mechanism with syntax - guided constraint , to capture syntax related parts with each concerned word .",27,0,27
dataset/preprocessed/training-data/natural_language_inference/87,"Specifically , we adopt pre-trained dependency syntactic parse tree structure to produce the related nodes for each word in a sentence , namely syntactic dependency of interest ( SDOI ) , by regarding each word as a child node and the SDOI consists all its ancestor nodes and itself in the dependency parsing tree .",28,0,55
dataset/preprocessed/training-data/natural_language_inference/87,An example is shown in .,29,0,6
dataset/preprocessed/training-data/natural_language_inference/87,"To effectively accommodate such SDOI information , we propose a novel syntax - guided network ( SG - Net ) , which fuses the original SAN and SDOI - SAN , to provide more linguistically inspired representation for challenging reading comprehension tasks 1 .",30,0,44
dataset/preprocessed/training-data/natural_language_inference/87,"To our best knowledge , we are the first to integrate syntactic relationship as attentive guidance for enhancing state - of - the - art SAN in Transformer encoder .",31,0,30
dataset/preprocessed/training-data/natural_language_inference/87,"The proposed SG - Net design is applied to pre-trained ) and evaluated on challenging MRC tasks , which shows its effectiveness by boosting the strong baseline substantially .",32,0,29
dataset/preprocessed/training-data/natural_language_inference/87,Machine Reading Comprehension,34,0,3
dataset/preprocessed/training-data/natural_language_inference/87,"In the last decade , the MRC tasks have evolved from the early cloze - style test to span - based answer extraction from passage ) .",35,0,27
dataset/preprocessed/training-data/natural_language_inference/87,The latest evaluation shows that BERT is powerful and convenient for downstream tasks .,36,0,14
dataset/preprocessed/training-data/natural_language_inference/87,"Following this line , we extract context - sensitive syntactic features and take pre-trained BERT as our backbone encoder to verify the effectiveness of our proposed SG - Net .",37,0,30
dataset/preprocessed/training-data/natural_language_inference/87,"Recently , dependency syntactic parsing have been further developed with neural network and attained new state - of the - art results .",39,0,23
dataset/preprocessed/training-data/natural_language_inference/87,"Benefiting from the highly accurate parser , neural network models could enjoy even higher accuracy gains by leveraging syntactic information rather than ignoring it .",40,0,25
dataset/preprocessed/training-data/natural_language_inference/87,"Syntactic dependency parse tree provides a form that is capable of indicating the existence and type of linguistic dependency relation among words , which has been shown generally beneficial in various natural language understanding tasks ( Bowman et al. 2016 ) .",41,0,42
dataset/preprocessed/training-data/natural_language_inference/87,"To effectively exploit syntactic clue , most of previous works ) absorb parse tree information by transforming dependency labels into vectors and simply concatenate the label embedding with word representation .",42,0,31
dataset/preprocessed/training-data/natural_language_inference/87,"However , such simplified and straightforward processing would result in higher dimension of joint word and label embeddings and is too coarse to capture contextual interactions between the associated labels and the mutual connections between labels and words .",43,0,39
dataset/preprocessed/training-data/natural_language_inference/87,This inspires us to seek for an attentive way to enrich the contextual representation from the syntactic source .,44,0,19
dataset/preprocessed/training-data/natural_language_inference/87,"A related work is from Strubell et al. , which proposed to incorporate syntax with multi-task learning for semantic role labeling .",45,0,22
dataset/preprocessed/training-data/natural_language_inference/87,"However , their syntax is incorporated by training one extra attention head to attend to syntactic ancestors for each token while we use all the existing heads rather than add an extra one .",46,0,34
dataset/preprocessed/training-data/natural_language_inference/87,"Besides , this work is based on the remarkable representation capacity of recent language models such as BERT , which have been suggested to be endowed with some syntax to an extent .",47,0,33
dataset/preprocessed/training-data/natural_language_inference/87,"Therefore , we are motivated to apply syntactic constraints through syntax guided method to prune the self - attention instead of purely adding dependency features .",48,0,26
dataset/preprocessed/training-data/natural_language_inference/87,"In this work , we form a general approach to benefit from syntax - guided representations , which is the first attempt for the SAN architecture improvement in Transformer encoder to our best knowledge .",49,0,35
dataset/preprocessed/training-data/natural_language_inference/44,Efficient and Robust Question Answering from Minimal Context over Documents,2,1,10
dataset/preprocessed/training-data/natural_language_inference/44,Neural models for question answering ( QA ) over documents have achieved significant performance improvements .,4,1,16
dataset/preprocessed/training-data/natural_language_inference/44,"Although effective , these models do not scale to large corpora due to their complex modeling of interactions between the document and the question .",5,0,25
dataset/preprocessed/training-data/natural_language_inference/44,"Moreover , recent work has shown that such models are sensitive to adversarial inputs .",6,0,15
dataset/preprocessed/training-data/natural_language_inference/44,"In this paper , we study the minimal context required to answer the question , and find that most questions in existing datasets can be answered with a small set of sentences .",7,0,33
dataset/preprocessed/training-data/natural_language_inference/44,"Inspired by this observation , we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model .",8,1,25
dataset/preprocessed/training-data/natural_language_inference/44,"Our over all system achieves significant reductions in training ( up to 15 times ) and inference times ( up to 13 times ) , with accuracy comparable to or better than the state - of - the - art on SQuAD , News QA , Trivia QA and SQuAD - Open .",9,0,53
dataset/preprocessed/training-data/natural_language_inference/44,"Furthermore , our experimental results and analyses show that our approach is more robust to adversarial inputs .",10,0,18
dataset/preprocessed/training-data/natural_language_inference/44,"The task of textual question answering ( QA ) , in which a machine reads a document and answers a question , is an important and challenging problem in natural language processing .",12,0,33
dataset/preprocessed/training-data/natural_language_inference/44,Recent progress in performance of QA models has been largely due to the variety of available QA datasets .,13,0,19
dataset/preprocessed/training-data/natural_language_inference/44,All work was done while the author was an intern at Salesforce Research .,15,0,14
dataset/preprocessed/training-data/natural_language_inference/44,"Many neural QA models have been proposed for these datasets , the most successful of which tend to leverage coattention or bidirectional attention mechanisms that build codependent representations of the document and the question .",16,0,35
dataset/preprocessed/training-data/natural_language_inference/44,"Yet , learning the full context over the document is challenging and inefficient .",17,0,14
dataset/preprocessed/training-data/natural_language_inference/44,"In particular , when the model is given along document , or multiple documents , learning the full context is intractably slow and hence difficult to scale to large corpora .",18,0,31
dataset/preprocessed/training-data/natural_language_inference/44,"In addition , show that , given adversarial inputs , such models tend to focus on wrong parts of the context and produce incorrect answers .",19,0,26
dataset/preprocessed/training-data/natural_language_inference/44,"In this paper , we aim to develop a QA system that is scalable to large documents as well as robust to adversarial inputs .",20,0,25
dataset/preprocessed/training-data/natural_language_inference/44,"First , we study the context required to answer the question by sampling examples in the dataset and carefully analyzing them .",21,0,22
dataset/preprocessed/training-data/natural_language_inference/44,"We find that most questions can be answered using a few sentences , without the consideration of context over entire document .",22,0,22
dataset/preprocessed/training-data/natural_language_inference/44,"In particular , we observe that on the SQuAD dataset , 92 % of answerable questions can be answered using a single sentence .",23,0,24
dataset/preprocessed/training-data/natural_language_inference/44,"Second , inspired by this observation , we propose a sentence selector to select the minimal set of sentences to give to the QA model in order to answer the question .",24,0,32
dataset/preprocessed/training-data/natural_language_inference/44,"Since the minimum number of sentences depends on the question , our sentence selector chooses a different number of sentences for each question , in contrast with previous models that select a fixed number of sentences .",25,0,37
dataset/preprocessed/training-data/natural_language_inference/44,"Our sentence selector leverages three simple techniques - weight transfer , data modification and score normalization , which we show to be highly effective on the task of sentence selection .",26,0,31
dataset/preprocessed/training-data/natural_language_inference/44,We compare the standard QA model given the full document Writers whose papers are in the library are as diverse as Charles Dickens and The papers of which famous English Victorian Beatrix Potter .,27,0,34
dataset/preprocessed/training-data/natural_language_inference/44,Illuminated manuscripts in the library dating from author are collected in the library ?,28,0,14
dataset/preprocessed/training-data/natural_language_inference/44,Existing QA models focus on learning the context over different parts in the full document .,30,0,16
dataset/preprocessed/training-data/natural_language_inference/44,"Although effective , learning the context within the full document is challenging and inefficient .",31,0,15
dataset/preprocessed/training-data/natural_language_inference/44,"Consequently , we study the minimal context in the document required to answer the question .",32,0,16
dataset/preprocessed/training-data/natural_language_inference/44,"First , we randomly sample 50 examples from the SQuAD development set , and analyze the minimum number of sentences required to answer the question , as shown in .",34,0,30
dataset/preprocessed/training-data/natural_language_inference/44,We observed that 98 % of questions are answerable given the document .,35,0,13
dataset/preprocessed/training-data/natural_language_inference/44,The remaining 2 % of questions are not answerable even given the entire document .,36,0,15
dataset/preprocessed/training-data/natural_language_inference/44,"For instance , in the last example in , the question requires the background knowledge that Charles Dickens is an English Victorian author .",37,0,24
dataset/preprocessed/training-data/natural_language_inference/44,"Among the answerable examples , 92 % are answerable with a single sentence , 6 % with two sentences , and 2 % with three or more sentences .",38,0,29
dataset/preprocessed/training-data/natural_language_inference/44,We perform a similar analysis on the TriviaQA ( Wikipedia ) development ( verified ) set .,39,0,17
dataset/preprocessed/training-data/natural_language_inference/44,"Finding the sentences to answer the question on TriviaQA is more challenging than on SQuAD , since Triv - ia QA documents are much longer than SQuAD documents ( 488 vs 5 sentences per document ) .",40,0,37
dataset/preprocessed/training-data/natural_language_inference/44,"Nevertheless , we find that most examples are answerable with one or two sentences - among the 88 % of examples thatare answerable given the full document , 95 % can be answered with one or two sentences .",41,0,39
dataset/preprocessed/training-data/natural_language_inference/44,Analyses on existing QA model,42,0,5
dataset/preprocessed/training-data/natural_language_inference/44,"Given that the majority of examples are answerable with a single oracle sentence on SQuAD , we analyze the performance of an existing , competitive QA model when it is given the oracle sentence .",43,0,35
dataset/preprocessed/training-data/natural_language_inference/44,"We train DCN + , one of the state - of - the - art models on SQuAD ( details in Section 3.1 ) , on the oracle sentence .",44,0,30
dataset/preprocessed/training-data/natural_language_inference/44,The model achieves 83.1 F1 when trained and evaluated using the full document and 85.1 F1 when trained and evaluated using the oracle sentence .,45,0,25
dataset/preprocessed/training-data/natural_language_inference/44,We analyze 50 randomly sampled examples in which the model fails on exact match ( EM ) despite using the oracle sentence .,46,0,23
dataset/preprocessed/training-data/natural_language_inference/44,"We classify these errors into 4 categories , as shown in .",47,0,12
dataset/preprocessed/training-data/natural_language_inference/44,"In these examples , we observed that 40 % of questions are answerable given the oracle sentence but the model unexpectedly fails to find the answer .",48,0,27
dataset/preprocessed/training-data/natural_language_inference/44,"58 % are those in which the model 's prediction is correct but does not lexically match the groundtruth answer , as shown in the first example in .",49,0,29
dataset/preprocessed/training-data/natural_language_inference/38,MEMEN : Multi-layer Embedding with Memory Networks for Machine Comprehension,2,1,10
dataset/preprocessed/training-data/natural_language_inference/38,Machine comprehension ( MC ) style question answering is a representative problem in natural language processing .,4,1,17
dataset/preprocessed/training-data/natural_language_inference/38,"Previous methods rarely spend time on the improvement of encoding layer , especially the embedding of syntactic information and name entity of the words , which are very crucial to the quality of encoding .",5,0,35
dataset/preprocessed/training-data/natural_language_inference/38,"Moreover , existing attention methods represent each query word as a vector or use a single vector to represent the whole query sentence , neither of them can handle the proper weight of the key words in query sentence .",6,0,40
dataset/preprocessed/training-data/natural_language_inference/38,"In this paper , we introduce a novel neural network architecture called Multi - layer Embedding with Memory Network ( MEMEN ) for machine reading task .",7,0,27
dataset/preprocessed/training-data/natural_language_inference/38,"In the encoding layer , we employ classic skip - gram model to the syntactic and semantic information of the words to train a new kind of embedding layer .",8,0,30
dataset/preprocessed/training-data/natural_language_inference/38,We also propose a memory network of full - orientation matching of the query and passage to catch more pivotal information .,9,0,22
dataset/preprocessed/training-data/natural_language_inference/38,Experiments show that our model has competitive results both from the perspectives of precision and efficiency in Stanford Question Answering Dataset ( SQuAD ) among all published results and achieves the state - of - the - art results on Trivia QA dataset .,10,0,44
dataset/preprocessed/training-data/natural_language_inference/38,Machine comprehension ( MC ) has gained significant popularity over the past few years and it is a coveted goal in the field of natural language processing and artificial intelligence .,12,1,31
dataset/preprocessed/training-data/natural_language_inference/38,It s task is to teach machine to understand the content of a given passage and then answer the question related to it .,13,0,24
dataset/preprocessed/training-data/natural_language_inference/38,shows a simple example from the popular dataset SQuAD .,14,0,10
dataset/preprocessed/training-data/natural_language_inference/38,"Many significant works are based on this task , and most of them focus on the improvement of a sequence model that is augmented with an attention mechanism .",15,0,29
dataset/preprocessed/training-data/natural_language_inference/38,"However , the encoding of the words is also crucial and a better encoding layer can lead to substantial difference to the final performance .",16,0,25
dataset/preprocessed/training-data/natural_language_inference/38,"Many powerful methods only represent their words in two ways , word - level embeddings and character - level embeddings .",17,0,21
dataset/preprocessed/training-data/natural_language_inference/38,"They use pre-train vectors , like GloVe ( Pennington",18,0,9
dataset/preprocessed/training-data/natural_language_inference/38,"The majority of the work was done while the authors were interning at the Eigen Technologies . , to do the word - level embeddings , which ignore syntactic information and name entity of the words .",19,0,37
dataset/preprocessed/training-data/natural_language_inference/38,construct a sequence of syntactic nodes for the words and encodes the sequence into a vector representation .,20,0,18
dataset/preprocessed/training-data/natural_language_inference/38,"However , they neglected the optimization of the initial embedding and did n't take the semantic information of the words into account , which are very important parts in the vector representations of the words .",21,0,36
dataset/preprocessed/training-data/natural_language_inference/38,"For example , the word "" Apple "" is a fixed vector in GloVe and noun in syntactics whatever it represents the fruit or the company , but name entity tags can help recognize .",22,0,35
dataset/preprocessed/training-data/natural_language_inference/38,"Moreover , the attention mechanism can be divided into two categories : one dimensional attention and two dimensional attention .",23,0,20
dataset/preprocessed/training-data/natural_language_inference/38,"In one dimensional attention , the whole query is represented by one embedding vector , which is usually the last hidden state in the neural network .",24,0,27
dataset/preprocessed/training-data/natural_language_inference/38,"However , using only one vector to represent the whole query will attenuate the attention of key words .",25,0,19
dataset/preprocessed/training-data/natural_language_inference/38,"On the contrary , every word in the query has its own embedding vector in the situation of two dimensional attention , but many words in the question sentence are useless even if disturbing , such as the stopwords .",26,0,40
dataset/preprocessed/training-data/natural_language_inference/38,"In this paper , we introduce the Multi - layer Embedding with Memory Networks ( MEMEN ) , an end - to - end neural network for machine comprehension task .",27,0,31
dataset/preprocessed/training-data/natural_language_inference/38,Our model consists of three parts :,28,0,7
dataset/preprocessed/training-data/natural_language_inference/38,"1 ) the encoding of context and query , in which we add useful syntactic and semantic information in the embedding of every word , 2 ) the high - efficiency multilayer memory network of full - orientation matching to match the question and context , 3 ) the pointer - network based answer boundary prediction layer to get the location of the answer in the passage .",29,0,68
dataset/preprocessed/training-data/natural_language_inference/38,The contributions of this paper can be summarized as follows .,30,0,11
dataset/preprocessed/training-data/natural_language_inference/38,"First , we propose a novel multi - layer embedding of the words in the passage and query .",31,0,19
dataset/preprocessed/training-data/natural_language_inference/38,We use skip - gram model to train the part - of - speech ( POS ) tags and name - entity recognition ( NER ) tags embedding that represent the syntactic and semantic information of the words respectively .,32,0,40
dataset/preprocessed/training-data/natural_language_inference/38,The analogy inference provided by skip - gram model can make the similar attributes close in their embedding space such that more adept at helping find the answer .,33,0,29
dataset/preprocessed/training-data/natural_language_inference/38,"Second , we introduce a memory networks of fullorientation matching .",34,0,11
dataset/preprocessed/training-data/natural_language_inference/38,"To combines the advantages of one dimensional attention and two dimensional attention , our novel hierarchical attention vectors contain both of them .",35,0,23
dataset/preprocessed/training-data/natural_language_inference/38,"Because key words in query often appear at ends of the sentence , one - dimensional attention , in which the bidirectional last hidden states are regarded as representation , is able to capture more useful information compared to only applying two dimensional attention .",36,0,45
dataset/preprocessed/training-data/natural_language_inference/38,"In order to deepen the memory and better understand the passage according to the query , we employ the structure of multihops to repeatedly read the passage .",37,0,28
dataset/preprocessed/training-data/natural_language_inference/38,"Moreover , we add agate to the end of each memory to improve the speed of convergence .",38,0,18
dataset/preprocessed/training-data/natural_language_inference/38,"Finally , the proposed method yields competitive results on the large machine comprehension bench marks SQuAD and the state - of - the - art results on TriviaQA dataset .",39,0,30
dataset/preprocessed/training-data/natural_language_inference/38,"On SQuAD , our model achieves 75.37 % exact match and 82 . 66 % F1 score .",40,0,18
dataset/preprocessed/training-data/natural_language_inference/38,"Moreover , our model avoids the high computation complexity self - matching mechanism which is popular in many previous works , thus we spend much less time and memory when training the model .",41,0,34
dataset/preprocessed/training-data/natural_language_inference/38,"As shows , our machine reading model consists of three parts .",43,0,12
dataset/preprocessed/training-data/natural_language_inference/38,"First , we concatenate several layers of embedding of questions and contexts and pass them into a bidirectional RNN .",44,0,20
dataset/preprocessed/training-data/natural_language_inference/38,Then we obtain the relationship between query and context through a novel fullorientation matching and apply memory networks in order to deeply understand .,45,0,24
dataset/preprocessed/training-data/natural_language_inference/38,"In the end , the output layer helps locate the answer in the passage .",46,0,15
dataset/preprocessed/training-data/natural_language_inference/38,Encoding of Context and Query,47,0,5
dataset/preprocessed/training-data/natural_language_inference/38,"In the encoding layer , we represent all tokens in the context and question as a sequence of embeddings and pass them as the input to a recurrent neural network .",48,0,31
dataset/preprocessed/training-data/natural_language_inference/38,Word - level embeddings and character - level embeddings are first applied .,49,0,13
dataset/preprocessed/training-data/natural_language_inference/9,Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING,2,1,15
dataset/preprocessed/training-data/natural_language_inference/9,"In this paper , we study the problem of question answering when reasoning over multiple facts is required .",4,1,19
dataset/preprocessed/training-data/natural_language_inference/9,"We propose Query - Reduction Network ( QRN ) , a variant of Recurrent Neural Network ( RNN ) that effectively handles both short - term ( local ) and long - term ( global ) sequential dependencies to reason over multiple facts .",5,0,44
dataset/preprocessed/training-data/natural_language_inference/9,"QRN considers the context sentences as a sequence of state - changing triggers , and reduces the original query to a more informed query as it observes each trigger ( context sentence ) through time .",6,0,36
dataset/preprocessed/training-data/natural_language_inference/9,"Our experiments show that QRN produces the state - of - the - art results in bAbI QA and dialog tasks , and in are al goal - oriented dialog dataset .",7,0,32
dataset/preprocessed/training-data/natural_language_inference/9,"In addition , QRN formulation allows parallelization on RNN 's time axis , saving an order of magnitude in time complexity for training and inference .",8,0,26
dataset/preprocessed/training-data/natural_language_inference/9,"In this paper , we address the problem of question answering ( QA ) when reasoning over multiple facts is required .",10,0,22
dataset/preprocessed/training-data/natural_language_inference/9,"For example , consider we know that Frogs eat insects and Flies are insects .",11,0,15
dataset/preprocessed/training-data/natural_language_inference/9,Then answering Do frogs eat flies ?,12,0,7
dataset/preprocessed/training-data/natural_language_inference/9,requires reasoning over both of the above facts .,13,0,9
dataset/preprocessed/training-data/natural_language_inference/9,"Question answering , more specifically context - based QA , has been extensively studied in machine comprehension tasks .",14,0,19
dataset/preprocessed/training-data/natural_language_inference/9,"However , most of the datasets are primarily focused on lexical and syntactic understanding , and hardly concentrate on inference over multiple facts .",15,0,24
dataset/preprocessed/training-data/natural_language_inference/9,"Recently , several datasets aimed for testing multi-hop reasoning have emerged ; among them are story - based QA and the dialog task .",16,0,24
dataset/preprocessed/training-data/natural_language_inference/9,"Recurrent Neural Network ( RNN ) and its variants , such as Long Short - Term Memory ( LSTM ) and Gated Recurrent Unit ( GRU ) , are popular choices for modeling natural language .",17,0,36
dataset/preprocessed/training-data/natural_language_inference/9,"However , when used for multi-hop reasoning in question answering , purely RNN - based models have shown to perform poorly .",18,0,22
dataset/preprocessed/training-data/natural_language_inference/9,This is largely due to the fact that RNN 's internal memory is inherently unstable over along term .,19,0,19
dataset/preprocessed/training-data/natural_language_inference/9,"For this reason , most recent approaches in the literature have mainly relied on global attention mechanism and shared external memory .",20,0,22
dataset/preprocessed/training-data/natural_language_inference/9,The attention mechanism allows these models to focus on a single sentence in each layer .,21,0,16
dataset/preprocessed/training-data/natural_language_inference/9,They can sequentially read multiple relevant sentences from the memory with multiple layers to perform multi-hop reasoning .,22,0,18
dataset/preprocessed/training-data/natural_language_inference/9,"However , one major drawback of these standard attention mechanisms is that they are insensitive to the time step ( memory address ) of the sentences when accessing them .",23,0,30
dataset/preprocessed/training-data/natural_language_inference/9,"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .",24,0,52
dataset/preprocessed/training-data/natural_language_inference/9,"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .",25,0,35
dataset/preprocessed/training-data/natural_language_inference/9,"For instance in , the original question , Where is the apple ? , can not be directly answered by any single sentence from the story .",26,0,27
dataset/preprocessed/training-data/natural_language_inference/9,"After observing the first sentence , Sandra got the apple there , QRN transforms the original question to a reduced query",27,0,21
dataset/preprocessed/training-data/natural_language_inference/9,"Where is Sandra ? , which is presumably . ? and ?",28,0,12
dataset/preprocessed/training-data/natural_language_inference/9,"are update gate and reduce functions , respectively .?",29,0,9
dataset/preprocessed/training-data/natural_language_inference/9,"is assigned to be h 2 5 , the local query at the last time step in the last layer .",30,0,21
dataset/preprocessed/training-data/natural_language_inference/9,"Also , red-colored text is the inferred meanings of the vectors ( see. easier to answer than the original question given the context provided by the first sentence .",31,0,29
dataset/preprocessed/training-data/natural_language_inference/9,"2 Unlike RNN - based models , QRN 's candidate state ( h t in ) does not depend on the previous hidden state ( h t?1 ) .",32,0,29
dataset/preprocessed/training-data/natural_language_inference/9,"Compared to memory - based approaches , QRN can better encodes locality information because it does not use a global memory access controller ( circle nodes in ) , and the query updates are performed locally .",33,0,37
dataset/preprocessed/training-data/natural_language_inference/9,"In short , the main contribution of QRN is threefold .",34,0,11
dataset/preprocessed/training-data/natural_language_inference/9,"First , QRN is a simple variant of RNN that reduces the query given the context sentences in a differentiable manner .",35,0,22
dataset/preprocessed/training-data/natural_language_inference/9,"Second , QRN is situated between the attention mechanism and RNN , effectively handling time dependency and long - term dependency problems of each technique , respectively .",36,0,28
dataset/preprocessed/training-data/natural_language_inference/9,"Hence it is well - suited for sequential data with both local and global interactions ( note that QRN is not the replacement of RNN , which is arguably better for modeling complex local interactions ) .",37,0,37
dataset/preprocessed/training-data/natural_language_inference/9,"Third , unlike most RNN - based models , QRN can be parallelized overtime by computing candidate reduced queries ( h t ) directly from local input queries ( q t ) and context sentence vectors ( x t ) .",38,0,41
dataset/preprocessed/training-data/natural_language_inference/9,"In fact , the parallelizability of QRN implies that QRN does not suffer from the vanishing gradient problem of RNN , hence effectively addressing the long - term dependency .",39,0,30
dataset/preprocessed/training-data/natural_language_inference/9,We experimentally demonstrate these contributions by achieving the state - of - the - art results on story - based QA and interactive dialog datasets .,40,0,26
dataset/preprocessed/training-data/natural_language_inference/9,"In story - based QA ( or dialog dataset ) , the input is the context as a sequence of sentences ( story or past conversations ) and a question in natural language ( equivalent to the user 's last utterance in the dialog ) .",42,0,46
dataset/preprocessed/training-data/natural_language_inference/9,The output is the predicted answer to the question in natural language ( the system 's next utterance in the dialog ) .,43,0,23
dataset/preprocessed/training-data/natural_language_inference/9,The only supervision provided during training is the answer to the question .,44,0,13
dataset/preprocessed/training-data/natural_language_inference/9,"In this paper we particularly focus on end - to - end solutions , i.e. , the only supervision comes from questions and answers , and we restrain from using manually defined rules or external language resources , such as lexicon or dependency parser .",45,0,45
dataset/preprocessed/training-data/natural_language_inference/9,"Let x 1 , . . . , x",46,0,9
dataset/preprocessed/training-data/natural_language_inference/9,"T denote the sequence of sentences , where T is the number of sentences in the story , and let q denote the question .",47,0,25
dataset/preprocessed/training-data/natural_language_inference/9,"denote the predicted answer , and y denote the true answer .",49,0,12
dataset/preprocessed/training-data/natural_language_inference/19,Distance - based Self - Attention Network for Natural Language Inference,2,1,11
dataset/preprocessed/training-data/natural_language_inference/19,Attention mechanism has been used as an ancillary means to help RNN or CNN .,4,0,15
dataset/preprocessed/training-data/natural_language_inference/19,"However , the Transformer ( Vaswani et al. , 2017 ) recently recorded the state - of - theart performance in machine translation with a dramatic reduction in training time by solely using attention .",5,0,35
dataset/preprocessed/training-data/natural_language_inference/19,"Motivated by the Transformer , Directional Self Attention Network ( Shen et al. , 2017 ) , a fully attention - based sentence encoder , was proposed .",6,0,28
dataset/preprocessed/training-data/natural_language_inference/19,It showed good performance with various data by using forward and backward directional information in a sentence .,7,0,18
dataset/preprocessed/training-data/natural_language_inference/19,"But in their study , not considered at all was the distance between words , an important feature when learning the local dependency to help understand the context of input text .",8,0,32
dataset/preprocessed/training-data/natural_language_inference/19,"We propose Distance - based Self - Attention Network , which considers the word distance by using a simple distance mask in order to model the local dependency without losing the ability of modeling global dependency which attention has inherent .",9,0,41
dataset/preprocessed/training-data/natural_language_inference/19,"Our model shows good performance with NLI data , and it records the new state - of - the - art result with SNLI data .",10,1,26
dataset/preprocessed/training-data/natural_language_inference/19,"Additionally , we show that our model has a strength in long sentences or documents . *",11,0,17
dataset/preprocessed/training-data/natural_language_inference/19,The NLI task can be solved through two different approaches : sentence encoding - based models and joint models .,12,0,20
dataset/preprocessed/training-data/natural_language_inference/19,"The former separately encode each sentence , whereas the latter take into account the direct relationship between two sentences .",13,0,20
dataset/preprocessed/training-data/natural_language_inference/19,"Between them , sentence - encoding based models focus on training sentence encoder that can represent sentences in vector form well .",14,0,22
dataset/preprocessed/training-data/natural_language_inference/19,"We focus on the former approach , since the objective of our work is to develop an advanced sentenceencoding model .",15,0,21
dataset/preprocessed/training-data/natural_language_inference/19,Sequence modeling has been employing Recurrent Neural Networks ( RNN ) or Convolutional Neural Networks ( CNN ) mostly .,17,0,20
dataset/preprocessed/training-data/natural_language_inference/19,"More recently , models incorporating attention mechanisms have shown good performance in machine translation , Natural Language Inference ( NLI ) , and Question Answering ( QA ) etc .",18,1,30
dataset/preprocessed/training-data/natural_language_inference/19,Attention mechanisms used to be exploited in conjunction with RNN or CNN as an ancillary means to help improve performance .,19,0,21
dataset/preprocessed/training-data/natural_language_inference/19,"Lately , presented the first fully attention - based model , which recorded the state - of - the - art result in machine translation .",20,0,26
dataset/preprocessed/training-data/natural_language_inference/19,"As a fully attention - based model can consider all words in a sentence at once , parallelization leads to great reduction in training time .",21,0,26
dataset/preprocessed/training-data/natural_language_inference/19,"Motivated by , proposed the first fully attention - based sentence encoder .",22,0,13
dataset/preprocessed/training-data/natural_language_inference/19,recorded good performance in a variety of tasks .,23,0,9
dataset/preprocessed/training-data/natural_language_inference/19,"In particular , they recorded the state - of - the - art result with Stanford Natural Language Inference ( SNLI ) dataset which is a representative dataset of NLI .",24,0,31
dataset/preprocessed/training-data/natural_language_inference/19,"The NLI task aims to classify the relationship between two sentences as entailment , contradiction , or neutral .",25,0,19
dataset/preprocessed/training-data/natural_language_inference/19,One of the approaches to solving the NLI task is to use sentence - encoding based models .,26,0,18
dataset/preprocessed/training-data/natural_language_inference/19,presented a sentence - encoding based model reflecting directional information in a sentence .,27,0,14
dataset/preprocessed/training-data/natural_language_inference/19,"However , the distance between words was not considered at all in their model , and the directional information simply involved words before and after the reference word .",28,0,29
dataset/preprocessed/training-data/natural_language_inference/19,"Altogether , positional information of words was not fully taken into account .",29,0,13
dataset/preprocessed/training-data/natural_language_inference/19,"As a result , the difference of importance between the distant words and the nearby words was not appropriately reflected .",30,0,21
dataset/preprocessed/training-data/natural_language_inference/19,"Hence lo - cal dependency was not properly modeled , which in turn failed to capture the context information in long sentences .",31,0,23
dataset/preprocessed/training-data/natural_language_inference/19,"To tackle this limitation , we propose Distancebased Self - Attention Network which introduces a distance mask which models the relative distance between words .",32,0,25
dataset/preprocessed/training-data/natural_language_inference/19,"In conjunction with a directional mask , the distance mask allows us to incorporate complete positional information of words in our model .",33,0,23
dataset/preprocessed/training-data/natural_language_inference/19,"Our Distance - based Self - Attention Network achieved good performance with NLI data , and recorded the state - of - the - art result with SNLI .",34,0,29
dataset/preprocessed/training-data/natural_language_inference/19,"Our model worked exceptionally well with long sentences , in particular .",35,0,12
dataset/preprocessed/training-data/natural_language_inference/19,We also visualized the effect of the distance mask to show that our model can grasp both local dependency and global dependency .,36,0,23
dataset/preprocessed/training-data/natural_language_inference/19,NLI tasks have been studied through models of various structures .,38,0,11
dataset/preprocessed/training-data/natural_language_inference/19,"Most of all , models combining attention with Long Short - Term Memory ( LSTM ) have performed well .",39,0,20
dataset/preprocessed/training-data/natural_language_inference/19,improved the performance by adding the mean pooling vector to the conventional attention model in which attention is applied to hidden states of LSTM .,40,0,25
dataset/preprocessed/training-data/natural_language_inference/19,used the input gates of the LSTM as attention weights to simplify the model structure .,41,0,16
dataset/preprocessed/training-data/natural_language_inference/19,"In and , short - cut connections in stacked LSTM , in combination with max - pooling originally suggested by , were proven effective in improving performance , recording the state - of - the - art performance in MultiNLI .",42,0,41
dataset/preprocessed/training-data/natural_language_inference/19,And used the memory for sentence encoding motivated by Neural Turing Machine .,43,0,13
dataset/preprocessed/training-data/natural_language_inference/19,"was the first study to construct an end - to - end model with attention alone , and recorded the state - of - the - art performance in machine translation tasks . 's encoder - decoder framework consists of a multihead attention and a position - wise feed forward network as a basic building block which is deeply stacked combined with residual connection .",44,0,65
dataset/preprocessed/training-data/natural_language_inference/19,The multi-head attention projects the input sentences to multiple subspaces and then computes the scaled dot -product attention in each subspace .,45,0,22
dataset/preprocessed/training-data/natural_language_inference/19,The results in each subspace are then concatenated and projected again .,46,0,12
dataset/preprocessed/training-data/natural_language_inference/19,Position - wise feed forward network adds non-linearity to vector representations of each position .,47,0,15
dataset/preprocessed/training-data/natural_language_inference/19,"In this way , the fully attentionbased model was constructed without using RNN or CNN , and the training cost was greatly reduced. , a very recent work , constructed a fully attention - based sentence encoder motivated by .",48,0,40
dataset/preprocessed/training-data/natural_language_inference/19,They proposed a multi-dimensional attention mechanism that computes the attention by each dimension through modification of additive attention .,49,0,19
dataset/preprocessed/training-data/natural_language_inference/22,Ruminating Reader : Reasoning with Gated Multi - Hop Attention,2,0,10
dataset/preprocessed/training-data/natural_language_inference/22,"To answer the question in machine comprehension ( MC ) task , the models need to establish the interaction between the question and the context .",4,1,26
dataset/preprocessed/training-data/natural_language_inference/22,"To tackle the problem that the single - pass model can not reflect on and correct its answer , we present Ruminating Reader .",5,0,24
dataset/preprocessed/training-data/natural_language_inference/22,Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model ( BIDAF ) .,6,0,24
dataset/preprocessed/training-data/natural_language_inference/22,We propose novel layer structures that construct an query - aware context vector representation and fuse encoding representation with intermediate representation on top of BIDAF model .,7,0,27
dataset/preprocessed/training-data/natural_language_inference/22,We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure .,8,0,16
dataset/preprocessed/training-data/natural_language_inference/22,"In experiments on SQuAD , we find that the Reader outperforms the BIDAF baseline by a substantial margin , and matches or surpasses the performance of all other published systems .",9,0,31
dataset/preprocessed/training-data/natural_language_inference/22,1 The latest results are listed at https://rajpurkar.github.io/SQuAD -explorer/,10,0,9
dataset/preprocessed/training-data/natural_language_inference/22,The majority of recorded human knowledge is circulated in unstructured natural language .,12,0,13
dataset/preprocessed/training-data/natural_language_inference/22,It is tremendously valuable to allow machines to read and comprehend the text knowledge .,13,0,15
dataset/preprocessed/training-data/natural_language_inference/22,Machine comprehension ( MC ) - especially in the form of question answering ( QA ) - is therefore attracting a significant amount of attention from the machine learning community .,14,1,31
dataset/preprocessed/training-data/natural_language_inference/22,"Recently introduced large - scale datasets like CNN / Daily Mail , the Stanford Question Answering Dataset ( SQuAD ; allow data - driven methods , including deep learning , to become viable .",15,0,34
dataset/preprocessed/training-data/natural_language_inference/22,Recent approaches toward solving machine comprehension tasks using neural networks can be viewed as falling into two broad categories : single - pass reasoners and multiple - pass reasoners .,16,0,30
dataset/preprocessed/training-data/natural_language_inference/22,Single - pass models read a question and a source text once and often adopt the differentiable attention mechanism that emphasizes important parts of the context related to the question .,17,0,31
dataset/preprocessed/training-data/natural_language_inference/22,BIDAF represents one of the state - of - the - art single - pass models in Machine Comprehension .,18,0,20
dataset/preprocessed/training-data/natural_language_inference/22,BIDAF uses a bi-directional attention matrix which calculates the correlations between each word pair in context and query to build query - aware context representation .,19,0,26
dataset/preprocessed/training-data/natural_language_inference/22,"However , BIDAF and some similar models miss some questions because they do n't have the capacity to reflect on problematic candidate answers and revise their decisions .",20,0,28
dataset/preprocessed/training-data/natural_language_inference/22,"When humans are reading a text with the goal of answering a question , they tend to read it multiple times to get a better understanding of the context and question , and to give a better response .",21,0,39
dataset/preprocessed/training-data/natural_language_inference/22,"With this intuition , recent multi-pass models revisit the question and the context passage ( or ruminate ) to infer the relations between the context , the question and the answer .",22,0,32
dataset/preprocessed/training-data/natural_language_inference/22,"We propose an extension of BIDAF , called Ruminating Reader , which uses a second pass of reading and reasoning to allow it to learn to avoid mistakes and to ensure that it is able to effectively use the full context when selecting an answer .",23,0,46
dataset/preprocessed/training-data/natural_language_inference/22,"In addition to adding a second pass , we also introduce two novel layer types , the ruminate layers , which use gating mechanisms to fuse the obtained from the first and second passes .",24,0,35
dataset/preprocessed/training-data/natural_language_inference/22,"We observe a surprising phenomenon that when an LSTM layer in the context ruminate layer takes same input in each timestep , it can produce useful representation for the gates .",25,0,31
dataset/preprocessed/training-data/natural_language_inference/22,"In addition , we introduce an answer-question similarity loss to penalize overlap between question and predicted answer , a common feature in the errors of our base model .",26,0,29
dataset/preprocessed/training-data/natural_language_inference/22,"This allows us to achieve an F 1 score of 79.5 and Exact Match ( EM ) score of 70.6 on hidden test set , 1 an improvement of 2.2 F1 score and 2.9 EM on BIDAF .",27,0,38
dataset/preprocessed/training-data/natural_language_inference/22,shows a high - level comparison between BIDAF and Ruminating Reader .,28,0,12
dataset/preprocessed/training-data/natural_language_inference/22,This paper is organized as follows :,29,0,7
dataset/preprocessed/training-data/natural_language_inference/22,In Section 2 we define the problem to be solved and introduce the SQuAD task .,30,0,16
dataset/preprocessed/training-data/natural_language_inference/22,"In Section 3 we introduce Ruminating Reader , focusing on the informationextracting and information - digesting components and how they integrate .",31,0,22
dataset/preprocessed/training-data/natural_language_inference/22,Section 4 discusses related work .,32,0,6
dataset/preprocessed/training-data/natural_language_inference/22,"Section 5 presents the experimental setting , results and analysis .",33,0,11
dataset/preprocessed/training-data/natural_language_inference/22,Section 6 concludes .,34,0,4
dataset/preprocessed/training-data/natural_language_inference/22,The task of the Ruminate Reader is to answer a question by reading and understanding a paragraph of text and selecting a span of words within the context .,36,0,29
dataset/preprocessed/training-data/natural_language_inference/22,"Formally , the Training and development data consist of tuples ( Q , P , A ) , where Q = ( q 1 , ... , q i , ...q | Q | ) is the question , a sequence of words with length | Q| , C = ( c 1 , ...c j , ... , c | C | ) is the context , a sequence of words with length | C| , and A = ( a b , a e ) is the answer span marking the beginning and end indices of the the answer in the context ( 1 <= ab <= a e <= | C | ) .",37,0,117
dataset/preprocessed/training-data/natural_language_inference/22,The SQuAD corpus is built using 536 articles randomly selected from English Wikipedia .,39,0,14
dataset/preprocessed/training-data/natural_language_inference/22,"Images , figures , tables are stripped and any paragraphs shorter than 500 characters are discarded .",40,0,17
dataset/preprocessed/training-data/natural_language_inference/22,"Unlike other datasets that such as CNN / Daily Mail whose questions are synthesized , uses a crowdsourcing platform to generate realistic question and answer pairs .",41,0,27
dataset/preprocessed/training-data/natural_language_inference/22,"SQuAD contains 107,785 question - answer pairs .",42,0,8
dataset/preprocessed/training-data/natural_language_inference/22,The typical context length spans from 50 tokens to 250 tokens .,43,0,12
dataset/preprocessed/training-data/natural_language_inference/22,The typical length of a question is around 10 tokens .,44,0,11
dataset/preprocessed/training-data/natural_language_inference/22,"The answer be any span of words from the context , resulting in O ( | C | 2 ) possible outputs .",45,0,23
dataset/preprocessed/training-data/natural_language_inference/22,3 Our Model,46,0,3
dataset/preprocessed/training-data/natural_language_inference/22,"In this section , we review the BIDAF model and introduce our extension , the Ruminating Reader .",48,0,18
dataset/preprocessed/training-data/natural_language_inference/22,"Our additions to the base model are motivated by the intuition that adding an additional pass of reading will allow the model to better integrate information from the question and answer and to better weigh possible answers , and that by interpolating the results of the second pass with those of the first pass through gating , we can prevent the additional complexity that we add to the model from substantially increasing the difficulty of training .",49,0,77
dataset/preprocessed/training-data/natural_language_inference/1,Large - scale Simple Question Answering with Memory Networks,2,1,9
dataset/preprocessed/training-data/natural_language_inference/1,Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .,4,1,24
dataset/preprocessed/training-data/natural_language_inference/1,"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions .",5,1,51
dataset/preprocessed/training-data/natural_language_inference/1,"To this end , we introduce a new dataset of 100 k questions that we use in conjunction with existing benchmarks .",6,0,22
dataset/preprocessed/training-data/natural_language_inference/1,"We conduct our study within the framework of Memory Networks ( Weston et al. , 2015 ) because this perspective allows us to eventually scale up to more complex reasoning , and show that Memory Networks can be successfully trained to achieve excellent performance .",7,0,45
dataset/preprocessed/training-data/natural_language_inference/1,"Open-domain Question Answering ( QA ) systems aim at providing the exact answer ( s ) to questions formulated in natural language , without restriction of domain .",9,0,28
dataset/preprocessed/training-data/natural_language_inference/1,"While there is along history of QA systems that search for textual documents or on the Web and extract answers from them ( see e.g. ) , recent progress has been made with the release of large Knowledge Bases ( KBs ) such as Freebase , which contain consolidated knowledge stored as atomic facts , and extracted from different sources , such as free text , tables in webpages or collaborative input .",10,0,73
dataset/preprocessed/training-data/natural_language_inference/1,Existing approaches for QA from KBs use learnable components to either transform the question into a structured KB query or learn to embed questions and facts in a low dimensional vector space and retrieve the answer by computing similarities in this embedding space .,11,0,44
dataset/preprocessed/training-data/natural_language_inference/1,"However , while most recent efforts have focused on designing systems with higher reasoning capabilities , that could jointly retrieve and use multiple facts to answer , the simpler problem of answering questions that refer to a single fact of the KB , which we call Simple Question Answering in this paper , is still far from solved .",12,1,59
dataset/preprocessed/training-data/natural_language_inference/1,"Hence , existing benchmarks are small ; they mostly cover the head of the distributions of facts , and are restricted in their question types and their syntactic and lexical variations .",13,0,32
dataset/preprocessed/training-data/natural_language_inference/1,"As such , it is still unknown how much the existing systems perform outside the range of the specific question templates of a few , small benchmark datasets , and it is also unknown whether learning on a single dataset transfers well on other ones , and whether such systems can learn from different training sources , which we believe is necessary to capture the whole range of possible questions .",14,0,71
dataset/preprocessed/training-data/natural_language_inference/1,"Besides , the actual need for reasoning , i.e. constructing the answer from more than a single fact from the KB , depends on the actual structure of the KB .",15,0,31
dataset/preprocessed/training-data/natural_language_inference/1,"As we shall see , for instance , a simple preprocessing of Freebase tremendously increases the coverage of simple QA in terms of possible questions that can be answered with a single fact , including list questions that expect more than a single answer .",16,0,45
dataset/preprocessed/training-data/natural_language_inference/1,"In fact , the task of simple QA itself might already cover a wide range of practical usages , if the KB is properly organized .",17,0,26
dataset/preprocessed/training-data/natural_language_inference/1,This paper presents two contributions .,18,0,6
dataset/preprocessed/training-data/natural_language_inference/1,"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",19,0,45
dataset/preprocessed/training-data/natural_language_inference/1,"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ?",20,0,29
dataset/preprocessed/training-data/natural_language_inference/1,Which forest is Fires Creek in ?,21,0,7
dataset/preprocessed/training-data/natural_language_inference/1,What is an active ingredient in childrens earache relief ?,22,0,10
dataset/preprocessed/training-data/natural_language_inference/1,"tators and associated to Freebase facts , while the largest existing benchmark , WebQuestions , contains less than 6 k questions created automatically using the Google suggest API .",23,0,29
dataset/preprocessed/training-data/natural_language_inference/1,"Second , in sections 3 and 4 , we present an embedding - based QA system developed under the framework of Memory Networks ( Mem NNs ) .",24,0,28
dataset/preprocessed/training-data/natural_language_inference/1,"Memory Networks are learning systems centered around a memory component that can be read and written to , with a particular focus on cases where the relationship between the input and response languages ( here natural language ) and the storage language ( here , the facts from KBs ) is performed by embedding all of them in the same vector space .",25,0,63
dataset/preprocessed/training-data/natural_language_inference/1,The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,26,0,20
dataset/preprocessed/training-data/natural_language_inference/1,"While our model bares similarity with previous embedding models for QA , using the framework of MemNNs opens the perspective to more involved inference schemes in future work , since MemNNs were shown to perform well on complex reasoning toy QA tasks .",27,0,43
dataset/preprocessed/training-data/natural_language_inference/1,We discuss related work in Section 5 .,28,0,8
dataset/preprocessed/training-data/natural_language_inference/1,"We report experimental results in Section 6 , where we show that our model achieves excellent results on the benchmark WebQuestions .",29,0,22
dataset/preprocessed/training-data/natural_language_inference/1,We also show that it can learn from two different QA datasets to improve its performance on both .,30,0,19
dataset/preprocessed/training-data/natural_language_inference/1,We also present the first successful application of transfer learning for QA .,31,0,13
dataset/preprocessed/training-data/natural_language_inference/1,"Using the Reverb KB and QA datasets , we show that Reverb facts can be added to the memory and used to answer without retraining , and that MemNNs achieve better results than some systems designed on this dataset .",32,0,40
dataset/preprocessed/training-data/natural_language_inference/1,Simple Question Answering,33,0,3
dataset/preprocessed/training-data/natural_language_inference/1,Knowledge Bases contain facts expressed as triples where subject and object are entities and relationship describes the type of ( directed ) link between these entities .,34,0,27
dataset/preprocessed/training-data/natural_language_inference/1,"The simple QA prob - lem we address here consist in finding the answer to questions that can be rephrased as queries of the form , asking for all objects linked to subject by relationship .",35,0,36
dataset/preprocessed/training-data/natural_language_inference/1,"What do Jamaican people speak ? , for instance , could be rephrased as the Freebase query ( jamaica , language spoken , ? ) .",37,0,26
dataset/preprocessed/training-data/natural_language_inference/1,"In other words , fetching a single fact from a KB is sufficient to answer correctly .",38,0,17
dataset/preprocessed/training-data/natural_language_inference/1,"The term simple QA refers to the simplicity of the reasoning process needed to answer questions , since it involves a single fact .",39,0,24
dataset/preprocessed/training-data/natural_language_inference/1,"However , this does not mean that the QA problem is easy per se , since retrieving this single supporting fact can be very challenging as it involves to search over millions of alternatives given a query expressed in natural language .",40,0,42
dataset/preprocessed/training-data/natural_language_inference/1,"shows that , with a KB with many types of relationships like",41,0,12
dataset/preprocessed/training-data/natural_language_inference/1,"Freebase , the range of questions that can be answered with a single fact is already very broad .",42,0,19
dataset/preprocessed/training-data/natural_language_inference/1,"Besides , as we shall see , modiying slightly the structure of the KB can make some QA problems simpler by adding direct connections between entities and hence allow to bypass the need for more complex reasoning .",43,0,38
dataset/preprocessed/training-data/natural_language_inference/1,"We use the KB Freebase 1 as the basis of our QA system , our source of facts and answers .",45,0,21
dataset/preprocessed/training-data/natural_language_inference/1,All Freebase entities and relationships are typed and the lexicon for types and relationships is closed .,46,0,17
dataset/preprocessed/training-data/natural_language_inference/1,"Freebase data is collaboratively collected and curated , to ensure a high reliability of the facts .",47,0,17
dataset/preprocessed/training-data/natural_language_inference/1,"Each entity has an internal identifier and a set of strings thatare usually used to refer to that entity in text , termed aliases .",48,0,25
dataset/preprocessed/training-data/natural_language_inference/1,"We consider two extracts of Freebase , whose statistics are given in .",49,0,13
dataset/preprocessed/training-data/natural_language_inference/83,Story Comprehension for Predicting What Happens Next,2,1,7
dataset/preprocessed/training-data/natural_language_inference/83,"Automatic story comprehension is a fundamental challenge in Natural Language Understanding , and can enable computers to learn about social norms , human behavior and commonsense .",4,1,27
dataset/preprocessed/training-data/natural_language_inference/83,"In this paper , we present a story comprehension model that explores three distinct semantic aspects : ( i ) the sequence of events described in the story , ( ii ) its emotional trajectory , and ( iii ) its plot consistency .",5,0,44
dataset/preprocessed/training-data/natural_language_inference/83,"We judge the model 's understanding of real - world stories by inquiring if , like humans , it can develop an expectation of what will happen next in a given story .",6,0,33
dataset/preprocessed/training-data/natural_language_inference/83,"Specifically , we use it to predict the correct ending of a given short story from possible alternatives .",7,0,19
dataset/preprocessed/training-data/natural_language_inference/83,The model uses a hidden variable to weigh the semantic aspects in the context of the story .,8,0,18
dataset/preprocessed/training-data/natural_language_inference/83,"Our experiments demonstrate the potential of our approach to characterize these semantic aspects , and the strength of the hidden variable based approach .",9,0,24
dataset/preprocessed/training-data/natural_language_inference/83,The model outperforms the stateof - the - art approaches and achieves best results on a publicly available dataset .,10,0,20
dataset/preprocessed/training-data/natural_language_inference/83,Narratives are a fundamental part of human language and culture .,12,0,11
dataset/preprocessed/training-data/natural_language_inference/83,"They serve as vehicles to share experiences , information and goals .",13,0,12
dataset/preprocessed/training-data/natural_language_inference/83,"For these reasons , automatically understanding stories is an interesting but challenging task for Computational Linguists .",14,1,17
dataset/preprocessed/training-data/natural_language_inference/83,"Story comprehension involves not only an array of NLP capabilities , but also some commonsense knowledge and an understanding of normative social behavior .",15,0,24
dataset/preprocessed/training-data/natural_language_inference/83,Past research has focused on various aspects of story understand - Context :,16,0,13
dataset/preprocessed/training-data/natural_language_inference/83,One day Wesley 's auntie came over to visit .,17,0,10
dataset/preprocessed/training-data/natural_language_inference/83,"He was happy to see her , because he liked to play with her .",18,0,15
dataset/preprocessed/training-data/natural_language_inference/83,"When she started to give his little sister attention , he got jealous .",19,0,14
dataset/preprocessed/training-data/natural_language_inference/83,He got angry at his auntie and bit her hand when she was n't looking .,20,0,16
dataset/preprocessed/training-data/natural_language_inference/83,Incorrect Ending :,21,0,3
dataset/preprocessed/training-data/natural_language_inference/83,She gave him a cookie for being so nice .,22,0,10
dataset/preprocessed/training-data/natural_language_inference/83,Correct Ending : He was scolded . :,23,0,8
dataset/preprocessed/training-data/natural_language_inference/83,Example from the story - cloze task : predict the correct ending to a given short story out of provided options .,24,0,22
dataset/preprocessed/training-data/natural_language_inference/83,"ing such as identifying character personas , interpersonal relationships , plotpatterns , narrative structures ) .",25,0,16
dataset/preprocessed/training-data/natural_language_inference/83,There has also been an interest in predicting what is expected to happen next in apiece of text .,26,0,19
dataset/preprocessed/training-data/natural_language_inference/83,Human readers are good at filling - in - the - gaps or inferring information that is not explicitly stated in the text .,27,0,24
dataset/preprocessed/training-data/natural_language_inference/83,"However , computers are not yet able to match their performance on predicting what could be the likely next step in a given sequence of events described in a story .",28,0,31
dataset/preprocessed/training-data/natural_language_inference/83,"Recently , introduced the story - cloze task for testing this ability , albeit without the aspect of language generation .",29,1,21
dataset/preprocessed/training-data/natural_language_inference/83,This task requires choosing the correct ending to a given four sentences long story ( also referred to as context ) from two provided alternatives .,30,0,26
dataset/preprocessed/training-data/natural_language_inference/83,"shows an example story consisting of a short context , and two ending options .",31,0,15
dataset/preprocessed/training-data/natural_language_inference/83,In this work we address this story - cloze task .,32,0,11
dataset/preprocessed/training-data/natural_language_inference/83,"While the short nature and third person narrative style of these stories help us circumvent the problem of speaker identification and processing long dialogues , the crowdsourced dataset ensures that they reflect real - world and commonsense stories .",33,0,39
dataset/preprocessed/training-data/natural_language_inference/83,"Our approach emphasizes the joint contribution of multiple aspects to story understanding , which future research can build upon .",34,0,20
dataset/preprocessed/training-data/natural_language_inference/83,"In this paper we explore three semantic aspects of story understanding : ( i ) the sequence of events described in the story , ( ii ) the evolution of sentiment and emotional trajectories , and ( iii ) topical consistency .",35,0,42
dataset/preprocessed/training-data/natural_language_inference/83,"The first aspect is motivated from approaches in semantic script induction , and evaluates if events described in an ending - alternative are likely to occur within the sequence of events described in the preceding context .",36,0,37
dataset/preprocessed/training-data/natural_language_inference/83,"For example , in the story in , Wesley gets angry and bites his sister 's hand .",37,0,18
dataset/preprocessed/training-data/natural_language_inference/83,"So , a next likely step might suggest that he would be scolded .",38,0,14
dataset/preprocessed/training-data/natural_language_inference/83,"However , there are multiple semantic aspects to story understanding beyond analyzing events and scripts .",39,0,16
dataset/preprocessed/training-data/natural_language_inference/83,Stories often describe characters ( e.g. Wesley ) who need to be viewed as social and emotional agents .,40,0,19
dataset/preprocessed/training-data/natural_language_inference/83,"They not only describe events involving these characters , but also reflect their social lives and emotional states .",41,0,19
dataset/preprocessed/training-data/natural_language_inference/83,Our model captures this by evaluating if the sentiment described in an ending option makes sense considering the context of the story .,42,0,23
dataset/preprocessed/training-data/natural_language_inference/83,"For example , in the story in , the general sentiment of being scolded is better aligned with the sentiment of Wesley being angry and jealous , compared to that of being nice .",43,0,34
dataset/preprocessed/training-data/natural_language_inference/83,"Also , stories generally revolve around coherent themes and topics .",44,0,11
dataset/preprocessed/training-data/natural_language_inference/83,Our model accounts for that by analyzing if the topic of an ending option is consistent with the preceding context .,45,0,21
dataset/preprocessed/training-data/natural_language_inference/83,We present a log - linear model that is used to weigh the various aspects of the story using a hidden variable .,46,0,23
dataset/preprocessed/training-data/natural_language_inference/83,It then uses this hidden variable to predict the correct ending for the given story .,47,0,16
dataset/preprocessed/training-data/natural_language_inference/83,We demonstrate the strength of our approach by comparing it with the existing state - of - the - art methods for this task .,48,0,25
dataset/preprocessed/training-data/natural_language_inference/83,We first validate the predictive potential of the features that correspond to the three semantic aspects through a simple classifier trained using these features .,49,0,25
dataset/preprocessed/training-data/natural_language_inference/5,A Compare - Aggregate Model with Latent Clustering for Answer Selection,2,1,11
dataset/preprocessed/training-data/natural_language_inference/5,"In this paper , we propose a novel method for a sentence - level answer- selection task that is a fundamental problem in natural language processing .",4,1,27
dataset/preprocessed/training-data/natural_language_inference/5,"First , we explore the effect of additional information by adopting a pretrained language model to compute the vector representation of the input text and by applying transfer learning from a large - scale corpus .",5,0,36
dataset/preprocessed/training-data/natural_language_inference/5,"Second , we enhance the compare - aggregate model by proposing a novel latent clustering method to compute additional information within the target corpus and by changing the objective function from listwise to pointwise .",6,0,35
dataset/preprocessed/training-data/natural_language_inference/5,"To evaluate the performance of the proposed approaches , experiments are performed with the WikiQA and TREC - QA datasets .",7,0,21
dataset/preprocessed/training-data/natural_language_inference/5,"The empirical results demonstrate the superiority of our proposed approach , which achieve state - of - the - art performance for both datasets .",8,0,25
dataset/preprocessed/training-data/natural_language_inference/5,Automatic question answering ( QA ) is a primary objective of artificial intelligence .,10,1,14
dataset/preprocessed/training-data/natural_language_inference/5,"Recently , research on this task has taken two major directions based on the answer span considered by the model .",11,0,21
dataset/preprocessed/training-data/natural_language_inference/5,"The first direction ( i.e. , the fine - grained approach ) finds an exact answer to a question within a given passage .",12,0,24
dataset/preprocessed/training-data/natural_language_inference/5,"The second direction ( i.e. , the coarse - level approach ) is an information retrieval ( IR ) - based approach that provides the most relevant sentence from a given document in response to a question .",13,0,38
dataset/preprocessed/training-data/natural_language_inference/5,"In this study , we are interested in building a model that computes a matching score between two text inputs .",14,0,21
dataset/preprocessed/training-data/natural_language_inference/5,"In particular , our model is designed to undertake an answer-selection task that chooses the sentence that is most relevant to the question from a list of answer candidates .",15,0,30
dataset/preprocessed/training-data/natural_language_inference/5,This task has been extensively investigated by researchers because it is a fundamental task that can be applied to other QA - related tasks .,16,0,25
dataset/preprocessed/training-data/natural_language_inference/5,"However , most previous answer-selection studies have employed small datasets compared with the large datasets employed for other natural language processing ( NLP ) tasks .",17,0,26
dataset/preprocessed/training-data/natural_language_inference/5,"Therefore , * Work conducted while the author was an intern at Adobe Research .",18,0,15
dataset/preprocessed/training-data/natural_language_inference/5,the exploration of sophisticated deep learning models for this task is difficult .,19,0,13
dataset/preprocessed/training-data/natural_language_inference/5,"To fill this gap , we conduct an intensive investigation with the following directions to obtain the best performance in the answerselection task .",20,0,24
dataset/preprocessed/training-data/natural_language_inference/5,"First , we explore the effect of additional information by adopting a pretrained language model ( LM ) to compute the vector representation of the input text .",21,0,28
dataset/preprocessed/training-data/natural_language_inference/5,Recent studies have shown that replacing the word - embedding layer with a pretrained language model helps the model capture the contextual meaning of words in the sentence .,22,0,29
dataset/preprocessed/training-data/natural_language_inference/5,"Following this study , we select an ELMo language model for this study .",23,0,14
dataset/preprocessed/training-data/natural_language_inference/5,"We investigate the applicability of transfer learning ( TL ) using a large - scale corpus that is created for a relevant - sentence - selection task ( i.e. , question - answering NLI ( QNLI ) dataset ) .",24,0,40
dataset/preprocessed/training-data/natural_language_inference/5,"Second , we further enhance one of the baseline models , Comp - Clip ( refer to the discussion in 3.1 ) , for the target QA task by proposing a novel latent clustering ( LC ) method .",25,0,39
dataset/preprocessed/training-data/natural_language_inference/5,The LC method computes latent cluster information for target samples by creating a latent memory space and calculating the similarity between the sample and the memory .,26,0,27
dataset/preprocessed/training-data/natural_language_inference/5,"By an endto - end learning process with the answer-selection task , the LC method assigns true - label question - answer pairs to similar clusters .",27,0,27
dataset/preprocessed/training-data/natural_language_inference/5,"In this manner , a model will have further information for matching sentence pairs , which increases the total model performance .",28,0,22
dataset/preprocessed/training-data/natural_language_inference/5,"Last , we explore the effect of different objective functions ( listwise and pointwise learning ) .",29,0,17
dataset/preprocessed/training-data/natural_language_inference/5,"In contrast to previous research , we observe that the pointwise learning approach performs better than the listwise learning approach when we apply our proposed methods .",30,0,27
dataset/preprocessed/training-data/natural_language_inference/5,Extensive experiments are conducted to investigate the efficacy and properties of the proposed methods and show the superiority of our proposed approaches for achieving state - of - the - art performance with the WikiQA and TREC - QA datasets .,31,0,41
dataset/preprocessed/training-data/natural_language_inference/5,Researchers have investigated models based on neural networks for question - answering tasks .,33,0,14
dataset/preprocessed/training-data/natural_language_inference/5,"One study employs a Siamese architecture that utilizes an encoder ( e.g. , RNN or CNN ) to compute vector representations of the question and the answer .",34,0,28
dataset/preprocessed/training-data/natural_language_inference/5,The affinity score is calculated based on these vector representations .,35,0,11
dataset/preprocessed/training-data/natural_language_inference/5,"To improve the model performance by enabling the use of information from one sentence ( e.g. , a question or an answer ) in computing the representation of another sentence , researchers included the attention mechanism in their models .",36,0,40
dataset/preprocessed/training-data/natural_language_inference/5,1 : : The architecture of the model .,37,0,9
dataset/preprocessed/training-data/natural_language_inference/5,The dotted box on the right shows the process through which the latent - cluster information is computed and added to the answer .,38,0,24
dataset/preprocessed/training-data/natural_language_inference/5,This process is also performed in the question part but is omitted in the figure .,39,0,16
dataset/preprocessed/training-data/natural_language_inference/5,The latent memory is shared in both processes .,40,0,9
dataset/preprocessed/training-data/natural_language_inference/5,Another line of research includes the compare - aggregate framework .,41,0,11
dataset/preprocessed/training-data/natural_language_inference/5,"In this framework , first , vector representations of each sentence are computed .",42,0,14
dataset/preprocessed/training-data/natural_language_inference/5,"Second , these representations are compared .",43,0,7
dataset/preprocessed/training-data/natural_language_inference/5,"Last , the results are aggregated to calculate the matching score between the question and the answer .",44,0,18
dataset/preprocessed/training-data/natural_language_inference/5,"In this study , unlike the previous research , we employ a pretrained language model and a latent - cluster method to help the model understand the information in the question and the answer .",45,0,35
dataset/preprocessed/training-data/natural_language_inference/5,Comp - Clip Model,47,0,4
dataset/preprocessed/training-data/natural_language_inference/5,"In this paper , we are interested in estimating the matching score f ( y|Q , A ) , where y , Q = {q 1 , ... , q n } and A = {a 1 , ... , am } represent the label , the question and the answer , respectfully .",48,0,54
dataset/preprocessed/training-data/natural_language_inference/5,"We select the model from , which is referred to as the Comp - Clip model , as our baseline model .",49,0,22
dataset/preprocessed/training-data/natural_language_inference/27,Published as a conference paper at ICLR 2018 QANET :,2,0,10
dataset/preprocessed/training-data/natural_language_inference/27,COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF - ATTENTION FOR READING COMPRE - HENSION,3,1,13
dataset/preprocessed/training-data/natural_language_inference/27,Current end - to - end machine reading and question answering ( Q&A ) models are primarily based on recurrent neural networks ( RNNs ) with attention .,5,1,28
dataset/preprocessed/training-data/natural_language_inference/27,"Despite their success , these models are often slow for both training and inference due to the sequential nature of RNNs .",6,0,22
dataset/preprocessed/training-data/natural_language_inference/27,"We propose a new Q&A architecture called QANet , which does not require recurrent networks :",7,0,16
dataset/preprocessed/training-data/natural_language_inference/27,"Its encoder consists exclusively of convolution and self - attention , where convolution models local interactions and self - attention models global interactions .",8,0,24
dataset/preprocessed/training-data/natural_language_inference/27,"On the SQuAD dataset , our model is 3x to 13x faster in training and 4x to 9 x faster in inference , while achieving equivalent accuracy to recurrent models .",9,0,31
dataset/preprocessed/training-data/natural_language_inference/27,The speed - up gain allows us to train the model with much more data .,10,0,16
dataset/preprocessed/training-data/natural_language_inference/27,We hence combine our model with data generated by backtranslation from a neural machine translation model .,11,0,17
dataset/preprocessed/training-data/natural_language_inference/27,"On the SQuAD dataset , our single model , trained with augmented data , achieves 84.6 F1 score 1 on the test set , which is significantly better than the best published F 1 score of 81.8 .",12,0,38
dataset/preprocessed/training-data/natural_language_inference/27,There is growing interest in the tasks of machine reading comprehension and automated question answering .,14,1,16
dataset/preprocessed/training-data/natural_language_inference/27,"Over the past few years , significant progress has been made with end - to - end models showing promising results on many challenging datasets .",15,0,26
dataset/preprocessed/training-data/natural_language_inference/27,"The most successful models generally employ two key ingredients : ( 1 ) a recurrent model to process sequential inputs , and ( 2 ) an attention component to cope with long term interactions .",16,0,35
dataset/preprocessed/training-data/natural_language_inference/27,"A successful combination of these two ingredients is the Bidirectional Attention Flow ( BiDAF ) model by , which achieve strong results on the SQuAD dataset .",17,0,27
dataset/preprocessed/training-data/natural_language_inference/27,"A weakness of these models is that they are often slow for both training and inference due to their recurrent nature , especially for long texts .",18,0,27
dataset/preprocessed/training-data/natural_language_inference/27,The expensive training not only leads to high turnaround time for experimentation and limits researchers from rapid iteration but also prevents the models from being used for larger dataset .,19,0,30
dataset/preprocessed/training-data/natural_language_inference/27,Meanwhile the slow inference prevents the machine comprehension systems from being deployed in real - time applications .,20,0,18
dataset/preprocessed/training-data/natural_language_inference/27,"In this paper , aiming to make the machine comprehension fast , we propose to remove the recurrent nature of these models .",21,1,23
dataset/preprocessed/training-data/natural_language_inference/27,We instead exclusively use convolutions and self - attentions as the building blocks of encoders that separately encodes the query and context .,22,0,23
dataset/preprocessed/training-data/natural_language_inference/27,Then we learn the interactions between context and question by standard attentions .,23,0,13
dataset/preprocessed/training-data/natural_language_inference/27,The resulting representation is encoded again with our recurrency - free encoder before finally decoding to the probability of each position being the start or end of the answer span .,24,0,31
dataset/preprocessed/training-data/natural_language_inference/27,"We call this architecture QANet , which is shown in .",25,0,11
dataset/preprocessed/training-data/natural_language_inference/27,"The key motivation behind the design of our model is the following : convolution captures the local structure of the text , while the self - attention learns the global interaction between each pair of words .",26,0,37
dataset/preprocessed/training-data/natural_language_inference/27,"The additional context - query attention is a standard module to construct the query - aware context vector for each position in the context paragraph , which is used in the subsequent modeling layers .",27,0,35
dataset/preprocessed/training-data/natural_language_inference/27,The feed - forward nature of our architecture speeds up the model significantly .,28,0,14
dataset/preprocessed/training-data/natural_language_inference/27,"In our experiments on the SQuAD dataset , our model is 3x to 13x faster in training and 4x to 9 x faster in inference .",29,0,26
dataset/preprocessed/training-data/natural_language_inference/27,"As a simple comparison , our model can achieve the same accuracy ( 77.0 F1 score ) as BiDAF model within 3 hours training that otherwise should have taken 15 hours .",30,0,32
dataset/preprocessed/training-data/natural_language_inference/27,The speed - up gain also allows us to train the model with more iterations to achieve better results than competitive models .,31,0,23
dataset/preprocessed/training-data/natural_language_inference/27,"For instance , if we allow our model to train for 18 hours , it achieves an F1 score of 82.7 on the dev set , which is much better than , and is on par with best published results .",32,0,41
dataset/preprocessed/training-data/natural_language_inference/27,"As our model is fast , we can train it with much more data than other models .",33,0,18
dataset/preprocessed/training-data/natural_language_inference/27,"To further improve the model , we propose a complementary data augmentation technique to enhance the training data .",34,0,19
dataset/preprocessed/training-data/natural_language_inference/27,"This technique paraphrases the examples by translating the original sentences from English to another language and then back to English , which not only enhances the number of training instances but also diversifies the phrasing .",35,0,36
dataset/preprocessed/training-data/natural_language_inference/27,"On the SQuAD dataset , QANet trained with the augmented data achieves 84.6 F1 score on the test set , which is significantly better than the best published result of 81.8 by .",36,0,33
dataset/preprocessed/training-data/natural_language_inference/27,We also conduct ablation test to justify the usefulness of each component of our model .,37,0,16
dataset/preprocessed/training-data/natural_language_inference/27,"In summary , the contribution of this paper are as follows :",38,0,12
dataset/preprocessed/training-data/natural_language_inference/27,We propose an efficient reading comprehension model that exclusively built upon convolutions and self - attentions .,39,0,17
dataset/preprocessed/training-data/natural_language_inference/27,"To the best of our knowledge , we are the first to do so .",40,0,15
dataset/preprocessed/training-data/natural_language_inference/27,"This combination maintains good accuracy , while achieving up to 13x speedup in training and 9x per training iteration , compared to the RNN counterparts .",41,0,26
dataset/preprocessed/training-data/natural_language_inference/27,The speedup gain makes our model the most promising candidate for scaling up to larger datasets .,42,0,17
dataset/preprocessed/training-data/natural_language_inference/27,"To improve our result on SQuAD , we propose a novel data augmentation technique to enrich the training data by paraphrasing .",43,0,22
dataset/preprocessed/training-data/natural_language_inference/27,It allows the model to achieve higher accuracy that is better than the state - of - the - art .,44,0,21
dataset/preprocessed/training-data/natural_language_inference/27,"In this section , we first formulate the reading comprehension problem and then describe the proposed model QANet : it is a feedforward model that consists of only convolutions and self - attention , a combination that is empirically effective , and is also a novel contribution of our work .",46,0,51
dataset/preprocessed/training-data/natural_language_inference/27,"The reading comprehension task considered in this paper , is defined as follows .",48,0,14
dataset/preprocessed/training-data/natural_language_inference/27,"Given a context paragraph with n words C = {c 1 , c 2 , ... , c n } and the query sentence with m words Q = {q 1 , q 2 , ... , q m } , output a span S =",49,0,46
dataset/preprocessed/training-data/natural_language_inference/47,A large annotated corpus for learning natural language inference,2,1,9
dataset/preprocessed/training-data/natural_language_inference/47,"Understanding entailment and contradiction is fundamental to understanding natural language , and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations .",4,0,29
dataset/preprocessed/training-data/natural_language_inference/47,"However , machine learning research in this are a has been dramatically limited by the lack of large - scale resources .",5,0,22
dataset/preprocessed/training-data/natural_language_inference/47,"To address this , we introduce the Stanford Natural Language Inference corpus , a new , freely available collection of labeled sentence pairs , written by humans doing a novel grounded task based on image captioning .",6,0,37
dataset/preprocessed/training-data/natural_language_inference/47,"At 570 K pairs , it is two orders of magnitude larger than all other resources of its type .",7,0,20
dataset/preprocessed/training-data/natural_language_inference/47,"This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models , and it allows a neural network - based model to perform competitively on natural language inference benchmarks for the first time .",8,0,37
dataset/preprocessed/training-data/natural_language_inference/47,"The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning , from the lexicon to the content of entire texts .",10,0,27
dataset/preprocessed/training-data/natural_language_inference/47,"Thus , natural language inference ( NLI ) - characterizing and using these relations in computational systems ) - is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning .",11,1,34
dataset/preprocessed/training-data/natural_language_inference/47,"NLI has been addressed using a variety of techniques , including those based on symbolic logic , knowledge bases , and neural networks .",12,1,24
dataset/preprocessed/training-data/natural_language_inference/47,"In recent years , it has become an important testing ground for approaches employing distributed word and phrase representations .",13,0,20
dataset/preprocessed/training-data/natural_language_inference/47,"Distributed representations excel at capturing relations based in similarity , and have proven effective at modeling simple dimensions of meaning like evaluative sentiment ( e.g. , ) , but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI .",14,0,52
dataset/preprocessed/training-data/natural_language_inference/47,"In a SemEval 2014 task aimed at evaluating distributed representations for NLI , the best - performing systems relied heavily on additional features and reasoning capabilities .",15,0,27
dataset/preprocessed/training-data/natural_language_inference/47,"Our ultimate objective is to provide an empirical evaluation of learning - centered approaches to NLI , advancing the case for NLI as a tool for the evaluation of domain - general approaches to semantic representation .",16,0,37
dataset/preprocessed/training-data/natural_language_inference/47,"However , in our view , existing NLI corpora do not permit such an assessment .",17,0,16
dataset/preprocessed/training-data/natural_language_inference/47,"They are generally too small for training modern data-intensive , wide - coverage models , many contain sentences that were algorithmically generated , and they are often beset with indeterminacies of event and entity coreference that significantly impact annotation quality .",18,0,41
dataset/preprocessed/training-data/natural_language_inference/47,"To address this , this paper introduces the Stanford Natural Language Inference ( SNLI ) corpus , a collection of sentence pairs labeled for entailment , contradiction , and semantic independence .",19,0,32
dataset/preprocessed/training-data/natural_language_inference/47,"At 570,152 sentence pairs , SNLI is two orders of magnitude larger than all other resources of its type .",20,0,20
dataset/preprocessed/training-data/natural_language_inference/47,"And , in contrast to many such resources , all of its sentences and labels were written by humans in a grounded , naturalistic context .",21,0,26
dataset/preprocessed/training-data/natural_language_inference/47,"In a separate validation phase , we collected four additional judgments for each label for 56,941 of the examples .",22,0,20
dataset/preprocessed/training-data/natural_language_inference/47,"Of these , 98 % of cases emerge with a threeannotator consensus , and 58 % see a unanimous consensus from all five annotators .",23,0,25
dataset/preprocessed/training-data/natural_language_inference/47,"In this paper , we use this corpus to evaluate A man inspects the uniform of a figure in some East Asian country .",24,0,24
dataset/preprocessed/training-data/natural_language_inference/47,The man is sleeping,26,0,4
dataset/preprocessed/training-data/natural_language_inference/47,An older and younger man smiling .,27,0,7
dataset/preprocessed/training-data/natural_language_inference/47,neutral N NE N N Two men are smiling and laughing at the cats playing on the floor .,28,0,19
dataset/preprocessed/training-data/natural_language_inference/47,A black race car starts up in front of a crowd of people .,29,0,14
dataset/preprocessed/training-data/natural_language_inference/47,contradiction C C C C CA man is driving down a lonely road .,30,0,14
dataset/preprocessed/training-data/natural_language_inference/47,A soccer game with multiple males playing .,31,0,8
dataset/preprocessed/training-data/natural_language_inference/47,entailment E E E E E Some men are playing a sport .,32,0,13
dataset/preprocessed/training-data/natural_language_inference/47,A smiling costumed woman is holding an umbrella .,33,0,9
dataset/preprocessed/training-data/natural_language_inference/47,neutral N NE C N A happy woman in a fairy costume holds an umbrella ..,34,0,16
dataset/preprocessed/training-data/natural_language_inference/47,"We further evaluate the LSTM model by taking advantage of its ready support for transfer learning , and show that it can be adapted to an existing NLI challenge task , yielding the best reported performance by a neural network model and approaching the over all state of the art .",35,0,51
dataset/preprocessed/training-data/natural_language_inference/47,A new corpus for NLI,36,0,5
dataset/preprocessed/training-data/natural_language_inference/47,"To date , the primary sources of annotated NLI corpora have been the Recognizing Textual Entailment ( RTE ) challenge tasks .",37,0,22
dataset/preprocessed/training-data/natural_language_inference/47,"These are generally high - quality , hand - labeled data sets , and they have stimulated innovative logical and statistical models of natural language reasoning , but their small size ( fewer than a thousand examples each ) limits their utility as a testbed for learned distributed representations .",39,0,50
dataset/preprocessed/training-data/natural_language_inference/47,"The data for the SemEval 2014 task called Sentences Involving Compositional Knowledge ( SICK ) is a step up in terms of size , but only to 4,500 training examples , and its partly automatic construction introduced some spurious patterns into the data .",40,0,44
dataset/preprocessed/training-data/natural_language_inference/47,"The Denotation Graph entailment set contains millions of examples of entailments between sentences and artificially constructed short phrases , but it was labeled using fully automatic methods , and is noisy enough that it is probably suitable only as a source of sup - 1 http://aclweb.org/aclwiki/index.php?",41,0,46
dataset/preprocessed/training-data/natural_language_inference/47,title=Textual_Entailment_Resource_ Pool plementary training data .,42,0,6
dataset/preprocessed/training-data/natural_language_inference/47,"Outside the domain of sentence - level entailment , introduce a large corpus of semi-automatically annotated entailment examples between subject - verbobject relation triples , and the second release of the Paraphrase Data base includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases .",43,0,51
dataset/preprocessed/training-data/natural_language_inference/47,Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations : indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label .,44,0,32
dataset/preprocessed/training-data/natural_language_inference/47,"For an example of the pitfalls surrounding entity coreference , consider the sentence pair A boat sank in the Pacific Ocean and A boat sank in the Atlantic Ocean .",45,0,30
dataset/preprocessed/training-data/natural_language_inference/47,"The pair could be labeled as a contradiction if one assumes that the two sentences refer to the same single event , but could also be reasonably labeled as neutral if that assumption is not made .",46,0,37
dataset/preprocessed/training-data/natural_language_inference/47,"In order to ensure that our labeling scheme assigns a single correct label to every pair , we must select one of these approaches across the board , but both choices present problems .",47,0,34
dataset/preprocessed/training-data/natural_language_inference/47,"If we opt not to assume that events are coreferent , then we will only ever find contradictions between sentences that make broad universal assertions , but if we opt to assume coreference , new counterintuitive predictions emerge .",48,0,39
dataset/preprocessed/training-data/natural_language_inference/47,"For example , Ruth Bader Ginsburg was appointed to the US Supreme Court and I had a sandwich for lunch today would unintuitively be labeled as a contradiction , rather than neutral , under this assumption .",49,0,37
dataset/preprocessed/training-data/natural_language_inference/77,Making Neural QA as Simple as Possible but not Simpler,2,1,10
dataset/preprocessed/training-data/natural_language_inference/77,Recent development of large - scale question answering ( QA ) datasets triggered a substantial amount of research into end - toend neural architectures for QA .,4,1,27
dataset/preprocessed/training-data/natural_language_inference/77,Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity .,5,0,19
dataset/preprocessed/training-data/natural_language_inference/77,"In this work , we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task .",6,0,23
dataset/preprocessed/training-data/natural_language_inference/77,"We find that there are two ingredients necessary for building a high - performing neural QA system : first , the awareness of question words while processing the context and second , a composition function that goes beyond simple bag - of - words modeling , such as recurrent neural networks .",7,0,52
dataset/preprocessed/training-data/natural_language_inference/77,"Our results show that FastQA , a system that meets these two requirements , can achieve very competitive performance compared with existing models .",8,0,24
dataset/preprocessed/training-data/natural_language_inference/77,We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective .,9,0,21
dataset/preprocessed/training-data/natural_language_inference/77,Question answering is an important end - user task at the intersection of natural language processing ( NLP ) and information retrieval ( IR ) .,11,1,26
dataset/preprocessed/training-data/natural_language_inference/77,QA systems can bridge the gap between IR - based search engines and sophisticated intelligent assistants that enable a more directed information retrieval process .,12,0,25
dataset/preprocessed/training-data/natural_language_inference/77,Such systems aim at finding precisely the piece of information sought by the user instead of documents or snippets containing the answer .,13,0,23
dataset/preprocessed/training-data/natural_language_inference/77,"A special form of QA , namely extractive QA , deals with the extraction of a direct answer to a question from a given textual context .",14,0,27
dataset/preprocessed/training-data/natural_language_inference/77,"The creation of large - scale , extractive QA datasets sparked research interest into the development of end - to - end neural QA systems .",15,0,26
dataset/preprocessed/training-data/natural_language_inference/77,"A typical neural architecture consists of an embedding - , encoding - , interaction - and answer layer .",16,0,19
dataset/preprocessed/training-data/natural_language_inference/77,Most such systems describe several innovations for the different layers of the architecture with a special focus on developing powerful interaction layer that aims at modeling word - by - word interaction between question and context .,17,0,37
dataset/preprocessed/training-data/natural_language_inference/77,"Although a variety of extractive QA systems have been proposed , there is no competitive neural baseline .",18,0,18
dataset/preprocessed/training-data/natural_language_inference/77,Most systems were builtin what we calla top - down process that proposes a complex architecture and validates design decisions by an ablation study .,19,0,25
dataset/preprocessed/training-data/natural_language_inference/77,"Most ablation studies , however , remove only a single part of an over all complex architecture and therefore lack comparison to a reasonable neural baseline .",20,0,27
dataset/preprocessed/training-data/natural_language_inference/77,This gap raises the question whether the complexity of current systems is justified solely by their empirical results .,21,0,19
dataset/preprocessed/training-data/natural_language_inference/77,Another important observation is the fact that seemingly complex questions might be answerable by simple heuristics .,22,0,17
dataset/preprocessed/training-data/natural_language_inference/77,Let 's consider the following example :,23,0,7
dataset/preprocessed/training-data/natural_language_inference/77,"relation between the answer and the question , answering this question is easily possible by applying a simple context / type matching heuristic .",24,0,24
dataset/preprocessed/training-data/natural_language_inference/77,"The heuristic aims at selecting answer spans that a ) match the expected answer type ( a time as indicated by "" When "" ) and b ) are close to important question words .",25,0,35
dataset/preprocessed/training-data/natural_language_inference/77,"The actual answer "" 1688 - 1692 "" would easily be extracted by such a heuristic .",26,0,17
dataset/preprocessed/training-data/natural_language_inference/77,"In this work , we propose to use the aforementioned context / type matching heuristic as a guideline to derive simple neural baseline architectures for the extractive QA task .",27,0,30
dataset/preprocessed/training-data/natural_language_inference/77,"In particular , we develop a simple neural , bag - of - words ( BoW ) - and a recurrent neural network ( RNN ) baseline , namely FastQA .",28,0,31
dataset/preprocessed/training-data/natural_language_inference/77,"Crucially , both models do not make use of a complex interaction layer but model interaction between question and context only through computable features on the word level .",29,0,29
dataset/preprocessed/training-data/natural_language_inference/77,"FastQA 's strong performance questions the necessity of additional complexity , especially in the interaction layer , which is exhibited by recently developed models .",30,0,25
dataset/preprocessed/training-data/natural_language_inference/77,We address this question by evaluating the impact of extending FastQA with an additional interaction layer ( FastQAExt ) and find that it does n't lead to systematic improvements .,31,0,30
dataset/preprocessed/training-data/natural_language_inference/77,"Finally , our contributions are the following : i ) definition and evaluation of a BoW - and RNN - based neural QA baselines guided by a simple heuristic ; i i ) bottom - up evaluation of our FastQA system with increasing architectural complexity , revealing that the awareness of question words and the application of a RNN are enough to reach stateof - the - art results ; iii ) a complexity comparison between FastQA and more complex architectures as well as an in - depth discussion of usefulness of an interaction layer ; iv ) a qualitative analysis indicating that FastQA mostly follows our heuristic which thus constitutes a strong baseline for extractive QA .",32,0,118
dataset/preprocessed/training-data/natural_language_inference/77,A Bag - of - Words Neural QA System,33,0,9
dataset/preprocessed/training-data/natural_language_inference/77,"We begin by motivating our architectures by defining our proposed context / type matching heuristic : a) the type of the answer span should correspond to the expected answer type given by the question , and b) the correct answer should further be surrounded by a context that fits the question , or , more precisely , it should be surrounded by many question words .",34,0,66
dataset/preprocessed/training-data/natural_language_inference/77,"Similar heuristics were frequently implemented explicitly in traditional QA systems , e.g. , in the answer extraction step of , however , in this work our heuristic is merely used as a guideline for the construction of neural QA systems .",35,0,41
dataset/preprocessed/training-data/natural_language_inference/77,"In the following , we denote the hidden dimensionality of the model by n , the question tokens by Q = ( q 1 , ... , q L Q ) , and the context tokens by X = ( x 1 , ... , x L X ) .",36,0,50
dataset/preprocessed/training-data/natural_language_inference/77,The embedding layer is responsible for mapping tokens x to their corresponding n-dimensional representation x .,38,0,16
dataset/preprocessed/training-data/natural_language_inference/77,"Typically this is done by mapping each word x to its corresponding word embedding x w ( lookup - embedding ) using an embedding matrix E , s.t. x w = Ex. Another approach is to embed each word by encoding their corresponding character sequence x c = ( c 1 , ... , c L X ) with C , s.t. x c = C (x c ) ( char-embedding ) .",39,0,73
dataset/preprocessed/training-data/natural_language_inference/77,"In this work , we use a convolutional neural network for C of filter width 5 with max - pooling overtime as explored by , to which we refer the reader for additional details .",40,0,35
dataset/preprocessed/training-data/natural_language_inference/77,"Both approaches are combined via concatenation , s.t. the final embedding becomes",41,0,12
dataset/preprocessed/training-data/natural_language_inference/77,"For the BoW baseline , we extract the span in the question that refers to the expected , lexical answer type ( LAT ) by extracting either the question word ( s ) ( e.g. , who , when , why , how , how many , etc. ) or the first noun phrase of the question after the question words "" what "" or "" which "" ( e.g. , "" what year did ... "" ) .",43,0,79
dataset/preprocessed/training-data/natural_language_inference/77,This leads to a correct LAT for most questions .,45,0,10
dataset/preprocessed/training-data/natural_language_inference/77,We encode the LAT by concatenating the embedding of the first - and last word together with the average embedding of all words within the LAT .,46,0,27
dataset/preprocessed/training-data/natural_language_inference/77,The concatenated representations are further transformed by a fully - connected layer followed by a tanh non-linearity intoz ?,47,0,19
dataset/preprocessed/training-data/natural_language_inference/77,"Note that we refer to a fully - connected layer in the following by FC ,",49,0,16
dataset/preprocessed/training-data/natural_language_inference/64,TANDA : Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection,2,1,12
dataset/preprocessed/training-data/natural_language_inference/64,"We propose TANDA , an effective technique for fine - tuning pre-trained Transformer models for natural language tasks .",4,0,19
dataset/preprocessed/training-data/natural_language_inference/64,"Specifically , we first transfer a pre-trained model into a model for a general task by fine - tuning it with a large and highquality dataset .",5,0,27
dataset/preprocessed/training-data/natural_language_inference/64,We then perform a second fine - tuning step to adapt the transferred model to the target domain .,6,0,19
dataset/preprocessed/training-data/natural_language_inference/64,"We demonstrate the benefits of our approach for answer sentence selection , which is a well - known inference task in Question Answering .",7,1,24
dataset/preprocessed/training-data/natural_language_inference/64,"We built a large scale dataset to enable the transfer step , exploiting the Natural Questions dataset .",8,0,18
dataset/preprocessed/training-data/natural_language_inference/64,"Our approach establishes the state of the art on two well - known benchmarks , WikiQA and TREC - QA , achieving MAP scores of 92 % and 94.3 % , respectively , which largely outperform the previous highest scores of 83.4 % and 87.5 % , obtained in very recent work .",9,0,53
dataset/preprocessed/training-data/natural_language_inference/64,We empirically show that TANDA generates more stable and robust models reducing the effort required for selecting optimal hyper - parameters .,10,0,22
dataset/preprocessed/training-data/natural_language_inference/64,"Additionally , we show that the transfer step of TANDA makes the adaptation step more robust to noise .",11,0,19
dataset/preprocessed/training-data/natural_language_inference/64,This enables a more effective use of noisy datasets for fine - tuning .,12,0,14
dataset/preprocessed/training-data/natural_language_inference/64,"Finally , we also confirm the positive impact of TANDA in an industrial setting , using domain specific datasets subject to different types of noise .",13,0,26
dataset/preprocessed/training-data/natural_language_inference/64,"In recent years , virtual assistants have become a central asset for technological companies .",15,0,15
dataset/preprocessed/training-data/natural_language_inference/64,"This has increased the interest of AI researchers in studying and developing conversational agents , some popular examples being Google Home , Siri and Alexa .",16,0,26
dataset/preprocessed/training-data/natural_language_inference/64,"This has renewed the research interest in Question Answering ( QA ) and , in particular , in two main tasks :",17,1,22
dataset/preprocessed/training-data/natural_language_inference/64,"( i ) answer sentence selection ( AS2 ) , which , given a question and a set of answer sentence candidates , consists in selecting sentences ( e.g. , retrieved by a search engine ) correctly answering the question ; and ( ii ) machine reading ( MR ) or reading comprehension , which , given a question and a reference text , consists in finding a text span answering it .",18,1,73
dataset/preprocessed/training-data/natural_language_inference/64,"Even though the latter is gaining more and more popularity , AS2 is more relevant to a production scenario since , a combination of a search engine and an AS2 model already implements an initial QA system .",19,1,38
dataset/preprocessed/training-data/natural_language_inference/64,Work done while the author was an intern at Amazon Alexa .,21,0,12
dataset/preprocessed/training-data/natural_language_inference/64,The AS2 task was originally defined in the TREC competition .,22,0,11
dataset/preprocessed/training-data/natural_language_inference/64,"With the advent of neural models , it has had significant contributions from techniques such as .",23,0,17
dataset/preprocessed/training-data/natural_language_inference/64,"Recently , approaches for training neural language models , e.g. , ELMO , GPT , BERT , Ro BERTa , XL - Net ) have led to major advancements in several NLP subfields .",24,0,34
dataset/preprocessed/training-data/natural_language_inference/64,These methods capture dependencies between words and their compounds by pre-training neural networks on large amounts of data .,25,0,19
dataset/preprocessed/training-data/natural_language_inference/64,"Interestingly , the resulting models can be easily applied to solve different NLP applications by just fine - tuning them on the training data of the target tasks .",26,0,29
dataset/preprocessed/training-data/natural_language_inference/64,"For example , the Transformer can be pre-trained on a large amount of data obtaining a powerful language model , which can then be specialized for solving specific NLP tasks by just adding new layers and training them on the target data .",27,0,43
dataset/preprocessed/training-data/natural_language_inference/64,"Although the approach is simple , the procedure for fine - tuning a Transformer - based model is not completely understood , and can result in high accuracy variance , with a possible on - off behavior , e.g. , models may always predict a single label .",28,0,48
dataset/preprocessed/training-data/natural_language_inference/64,"As a consequence , researchers need to invest a considerable effort in selecting suitable parameters , with no theory or a well - assessed best practice helping them .",29,0,29
dataset/preprocessed/training-data/natural_language_inference/64,"Such problem also affects QA , and , in particular , AS2 since no large and and accurate dataset has been developed for it .",30,0,25
dataset/preprocessed/training-data/natural_language_inference/64,"In this paper , we study the use of Transformer - based models for AS2 and provide effective solutions to tackle the data scarceness problem for AS2 and the instability of the finetuning step .",31,0,35
dataset/preprocessed/training-data/natural_language_inference/64,"In detail , the contributions of our papers are :",32,0,10
dataset/preprocessed/training-data/natural_language_inference/64,"We improve stability of Transformer models by adding an intermediate fine - tuning step , which aims at specializing them to the target task ( AS2 ) , i.e. , this step transfers a pretrained language model to a model for the target task .",33,0,45
dataset/preprocessed/training-data/natural_language_inference/64,"We show that the transferred model can be effectively adapted to the target domain with a subsequent finetuning step , even when using target data of small size .",34,0,29
dataset/preprocessed/training-data/natural_language_inference/64,"Our Transfer and Adapt ( TANDA ) approach makes finetuning : ( i ) easier and more stable , without the need of cherry picking parameters ; and ( ii ) robust to noise , i.e. , noisy data from the target domain can be utilized to train an accurate model .",35,0,52
dataset/preprocessed/training-data/natural_language_inference/64,"We built ASNQ , a dataset for AS2 , by transforming the recently released Natural Questions ( NQ ) corpus ) from MR to AS2 task .",36,0,27
dataset/preprocessed/training-data/natural_language_inference/64,This was essential as our transfer step requires a large and accurate dataset .,37,0,14
dataset/preprocessed/training-data/natural_language_inference/64,ASNQ is an important contribution of our work to the research community .,38,0,13
dataset/preprocessed/training-data/natural_language_inference/64,"Finally , the generality of our approach and empirical investigation suggest that our TANDA findings also apply to other NLP tasks , especially , textual inference , although empirical analysis is essential to confirm these claims .",40,0,37
dataset/preprocessed/training-data/natural_language_inference/64,"We evaluate TANDA on well - known academic benchmarks , i.e. , TREC - QA and WikiQA , as well as three different industrial datasets , where questions are derived from Alexa Traffic and candidate sentences are selected from web data .",41,0,42
dataset/preprocessed/training-data/natural_language_inference/64,The results show that :,42,0,5
dataset/preprocessed/training-data/natural_language_inference/64,TANDA improves the stability of Transformer models .,43,0,8
dataset/preprocessed/training-data/natural_language_inference/64,"In the adapt step , the model accuracy throughout different epochs show a smooth and convex behavior , which is ideal for estimating optimal parameters .",44,0,26
dataset/preprocessed/training-data/natural_language_inference/64,We improved the state of the art in AS2 by just applying BERT and RoBERTa to AS2 and we further improved it by almost 10 absolute percent points in MAP scores with TANDA .,45,0,34
dataset/preprocessed/training-data/natural_language_inference/64,"TANDA achieves much higher accuracy than traditional fine - tuning , especially in case of noise data .",46,0,18
dataset/preprocessed/training-data/natural_language_inference/64,"For example , the drop in performance is up to one order of magnitude lower with TANDA , i.e. , 2.5 % , when we inject 20 % of noise in the WikiQA and TREC - QA datasets .",47,0,39
dataset/preprocessed/training-data/natural_language_inference/64,Our experiments with real - world datasets built from Alexa traffic data confirm all our above findings .,48,0,18
dataset/preprocessed/training-data/natural_language_inference/64,"Specifically , we observe the same robustness to noise , which , in this case , is generated by real sources .",49,0,22
dataset/preprocessed/training-data/natural_language_inference/18,Neural Variational Inference for Text Processing Phil Blunsom 12,2,1,9
dataset/preprocessed/training-data/natural_language_inference/18,Recent advances in neural variational inference have spawned a renaissance in deep latent variable models .,4,0,16
dataset/preprocessed/training-data/natural_language_inference/18,In this paper we introduce a generic variational inference framework for generative and conditional models of text .,5,0,18
dataset/preprocessed/training-data/natural_language_inference/18,"While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables , here we construct an inference network conditioned on the discrete text input to provide the variational distribution .",6,0,34
dataset/preprocessed/training-data/natural_language_inference/18,"We validate this framework on two very different text modelling applications , generative document modelling and supervised question answering .",7,0,20
dataset/preprocessed/training-data/natural_language_inference/18,Our neural variational document model combines a continuous stochastic document representation with a bagof - words generative model and achieves the lowest reported perplexities on two standard test corpora .,8,0,30
dataset/preprocessed/training-data/natural_language_inference/18,The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair .,9,0,25
dataset/preprocessed/training-data/natural_language_inference/18,On two question answering benchmarks this model exceeds all previous published benchmarks .,10,0,13
dataset/preprocessed/training-data/natural_language_inference/18,Probabilistic generative models underpin many successful applications within the field of natural language processing ( NLP ) .,12,0,18
dataset/preprocessed/training-data/natural_language_inference/18,"Their popularity stems from their ability to use unlabelled data effectively , to incorporate abundant linguistic features , and to learn interpretable dependencies among data .",13,0,26
dataset/preprocessed/training-data/natural_language_inference/18,"However these successes are tempered by the fact that as the structure of such generative models becomes deeper and more complex , true Bayesian inference becomes intractable due to the high dimensional integrals required .",14,0,35
dataset/preprocessed/training-data/natural_language_inference/18,Markov chain Monte Carlo ( MCMC ),15,0,7
dataset/preprocessed/training-data/natural_language_inference/18,"Proceedings of the 33 rd International Conference on Machine Learning , New York , NY , USA , 2016 .",16,0,20
dataset/preprocessed/training-data/natural_language_inference/18,JMLR : W&CP volume,17,0,4
dataset/preprocessed/training-data/natural_language_inference/18,48 . Copyright 2016 by the author ( s ) .,18,0,11
dataset/preprocessed/training-data/natural_language_inference/18,and variational inference are the standard approaches for approximating these integrals .,19,0,12
dataset/preprocessed/training-data/natural_language_inference/18,"However the computational cost of the former results in impractical training for the large and deep neural networks which are now fashionable , and the latter is conventionally confined due to the underestimation of posterior variance .",20,0,37
dataset/preprocessed/training-data/natural_language_inference/18,"The lack of effective and efficient inference methods hinders our ability to create highly expressive models of text , especially in the situation where the model is non-conjugate .",21,0,29
dataset/preprocessed/training-data/natural_language_inference/18,"This paper introduces a neural variational framework for generative models of text , inspired by the variational autoencoder .",22,0,19
dataset/preprocessed/training-data/natural_language_inference/18,"The principle idea is to build an inference network , implemented by a deep neural network conditioned on text , to approximate the intractable distributions over the latent variables .",23,0,30
dataset/preprocessed/training-data/natural_language_inference/18,"Instead of providing an analytic approximation , as in traditional variational Bayes , neural variational inference learns to model the posterior probability , thus endowing the model with strong generalis ation abilities .",24,0,33
dataset/preprocessed/training-data/natural_language_inference/18,"Due to the flexibility of deep neural networks , the inference network is capable of learning complicated non-linear distributions and processing structured inputs such as word sequences .",25,0,28
dataset/preprocessed/training-data/natural_language_inference/18,"Inference networks can be designed as , but not restricted to , multilayer perceptrons ( MLP ) , convolutional neural networks ( CNN ) , and recurrent neural networks ( RNN ) , approaches which are rarely used in conventional generative models .",26,0,43
dataset/preprocessed/training-data/natural_language_inference/18,"By using the reparameteris ation method , the inference network is trained through back - propagating unbiased and low variance gradients w.r.t. the latent variables .",27,0,26
dataset/preprocessed/training-data/natural_language_inference/18,"Within this framework , we propose a Neural Variational Document Model ( NVDM ) for document modelling and a Neural Answer Selection Model ( NASM ) for question answering , a task that selects the sentences that correctly answer a factoid question from a set of candidate sentences .",28,0,49
dataset/preprocessed/training-data/natural_language_inference/18,The NVDM is an unsupervised generative model of text which aims to extract a continuous semantic latent variable for each document .,29,0,22
dataset/preprocessed/training-data/natural_language_inference/18,"This model can be interpreted as a variational auto - encoder : an MLP encoder ( inference network ) compresses the bag - of - words document representation into a continuous latent distribution , and a softmax decoder ( generative model ) reconstructs the document by generating the words independently .",30,0,51
dataset/preprocessed/training-data/natural_language_inference/18,A primary feature of NVDM is that each word is generated directly from a dense continuous document representation instead of the more common binary semantic vector .,31,0,27
dataset/preprocessed/training-data/natural_language_inference/18,Our experiments demonstrate that our neural document model achieves the stateof - the - art perplexities on the 20 New s Groups and RCV1 - v 2 .,32,0,28
dataset/preprocessed/training-data/natural_language_inference/18,The NASM ( is a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,33,0,31
dataset/preprocessed/training-data/natural_language_inference/18,The attention model is designed to focus on the phrases of an answer that are strongly connected to the question semantics and is modelled by a latent distribution .,34,0,29
dataset/preprocessed/training-data/natural_language_inference/18,"This mechanism allows the model to deal with the ambiguity inherent in the task and learns pair- specific representations thatare more effective at predicting answer matches , rather than independent embeddings of question and answer sentences .",35,0,37
dataset/preprocessed/training-data/natural_language_inference/18,"Bayesian inference provides a natural safeguard against overfitting , especially as the training sets available for this task are small .",36,0,21
dataset/preprocessed/training-data/natural_language_inference/18,"The experiments show that the LSTM with a latent stochastic attention mechanism learns an effective attention model and outperforms both previously published results , and our own strong nonstochastic attention baselines .",37,0,32
dataset/preprocessed/training-data/natural_language_inference/18,"In summary , we demonstrate the effectiveness of neural variational inference for text processing on two diverse tasks .",38,0,19
dataset/preprocessed/training-data/natural_language_inference/18,"These models are simple , expressive and can be trained efficiently with the highly scalable stochastic gradient back - propagation .",39,0,21
dataset/preprocessed/training-data/natural_language_inference/18,"Our neural variational framework is suitable for both unsupervised and supervised learning tasks , and can be generalised to incorporate any type of neural networks .",40,0,26
dataset/preprocessed/training-data/natural_language_inference/18,Neural Variational Inference Framework,41,0,4
dataset/preprocessed/training-data/natural_language_inference/18,"Latent variable modelling is popular in many NLP problems , but it is non-trivial to carry out effective and efficient inference for models with complex and deep structure .",42,0,29
dataset/preprocessed/training-data/natural_language_inference/18,In this section we introduce a generic neural variational inference framework that we apply to both the unsupervised NVDM and supervised NASM in the follow sections .,43,0,27
dataset/preprocessed/training-data/natural_language_inference/18,"We define a generative model with a latent variable h , which can be considered as the stochastic units in deep neural networks .",44,0,24
dataset/preprocessed/training-data/natural_language_inference/18,We designate the observed parent and child nodes of h as x and y respectively .,45,0,16
dataset/preprocessed/training-data/natural_language_inference/18,"Hence , the joint distribution of the generative model is p ? ( x , y ) = hp ? ( y|h ) p ? ( h|x ) p ( x ) , and the variational lower bound L is derived as :",46,0,43
dataset/preprocessed/training-data/natural_language_inference/18,parameterises the generative distributions p ? ( y|h ) and p ? ( h|x ) .,48,0,16
dataset/preprocessed/training-data/natural_language_inference/18,"In order to have a tight lower bound , the variational distribution q ( h ) should approach the true posterior p ( h|x , y ) .",49,0,28
dataset/preprocessed/training-data/natural_language_inference/15,Semantic Sentence Matching with Densely - connected Recurrent and Co - attentive Information,2,1,13
dataset/preprocessed/training-data/natural_language_inference/15,"Sentence matching is widely used in various natural language tasks such as natural language inference , paraphrase identification , and question answering .",4,1,23
dataset/preprocessed/training-data/natural_language_inference/15,"For these tasks , understanding logical and semantic relationship between two sentences is required but it is yet challenging .",5,0,20
dataset/preprocessed/training-data/natural_language_inference/15,"Although attention mechanism is useful to capture the semantic relationship and to properly align the elements of two sentences , previous methods of attention mechanism simply use a summation operation which does not retain original features enough .",6,0,38
dataset/preprocessed/training-data/natural_language_inference/15,"Inspired by DenseNet , a densely connected convolutional network , we propose a densely - connected co-attentive recurrent neural network , each layer of which uses concatenated information of attentive features as well as hidden features of all the preceding recurrent layers .",7,0,43
dataset/preprocessed/training-data/natural_language_inference/15,It enables preserving the original and the co-attentive feature information from the bottommost word embedding layer to the uppermost recurrent layer .,8,0,22
dataset/preprocessed/training-data/natural_language_inference/15,"To alleviate the problem of an ever - increasing size of feature vectors due to dense concatenation operations , we also propose to use an autoencoder after dense concatenation .",9,0,30
dataset/preprocessed/training-data/natural_language_inference/15,We evaluate our proposed architecture on highly competitive benchmark datasets related to sentence matching .,10,0,15
dataset/preprocessed/training-data/natural_language_inference/15,"Experimental results show that our architecture , which retains recurrent and attentive features , achieves state - of - the - art performances for most of the tasks .",11,0,29
dataset/preprocessed/training-data/natural_language_inference/15,"Semantic sentence matching , a fundamental technology in natural language processing , requires lexical and compositional semantics .",13,0,18
dataset/preprocessed/training-data/natural_language_inference/15,"In paraphrase identification , sentence matching is utilized to identify whether two sentences have identical meaning or not .",14,0,19
dataset/preprocessed/training-data/natural_language_inference/15,"In natural language inference also known as recognizing textual entailment , it determines whether a hypothesis sentence can reasonably be inferred from a given premise sentence .",15,0,27
dataset/preprocessed/training-data/natural_language_inference/15,"In question answering , sentence matching is required to determine the degree of matching 1 ) between a query and a question for question retrieval , and 2 ) between a question and an answer for answer selection .",16,0,39
dataset/preprocessed/training-data/natural_language_inference/15,However identifying logical and semantic relationship between two sentences is not trivial due to the problem of the semantic gap .,17,0,21
dataset/preprocessed/training-data/natural_language_inference/15,Recent advances of deep neural network enable to learn textual semantics for sentence matching .,18,0,15
dataset/preprocessed/training-data/natural_language_inference/15,"Large amount of annotated data such as , SNLI ( Bowman Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",19,0,26
dataset/preprocessed/training-data/natural_language_inference/15,"All rights reserved. , and MultiNLI have contributed significantly to learning semantics as well .",20,0,15
dataset/preprocessed/training-data/natural_language_inference/15,"In the conventional methods , a matching model can be trained in two different ways .",21,0,16
dataset/preprocessed/training-data/natural_language_inference/15,The first methods are sentence - encoding - based ones where each sentence is encoded to a fixed - sized vector in a complete isolated manner and the two vectors for the corresponding sentences are used in predicting the degree of matching .,22,0,43
dataset/preprocessed/training-data/natural_language_inference/15,The others are joint methods that allow to utilize interactive features like attentive information between the sentences .,23,0,18
dataset/preprocessed/training-data/natural_language_inference/15,"In the former paradigm , because two sentences have no interaction , they can not utilize interactive information during the encoding procedure .",24,0,23
dataset/preprocessed/training-data/natural_language_inference/15,"In our work , we adopted a joint method which enables capturing interactive information for performance improvements .",25,0,18
dataset/preprocessed/training-data/natural_language_inference/15,"Furthermore , we employ a substantially deeper recurrent network for sentence matching like deep neural machine translator ( NMT ) .",26,0,21
dataset/preprocessed/training-data/natural_language_inference/15,Deep recurrent models are more advantageous for learning long sequences and outperform the shallower architectures .,27,0,16
dataset/preprocessed/training-data/natural_language_inference/15,"However , the attention mechanism is unstable in deeper models with the well - known vanishing gradient problem .",28,0,19
dataset/preprocessed/training-data/natural_language_inference/15,"Though ) uses residual connection between recurrent layers to allow better information and gradient flow , there are some limitations .",29,0,21
dataset/preprocessed/training-data/natural_language_inference/15,The recurrent hidden or attentive features are not preserved intact through residual connection because the summation operation may impede the information flow in deep networks .,30,0,26
dataset/preprocessed/training-data/natural_language_inference/15,"Inspired by Densenet ) , we propose a densely - connected recurrent network where the recurrent hidden features are retained to the uppermost layer .",31,0,25
dataset/preprocessed/training-data/natural_language_inference/15,"In addition , instead of the conventional summation operation , the concatenation operation is used in combination with the attention mechanism to preserve co-attentive information better .",32,0,27
dataset/preprocessed/training-data/natural_language_inference/15,The proposed architecture shown in is called DRCN which is an abbreviation for Densely - connected Recurrent and Co -attentive neural Network .,33,0,23
dataset/preprocessed/training-data/natural_language_inference/15,The proposed DRCN can utilize the increased representational power of deeper recurrent networks and attentive information .,34,0,17
dataset/preprocessed/training-data/natural_language_inference/15,"Furthermore , to alleviate the problem of an ever- increasing feature vector size due to concatenation operations , we adopted an autoencoder and forwarded a fixed length vector to the higher layer recurrent module as shown in the figure .",35,0,40
dataset/preprocessed/training-data/natural_language_inference/15,"DRCN is , to our best knowledge , the first generalized version of DenseRNN which is expandable to deeper layers with the property of Dashed arrows indicate that a group of RNN - layer , concatenation and AE can be repeated multiple ( N ) times ( like a repeat mark in a music score ) .",36,0,57
dataset/preprocessed/training-data/natural_language_inference/15,"The bottleneck component denoted as AE , inserted to prevent the ever - growing size of a feature vector , is optional for each repetition .",37,0,26
dataset/preprocessed/training-data/natural_language_inference/15,The upper right diagram is our specific architecture for experiments with 5 RNN layers ( N = 4 ) .,38,0,20
dataset/preprocessed/training-data/natural_language_inference/15,controllable feature sizes by the use of an autoencoder .,39,0,10
dataset/preprocessed/training-data/natural_language_inference/15,"We evaluate our model on three sentence matching tasks : natural language inference , paraphrase identification and answer sentence selection .",40,0,21
dataset/preprocessed/training-data/natural_language_inference/15,"Experimental results on five highly competitive benchmark datasets ( SNLI , MultiNLI , QUORA , TrecQA and SelQA ) show that our model significantly outperforms the current state - of - the - art results on most of the tasks .",41,0,41
dataset/preprocessed/training-data/natural_language_inference/15,"Earlier approaches of sentence matching mainly relied on conventional methods such as syntactic features , transformations or relation extraction .",43,0,20
dataset/preprocessed/training-data/natural_language_inference/15,These are restrictive in that they work only on very specific tasks .,44,0,13
dataset/preprocessed/training-data/natural_language_inference/15,The developments of large - scale annotated datasets and deep learning algorithms have led a big progress on matching natural language sentences .,45,0,23
dataset/preprocessed/training-data/natural_language_inference/15,"Furthermore , the wellestablished attention mechanisms endowed richer information for sentence matching by providing alignment and dependency relationship between two sentences .",46,0,22
dataset/preprocessed/training-data/natural_language_inference/15,The release of the large - scale datasets also has encouraged the developments of the learning - centered approaches to semantic representation .,47,0,23
dataset/preprocessed/training-data/natural_language_inference/15,The first type of these approaches is sentence - encoding - based methods where sentences are encoded into their own sentence representation without any cross-interaction .,48,0,26
dataset/preprocessed/training-data/natural_language_inference/15,"Then , a classifier such as a neural network is applied to decide the relationship based on these independent sentence representations .",49,0,22
dataset/preprocessed/training-data/natural_language_inference/57,ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,2,1,7
dataset/preprocessed/training-data/natural_language_inference/57,"Hypernymy , textual entailment , and image captioning can be seen as special cases of a single visual - semantic hierarchy over words , sentences , and images .",4,1,29
dataset/preprocessed/training-data/natural_language_inference/57,In this paper we advocate for explicitly modeling the partial order structure of this hierarchy .,5,0,16
dataset/preprocessed/training-data/natural_language_inference/57,"Towards this goal , we introduce a general method for learning ordered representations , and show how it can be applied to a variety of tasks involving images and language .",6,0,31
dataset/preprocessed/training-data/natural_language_inference/57,We show that the resulting representations improve performance over current approaches for hypernym prediction and image - caption retrieval .,7,0,20
dataset/preprocessed/training-data/natural_language_inference/57,Computer vision and natural language processing are becoming increasingly intertwined .,9,0,11
dataset/preprocessed/training-data/natural_language_inference/57,"Recent work in vision has moved beyond discriminating between a fixed set of object classes , to automatically generating open - ended lingual descriptions of images .",10,0,27
dataset/preprocessed/training-data/natural_language_inference/57,Recent methods for natural language processing such as learn the semantics of language by grounding it in the visual world .,11,0,21
dataset/preprocessed/training-data/natural_language_inference/57,"Looking to the future , autonomous artificial agents will need to jointly model vision and language in order to parse the visual world and communicate with people .",12,0,28
dataset/preprocessed/training-data/natural_language_inference/57,"But what , precisely , is the relationship between images and the words or captions we use to describe them ?",13,0,21
dataset/preprocessed/training-data/natural_language_inference/57,"It is akin to the hypernym relation between words , and textual entailment among phrases : captions are simply abstractions of images .",14,0,23
dataset/preprocessed/training-data/natural_language_inference/57,"In fact , all three relations can be seen as special cases of a partial order over images and language , illustrated in , which we refer to as the visualsemantic hierarchy .",15,1,33
dataset/preprocessed/training-data/natural_language_inference/57,"As a partial order , this relation is transitive : "" woman walking her dog "" , "" woman walking "" , "" person walking "" , "" person "" , and "" entity "" are all valid abstractions of the rightmost image .",16,0,44
dataset/preprocessed/training-data/natural_language_inference/57,Our goal in this work is to learn representations that respect this partial order structure .,17,0,16
dataset/preprocessed/training-data/natural_language_inference/57,"Most recent approaches to modeling the hypernym , entailment , and image - caption relations involve learning distributed representations or embeddings .",18,0,22
dataset/preprocessed/training-data/natural_language_inference/57,"This is a very powerful and general approach which maps the objects of interest - words , phrases , images - to points in a high - dimensional vector space .",19,0,31
dataset/preprocessed/training-data/natural_language_inference/57,"One line of work , exemplified by and first applied to the caption - image relationship by , requires the mapping to be distance - preserving : semantically 1 ar Xiv : 1511.06361v6LG ] 1 Mar 2016",20,0,37
dataset/preprocessed/training-data/natural_language_inference/57,Published as a conference paper at ICLR 2016 similar objects are mapped to points thatare nearby in the embedding space .,21,0,21
dataset/preprocessed/training-data/natural_language_inference/57,A symmetric distance measure such as Euclidean or cosine distance is typically used .,22,0,14
dataset/preprocessed/training-data/natural_language_inference/57,"Since the visual - semantic hierarchy is an antisymmetric relation , we expect this approach to introduce systematic model error .",23,0,21
dataset/preprocessed/training-data/natural_language_inference/57,"Other approaches do not have such explicit constraints , learning a more - or - less general binary relation between the objects of interest , e.g. ; ; .",24,0,29
dataset/preprocessed/training-data/natural_language_inference/57,"Notably , no existing approach directly imposes the transitivity and antisymmetry of the partial order , leaving the model to induce these properties from data .",25,0,26
dataset/preprocessed/training-data/natural_language_inference/57,"In contrast , we propose to exploit the partial order structure of the visual - semantic hierarchy by learning a mapping which is not distance - preserving but order - preserving between the visualsemantic hierarchy and a partial order over the embedding space .",26,0,44
dataset/preprocessed/training-data/natural_language_inference/57,We call embeddings learned in this way order- embeddings .,27,0,10
dataset/preprocessed/training-data/natural_language_inference/57,This idea can be integrated into existing relational learning methods simply by replacing their comparison operation with ours .,28,0,19
dataset/preprocessed/training-data/natural_language_inference/57,"By modifying existing methods in this way , we find that order - embeddings provide a marked improvement over the state - of - art for hypernymy prediction and caption - image retrieval , and near state - of - the - art performance for natural language inference .",29,0,49
dataset/preprocessed/training-data/natural_language_inference/57,This paper is structured as follows .,30,0,7
dataset/preprocessed/training-data/natural_language_inference/57,"We begin , in Section 2 , by giving a unified mathematical treatment of our tasks , and describing the general approach of learning order- embeddings .",31,0,27
dataset/preprocessed/training-data/natural_language_inference/57,"In the next three sections we describe in detail the tasks we tackle , how we apply the order - embeddings idea to each of them , and the results we obtain .",32,0,33
dataset/preprocessed/training-data/natural_language_inference/57,"The tasks are hypernym prediction ( Section 3 ) , captionimage retrieval ( Section 4 ) , and textual entailment ( Section 5 ) .",33,0,25
dataset/preprocessed/training-data/natural_language_inference/57,"In the supplementary material , we visualize novel vector regularities that emerge in our learned embeddings of images and language .",34,0,21
dataset/preprocessed/training-data/natural_language_inference/57,LEARNING ORDER - EMBEDDINGS,35,0,4
dataset/preprocessed/training-data/natural_language_inference/57,"To unify our treatment of various tasks , we introduce the problem of partial order completion .",36,0,17
dataset/preprocessed/training-data/natural_language_inference/57,"In partial order completion , we are given a set of positive examples P = { ( u , v ) } of ordered pairs drawn from a partially ordered set ( X , X ) , and a set of negative examples N which we know to be unordered .",37,0,51
dataset/preprocessed/training-data/natural_language_inference/57,"Our goal is to predict whether an unseen pair ( u , v ) is ordered .",38,0,17
dataset/preprocessed/training-data/natural_language_inference/57,"Note that hypernym prediction , caption - image retrieval , and textual entailment are all special cases of this task , since they all involve classifying pairs of concepts in the ( partially ordered ) visual - semantic hierarchy .",39,0,40
dataset/preprocessed/training-data/natural_language_inference/57,"We tackle this problem by learning a mapping from X into a partially ordered embedding space ( Y , Y ) .",40,0,22
dataset/preprocessed/training-data/natural_language_inference/57,The idea is to predict the ordering of an unseen pair in X based on its ordering in the embedding space .,41,0,22
dataset/preprocessed/training-data/natural_language_inference/57,This is possible only if the mapping satisfies the following crucial property :,42,0,13
dataset/preprocessed/training-data/natural_language_inference/57,"This definition implies that each combination of embedding space Y , order Y , and orderembedding f determines a unique completion of our data as a partial order X .",43,0,30
dataset/preprocessed/training-data/natural_language_inference/57,"In the following , we first consider the choice of Y and Y , and then discuss how to find an appropriate f .",44,0,24
dataset/preprocessed/training-data/natural_language_inference/57,THE REVERSED PRODUCT ORDER,45,0,4
dataset/preprocessed/training-data/natural_language_inference/57,The choice of Y and Y is somewhat application - dependent .,46,0,12
dataset/preprocessed/training-data/natural_language_inference/57,"For the purpose of modeling the semantic hierarchy , our choices are narrowed by the following considerations .",47,0,18
dataset/preprocessed/training-data/natural_language_inference/57,Much of the expressive power of human language comes from abstraction and composition .,48,0,14
dataset/preprocessed/training-data/natural_language_inference/57,"For any two concepts , say "" dog "" and "" cat "" , we can name a concept that is an abstraction of the two , such as "" mammal "" , as well as a concept that composes the two , such as "" dog chasing cat "" .",49,0,51
dataset/preprocessed/training-data/natural_language_inference/61,A ention Boosted Sequential Inference Model,2,0,6
dataset/preprocessed/training-data/natural_language_inference/61,A ention mechanism has been proven e ective on natural language processing .,4,0,13
dataset/preprocessed/training-data/natural_language_inference/61,"is paper proposes an a ention boosted natural language inference model named a ESIM by adding word a ention and adaptive direction - oriented a ention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM . is makes the inference model a ESIM has the ability toe ectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .",5,1,73
dataset/preprocessed/training-data/natural_language_inference/61,"e empirical studies on the SNLI , MultiNLI and ora benchmarks manifest that a ESIM is superior to the original ESIM model .",6,0,23
dataset/preprocessed/training-data/natural_language_inference/61,"natural language processing , deep learning , natural language inference , Bi - LSTM",7,0,14
dataset/preprocessed/training-data/natural_language_inference/61,Natural language inference ( NLI ) is an important and signicant task in natural language processing ( NLP ) .,9,1,20
dataset/preprocessed/training-data/natural_language_inference/61,"It concerns whether a hypothesis can be inferred from a premise , requiring understanding of the semantic similarity between the hypothesis and the premise to discriminate their relation .",10,0,29
dataset/preprocessed/training-data/natural_language_inference/61,shows several samples of natural language inference from SNLI ( Stanford Natural Language Inference ) corpus .,11,0,17
dataset/preprocessed/training-data/natural_language_inference/61,"In the literature , the task of NLI is usually viewed as a relation classi cation .",12,1,17
dataset/preprocessed/training-data/natural_language_inference/61,It learns the relation between a premise Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro tor commercial advantage and that copies bear this notice and the full citation on the rst page .,13,0,58
dataset/preprocessed/training-data/natural_language_inference/61,Copyrights for components of this work owned by others than the author ( s ) must be honored .,14,0,19
dataset/preprocessed/training-data/natural_language_inference/61,Abstracting with credit is permi ed .,15,0,7
dataset/preprocessed/training-data/natural_language_inference/61,"To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior speci c permission and / or a fee .",16,0,28
dataset/preprocessed/training-data/natural_language_inference/61,Request permissions from permissions@acm.org .,17,0,5
dataset/preprocessed/training-data/natural_language_inference/61,"DAPA '19 , Melbourne , Australia 2019",18,0,7
dataset/preprocessed/training-data/natural_language_inference/61,Copyright held by the owner / author ( s ) .,19,0,11
dataset/preprocessed/training-data/natural_language_inference/61,Publication rights licensed to ACM .,20,0,6
dataset/preprocessed/training-data/natural_language_inference/61,". and a hypothesis in a large training set , then predicts the relation between a new pair of premise and hypothesis .",21,0,23
dataset/preprocessed/training-data/natural_language_inference/61,e existing methods of NLI can be roughly partitioned into two categories : feature - based models and neural network - based models .,22,0,24
dataset/preprocessed/training-data/natural_language_inference/61,"Feature - based models represent a premise and a hypothesis by their unlexicalized and lexicalized features , such as n-gram length and the real - valued feature of length di erence , then train a classi er to perform relation classi cation .",23,0,43
dataset/preprocessed/training-data/natural_language_inference/61,"Recently , end - to - end neural network - based models have drawn worldwide a ention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .",24,0,39
dataset/preprocessed/training-data/natural_language_inference/61,"On the basis of their model structures , we can divide neural network - based models for NLI into two classes , sentence encoding models and sentence interaction - aggregation models .",25,0,32
dataset/preprocessed/training-data/natural_language_inference/61,e architectures of the two types of models are shown in .,26,0,12
dataset/preprocessed/training-data/natural_language_inference/61,"Sentence encoding models ( their main architecture is shown in . a ) independently encode a pair of sentences , a premise and a hypothesis using pre-trained word embedding vectors , then learn semantic relation between two sentences with a multi - layer perceptron ( MLP ) .",27,0,48
dataset/preprocessed/training-data/natural_language_inference/61,"In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .",28,0,47
dataset/preprocessed/training-data/natural_language_inference/61,"scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , selfa ention network and hierarchical convolutional networks .",29,0,29
dataset/preprocessed/training-data/natural_language_inference/61,e experimental results demonstrated that the Bi - LSTM with max pooling achieved the best performance .,30,0,17
dataset/preprocessed/training-data/natural_language_inference/61,Talman et al .,31,0,4
dataset/preprocessed/training-data/natural_language_inference/61,designed a hierarchical Bi - LSTM max pooling ( HBMP ) model to encode sentences .,32,0,16
dataset/preprocessed/training-data/natural_language_inference/61,"is model applied parameters of one Bi - LSTM to initialize the next Bi - LSTM to convey information , which shown beer results than the model with a single Bi - LSTM .",33,0,34
dataset/preprocessed/training-data/natural_language_inference/61,"Besides LSTM , a ention mechanisms could also be used to boost thee ectiveness of sentence encoding .",34,0,18
dataset/preprocessed/training-data/natural_language_inference/61,e model developed by Ghaeini et al .,35,0,8
dataset/preprocessed/training-data/natural_language_inference/61,"added selfa ention to LSTM model , and achieved beer performance .",36,0,12
dataset/preprocessed/training-data/natural_language_inference/61,"Sentence interaction - aggregation models ( their main architecture is shown in . b ) learn vector representations of pairs of sentences in the way similar to sentence encoding models and calculate pairwise word interaction matrix between two sentences using the newly updated word vectors , and then the matching results are aggregated into a vector to make the nal decision .",37,0,62
dataset/preprocessed/training-data/natural_language_inference/61,"Compared with sentence encoding model , sentence interaction - aggregation models aggregate word similarities between a pair of sentences , are capable of capturing the relevant information between two sentences , a premise and a hypothesis .",38,0,37
dataset/preprocessed/training-data/natural_language_inference/61,"Bahdanau et al. translated and aligned text simultaneously in machine translation task , innovatively introducing a ention mechanism to natural language process ( NLP ) .",39,0,26
dataset/preprocessed/training-data/natural_language_inference/61,"He et al . designed a pairwise word interaction model ( PWIM ) , which made full use of word - level ne -grained information .",40,0,26
dataset/preprocessed/training-data/natural_language_inference/61,"Wang et al . put forward a bilateral multi-perspective matching ( BiMPM ) model , focusing on various matching strategies that could be seen as di erent types of a ention .",41,0,32
dataset/preprocessed/training-data/natural_language_inference/61,e empirical studies of Lan et al .,42,0,8
dataset/preprocessed/training-data/natural_language_inference/61,and Chen et al .,43,0,5
dataset/preprocessed/training-data/natural_language_inference/61,"concluded that sentence interation - aggregation models , especially ESIM ( Enhanced Sequential Inference Model ) , a carefully designed sequential inference model based on chain LSTMs , outperformed all previous sentence encoding models .",44,0,35
dataset/preprocessed/training-data/natural_language_inference/61,"Although ESIM has achieved excellent achievements , this model does n't consider the a ention along the words in a sentence in its Bi - LSTM layer .",45,0,28
dataset/preprocessed/training-data/natural_language_inference/61,Word a ention can characterize the di erent contribution of each word .,46,0,13
dataset/preprocessed/training-data/natural_language_inference/61,"erefore , it will be bene cial to put word a ention into the Bi - LTSM layer .",47,0,19
dataset/preprocessed/training-data/natural_language_inference/61,"Moreover , the orientation of the words represents the direction of the information ow , either forward or backward , should not be ignored .",48,0,25
dataset/preprocessed/training-data/natural_language_inference/61,"In traditional Bi - LSTM model , the forward and the backward vectors learnt by Bi - LSTM are simply jointed .",49,0,22
dataset/preprocessed/training-data/natural_language_inference/53,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,2,1,11
dataset/preprocessed/training-data/natural_language_inference/53,"Many modern NLP systems rely on word embeddings , previously trained in an unsupervised manner on large corpora , as base features .",4,0,23
dataset/preprocessed/training-data/natural_language_inference/53,"Efforts to obtain embeddings for larger chunks of text , such as sentences , have however not been so successful .",5,0,21
dataset/preprocessed/training-data/natural_language_inference/53,Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted .,6,0,19
dataset/preprocessed/training-data/natural_language_inference/53,"In this paper , we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors ( Kiros et al. , 2015 ) on a wide range of transfer tasks .",7,0,45
dataset/preprocessed/training-data/natural_language_inference/53,"Much like how computer vision uses ImageNet to obtain features , which can then be transferred to other tasks , our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks .",8,0,39
dataset/preprocessed/training-data/natural_language_inference/53,Our encoder is publicly available 1 .,9,0,7
dataset/preprocessed/training-data/natural_language_inference/53,Distributed representations of words ( or word embeddings ) have shown to provide useful features for various tasks in natural language processing and computer vision .,11,0,26
dataset/preprocessed/training-data/natural_language_inference/53,"While there seems to be a consensus concerning the usefulness of word embeddings and how to learn them , this is not yet clear with regard to representations that carry the meaning of a full sentence .",12,0,37
dataset/preprocessed/training-data/natural_language_inference/53,"That is , how to capture the relationships among multiple words and phrases in a single vector remains an question to be solved .",13,0,24
dataset/preprocessed/training-data/natural_language_inference/53,"In this paper , we study the task of learning universal representations of sentences , i.e. , a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks .",15,0,35
dataset/preprocessed/training-data/natural_language_inference/53,"Two questions need to be solved in order to build such an encoder , namely : what is the preferable neural network architecture ; and how and on what task should such a network be trained .",16,0,37
dataset/preprocessed/training-data/natural_language_inference/53,"Following existing work on learning word embeddings , most current approaches consider learning sentence encoders in an unsupervised manner like SkipThought or FastSent .",17,0,24
dataset/preprocessed/training-data/natural_language_inference/53,"Here , we investigate whether supervised learning can be leveraged instead , taking inspiration from previous results in computer vision , where many models are pretrained on the ImageNet ) before being transferred .",18,0,34
dataset/preprocessed/training-data/natural_language_inference/53,"We compare sentence embeddings trained on various supervised tasks , and show that sentence embeddings generated from models trained on a natural language inference ( NLI ) task reach the best results in terms of transfer accuracy .",19,0,38
dataset/preprocessed/training-data/natural_language_inference/53,We hypothesize that the suitability of NLI as a training task is caused by the fact that it is a high - level understanding task that involves reasoning about the semantic relationships within sentences .,20,0,35
dataset/preprocessed/training-data/natural_language_inference/53,"Unlike in computer vision , where convolutional neural networks are predominant , there are multiple ways to encode a sentence using neural networks .",21,0,24
dataset/preprocessed/training-data/natural_language_inference/53,"Hence , we investigate the impact of the sentence encoding architecture on representational transferability , and compare convolutional , recurrent and even simpler word composition schemes .",22,0,27
dataset/preprocessed/training-data/natural_language_inference/53,"Our experiments show that an encoder based on a bi-directional LSTM architecture with max pooling , trained on the Stanford Natural Language Inference ( SNLI ) dataset , yields state - of - the - art sentence embeddings compared to all existing alternative unsupervised approaches like SkipThought or FastSent , while be-ing much faster to train .",23,0,57
dataset/preprocessed/training-data/natural_language_inference/53,We establish this finding on a broad and diverse set of transfer tasks that measures the ability of sentence representations to capture general and useful information .,24,0,27
dataset/preprocessed/training-data/natural_language_inference/53,Transfer learning using supervised features has been successful in several computer vision applications .,26,0,14
dataset/preprocessed/training-data/natural_language_inference/53,"Striking examples include face recognition and visual question answering , where image features trained on ImageNet ) and word embeddings trained on large unsupervised corpora are combined .",27,0,28
dataset/preprocessed/training-data/natural_language_inference/53,"In contrast , most approaches for sentence representation learning are unsupervised , arguably because the NLP community has not yet found the best supervised task for embedding the semantics of a whole sentence .",28,0,34
dataset/preprocessed/training-data/natural_language_inference/53,"Another reason is that neural networks are very good at capturing the biases of the task on which they are trained , but can easily forget the over all information or semantics of the input data by specializing too much on these biases .",29,0,44
dataset/preprocessed/training-data/natural_language_inference/53,Learning models on large unsupervised task makes it harder for the model to specialize .,30,0,15
dataset/preprocessed/training-data/natural_language_inference/53,"showed that co-adaptation of encoders and classifiers , when trained end - to - end , can negatively impact the generalization power of image features generated by an encoder .",31,0,30
dataset/preprocessed/training-data/natural_language_inference/53,They propose a loss that incorporates multiple orthogonal classifiers to counteract this effect .,32,0,14
dataset/preprocessed/training-data/natural_language_inference/53,Recent work on generating sentence embeddings range from models that compose word embeddings to more complex neural network architectures .,33,0,20
dataset/preprocessed/training-data/natural_language_inference/53,SkipThought vectors propose an objective function that adapts the skip - gram model for words to the sentence level .,34,0,20
dataset/preprocessed/training-data/natural_language_inference/53,"By encoding a sentence to predict the sentences around it , and using the features in a linear model , they were able to demonstrate good performance on 8 transfer tasks .",35,0,32
dataset/preprocessed/training-data/natural_language_inference/53,They further obtained better results using layer - norm regularization of their model in .,36,0,15
dataset/preprocessed/training-data/natural_language_inference/53,showed that the task on which sentence embeddings are trained significantly impacts their quality .,37,0,15
dataset/preprocessed/training-data/natural_language_inference/53,"In addition to unsupervised methods , they included supervised training in their comparisonnamely , on machine translation data ( using the WMT ' 14 English / French and English / German pairs ) , dictionary definitions and image captioning data ( see also ) from the COCO dataset .",38,0,49
dataset/preprocessed/training-data/natural_language_inference/53,These models obtained significantly lower results compared to the unsupervised Skip - Thought approach .,39,0,15
dataset/preprocessed/training-data/natural_language_inference/53,"Recent work has explored training sentence encoders on the SNLI corpus and applying them on the SICK corpus , either using multi-task learning or pretraining .",40,0,26
dataset/preprocessed/training-data/natural_language_inference/53,The results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead .,41,0,27
dataset/preprocessed/training-data/natural_language_inference/53,"To our knowledge , this work is the first attempt to fully exploit the SNLI corpus for building generic sentence encoders .",42,0,22
dataset/preprocessed/training-data/natural_language_inference/53,"As we show in our experiments , we are able to consistently outperform unsupervised approaches , even if our models are trained on much less ( but humanannotated ) data .",43,0,31
dataset/preprocessed/training-data/natural_language_inference/53,"This work combines two research directions , which we describe in what follows .",45,0,14
dataset/preprocessed/training-data/natural_language_inference/53,"First , we explain how the NLI task can be used to train universal sentence encoding models using the SNLI task .",46,0,22
dataset/preprocessed/training-data/natural_language_inference/53,"We subsequently describe the architectures that we investigated for the sentence encoder , which , in our opinion , covers a suitable range of sentence encoders currently in use .",47,0,30
dataset/preprocessed/training-data/natural_language_inference/53,"Specifically , we examine standard recurrent models such as LSTMs and GRUs , for which we investigate mean and maxpooling over the hidden representations ; a selfattentive network that incorporates different views of the sentence ; and a hierarchical convolutional network that can be seen as a tree - based method that blends different levels of abstraction .",48,0,58
dataset/preprocessed/training-data/natural_language_inference/53,The Natural Language Inference task,49,0,5
dataset/preprocessed/training-data/natural_language_inference/8,Deep Learning for Answer Sentence Selection,2,1,6
dataset/preprocessed/training-data/natural_language_inference/8,Answer sentence selection is the task of identifying sentences that contain the answer to a given question .,4,0,18
dataset/preprocessed/training-data/natural_language_inference/8,This is an important problem in its own right as well as in the larger context of open domain question answering .,5,0,22
dataset/preprocessed/training-data/natural_language_inference/8,"We propose a novel approach to solving this task via means of distributed representations , and learn to match questions with answers by considering their semantic encoding .",6,0,28
dataset/preprocessed/training-data/natural_language_inference/8,"This contrasts prior work on this task , which typically relies on classifiers with large numbers of hand - crafted syntactic and semantic features and various external resources .",7,0,29
dataset/preprocessed/training-data/natural_language_inference/8,"Our approach does not require any feature engineering nor does it involve specialist linguistic data , making this model easily applicable to a wide range of domains and languages .",8,0,30
dataset/preprocessed/training-data/natural_language_inference/8,Experimental results on a standard benchmark dataset from TREC demonstrate that - despite its simplicity - our model matches state of the art performance on the answer sentence selection task .,9,0,31
dataset/preprocessed/training-data/natural_language_inference/8,Question answering can broadly be divided into two categories .,11,0,10
dataset/preprocessed/training-data/natural_language_inference/8,"One approach focuses on semantic parsing , where answers are retrieved by turning a question into a data base query and subsequently applying that query to an existing knowledge base .",12,0,31
dataset/preprocessed/training-data/natural_language_inference/8,"The other category is open domain question answering , which is more closely related to the field of information retrieval .",13,0,21
dataset/preprocessed/training-data/natural_language_inference/8,Open domain question answering requires a number of intermediate steps .,14,0,11
dataset/preprocessed/training-data/natural_language_inference/8,"For instance , to answer a question such as "" Who wrote the book Harry Potter ? "" , a system would first identify the question type and retrieve relevant documents .",15,0,32
dataset/preprocessed/training-data/natural_language_inference/8,"Subsequently , within the retrieved documents , a sentence containing the answer is selected , and finally the answer ( J.K. Rowling ) itself is extracted from the relevant sentence .",16,0,31
dataset/preprocessed/training-data/natural_language_inference/8,"In this paper , we focus on answer sentence selection , the task that selects the correct sentences answering a factual question from a set of candidate sentences .",17,0,29
dataset/preprocessed/training-data/natural_language_inference/8,"Beyond it s role in open domain question answering , answer sentence selection is also a stand - alone task with applications in knowledge base construction and information extraction .",18,0,30
dataset/preprocessed/training-data/natural_language_inference/8,"The correct sentence may not answer the question directly and perhaps also contain extraneous information , for example :",19,0,19
dataset/preprocessed/training-data/natural_language_inference/8,Q : When did Amtrak begin operations ?,20,0,8
dataset/preprocessed/training-data/natural_language_inference/8,A : Amtrak has not turned a profit since it was founded in 1971 .,21,0,15
dataset/preprocessed/training-data/natural_language_inference/8,The relevance of an answer sentence to a question is typically determined by measuring the semantic similarity between question and answer .,22,0,22
dataset/preprocessed/training-data/natural_language_inference/8,Prior work in this field mainly attempted this via syntactic matching of parse trees .,23,0,15
dataset/preprocessed/training-data/natural_language_inference/8,This can be achieved with generative models that syntactically transform answers to questions .,24,0,14
dataset/preprocessed/training-data/natural_language_inference/8,Another option is discriminative models over features produced from minimal edit sequences between dependency parse trees .,25,0,17
dataset/preprocessed/training-data/natural_language_inference/8,"Beyond syntactic information , some prior work has also included semantic features from resources such as WordNet , with the previous state - of - the - art model for this task relying on a variety of such lexical semantic resources .",26,0,42
dataset/preprocessed/training-data/natural_language_inference/8,"While empirically the inclusion of large amounts of semantic information has been shown to improve performance , such approaches rely on a significant amount of feature engineering and require expensive semantic resources which maybe difficult to obtain , particularly for resource - low languages .",27,0,45
dataset/preprocessed/training-data/natural_language_inference/8,"A second limitation of such feature - based semantic models is the difficulty of adapting to new domains , requiring separate feature extraction and resource development or identification steps for every domain .",28,0,33
dataset/preprocessed/training-data/natural_language_inference/8,"At the same time , neural network - based distributional sentence models have achieved successes in many natural language processing tasks such as sentiment analysis , paraphrase detection and document classification .",29,0,32
dataset/preprocessed/training-data/natural_language_inference/8,"As a consequence of this success , it appears natural to attempt to solve question answering using similar techniques .",30,0,20
dataset/preprocessed/training-data/natural_language_inference/8,"There has been some work on this in the context of semantic parsing for knowledge base question answering , where the focus is on learning semantic representations for knowledge base tuples .",31,0,32
dataset/preprocessed/training-data/natural_language_inference/8,Another line of work - closely related to the model presented here - is the application of recursive neural networks to factoid question answering over paragraphs .,32,0,27
dataset/preprocessed/training-data/natural_language_inference/8,"A key difference to our approach is that this model , given a question , selects answers from a relatively small fixed set of candidates encountered during training .",33,0,29
dataset/preprocessed/training-data/natural_language_inference/8,"On the other hand , the task of answer sentence selection that we address here , requires picking an answer from among a set of candidate sentences not encountered during training .",34,0,32
dataset/preprocessed/training-data/natural_language_inference/8,"In addition , each question has different numbers of candidate sentences .",35,0,12
dataset/preprocessed/training-data/natural_language_inference/8,"In this paper , we show that a neural network - based sentence model can be applied to the task of answer sentence selection .",36,0,25
dataset/preprocessed/training-data/natural_language_inference/8,"We construct two distributional sentence models ; first a bag - of - words model , and second , a bigram model based on a convolutional neural network .",37,0,29
dataset/preprocessed/training-data/natural_language_inference/8,"Assuming a set of pre-trained semantic word embeddings , we train a supervised model to learn a semantic matching between question and answer pairs .",38,0,25
dataset/preprocessed/training-data/natural_language_inference/8,The underlying distributional models provide the semantic information necessary for this matching function .,39,0,14
dataset/preprocessed/training-data/natural_language_inference/8,"We also present an enhanced version of this model , which combines the signal of the distributed matching algorithm with two simple word matching features .",40,0,26
dataset/preprocessed/training-data/natural_language_inference/8,Note that these features still do not require any external linguistic annotation .,41,0,13
dataset/preprocessed/training-data/natural_language_inference/8,"Compared with previous work on answer sentence selection , our approach is applicable to any language , and does not require feature - engineering and hand - coded resources beyond some large corpus on which to train our initial word embeddings .",42,0,42
dataset/preprocessed/training-data/natural_language_inference/8,We conduct experiments on an answer selection dataset created from TREC QA track .,43,0,14
dataset/preprocessed/training-data/natural_language_inference/8,The results show that our models are very effective on this task - matching the state - of - the - art results .,44,0,24
dataset/preprocessed/training-data/natural_language_inference/8,There are three threads of related work relevant to our approach .,46,0,12
dataset/preprocessed/training-data/natural_language_inference/8,We first review composition methods for distributional semantics and then discuss the existing work on answer sentence selection .,47,0,19
dataset/preprocessed/training-data/natural_language_inference/8,"Finally , we introduce existing work on applying neural networks to question answering .",48,0,14
dataset/preprocessed/training-data/natural_language_inference/8,Compositional distributional semantics .,49,0,4
dataset/preprocessed/training-data/natural_language_inference/89,Read + Verify : Machine Reading Comprehension with Unanswerable Questions,2,1,10
dataset/preprocessed/training-data/natural_language_inference/89,Machine reading comprehension with unanswerable questions aims to abstain from answering when no answer can be inferred .,4,0,18
dataset/preprocessed/training-data/natural_language_inference/89,"In addition to extract answers , previous works usually predict an additional "" no - answer "" probability to detect unanswerable cases .",5,0,23
dataset/preprocessed/training-data/natural_language_inference/89,"However , they fail to validate the answerability of the question by verifying the legitimacy of the predicted answer .",6,0,20
dataset/preprocessed/training-data/natural_language_inference/89,"To address this problem , we propose a novel read - then - verify system , which not only utilizes a neural reader to extract candidate answers and produce noanswer probabilities , but also leverages an answer verifier to decide whether the predicted answer is entailed by the input snippets .",7,0,51
dataset/preprocessed/training-data/natural_language_inference/89,"Moreover , we introduce two auxiliary losses to help the reader better handle answer extraction as well as noanswer detection , and investigate three different architectures for the answer verifier .",8,0,31
dataset/preprocessed/training-data/natural_language_inference/89,"Our experiments on the SQuAD 2.0 dataset show that our system obtains a score of 74.2 F1 on test set , achieving state - of - the - art results at the time of submission ( Aug. 28th , 2018 ) .",9,0,42
dataset/preprocessed/training-data/natural_language_inference/89,The ability to comprehend text and answer questions is crucial for natural language processing .,11,0,15
dataset/preprocessed/training-data/natural_language_inference/89,"Due to the creation of various large - scale datasets ) , remarkable advancements have been made in the task of machine reading comprehension .",12,0,25
dataset/preprocessed/training-data/natural_language_inference/89,"Nevertheless , one important hypothesis behind current approaches is that there always exists a correct answer in the context passage .",13,0,21
dataset/preprocessed/training-data/natural_language_inference/89,"Therefore , the models only need to choose a most plausible text span based on the question , instead of checking if there exists an answer in the first place .",14,0,31
dataset/preprocessed/training-data/natural_language_inference/89,"Recently , a new version of Stanford Question Answering Dataset ( SQuAD ) , namely SQuAD 2.0 ( Rajpurkar , Jia , and Liang 2018 ) , has been proposed to test the ability of answering answerable questions as well as detecting unanswerable cases .",15,0,45
dataset/preprocessed/training-data/natural_language_inference/89,"To deal with unanswerable cases , systems must learn to identify a wide range of linguistic phenomena such as negation , antonymy and entity changes between the passage and the question .",16,0,32
dataset/preprocessed/training-data/natural_language_inference/89,Previous works all apply a shared - normalization :,17,0,9
dataset/preprocessed/training-data/natural_language_inference/89,An overview of our approach .,18,0,6
dataset/preprocessed/training-data/natural_language_inference/89,The reader first extracts a candidate answer and produces a no-answer probability ( NA Prob ) .,19,0,17
dataset/preprocessed/training-data/natural_language_inference/89,The answer verifier then checks whether the extracted answer is legitimate or not .,20,0,14
dataset/preprocessed/training-data/natural_language_inference/89,"Finally , the system aggregates previous results and outputs the final prediction .",21,0,13
dataset/preprocessed/training-data/natural_language_inference/89,"operation between a "" no-answer "" score and answer span scores , so as to produce a probability that a question is unanswerable as well as output a candidate answer .",22,0,31
dataset/preprocessed/training-data/natural_language_inference/89,"However , they have not considered further validating the answerability of the question by verifying the legitimacy of the predicted answer .",23,0,22
dataset/preprocessed/training-data/natural_language_inference/89,"Here , answerability denotes whether the question has an answer , and legitimacy means whether the extracted text can be supported by the passage and the question .",24,0,28
dataset/preprocessed/training-data/natural_language_inference/89,"Human , on the contrary , tends to first find a plausible answer given a question , and then checks if there exists any contradictory semantics .",25,0,27
dataset/preprocessed/training-data/natural_language_inference/89,"To address the above issue , we propose a read - then - verify system that aims to be robust to unanswerable questions in this paper .",26,0,27
dataset/preprocessed/training-data/natural_language_inference/89,"As shown in , our system consists of two components : ( 1 ) a no-answer reader for extracting candidate answers and detecting unanswerable questions , and ( 2 ) an answer verifier for deciding whether or not the extracted candidate is legitimate .",27,0,44
dataset/preprocessed/training-data/natural_language_inference/89,The key contributions of our work are three - fold .,28,0,11
dataset/preprocessed/training-data/natural_language_inference/89,"First , we augment existing readers with two auxiliary losses , to better handle answer extraction and no - answer detection respectively .",29,0,23
dataset/preprocessed/training-data/natural_language_inference/89,"Since the downstream verifying stage always requires a candidate answer , the reader must be able to extract plausible answers for all questions .",30,0,24
dataset/preprocessed/training-data/natural_language_inference/89,"However , previous approaches are not trained to find potential candidates for unanswerable questions .",31,0,15
dataset/preprocessed/training-data/natural_language_inference/89,We solve this problem by introducing an independent span loss that aims to concentrate on the answer extraction task regardless of the answerability of the question .,32,0,27
dataset/preprocessed/training-data/natural_language_inference/89,"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the no -answer score and the other is used for our auxiliary loss .",33,0,45
dataset/preprocessed/training-data/natural_language_inference/89,"Besides , we present another independent noanswer loss to further alleviate the confliction , by focusing on the no-answer detection task without considering the shared normalization of answer extraction .",34,0,30
dataset/preprocessed/training-data/natural_language_inference/89,"Second , in addition to the standard reading phase , we introduce an additional answer verifying phase , which aims at finding local entailment that supports the answer by comparing the answer sentence with the question .",35,0,37
dataset/preprocessed/training-data/natural_language_inference/89,This is based on the observation that the core phenomenon of unanswerable questions usually occurs between a few passage words and question words .,36,0,24
dataset/preprocessed/training-data/natural_language_inference/89,"Take for example , after comparing the passage snippet "" Normandy , a region in France "" with the question , we can easily determine that no answer exists since the question asks for an impossible condition 1 .",37,0,39
dataset/preprocessed/training-data/natural_language_inference/89,"This observation is even more obvious when antonym or mutual exclusion occurs , such as the question asks for "" the decline of rainforests "" but the passage mentions that "" the rainforests spread out "" .",38,0,37
dataset/preprocessed/training-data/natural_language_inference/89,"Inspired by recent advances in natural language inference ( NLI ) , we investigate three different architectures for the answer verifying task .",39,0,23
dataset/preprocessed/training-data/natural_language_inference/89,"The first one is a sequential model that takes two sentences as along sequence , while the second one attempts to capture interactions between two sentences .",40,0,27
dataset/preprocessed/training-data/natural_language_inference/89,The last one is a hybrid model that combines the above two models to test if the performance can be further improved .,41,0,23
dataset/preprocessed/training-data/natural_language_inference/89,"Lastly , we evaluate our system on the SQuAD 2.0 dataset , a reading comprehension benchmark augmented with unanswerable questions .",42,0,21
dataset/preprocessed/training-data/natural_language_inference/89,"Our best reader achieves a F 1 score of 73.7 and 69.1 on the development set , with or without ELMo embeddings .",43,0,23
dataset/preprocessed/training-data/natural_language_inference/89,"When combined with the answer verifier , the whole system improves to 74.8 F1 and 71.5 F1 respectively .",44,0,19
dataset/preprocessed/training-data/natural_language_inference/89,"Moreover , the best system obtains a score of 74.2 F1 on test set , achieving state - of - the - art results at the time of submission ( Aug. 28th , 2018 ) .",45,0,36
dataset/preprocessed/training-data/natural_language_inference/89,Existing reading comprehension models focus on answering questions where a correct answer is guaranteed to exist .,47,0,17
dataset/preprocessed/training-data/natural_language_inference/89,"However , they are notable to identify unanswerable questions but tend to return an unreliable text span .",48,0,18
dataset/preprocessed/training-data/natural_language_inference/89,"Consequently , we first give a brief introduction on the unanswer - 1 Impossible condition means that the question asks for something that is not satisfied by anything in the given passage .",49,0,33
dataset/preprocessed/training-data/natural_language_inference/84,LEARNING TO COMPUTE WORD EMBEDDINGS ON THE FLY,2,1,8
dataset/preprocessed/training-data/natural_language_inference/84,Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare .,4,0,18
dataset/preprocessed/training-data/natural_language_inference/84,"Learning representations for words in the "" long tail "" of this distribution requires enormous amounts of data .",5,0,19
dataset/preprocessed/training-data/natural_language_inference/84,"Representations of rare words trained directly on end tasks are usually poor , requiring us to pre-train embeddings on external data , or treat all rare words as out - of - vocabulary words with a unique representation .",6,0,39
dataset/preprocessed/training-data/natural_language_inference/84,We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained end - to - end for the downstream task .,7,0,33
dataset/preprocessed/training-data/natural_language_inference/84,"We show that this improves results against baselines where embeddings are trained on the end task for reading comprehension , recognizing textual entailment and language modeling .",8,0,27
dataset/preprocessed/training-data/natural_language_inference/84,* Work partly done at Element AI . Equal contribution . CIFAR Associate Fellow CIFAR Senior Fellow,9,0,17
dataset/preprocessed/training-data/natural_language_inference/84,"Natural language yields a Zipfian distribution which tells us that a core set of words ( at the head of the distribution ) are frequent and ubiquitous , while a significantly larger number ( in the long tail ) are rare .",11,0,42
dataset/preprocessed/training-data/natural_language_inference/84,"Learning representations for rare words is a well - known challenge of natural language understanding , since the standard end - to - end supervised learning methods require many occurrences of each word to generalize well .",12,1,37
dataset/preprocessed/training-data/natural_language_inference/84,"The typical remedy to the rare word problem is to learn embeddings for some proportion of the head of the distribution , possibly shifted towards the domain - specific vocabulary of the dataset or task at hand , and to treat all other words as out - of - vocabulary ( OOV ) , replacing them with an unknown word "" UNK "" token with a shared embedding .",13,0,69
dataset/preprocessed/training-data/natural_language_inference/84,"This essentially heuristic solution is inelegant , as words from technical domains , names of people , places , institutions , and soon will lack a specific representation unless sufficient data are available to justify their inclusion in the vocabulary .",14,0,41
dataset/preprocessed/training-data/natural_language_inference/84,"This forces model designers to rely on overly large vocabularies , as observed by , which are parametrically expensive , or to employ vocabulary selection strategies .",15,0,27
dataset/preprocessed/training-data/natural_language_inference/84,"In both cases , we face the issue that words in the tail of the Zipfian distribution will typically still be too rare to learn good representations for through standard embedding methods .",16,0,33
dataset/preprocessed/training-data/natural_language_inference/84,"Some models , such as in the work of , have sought to deal with the open vocabulary problem by obtaining representations of words from characters .",17,0,27
dataset/preprocessed/training-data/natural_language_inference/84,"This is successful at capturing the semantics of morphological derivations ( e.g. "" running "" from "" run "" ) but puts significant pressure on the encoder to capture semantic distinctions amongst syntactically similar but semantically unrelated words ( e.g. "" run "" vs. "" rung "" ) .",18,0,49
dataset/preprocessed/training-data/natural_language_inference/84,"Additionally , nothing about the spelling of named entities , e.g. "" The Beatles "" , tells you anything about their semantics ( namely that they are a rock band ) .",19,0,32
dataset/preprocessed/training-data/natural_language_inference/84,"In this paper we propose a new method for computing embeddings "" on the fly "" , which jointly addresses the large vocabulary problem and the paucity of data for learning representations in the long tail of the Zipfian distribution .",20,0,41
dataset/preprocessed/training-data/natural_language_inference/84,"This method , which we illustrate in , can be summarized as follows : instead of directly learning separate representations for all words in a potentially unbounded vocabulary , we train a network to predict the representations of words based on auxiliary data .",21,0,44
dataset/preprocessed/training-data/natural_language_inference/84,Such auxiliary data need only satisfy the general requirement that it describe some aspect of the semantics of the word for which a representation is needed .,22,0,27
dataset/preprocessed/training-data/natural_language_inference/84,"Examples of such data could be dictionary definitions , Wikipedia infoboxes , linguistic descriptions of named entities obtained from Wikipedia articles , or something as simple as the spelling of a word .",23,0,33
dataset/preprocessed/training-data/natural_language_inference/84,"We will refer to the content of auxiliary data as "" definitions "" throughout the paper , regardless of the source .",24,0,22
dataset/preprocessed/training-data/natural_language_inference/84,Several sources of auxiliary data can be used simultaneously as input to a neural network that will compute a combined representation .,25,0,22
dataset/preprocessed/training-data/natural_language_inference/84,"These representations can then be used for out - of - vocabulary words , or combined with withinvocabulary word embeddings directly trained on the task of interest or pretrained from an external data source .",26,0,35
dataset/preprocessed/training-data/natural_language_inference/84,"Importantly , the auxiliary data encoders are trained jointly with the objective , ensuring the preservation of semantic alignment with representations of within - vocabulary words .",27,0,27
dataset/preprocessed/training-data/natural_language_inference/84,"In the present paper , we will focus on a subset of these approaches and auxiliary data sources , restricting ourselves to producing out - of - vocabulary words embeddings from dictionary data , spelling , or both .",28,0,39
dataset/preprocessed/training-data/natural_language_inference/84,The obvious use case for our method would be datasets and tasks where there are many rare terms such as technical writing or bio / medical text .,29,0,28
dataset/preprocessed/training-data/natural_language_inference/84,"On such datasets , attempting to learn global vectors - for example GloVe embeddings ) - from external data , would only provide coverage for common words and would be unlikely to be exposed to sufficient ( or any ) examples of domain - specific technical terms to learn good enough representations .",30,0,53
dataset/preprocessed/training-data/natural_language_inference/84,"However , there are no ( or significantly fewer ) established neural network - based baselines on these tasks , which makes it harder to validate baseline results .",31,0,29
dataset/preprocessed/training-data/natural_language_inference/84,"Instead , we present results on a trio of well - established tasks , namely reading comprehension , recognizing textual entailment , and a variant on language modelling .",32,0,29
dataset/preprocessed/training-data/natural_language_inference/84,"For each task , we compare baseline models with embeddings trained directly only on the task objective to those same models with our on the fly embedding method .",33,0,29
dataset/preprocessed/training-data/natural_language_inference/84,"Additionally , we report results for the same models with pretrained GLoVe vectors as input which we do not update .",34,0,21
dataset/preprocessed/training-data/natural_language_inference/84,We aim to show how the gap in results between the baseline and the data - rich GLoVe - based models can be partially but substantially closed merely through the introduction of relatively small amounts of auxiliary definitions .,35,0,39
dataset/preprocessed/training-data/natural_language_inference/84,Quantitative results show that auxiliary data improves performance .,36,0,9
dataset/preprocessed/training-data/natural_language_inference/84,"Qualitative evaluation indicates our method allows models to draw and exploit connections defined in auxiliary data , along the lines of synonymy and semantic relatedness .",37,0,26
dataset/preprocessed/training-data/natural_language_inference/84,"Arguably , the most popular approach for representing rare words is by using word embeddings trained on very large corpora of raw text ..",39,0,24
dataset/preprocessed/training-data/natural_language_inference/84,Such embeddings are typically explicitly or implicitly based on word co-occurence statistics .,40,0,13
dataset/preprocessed/training-data/natural_language_inference/84,"Being a big step forward from the models thatare trained from scratch only on the task at hand , the approach can be criticized for being extremely data- hungry 1 .",41,0,31
dataset/preprocessed/training-data/natural_language_inference/84,"Obtaining the necessary amounts of data maybe difficult , e.g. in technical domains .",42,0,14
dataset/preprocessed/training-data/natural_language_inference/84,"Besides , auxiliary training criteria used in the pretraining approaches are not guaranteed to yield representations thatare useful for the task at hand .",43,0,24
dataset/preprocessed/training-data/natural_language_inference/84,proposed to represent OOV words by fixed random vectors .,44,0,10
dataset/preprocessed/training-data/natural_language_inference/84,"While this has shown to be effective for machine comprehension , this method does not account for word semantics at all , and therefore , does not cover the same ground as the method that we propose .",45,0,38
dataset/preprocessed/training-data/natural_language_inference/84,There have been a number of attempts to achieve out - of - vocabulary generalization by relying on the spelling .,46,0,21
dataset/preprocessed/training-data/natural_language_inference/84,used a bidirectional LSTM to read the spelling of rare words and showed that this can be helpful for language modeling and POS tagging .,47,0,25
dataset/preprocessed/training-data/natural_language_inference/84,We too will investigate spelling as a :,48,0,8
dataset/preprocessed/training-data/natural_language_inference/84,An example of how we deal with out - of - vocabulary words ( indicated in bold ) .,49,0,19
dataset/preprocessed/training-data/natural_language_inference/6,Massively Multilingual Sentence Embeddings for Zero - Shot Cross - Lingual Transfer and Beyond,2,1,14
dataset/preprocessed/training-data/natural_language_inference/6,"We introduce an architecture to learn joint multilingual sentence representations for 93 languages , belonging to more than 30 different families and written in 28 different scripts .",4,1,28
dataset/preprocessed/training-data/natural_language_inference/6,"Our system uses a single BiLSTM encoder with a shared BPE vocabulary for all languages , which is coupled with an auxiliary decoder and trained on publicly available parallel corpora .",5,0,31
dataset/preprocessed/training-data/natural_language_inference/6,"This enables us to learn a classifier on top of the resulting embeddings using English annotated data only , and transfer it to any of the 93 languages without any modification .",6,0,32
dataset/preprocessed/training-data/natural_language_inference/6,"Our experiments in cross-lingual natural language inference ( XNLI dataset ) , cross - lingual document classification ( ML - Doc dataset ) and parallel corpus mining ( BUCC dataset ) show the effectiveness of our approach .",7,0,38
dataset/preprocessed/training-data/natural_language_inference/6,"We also introduce a new test set of aligned sentences in 112 languages , and show that our sentence embeddings obtain strong results in multilingual similarity search even for low - resource languages .",8,0,34
dataset/preprocessed/training-data/natural_language_inference/6,"Our implementation , the pretrained encoder and the multilingual test set are available at https://github.com / facebookresearch/LASER . . 2018 .",9,0,21
dataset/preprocessed/training-data/natural_language_inference/6,Achieving human parity on automatic Chinese to English news translation .,10,0,11
dataset/preprocessed/training-data/natural_language_inference/6,"CoRR , abs / 1803.05567 .",11,0,6
dataset/preprocessed/training-data/natural_language_inference/6,Jeremy Howard and Sebastian Ruder . 2018 .,12,0,8
dataset/preprocessed/training-data/natural_language_inference/6,Universal language model fine - tuning for text classification .,13,0,10
dataset/preprocessed/training-data/natural_language_inference/6,"While the recent advent of deep learning has led to impressive progress in Natural Language Processing ( NLP ) , these techniques are known to be particularly data hungry , limiting their applicability in many practical scenarios .",16,0,38
dataset/preprocessed/training-data/natural_language_inference/6,"An increasingly popular approach to alleviate this issue is to first learn general language representations on unlabeled data , which are then integrated in task - specific downstream systems .",17,0,30
dataset/preprocessed/training-data/natural_language_inference/6,This approach was first popularized byword embeddings,18,0,7
dataset/preprocessed/training-data/natural_language_inference/6,"This work was performed during an internship at Facebook AI Research . , but has recently been superseded by sentence - level representations .",19,0,24
dataset/preprocessed/training-data/natural_language_inference/6,"Nevertheless , all these works learn a separate model for each language and are thus unable to leverage information across different languages , greatly limiting their potential performance for low - resource languages .",20,0,34
dataset/preprocessed/training-data/natural_language_inference/6,"In this work , we are interested in universal language agnostic sentence embeddings , that is , vector representations of sentences that are general with respect to two dimensions : the input language and the NLP task .",21,0,38
dataset/preprocessed/training-data/natural_language_inference/6,"The motivations for such representations are multiple : the hope that languages with limited resources benefit from joint training over many languages , the desire to perform zero - shot transfer of an NLP model from one language ( typically English ) to another , and the possibility to handle code-switching .",22,0,52
dataset/preprocessed/training-data/natural_language_inference/6,"To that end , we train a single encoder to handle multiple languages , so that semantically similar sentences in different languages are close in the embedding space .",23,0,29
dataset/preprocessed/training-data/natural_language_inference/6,"While previous work in multilingual NLP has been limited to either a few languages or specific applications like typology prediction or machine translation , we learn general purpose sentence representations for 93 languages ( see ) .",24,0,37
dataset/preprocessed/training-data/natural_language_inference/6,"Using a single pre-trained BiLSTM encoder for all the 93 languages , we obtain very strong results in various scenarios without any fine - tuning , including crosslingual natural language inference ( XNLI dataset ) , cross - lingual classification ( MLDoc dataset ) , bitext mining ( BUCC dataset ) and a new multilingual similarity search dataset we introduce covering 112 languages .",25,0,64
dataset/preprocessed/training-data/natural_language_inference/6,"To the best of our knowledge , this is the first exploration of general purpose massively multilingual sentence representations across a large variety of tasks .",26,0,26
dataset/preprocessed/training-data/natural_language_inference/6,ar Xiv: 1812.10464v2 [ cs. CL ] 25 Sep 2019,27,0,10
dataset/preprocessed/training-data/natural_language_inference/6,2 Related work,28,0,3
dataset/preprocessed/training-data/natural_language_inference/6,"Following the success of word embeddings , there has been an increasing interest in learning continuous vector representations of longer linguistic units like sentences .",29,0,25
dataset/preprocessed/training-data/natural_language_inference/6,"These sentence embeddings are commonly obtained using a Recurrent Neural Network ( RNN ) encoder , which is typically trained in an unsupervised way overlarge collections of unlabelled corpora .",30,0,30
dataset/preprocessed/training-data/natural_language_inference/6,"For instance , the skip - thought model of couple the encoder with an auxiliary decoder , and train the entire system to predict the surrounding sentences over a collection of books .",31,0,33
dataset/preprocessed/training-data/natural_language_inference/6,It was later shown that more competitive results could be obtained by training the encoder over labeled Natural Language Inference ( NLI ) data .,32,0,25
dataset/preprocessed/training-data/natural_language_inference/6,"This was later extended to multitask learning , combining different training objectives like that of skip - thought , NLI and machine translation .",33,0,24
dataset/preprocessed/training-data/natural_language_inference/6,"While the previous methods consider a single language at a time , multilingual representations have attracted a large attention in recent times .",34,0,23
dataset/preprocessed/training-data/natural_language_inference/6,"Most of this research focuses on cross - lingual word embeddings , which are commonly learned jointly from parallel corpora .",35,0,21
dataset/preprocessed/training-data/natural_language_inference/6,"An alternative approach that is becoming increasingly popular is to separately train word embeddings for each language , and map them to a shared space based on a bilingual dictionary or even in a fully unsupervised manner .",36,0,38
dataset/preprocessed/training-data/natural_language_inference/6,Cross - lingual word embeddings are often used to build bag - of - word representations of longer linguistic units by taking their respective ( IDF - weighted ) average .,37,0,31
dataset/preprocessed/training-data/natural_language_inference/6,"While this approach has the advantage of requiring weak or no cross - lingual signal , it has been shown that the resulting sentence embeddings work poorly in practical crosslingual transfer settings .",38,0,33
dataset/preprocessed/training-data/natural_language_inference/6,A more competitive approach that we follow here is to use a sequence - to - sequence encoderdecoder architecture .,39,0,20
dataset/preprocessed/training-data/natural_language_inference/6,"The full system is trained end - to - end on parallel corpora akin to multilingual neural machine translation : the encoder maps the source sequence into a fixed - length vector representation , which is used by the decoder to create the target sequence .",40,0,46
dataset/preprocessed/training-data/natural_language_inference/6,"This decoder is then discarded , and the encoder is kept to embed sentences in any of the training languages .",41,0,21
dataset/preprocessed/training-data/natural_language_inference/6,"While some proposals use a separate encoder for each language ( Schwenk and Douze , 2017 ) , sharing a single encoder for all languages also gives strong results .",42,0,30
dataset/preprocessed/training-data/natural_language_inference/6,"Nevertheless , most existing work is either limited to few , rather close languages or , more commonly , consider pairwise joint embeddings with English and one foreign language .",43,0,30
dataset/preprocessed/training-data/natural_language_inference/6,"To the best of our knowledge , existing work on learning multilingual representations for a large number of languages is limited to word embeddings or specific applications like typology prediction or machine translation , ours being the first paper exploring general purpose massively multilingual sentence representations .",44,0,47
dataset/preprocessed/training-data/natural_language_inference/6,All the previous approaches learn a fixed - length representation for each sentence .,45,0,14
dataset/preprocessed/training-data/natural_language_inference/6,"A recent research line has obtained very strong results using variable - length representations instead , consisting of contextualized embeddings of the words in the sentence .",46,0,27
dataset/preprocessed/training-data/natural_language_inference/6,"For that purpose , these methods train either an RNN or self - attentional encoder over unnanotated corpora using some form of language modeling .",47,0,25
dataset/preprocessed/training-data/natural_language_inference/6,"A classifier can then be learned on top of the resulting encoder , which is commonly further fine - tuned during this supervised training .",48,0,25
dataset/preprocessed/training-data/natural_language_inference/6,"Concurrent to our work , Lample and Conneau ( 2019 ) propose a cross - lingual extension of these models , and report strong results in cross -lingual natural language inference , machine translation and language modeling .",49,0,38
dataset/preprocessed/training-data/natural_language_inference/90,A Decomposable Attention Model for Natural Language Inference,2,1,8
dataset/preprocessed/training-data/natural_language_inference/90,We propose a simple neural architecture for natural language inference .,4,0,11
dataset/preprocessed/training-data/natural_language_inference/90,"Our approach uses attention to decompose the problem into subproblems that can be solved separately , thus making it trivially parallelizable .",5,0,22
dataset/preprocessed/training-data/natural_language_inference/90,"On the Stanford Natural Language Inference ( SNLI ) dataset , we obtain state - of - the - art results with almost an order of magnitude fewer parameters than previous work and without relying on any word - order information .",6,0,42
dataset/preprocessed/training-data/natural_language_inference/90,Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements .,7,0,16
dataset/preprocessed/training-data/natural_language_inference/90,Natural language inference ( NLI ) refers to the problem of determining entailment and contradiction relationships between a premise and a hypothesis .,9,1,23
dataset/preprocessed/training-data/natural_language_inference/90,NLI is a central problem in language understanding ) and recently the large SNLI corpus of 570K sentence pairs was created for this task .,10,1,25
dataset/preprocessed/training-data/natural_language_inference/90,We present a new model for NLI and leverage this corpus for comparison with prior work .,11,0,17
dataset/preprocessed/training-data/natural_language_inference/90,A large body of work based on neural networks for text similarity tasks including NLI has been published in recent years .,12,0,22
dataset/preprocessed/training-data/natural_language_inference/90,"The dominating trend in these models is to build complex , deep text representation models , for example , with convolutional networks or long short - term memory networks with the goal of deeper sentence comprehension .",13,0,37
dataset/preprocessed/training-data/natural_language_inference/90,"While these approaches have yielded impressive results , they are often computationally very expensive , and result in models having millions of parameters ( excluding embeddings ) .",14,0,28
dataset/preprocessed/training-data/natural_language_inference/90,"Here , we take a different approach , arguing that for natural language inference it can often suffice to simply align bits of local text substructure and then aggregate this information .",15,0,32
dataset/preprocessed/training-data/natural_language_inference/90,"For example , consider the following sentences :",16,0,8
dataset/preprocessed/training-data/natural_language_inference/90,"Bob is in his room , but because of the thunder and lightning outside , he can not sleep .",17,0,20
dataset/preprocessed/training-data/natural_language_inference/90,Bob is awake .,18,0,4
dataset/preprocessed/training-data/natural_language_inference/90,It is sunny outside .,19,0,5
dataset/preprocessed/training-data/natural_language_inference/90,The first sentence is complex in structure and it is challenging to construct a compact representation that expresses its entire meaning .,20,0,22
dataset/preprocessed/training-data/natural_language_inference/90,"However , it is fairly easy to conclude that the second sentence follows from the first one , by simply aligning Bob with Bob and can not sleep with awake and recognizing that these are synonyms .",21,0,37
dataset/preprocessed/training-data/natural_language_inference/90,"Similarly , one can conclude that It is sunny outside contradicts the first sentence , by aligning thunder and lightning with sunny and recognizing that these are most likely incompatible .",22,0,31
dataset/preprocessed/training-data/natural_language_inference/90,"We leverage this intuition to build a simpler and more lightweight approach to NLI within a neural framework ; with considerably fewer parameters , our model outperforms more complex existing neural architectures .",23,0,33
dataset/preprocessed/training-data/natural_language_inference/90,"In contrast to existing approaches , our approach only relies on alignment and is fully computationally decomposable with respect to the input text .",24,0,24
dataset/preprocessed/training-data/natural_language_inference/90,An overview of our approach is given in Figure 1 .,25,0,11
dataset/preprocessed/training-data/natural_language_inference/90,"Given two sentences , where each word is repre-sented by an embedding vector , we first create a soft alignment matrix using neural attention .",26,0,25
dataset/preprocessed/training-data/natural_language_inference/90,We then use the ( soft ) alignment to decompose the task into subproblems that are solved separately .,27,0,19
dataset/preprocessed/training-data/natural_language_inference/90,"Finally , the results of these subproblems are merged to produce the final classification .",28,0,15
dataset/preprocessed/training-data/natural_language_inference/90,"In addition , we optionally apply intra-sentence attention to endow the model with a richer encoding of substructures prior to the alignment step .",29,0,24
dataset/preprocessed/training-data/natural_language_inference/90,"Asymptotically our approach does the same total work as a vanilla LSTM encoder , while being trivially parallelizable across sentence length , which can allow for considerable speedups in low - latency settings .",30,0,34
dataset/preprocessed/training-data/natural_language_inference/90,"Empirical results on the SNLI corpus show that our approach achieves state - of - the - art results , while using almost an order of magnitude fewer parameters compared to complex LSTM - based approaches .",31,0,37
dataset/preprocessed/training-data/natural_language_inference/90,"Our method is motivated by the central role played by alignment in machine translation ) and previous approaches to sentence similarity modeling , natural language inference , and semantic parsing .",33,0,31
dataset/preprocessed/training-data/natural_language_inference/90,"The neural counterpart to alignment , attention , which is a key part of our approach , was originally proposed and has been predominantly used in conjunction with and to a lesser extent with CNNs .",34,0,36
dataset/preprocessed/training-data/natural_language_inference/90,"In contrast , our use of attention is purely based on word embeddings and our method essentially consists of feed - forward networks that operate largely independently of word order .",35,0,31
dataset/preprocessed/training-data/natural_language_inference/90,"Leta = ( a 1 , . . . , a a ) and b = ( b 1 , . . . , b b ) be the two input sentences of length a and b , respectively .",37,0,40
dataset/preprocessed/training-data/natural_language_inference/90,"We assume that each a i , b j ?",38,0,10
dataset/preprocessed/training-data/natural_language_inference/90,"Rd is a word embedding vector of dimension d and that each sentence is prepended with a "" NULL "" token .",39,0,22
dataset/preprocessed/training-data/natural_language_inference/90,"Our training data comes in the form of labeled pairs {a ( n ) , b ( n ) , y ( n ) } N n= 1 , where y ( n ) = ( y time , we receive a pair of sentences ( a , b ) and our goal is to predict the correct label y .",40,0,61
dataset/preprocessed/training-data/natural_language_inference/90,Input representation .,41,0,3
dataset/preprocessed/training-data/natural_language_inference/90,"Let ? = ( ? 1 , . . . ,? a ) and b = ( b 1 , . . . , b b ) denote the input representation of each fragment that is fed to subsequent steps of the algorithm .",42,0,44
dataset/preprocessed/training-data/natural_language_inference/90,The vanilla version of our model simply defines ? :=,43,0,10
dataset/preprocessed/training-data/natural_language_inference/90,a andb := b.,44,0,4
dataset/preprocessed/training-data/natural_language_inference/90,"With this input representation , our model does not make use of word order .",45,0,15
dataset/preprocessed/training-data/natural_language_inference/90,"However , we discuss an extension using intrasentence attention in Section 3.4 that uses a minimal amount of sequence information .",46,0,21
dataset/preprocessed/training-data/natural_language_inference/90,"The core model consists of the following three components ( see ) , which are trained jointly :",47,0,18
dataset/preprocessed/training-data/natural_language_inference/90,"First , soft - align the elements of ?",49,0,9
dataset/preprocessed/training-data/natural_language_inference/41,"BART : Denoising Sequence - to - Sequence Pre-training for Natural Language Generation , Translation , and Comprehension",2,1,18
dataset/preprocessed/training-data/natural_language_inference/41,"We present BART , a denoising autoencoder for pretraining sequence - to - sequence models .",4,1,16
dataset/preprocessed/training-data/natural_language_inference/41,"BART is trained by ( 1 ) corrupting text with an arbitrary noising function , and ( 2 ) learning a model to reconstruct the original text .",5,0,28
dataset/preprocessed/training-data/natural_language_inference/41,"It uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes .",6,0,51
dataset/preprocessed/training-data/natural_language_inference/41,"We evaluate a number of noising approaches , finding the best performance by both randomly shuffling the order of the original sentences and using a novel in - filling scheme , where spans of text are replaced with a single mask token .",7,0,43
dataset/preprocessed/training-data/natural_language_inference/41,BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks .,8,0,18
dataset/preprocessed/training-data/natural_language_inference/41,"It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD , achieves new stateof - the - art results on a range of abstractive dialogue , question answering , and summarization tasks , with gains of up to 6 ROUGE .",9,0,45
dataset/preprocessed/training-data/natural_language_inference/41,"BART also provides a 1.1 BLEU increase over a back - translation system for machine translation , with only target language pretraining .",10,0,23
dataset/preprocessed/training-data/natural_language_inference/41,"We also report ablation experiments that replicate other pretraining schemes within the BART framework , to better measure which factors most influence end - task performance .",11,0,27
dataset/preprocessed/training-data/natural_language_inference/41,Self - supervised methods have achieved remarkable success in a wide range of NLP tasks .,13,0,16
dataset/preprocessed/training-data/natural_language_inference/41,"The most successful approaches have been variants of masked language models , which are denoising autoencoders thatare trained to reconstruct text where a random subset of the words has been masked out .",14,0,33
dataset/preprocessed/training-data/natural_language_inference/41,"Recent work has shown gains by improving the distribution of masked tokens , the order in which masked tokens are predicted , and the available context for replacing masked tokens .",15,0,31
dataset/preprocessed/training-data/natural_language_inference/41,"However , these methods typically focus on particular types of end tasks ( e.g. span prediction , generation , etc. ) , limiting their applicability .",16,0,26
dataset/preprocessed/training-data/natural_language_inference/41,"In this paper , we present BART , which pre-trains a model combining Bidirectional and Auto - Regressive Transformers .",17,0,20
dataset/preprocessed/training-data/natural_language_inference/41,BART is a denoising autoencoder built with a sequence - to - sequence model that is applicable to a very wide range of end tasks .,18,0,26
dataset/preprocessed/training-data/natural_language_inference/41,"Pretraining has two stages ( 1 ) text is corrupted with an arbitrary noising function , and ( 2 ) a sequence - to - sequence model is learned to reconstruct the original text .",19,0,35
dataset/preprocessed/training-data/natural_language_inference/41,"BART uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes ( see .",20,0,53
dataset/preprocessed/training-data/natural_language_inference/41,"A key advantage of this setup is the noising flexibility ; arbitrary transformations can be applied to the original text , including changing its length .",21,0,26
dataset/preprocessed/training-data/natural_language_inference/41,"We evaluate a number of noising approaches , finding the best performance by both randomly shuffling the order of the original sentences and using a novel in - filling scheme , where arbitrary length spans of text ( including zero length ) are replaced with a single mask token .",22,0,50
dataset/preprocessed/training-data/natural_language_inference/41,This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about over all sentence length and make longer range transformations to the input .,23,0,35
dataset/preprocessed/training-data/natural_language_inference/41,BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks .,24,0,18
dataset/preprocessed/training-data/natural_language_inference/41,"It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD , and achieves new state - of - the - art results on a range of abstractive dialogue , question answering , and summarization tasks .",25,0,40
dataset/preprocessed/training-data/natural_language_inference/41,"For example , it improves performance by 6 ROUGE over previous work on XSum .",26,0,15
dataset/preprocessed/training-data/natural_language_inference/41,BART also opens up new ways of thinking about fine tuning .,27,0,12
dataset/preprocessed/training-data/natural_language_inference/41,We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers .,28,0,21
dataset/preprocessed/training-data/natural_language_inference/41,These layers are trained to essentially translate the foreign language to noised .,29,0,13
dataset/preprocessed/training-data/natural_language_inference/41,"English , by propagation through BART , thereby using BART as a pre-trained target - side language model .",30,0,19
dataset/preprocessed/training-data/natural_language_inference/41,This approach improves performance over a strong back - translation MT baseline by 1.1 BLEU on the WMT Romanian - English benchmark .,31,0,23
dataset/preprocessed/training-data/natural_language_inference/41,"To better understand these effects , we also report an ablation analysis that replicates other recently proposed training objectives .",32,0,20
dataset/preprocessed/training-data/natural_language_inference/41,"This study allows us to carefully control for a number of factors , including data and optimization parameters , which have been shown to be as important for over all performance as the selection of training objectives .",33,0,38
dataset/preprocessed/training-data/natural_language_inference/41,We find that BART exhibits the most consistently strong performance across the full range of tasks we consider .,34,0,19
dataset/preprocessed/training-data/natural_language_inference/41,BART is a denoising autoencoder that maps a corrupted document to the original document it was derived from .,36,0,19
dataset/preprocessed/training-data/natural_language_inference/41,It is implemented as a sequence - to - sequence model with a bidirectional encoder over corrupted text and a left - to - right autoregressive decoder .,37,0,28
dataset/preprocessed/training-data/natural_language_inference/41,"For pre-training , we optimize the negative log likelihood of the original document .",38,0,14
dataset/preprocessed/training-data/natural_language_inference/41,"BART uses the standard sequence - to - sequence Transformer architecture from , except , following GPT , that we modify ReLU activation functions to GeLUs and initialise parameters from N ( 0 , 0.02 ) .",40,0,37
dataset/preprocessed/training-data/natural_language_inference/41,"For our base model , we use 6 layers in the encoder and de-coder , and for our large model we use 12 layers in each .",41,0,27
dataset/preprocessed/training-data/natural_language_inference/41,"The architecture is closely related to that used in BERT , with the following differences : ( 1 ) each layer of the decoder additionally performs cross - attention over the final hidden layer of the encoder ( as in the transformer sequence - to - sequence model ) ; and ( 2 ) BERT uses an additional feed - forward network before wordprediction , which BART does not .",42,0,70
dataset/preprocessed/training-data/natural_language_inference/41,"In total , BART contains roughly 10 % more parameters than the equivalently sized BERT model .",43,0,17
dataset/preprocessed/training-data/natural_language_inference/41,BART is trained by corrupting documents and then optimizing a reconstruction loss - the cross - entropy between the decoder 's output and the original document .,45,0,27
dataset/preprocessed/training-data/natural_language_inference/41,"Unlike existing denoising autoencoders , which are tailored to specific noising schemes , BART allows us to apply any type of document corruption .",46,0,24
dataset/preprocessed/training-data/natural_language_inference/41,"In the extreme case , where all information about the source is lost , BART is equivalent to a language model .",47,0,22
dataset/preprocessed/training-data/natural_language_inference/41,"We experiment with several previously proposed and novel transformations , but we believe there is a significant potential for development of other new alternatives .",48,0,25
dataset/preprocessed/training-data/natural_language_inference/41,"The transformations we used are summarized below , and examples are shown in .",49,0,14
dataset/preprocessed/training-data/natural_language_inference/55,Question Answering with Subgraph Embeddings,2,1,5
dataset/preprocessed/training-data/natural_language_inference/55,This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features .,4,1,25
dataset/preprocessed/training-data/natural_language_inference/55,Our model learns low - dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .,5,0,27
dataset/preprocessed/training-data/natural_language_inference/55,"Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .",6,0,31
dataset/preprocessed/training-data/natural_language_inference/55,Teaching machines how to automatically answer questions asked in natural language on any topic or in any domain has always been along standing goal in Artificial Intelligence .,8,1,28
dataset/preprocessed/training-data/natural_language_inference/55,"With the rise of large scale structured knowledge bases ( KBs ) , this problem , known as open - domain question answering ( or open QA ) , boils down to being able to query efficiently such databases with natural language .",9,1,43
dataset/preprocessed/training-data/natural_language_inference/55,"These KBs , such as Freebase encompass huge ever growing amounts of information and ease open QA by organizing a great variety of answers in a structured format .",10,1,29
dataset/preprocessed/training-data/natural_language_inference/55,"However , the scale and the difficulty for machines to interpret natural language still makes this task a challenging problem .",11,0,21
dataset/preprocessed/training-data/natural_language_inference/55,"The state - of - the - art techniques in open QA can be classified into two main classes , namely , information retrieval based and semantic parsing based .",12,0,30
dataset/preprocessed/training-data/natural_language_inference/55,Information retrieval systems first retrieve a broad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid query and then use fine - grained detection heuristics to identify the exact answer .,13,0,42
dataset/preprocessed/training-data/natural_language_inference/55,"On the other hand , semantic parsing methods focus on the correct interpretation of the meaning of a question by a semantic parsing system .",14,0,25
dataset/preprocessed/training-data/natural_language_inference/55,A correct interpretation converts a question into the exact data base query that returns the correct answer .,15,0,18
dataset/preprocessed/training-data/natural_language_inference/55,"Interestingly , recent works have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large - scale regimes , while bypassing most of the annotation costs .",16,0,35
dataset/preprocessed/training-data/natural_language_inference/55,"Yet , even if both kinds of system have shown the ability to handle largescale KBs , they still require experts to hand - craft lexicons , grammars , and KB schema to be effective .",17,0,36
dataset/preprocessed/training-data/natural_language_inference/55,"This non-negligible human intervention might not be generic enough to conveniently scale up to new data bases with other schema , broader vocabularies or languages other than English .",18,0,29
dataset/preprocessed/training-data/natural_language_inference/55,"In contrast , proposed a framework for open QA requiring almost no human annotation .",19,0,15
dataset/preprocessed/training-data/natural_language_inference/55,"Despite being an interesting approach , this method is outperformed by other competing methods .",20,0,15
dataset/preprocessed/training-data/natural_language_inference/55,"introduced an embedding model , which learns low - dimensional vector representations of words and symbols ( such as KBs constituents ) and can be trained with even less supervision than the system of while being able to achieve better prediction performance .",21,0,43
dataset/preprocessed/training-data/natural_language_inference/55,"However , this approach is only compared with which operates in a simplified setting and has not been applied in more realistic conditions nor evaluated against the best performing methods .",22,0,31
dataset/preprocessed/training-data/natural_language_inference/55,"In this paper , we improve the model of by providing the ability to answer more complicated questions .",23,0,19
dataset/preprocessed/training-data/natural_language_inference/55,s The main contributions of the paper are : ( 1 ) a more sophisticated inference procedure that is both efficient and can consider longer paths ( considered only answers directly connected to the question in the graph ) ; and ( 2 ) a richer representation of the answers which encodes the question - answer path and surrounding subgraph of the KB .,24,0,64
dataset/preprocessed/training-data/natural_language_inference/55,"Our approach is competitive with the current state - of - the - art on the recent benchmark We-bQuestions without using any lexicon , rules or additional system for partof - speech tagging , syntactic or dependency parsing during training as most other systems do .",25,0,46
dataset/preprocessed/training-data/natural_language_inference/55,Our main motivation is to provide a system for open QA able to be trained as long as it has access to : ( 1 ) a training set of questions paired with answers and ( 2 ) a KB providing a structure among answers .,27,0,46
dataset/preprocessed/training-data/natural_language_inference/55,We suppose that all potential answers are entities in the KB and that questions are sequences of words that include one identified KB entity .,28,0,25
dataset/preprocessed/training-data/natural_language_inference/55,"When this entity is not given , plain string matching is used to perform entity resolution .",29,0,17
dataset/preprocessed/training-data/natural_language_inference/55,Smarter methods could be used but this is not our focus .,30,0,12
dataset/preprocessed/training-data/natural_language_inference/55,We use WebQuestions as our evaluation bemchmark .,31,0,8
dataset/preprocessed/training-data/natural_language_inference/55,"Since it contains few training samples , it is impossible to learn on it alone , and this section describes the various data sources that were used for training .",32,0,30
dataset/preprocessed/training-data/natural_language_inference/55,These are similar to those used in .,33,0,8
dataset/preprocessed/training-data/natural_language_inference/55,"This dataset is built using Freebase as the KB and contains 5,810 question - answer pairs .",35,0,17
dataset/preprocessed/training-data/natural_language_inference/55,"It was created by crawling questions through the Google Suggest API , and then obtaining answers using Amazon Mechanical Turk .",36,0,21
dataset/preprocessed/training-data/natural_language_inference/55,"We used the original split ( 3,778 examples for training and 2,032 for testing ) , and isolated 1 k questions from the training set for validation .",37,0,28
dataset/preprocessed/training-data/natural_language_inference/55,We-bQuestions is built on Freebase since all answers are defined as Freebase entities .,38,0,14
dataset/preprocessed/training-data/natural_language_inference/55,"In each question , we identified one Freebase entity using string matching between words of the question and entity names in Freebase .",39,0,23
dataset/preprocessed/training-data/natural_language_inference/55,"When the same string matches multiple entities , only the entity appearing in most triples , i.e. the most popular in Freebase , was kept .",40,0,26
dataset/preprocessed/training-data/natural_language_inference/55,"Example questions ( answers ) in the dataset include "" Where did Edgar Allan Poe died ? "" ( baltimore ) or "" What degrees did Barack Obama get ? "" ( bachelor of arts , juris doctor ) .",41,0,40
dataset/preprocessed/training-data/natural_language_inference/55,"Freebase Freebase is a huge and freely available data base of general facts ; data is organized as triplets ( subject , type1.type2.predicate , object ) , where two entities subject and object ( identified by mids ) are connected by the relation type type1. type2 . predicate .",42,0,49
dataset/preprocessed/training-data/natural_language_inference/55,"We used a subset , created by only keeping triples where one of the entities was appearing in either the WebQuestions training / validation set or in ClueWeb extractions .",43,0,30
dataset/preprocessed/training-data/natural_language_inference/55,We also removed all entities appearing less than 5 times and finally obtained a Freebase set containing 14M triples made of 2.2 M entities and 7 k relation types .,44,0,30
dataset/preprocessed/training-data/natural_language_inference/55,"Since the format of triples does not correspond to any structure one could find in language , we decided to transform them into automatically generated questions .",45,0,27
dataset/preprocessed/training-data/natural_language_inference/55,"Hence , all triples were converted into questions "" What is the predicate of the type2 subject ? "" ( using the mid of the subject ) with the answer being object .",46,0,33
dataset/preprocessed/training-data/natural_language_inference/55,"An example is "" What is the nationality of the person barack obama ? "" ( united states ) .",47,0,20
dataset/preprocessed/training-data/natural_language_inference/55,More examples and details are given in a longer version of this paper .,48,0,14
dataset/preprocessed/training-data/natural_language_inference/76,Published as a conference paper at ICLR 2016 REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION,2,0,14
dataset/preprocessed/training-data/natural_language_inference/76,"While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines , in practice their performance has been only slightly better than bag - of - word pair classifiers using only lexical similarity .",4,0,45
dataset/preprocessed/training-data/natural_language_inference/76,The only attempt so far to build an end - to - end differentiable neural network for entailment failed to outperform such a simple similarity classifier .,5,0,27
dataset/preprocessed/training-data/natural_language_inference/76,"In this paper , we propose a neural model that reads two sentences to determine entailment using long short - term memory units .",6,0,24
dataset/preprocessed/training-data/natural_language_inference/76,We extend this model with a word - by - word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases .,7,0,26
dataset/preprocessed/training-data/natural_language_inference/76,"Furthermore , we present a qualitative analysis of attention weights produced by this model , demonstrating such reasoning capabilities .",8,0,20
dataset/preprocessed/training-data/natural_language_inference/76,On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin .,9,0,24
dataset/preprocessed/training-data/natural_language_inference/76,It is the first generic end - to - end differentiable system that achieves state - of - the - art accuracy on a textual entailment dataset .,10,0,28
dataset/preprocessed/training-data/natural_language_inference/76,"Recently , Bowman et al. ( 2015 ) published the Stanford Natural Language Inference ( SNLI ) corpus accompanied by a neural network with long short - term memory units ( LSTM , Hochreiter and Schmidhuber , 1997 ) , which achieves an accuracy of 77.6 % for RTE on this dataset .",11,0,53
dataset/preprocessed/training-data/natural_language_inference/76,It is the first time a generic neural model without hand - crafted features got close to the accuracy of a simple lexicalized classifier with engineered features for RTE .,12,0,30
dataset/preprocessed/training-data/natural_language_inference/76,This can be explained by the high quality 1 ar Xiv : 1509.06664v4 [ cs.CL ],13,0,16
dataset/preprocessed/training-data/natural_language_inference/76,The ability to determine the semantic relationship between two sentences is an integral part of machines that understand and reason with natural language .,15,0,24
dataset/preprocessed/training-data/natural_language_inference/76,"Recognizing textual entailment ( RTE ) is the task of determining whether two natural language sentences are ( i ) contradicting each other , ( ii ) not related , or whether ( iii ) the first sentence ( called premise ) entails the second sentence ( called hypothesis ) .",16,1,51
dataset/preprocessed/training-data/natural_language_inference/76,"This task is important since many natural language processing ( NLP ) problems , such as information extraction , relation extraction , text summarization or machine translation , rely on it explicitly or implicitly and could benefit from more accurate RTE systems .",17,1,43
dataset/preprocessed/training-data/natural_language_inference/76,"State - of - the - art systems for RTE so far relied heavily on engineered NLP pipelines , extensive manual creation of features , as well as various external resources and specialized subcomponents such as negation detection ( e.g. .",18,0,41
dataset/preprocessed/training-data/natural_language_inference/76,"Despite the success of neural networks for paraphrase detection ( e.g. , end - to - end differentiable neural architectures failed to get close to acceptable performance for RTE due to the lack of large high - quality datasets .",19,0,40
dataset/preprocessed/training-data/natural_language_inference/76,"An end - to - end differentiable solution to RTE is desirable , since it avoids specific assumptions about the underlying language .",20,0,23
dataset/preprocessed/training-data/natural_language_inference/76,"In particular , there is no need for language features like part - of - speech tags or dependency parses .",21,0,21
dataset/preprocessed/training-data/natural_language_inference/76,"Furthermore , a generic sequence - to - sequence solution allows to extend the concept of capturing entailment across any sequential data , not only natural language .",22,0,28
dataset/preprocessed/training-data/natural_language_inference/76,and size of SNLI compared to the two orders of magnitude smaller and partly synthetic datasets so far used to evaluate RTE systems .,23,0,24
dataset/preprocessed/training-data/natural_language_inference/76,Bowman et al. 's LSTM encodes the premise and hypothesis as dense fixed - length vectors whose concatenation is subsequently used in a multi - layer perceptron ( MLP ) for classification .,24,0,33
dataset/preprocessed/training-data/natural_language_inference/76,"In contrast , we are proposing an attentive neural network that is capable of reasoning over entailments of pairs of words and phrases by processing the hypothesis conditioned on the premise .",25,0,32
dataset/preprocessed/training-data/natural_language_inference/76,"Our contributions are threefold : ( i ) We present a neural model based on LSTMs that reads two sentences in one go to determine entailment , as opposed to mapping each sentence independently into a semantic space ( 2.2 ) , ( ii ) We extend this model with a neural word - by - word attention mechanism to encourage reasoning over entailments of pairs of words and phrases ( 2.4 ) , and ( iii ) We provide a detailed qualitative analysis of neural attention for RTE ( 4.1 ) .",26,0,93
dataset/preprocessed/training-data/natural_language_inference/76,"Our benchmark LSTM achieves an accuracy of 80.9 % on SNLI , outperforming a simple lexicalized classifier tailored to RTE by 2.7 percentage points .",27,0,25
dataset/preprocessed/training-data/natural_language_inference/76,"An extension with word - by - word neural attention surpasses this strong benchmark LSTM result by 2.6 percentage points , setting a new state - of - the - art accuracy of 83.5 % for recognizing entailment on SNLI .",28,0,41
dataset/preprocessed/training-data/natural_language_inference/76,In this section we discuss LSTMs ( 2.1 ) and describe how they can be applied to RTE ( 2.2 ) .,30,0,22
dataset/preprocessed/training-data/natural_language_inference/76,We introduce an extension of an LSTM for RTE with neural attention ( 2.3 ) and word - by - word attention ( 2.4 ) .,31,0,26
dataset/preprocessed/training-data/natural_language_inference/76,"Finally , we show how such attentive models can easily be used for attending both ways : over the premise conditioned on the hypothesis and over the hypothesis conditioned on the premise ( 2.5 ) .",32,0,36
dataset/preprocessed/training-data/natural_language_inference/76,"Recurrent neural networks ( RNNs ) with long short - term memory ( LSTM ) units have been successfully applied to a wide range of NLP tasks , such as machine translation , constituency parsing , language modeling and recently .",34,0,41
dataset/preprocessed/training-data/natural_language_inference/76,"LSTMs encompass memory cells that can store information for a long period of time , as well as three types of gates that control the flow of information into and out of these cells : input gates ( Eq. 2 ) , forget gates ( Eq. 3 ) and output gates ( Eq. 4 ) .",35,0,56
dataset/preprocessed/training-data/natural_language_inference/76,"Given an input vector x tat time step t , the previous output h t?1 and cell state c t ?1 , an LSTM with hidden size k computes the next output ht and cell state ct as",36,0,38
dataset/preprocessed/training-data/natural_language_inference/76,"biases that parameterize the gates and transformations of the input , ?",38,0,12
dataset/preprocessed/training-data/natural_language_inference/76,denotes the element - wise application of the sigmoid function and the element - wise multiplication of two vectors .,39,0,20
dataset/preprocessed/training-data/natural_language_inference/76,RECOGNIZING TEXTUAL ENTAILMENT WITH LSTMS,40,0,5
dataset/preprocessed/training-data/natural_language_inference/76,LSTMs can readily be used for RTE by independently encoding the premise and hypothesis as dense vectors and taking their concatenation as input to an MLP classifier .,41,0,28
dataset/preprocessed/training-data/natural_language_inference/76,This demonstrates that LSTMs can learn semantically rich sentence representations thatare suitable for determining textual entailment .,42,0,17
dataset/preprocessed/training-data/natural_language_inference/76,"In contrast to learning sentence representations , we are interested in neural models that read both sentences to determine entailment , thereby reasoning over entailments of pairs of words and phrases .",44,0,32
dataset/preprocessed/training-data/natural_language_inference/76,shows the high - level structure of this model .,45,0,10
dataset/preprocessed/training-data/natural_language_inference/76,The premise ( left ) is read by an LSTM .,46,0,11
dataset/preprocessed/training-data/natural_language_inference/76,"A second LSTM with different parameters is reading a delimiter and the hypothesis ( right ) , but its memory state is initialized with the last cell state of the previous LSTM ( c 5 in the example ) , i.e. it is conditioned on the representation that the first LSTM built for the premise ( A ) .",47,0,59
dataset/preprocessed/training-data/natural_language_inference/76,"We use word2 vec vectors as word representations , which we do not optimize during training .",48,0,17
dataset/preprocessed/training-data/natural_language_inference/76,"Out-ofvocabulary words in the training set are randomly initialized by sampling values uniformly from ( ? 0.05 , 0.05 ) and optimized during training .",49,0,25
dataset/preprocessed/training-data/natural_language_inference/96,Swag : A Large - Scale Adversarial Dataset for Grounded Commonsense Inference,2,1,12
dataset/preprocessed/training-data/natural_language_inference/96,"Given a partial description like "" she opened the hood of the car , "" humans can reason about the situation and anticipate what might come next ( "" then , she examined the engine "" ) .",4,0,38
dataset/preprocessed/training-data/natural_language_inference/96,"In this paper , we introduce the task of grounded commonsense inference , unifying natural language inference and commonsense reasoning .",5,0,21
dataset/preprocessed/training-data/natural_language_inference/96,"We present Swag , a new dataset with 113 k multiple choice questions about a rich spectrum of grounded situations .",6,0,21
dataset/preprocessed/training-data/natural_language_inference/96,"To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets , we propose Adversarial Filtering ( AF ) , a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers , and using them to filter the data .",7,0,51
dataset/preprocessed/training-data/natural_language_inference/96,"To account for the aggressive adversarial filtering , we use state - of - theart language models to massively oversample a diverse set of potential counterfactuals .",8,0,27
dataset/preprocessed/training-data/natural_language_inference/96,"Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy ( 88 % ) , various competitive models struggle on our task .",9,0,28
dataset/preprocessed/training-data/natural_language_inference/96,We provide comprehensive analysis that indicates significant opportunities for future research .,10,0,12
dataset/preprocessed/training-data/natural_language_inference/96,"When we read a story , we bring to it a large body of implicit knowledge about the physical world .",12,0,21
dataset/preprocessed/training-data/natural_language_inference/96,"For instance , given the context "" on stage , a woman takes a seat at the piano , "" shown in , we can easily infer what the situation might look like : a woman is giving a piano performance , with a crowd watching her .",13,0,48
dataset/preprocessed/training-data/natural_language_inference/96,We can furthermore infer her likely next action : she will most likely set her fingers on the piano keys and start playing .,14,0,24
dataset/preprocessed/training-data/natural_language_inference/96,"This type of natural language inference requires commonsense reasoning , substantially broadening the scope of prior work that focused primarily on On stage , a woman takes a seat at the piano .",15,0,33
dataset/preprocessed/training-data/natural_language_inference/96,She a ) sits on a bench as her sister plays with the doll .,16,0,15
dataset/preprocessed/training-data/natural_language_inference/96,b ) smiles with someone as the music plays .,17,0,10
dataset/preprocessed/training-data/natural_language_inference/96,"c ) is in the crowd , watching the dancers .",18,0,11
dataset/preprocessed/training-data/natural_language_inference/96,d ) nervously sets her fingers on the keys .,19,0,10
dataset/preprocessed/training-data/natural_language_inference/96,A girl is going across a set of monkey bars .,20,0,11
dataset/preprocessed/training-data/natural_language_inference/96,She a ) jumps up across the monkey bars .,21,0,10
dataset/preprocessed/training-data/natural_language_inference/96,b) struggles onto the monkey bars to grab her head .,22,0,11
dataset/preprocessed/training-data/natural_language_inference/96,c ) gets to the end and stands on a wooden plank .,23,0,13
dataset/preprocessed/training-data/natural_language_inference/96,d ) jumps up and does aback flip .,24,0,9
dataset/preprocessed/training-data/natural_language_inference/96,The woman is now blow drying the dog .,25,0,9
dataset/preprocessed/training-data/natural_language_inference/96,The dog a ) is placed in the kennel next to a woman 's feet .,26,0,16
dataset/preprocessed/training-data/natural_language_inference/96,b ) washes her face with the shampoo .,27,0,9
dataset/preprocessed/training-data/natural_language_inference/96,"c) walks into frame and walks towards the dog . d) tried to cut her face , so she is trying to do something very close to her face .",28,0,30
dataset/preprocessed/training-data/natural_language_inference/96,linguistic entailment .,29,0,3
dataset/preprocessed/training-data/natural_language_inference/96,"Whereas the dominant entailment paradigm asks if two natural language sentences ( the ' premise ' and the ' hypothesis ' ) describe the same set of possible worlds , here we focus on whether a ( multiple - choice ) ending describes a possible ( future ) world that can be anticipated from the situation described in the premise , even when it is not strictly entailed .",30,0,69
dataset/preprocessed/training-data/natural_language_inference/96,"Making such inference necessitates a rich understanding about everyday physical situations , including object affordances and frame semantics .",31,0,19
dataset/preprocessed/training-data/natural_language_inference/96,A first step toward grounded commonsense inference with today 's deep learning machinery is to create a large - scale dataset .,32,0,22
dataset/preprocessed/training-data/natural_language_inference/96,"However , recent work has shown that human - written datasets are susceptible to annotation artifacts : unintended stylistic patterns that give out clues for the gold labels .",33,0,29
dataset/preprocessed/training-data/natural_language_inference/96,"As a result , models trained on such datasets with hu-man biases run the risk of over-estimating the actual performance on the underlying task , and are vulnerable to adversarial or out - of - domain examples .",34,0,38
dataset/preprocessed/training-data/natural_language_inference/96,"In this paper , we introduce Adversarial Filtering ( AF ) , a new method to automatically detect and reduce stylistic artifacts .",35,0,23
dataset/preprocessed/training-data/natural_language_inference/96,We use this method to construct Swag : an adversarial dataset with 113 k multiple - choice questions .,36,0,19
dataset/preprocessed/training-data/natural_language_inference/96,"We start with pairs of temporally adjacent video captions , each with a context and a follow - up event that we know is physically possible .",37,0,27
dataset/preprocessed/training-data/natural_language_inference/96,We then use a state - of - theart language model fine - tuned on this data to massively oversample a diverse set of possible negative sentence endings ( or counterfactuals ) .,38,0,33
dataset/preprocessed/training-data/natural_language_inference/96,"Next , we filter these candidate endings aggressively and adversarially using a committee of trained models to obtain a population of de-biased endings with similar stylistic features to the real ones .",39,0,32
dataset/preprocessed/training-data/natural_language_inference/96,"Finally , these filtered counterfactuals are validated by crowd workers to further ensure data quality .",40,0,16
dataset/preprocessed/training-data/natural_language_inference/96,"Extensive empirical results demonstrate unique contributions of our dataset , complementing existing datasets for natural langauge inference ( NLI ) and commonsense reasoning .",41,0,24
dataset/preprocessed/training-data/natural_language_inference/96,"First , our dataset poses a new challenge of grounded commonsense inference that is easy for humans ( 88 % ) while hard for current state - of the - art NLI models ( < 60 % ) .",42,0,39
dataset/preprocessed/training-data/natural_language_inference/96,"Second , our proposed adversarial filtering methodology allows for cost-effective construction of a large - scale dataset while substantially reducing known annotation artifacts .",43,0,24
dataset/preprocessed/training-data/natural_language_inference/96,"The generality of adversarial filtering allows it to be applied to build future datasets , ensuring that they serve as reliable benchmarks .",44,0,23
dataset/preprocessed/training-data/natural_language_inference/96,2 Swag :,45,0,3
dataset/preprocessed/training-data/natural_language_inference/96,Our new dataset,46,0,3
dataset/preprocessed/training-data/natural_language_inference/96,"We introduce a new dataset for studying physically grounded commonsense inference , called Swag .",47,0,15
dataset/preprocessed/training-data/natural_language_inference/96,Our task is to predict which event is most likely to occur next in a video .,49,0,17
dataset/preprocessed/training-data/natural_language_inference/11,Dynamic Integration of Background Knowledge in Neural NLU Systems,2,1,9
dataset/preprocessed/training-data/natural_language_inference/11,"Common- sense and background knowledge is required to understand natural language , but in most neural natural language understanding ( NLU ) systems , this knowledge must be acquired from training corpora during learning , and then it is static at test time .",4,1,44
dataset/preprocessed/training-data/natural_language_inference/11,We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models .,5,0,17
dataset/preprocessed/training-data/natural_language_inference/11,A general - purpose reading module reads background knowledge in the form of freetext statements ( together with task - specific text inputs ) and yields refined word representations to a task - specific NLU architecture that reprocesses the task inputs with these representations .,6,0,45
dataset/preprocessed/training-data/natural_language_inference/11,Experiments on document question answering ( DQA ) and recognizing textual entailment ( RTE ) demonstrate the effectiveness and flexibility of the approach .,7,0,24
dataset/preprocessed/training-data/natural_language_inference/11,Analysis shows that our model learns to exploit knowledge in a semantically appropriate way .,8,0,15
dataset/preprocessed/training-data/natural_language_inference/11,"Sungjin Ahn , Heeyoul Choi , Tanel Prnamaa , and Yoshua Bengio. 2016 .",9,0,14
dataset/preprocessed/training-data/natural_language_inference/11,A neural knowledge language model .,10,0,6
dataset/preprocessed/training-data/natural_language_inference/11,"Understanding natural language depends crucially on common - sense and background knowledge , for example , knowledge about what concepts are expressed by the words being read , and what relations hold between these concepts ( relational knowledge ) .",13,0,40
dataset/preprocessed/training-data/natural_language_inference/11,"As a simple illustration , if an agent needs to understand that the statement "" King Farouk signed his abdication "" is entailed by "" King Farouk was exiled to France in 1952 , after signing his resignation "" , it must know ( among other things ) that abdication means resignation of a king .",14,0,56
dataset/preprocessed/training-data/natural_language_inference/11,"In most neural natural language understanding ( NLU ) systems , the requisite background knowl - edge is implicitly encoded in the models ' parameters .",15,0,26
dataset/preprocessed/training-data/natural_language_inference/11,"That is , what background knowledge is present has been learned from task supervision and also by pre-training word embeddings ( where distributional properties correlate with certain kinds of useful background knowledge , such as semantic relatedness ) .",16,0,39
dataset/preprocessed/training-data/natural_language_inference/11,"However , acquisition of background knowledge from static training corpora is limiting for two reasons .",17,0,16
dataset/preprocessed/training-data/natural_language_inference/11,"First , it is unreasonable to expect that all background knowledge that could be important for solving an NLU task can be extracted from a limited amount of training data .",18,0,31
dataset/preprocessed/training-data/natural_language_inference/11,"Second , as the world changes , the facts that may influence how a text is understood will likewise change .",19,0,21
dataset/preprocessed/training-data/natural_language_inference/11,"In short : building suitably large corpora to capture all relevant information , and keeping the corpus and derived models up to date with changes to the world would be impractical .",20,0,32
dataset/preprocessed/training-data/natural_language_inference/11,"In this paper , we develop a new architecture for dynamically incorporating external background knowledge in NLU models .",21,0,19
dataset/preprocessed/training-data/natural_language_inference/11,"Rather than relying only on static knowledge implicitly present in the training data , supplementary knowledge is retrieved from external knowledge sources ( in this paper , ConceptNet and Wikipedia ) to assist with understanding text inputs .",22,0,38
dataset/preprocessed/training-data/natural_language_inference/11,"Since NLU systems must already read and understand text inputs , we assume that background knowledge will likewise be provided in text form ( 2 ) .",23,0,27
dataset/preprocessed/training-data/natural_language_inference/11,The retrieved supplementary texts are read together with the task inputs by an initial reading module whose outputs are contextually refined word embeddings ( 3 ) .,24,0,27
dataset/preprocessed/training-data/natural_language_inference/11,These refined embeddings are then used as input to a task - specific NLU architecture ( any architecture that reads text as a sequence of word embeddings can be used here ) .,25,0,33
dataset/preprocessed/training-data/natural_language_inference/11,"The initial reading module and the task module are learnt jointly , end - to - end .",26,0,18
dataset/preprocessed/training-data/natural_language_inference/11,We experiment with several different datasets on the tasks of document question answering ( DQA ) and recognizing textual entailment evaluating the impact of our proposed solution with both basic task architectures and a sophisticated task architecture for RTE ( 4 ) .,27,0,43
dataset/preprocessed/training-data/natural_language_inference/11,We find that our embedding refinement strategy is effective ( 5 ) .,28,0,13
dataset/preprocessed/training-data/natural_language_inference/11,"On four competitive benchmarks , we show that refinement helps .",29,0,11
dataset/preprocessed/training-data/natural_language_inference/11,"First , simply refining the embeddings just using the context ( and no additional background information ) can improve performance significantly , but adding background knowledge helps further .",30,0,29
dataset/preprocessed/training-data/natural_language_inference/11,"Our results are competitive with the best systems , achieving a new state of the art on the recent TriviaQA benchmarks .",31,0,22
dataset/preprocessed/training-data/natural_language_inference/11,"Our success on this task is especially noteworthy because the task - specific architecture is a simple reading architecture , in particular a single layer BiLSTM with a feed - forward neural network for span prediction .",32,0,37
dataset/preprocessed/training-data/natural_language_inference/11,"Finally , we provide an analysis demonstrating that our systems are able to exploit background knowledge in a semantically appropriate manner ( 5.3 ) .",33,0,25
dataset/preprocessed/training-data/natural_language_inference/11,"It includes , for instance , an experiment showing that our system is capable of making appropriate counterfactual inferences when provided with "" alternative facts "" .",34,0,27
dataset/preprocessed/training-data/natural_language_inference/11,External Knowledge as Supplementary,35,0,4
dataset/preprocessed/training-data/natural_language_inference/11,"Knowledge resources make information that could potentially be useful for improving NLU available in a variety different formats , such as natural language text , ( subject , predicate , object ) triples , relational data bases , and other structured formats .",37,0,43
dataset/preprocessed/training-data/natural_language_inference/11,"Rather than tailoring our solution to a particular structured representation , we assume that all supplementary information either already exists in natural language statements ( e.g. , encyclopedias ) or can easily be recoded as natural language .",38,0,38
dataset/preprocessed/training-data/natural_language_inference/11,"Furthermore , while mapping from unstructured to structured representations is hard , the inverse problem is easy .",39,0,18
dataset/preprocessed/training-data/natural_language_inference/11,"For example , given a triple ( abdication , ISA , resignation ) we can construct the free - text assertion "" Abdication is a resignation . "" using simple rules .",40,0,32
dataset/preprocessed/training-data/natural_language_inference/11,"Finally , the freetext format means that knowledge that exists only in unstructured text form such as encyclopedic knowledge ( e.g. , Wikipedia ) is usable by our system .",41,0,30
dataset/preprocessed/training-data/natural_language_inference/11,"An important question that remains to be answered is : given some text that is to be understood , what supplementary knowledge should be incorporated ?",42,0,26
dataset/preprocessed/training-data/natural_language_inference/11,"The retrieval and preparation of contextually relevant information from knowledge sources is a complex research topic by itself , and there are several statistical and more recently neural approaches as well as approaches based on reinforcement learning .",43,0,38
dataset/preprocessed/training-data/natural_language_inference/11,"Rather than learning both how to incorporate relevant information and which information is relevant , we use a heuristic retrieval mechanism ( 4 ) and focus on the integration model .",44,0,31
dataset/preprocessed/training-data/natural_language_inference/11,"In the next section , we turn to the question of how to leverage the retrieved supplementary knowledge ( encoded as text ) in a NLU system .",45,0,28
dataset/preprocessed/training-data/natural_language_inference/11,Refining Word Embeddings by Reading,46,0,5
dataset/preprocessed/training-data/natural_language_inference/11,Virtually every NLU task - from document classification to translation to question answeringshould in theory be able to benefit from supplementary knowledge .,47,0,23
dataset/preprocessed/training-data/natural_language_inference/11,"While one could develop custom architectures for each task so as to read supplementary inputs , we would like ours to augment any existing NLU task architectures with the ability to read relevant information with minimal effort .",48,0,38
dataset/preprocessed/training-data/natural_language_inference/11,"To realize this goal , we adopt the strategy of refining word embeddings ; that is , we replace static word embeddings with embeddings thatare functions of the task inputs and any supplementary inputs .",49,0,35
dataset/preprocessed/training-data/natural_language_inference/36,Explicit Contextual Semantics for Text Comprehension,2,1,6
dataset/preprocessed/training-data/natural_language_inference/36,"Who did what to whom is a major focus in natural language understanding , which is right the aim of semantic role labeling ( SRL ) task .",4,0,28
dataset/preprocessed/training-data/natural_language_inference/36,"Despite of sharing a lot of processing characteristics and even task purpose , it is surprisingly that jointly considering these two related tasks was never formally reported in previous work .",5,0,31
dataset/preprocessed/training-data/natural_language_inference/36,Thus this paper makes the first attempt to let SRL enhance text comprehension and inference through specifying verbal predicates and their corresponding semantic roles .,6,0,25
dataset/preprocessed/training-data/natural_language_inference/36,"In terms of deep learning models , our embeddings are enhanced by explicit contextual semantic role labels for more fine - grained semantics .",7,0,24
dataset/preprocessed/training-data/natural_language_inference/36,We show that the salient labels can be conveniently added to existing models and significantly improve deep learning models in challenging text comprehension tasks .,8,0,25
dataset/preprocessed/training-data/natural_language_inference/36,Extensive experiments on benchmark machine reading comprehension and inference datasets verify that the proposed semantic learning helps our system reach new state - of - the - art over strong baselines which have been enhanced by well pretrained language models from the latest progress .,9,0,45
dataset/preprocessed/training-data/natural_language_inference/36,"Text comprehension is challenging for it requires computers to read and understand natural language texts to answer questions or make inference , which is indispensable for advanced context - oriented dialogue and interactive systems .",11,0,35
dataset/preprocessed/training-data/natural_language_inference/36,"This paper focuses on two core text comprehension ( TC ) tasks , machine reading comprehension ( MRC ) and textual entailment ( TE ) .",12,1,26
dataset/preprocessed/training-data/natural_language_inference/36,One of the intrinsic challenges for text comprehension is semantic learning .,13,0,12
dataset/preprocessed/training-data/natural_language_inference/36,"Though deep learning has been applied to natural language processing ( NLP ) tasks with remarkable performance , recent studies have found deep learning models might not really understand the natural language texts and vulnerably suffer from adversarial attacks .",14,0,40
dataset/preprocessed/training-data/natural_language_inference/36,"Typically , an MRC model pays great attention to non-significant words and ignores important ones .",15,0,16
dataset/preprocessed/training-data/natural_language_inference/36,"To help model better understand natural language , we are motivated to discover an effective way to distill semantics inside the input sentence explicitly , such as semantic role labeling , instead of completely relying on uncontrollable model parameter learning or manual pruning .",16,0,44
dataset/preprocessed/training-data/natural_language_inference/36,"Semantic role labeling ( SRL ) is a shallow semantic parsing task aiming to discover who did what to whom , when and why , providing explicit contextual semantics , which naturally matches the task target of text comprehension .",17,0,40
dataset/preprocessed/training-data/natural_language_inference/36,"For MRC , questions are usually formed with who , what , how , when and why , whose predicate - argument relationship that is supposed to be from SRL is of the same importance as well .",18,0,38
dataset/preprocessed/training-data/natural_language_inference/36,"Be - sides , explicit semantics has been proved to be beneficial to a wide range of NLP tasks , including discourse relation sense classification , machine translation and question answering .",19,0,32
dataset/preprocessed/training-data/natural_language_inference/36,All the previous successful work indicates that explicit contextual semantics may hopefully help into reading comprehension and inference tasks .,20,0,20
dataset/preprocessed/training-data/natural_language_inference/36,"Some work studied question answering ( QA ) driven SRL , like QA - SRL parsing .",21,0,17
dataset/preprocessed/training-data/natural_language_inference/36,They focus on detecting argument spans for a predicate and generating questions to annotate the semantic relationship .,22,0,18
dataset/preprocessed/training-data/natural_language_inference/36,"However , our task is quite different .",23,0,8
dataset/preprocessed/training-data/natural_language_inference/36,"In QA - SRL , the focus is commonly simple and short factoid questions thatare less related to the context , let alone making inference .",24,0,26
dataset/preprocessed/training-data/natural_language_inference/36,"Actually , text comprehension and inference are quite challenging tasks in NLP , requiring to dig the deep semantics between the document and comprehensive question which are usually raised or re-written by humans , instead of shallow argument alignment around the same predicate in QA - SRL .",25,0,48
dataset/preprocessed/training-data/natural_language_inference/36,"In this work , to alleviate such an obvious shortcoming about semantics , we make attempt to explore integrative models for finer - grained text comprehension and inference .",26,0,29
dataset/preprocessed/training-data/natural_language_inference/36,"In this work , we propose a semantics enhancement framework for TC tasks , which boosts the strong baselines effectively .",27,0,21
dataset/preprocessed/training-data/natural_language_inference/36,We implement an easy and feasible scheme to integrate semantic signals in downstream neural models in end - to - end manner to boost strong baselines effectively .,28,0,28
dataset/preprocessed/training-data/natural_language_inference/36,An example about how contextual semantics helps MRC is illustrated in .,29,0,12
dataset/preprocessed/training-data/natural_language_inference/36,A series of detailed case studies are employed to analyze the robustness of the semantic role labeler .,30,0,18
dataset/preprocessed/training-data/natural_language_inference/36,"To our best knowledge , our work is the first attempt to apply explicit contextual semantics for text comprehension tasks , which have been ignored in previous works for a longtime .",31,0,32
dataset/preprocessed/training-data/natural_language_inference/36,The rest of this paper is organized as follows .,32,0,10
dataset/preprocessed/training-data/natural_language_inference/36,The next section reviews the related work .,33,0,8
dataset/preprocessed/training-data/natural_language_inference/36,Section 3 will demonstrate our semantic learning framework and implementation .,34,0,11
dataset/preprocessed/training-data/natural_language_inference/36,"Task details and experimental results are reported in Section 4 , followed by case studies and analysis in Section 5 and conclusion in Section 6 .",35,0,26
dataset/preprocessed/training-data/natural_language_inference/36,"As a challenging task in NLP , text comprehension is one of the key problems in artificial intelligence , which aims to read and comprehend a given text , and then answer questions or make inference based on it .",38,0,40
dataset/preprocessed/training-data/natural_language_inference/36,These tasks require a comprehensive understanding of natural languages and the ability to do further inference and reasoning .,39,0,19
dataset/preprocessed/training-data/natural_language_inference/36,"We focus on two types of text comprehension , document - based questionanswering ) and textual entailment ( .",40,0,19
dataset/preprocessed/training-data/natural_language_inference/36,"Textual entailment aims for a deep understanding of text and reasoning , which shares the similar genre of machine reading comprehension , though the task formations are slightly different .",41,0,30
dataset/preprocessed/training-data/natural_language_inference/36,"In the last decade , the MRC tasks have evolved from the early cloze - style test to spanbased answer extraction from passage .",42,0,24
dataset/preprocessed/training-data/natural_language_inference/36,The former has restrictions that each answer should be a single word in the document and the original sentence without the answer part is taken as the query .,43,0,29
dataset/preprocessed/training-data/natural_language_inference/36,"For the span - based one , the query is formed as questions in natural language whose answers are spans of texts .",44,0,23
dataset/preprocessed/training-data/natural_language_inference/36,"Various attentive models have been employed for text representation and relation discovery , including Attention Sum Reader , Gated attention Reader and Self - matching Network Passage",45,0,27
dataset/preprocessed/training-data/natural_language_inference/36,"There are three major types of rock : igneous , sedimentary , and metamorphic .",46,0,15
dataset/preprocessed/training-data/natural_language_inference/36,"The rock cycle is an important concept in geology which illustrates the relationships between these three types of rock , and magma .",47,0,23
dataset/preprocessed/training-data/natural_language_inference/36,"When a rock crystallizes from melt ( magma and / or lava ) , it is an igneous rock .",48,0,20
dataset/preprocessed/training-data/natural_language_inference/36,"This rock can be weathered and eroded , and then redeposited and lithified into a sedimentary rock , or be turned into a metamorphic rock due to heat and pressure that change the mineral content of the rock which gives it a characteristic fabric .",49,0,45
dataset/preprocessed/training-data/natural_language_inference/78,"Compare , Compress and Propagate : Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",2,1,16
dataset/preprocessed/training-data/natural_language_inference/78,This paper presents a new deep learning architecture for Natural Language Inference ( NLI ) .,4,1,16
dataset/preprocessed/training-data/natural_language_inference/78,"Firstly , we introduce a new architecture where alignment pairs are compared , compressed and then propagated to upper layers for enhanced representation learning .",5,0,25
dataset/preprocessed/training-data/natural_language_inference/78,"Secondly , we adopt factorization layers for efficient and expressive compression of alignment vectors into scalar features , which are then used to augment the base word representations .",6,0,29
dataset/preprocessed/training-data/natural_language_inference/78,"The design of our approach is aimed to be conceptually simple , compact and yet powerful .",7,0,17
dataset/preprocessed/training-data/natural_language_inference/78,"We conduct experiments on three popular benchmarks , SNLI , MultiNLI and SciTail , achieving competitive performance on all .",8,0,20
dataset/preprocessed/training-data/natural_language_inference/78,A lightweight parameterization of our model also enjoys a ?,9,0,10
dataset/preprocessed/training-data/natural_language_inference/78,"3 times reduction in parameter size compared to the existing state - of - the - art models , e.g. , ESIM and DIIN , while maintaining competitive performance .",10,0,30
dataset/preprocessed/training-data/natural_language_inference/78,"Additionally , visual analysis shows that our propagated features are highly interpretable .",11,0,13
dataset/preprocessed/training-data/natural_language_inference/78,Natural Language Inference ( NLI ) is a pivotal and fundamental task in language understanding and artificial intelligence .,13,0,19
dataset/preprocessed/training-data/natural_language_inference/78,"More concretely , given a premise and hypothesis , NLI aims to detect whether the latter entails or contradicts the former .",14,1,22
dataset/preprocessed/training-data/natural_language_inference/78,"As such , NLI is also commonly known as .",15,0,10
dataset/preprocessed/training-data/natural_language_inference/78,NLI is known to be a significantly challenging task for machines whose success often depends on a wide repertoire of reasoning techniques .,16,0,23
dataset/preprocessed/training-data/natural_language_inference/78,"In recent years , we observe a steep improvement in NLI systems , largely contributed by the release of the largest publicly available corpus for NLI - the Stanford Natural Language Inference ( SNLI ) corpus which comprises 570K hand labeled sentence pairs .",17,0,44
dataset/preprocessed/training-data/natural_language_inference/78,"This has improved the feasibility of training complex neural models , given the fact that neural models often require a relatively large amount of training data .",18,0,27
dataset/preprocessed/training-data/natural_language_inference/78,"Highly competitive neural models for NLI are mostly based on soft - attention alignments , popularized by .",19,0,18
dataset/preprocessed/training-data/natural_language_inference/78,The key idea is to learn an alignment of sub-phrases in both sentences and learn to compare the relationship between them .,20,0,22
dataset/preprocessed/training-data/natural_language_inference/78,Standard feed - forward neural networks are commonly used to model similarity between aligned ( decomposed ) sub-phrases and then aggregated into the final prediction layers .,21,0,27
dataset/preprocessed/training-data/natural_language_inference/78,Alignment between sentences has become a staple technique in NLI research and many recent state - of - the - art models such as the Enhanced Sequential Inference Model ( ESIM ) also incorporate the alignment strategy .,22,0,38
dataset/preprocessed/training-data/natural_language_inference/78,"The difference here is that ESIM considers a nonparameterized comparison scheme , i.e. , concatenating the subtraction and element - wise product of aligned sub-phrases , along with two original sub-phrases , into the final comparison vector .",23,0,38
dataset/preprocessed/training-data/natural_language_inference/78,A bidirectional LSTM is then used to aggregate the compared alignment vectors .,24,0,13
dataset/preprocessed/training-data/natural_language_inference/78,This paper presents a new neural model for NLI .,25,0,10
dataset/preprocessed/training-data/natural_language_inference/78,There are several new novel components in our work .,26,0,10
dataset/preprocessed/training-data/natural_language_inference/78,"Firstly , we propose a compare , compress and propagate ( Com Prop ) architecture where compressed alignment features are propagated to upper layers ( such as a RNN - based encoder ) for enhancing representation learning .",27,0,38
dataset/preprocessed/training-data/natural_language_inference/78,"Secondly , in order to achieve an efficient propagation of alignment features , we propose alignment factorization layers to reduce each alignment vector to a single scalar valued feature .",28,0,30
dataset/preprocessed/training-data/natural_language_inference/78,"Each scalar valued feature is used to augment the base word representation , allowing the subsequent RNN encoder layers to benefit from not only global but also cross sentence information .",29,0,31
dataset/preprocessed/training-data/natural_language_inference/78,There are several major advantages to our proposed architecture .,30,0,10
dataset/preprocessed/training-data/natural_language_inference/78,"Firstly , our model is relatively compact , i.e. , we compress alignment feature vectors and augment them to word representations instead .",31,0,23
dataset/preprocessed/training-data/natural_language_inference/78,This is to avoid large alignment ( or match ) vectors being propagated across the network .,32,0,17
dataset/preprocessed/training-data/natural_language_inference/78,"As a result , our model is more parameter efficient compared to ESIM since the width of the middle layers of the network is now much smaller .",33,0,28
dataset/preprocessed/training-data/natural_language_inference/78,"To the best of our knowledge , this is the first work that explicitly employs such a paradigm .",34,0,19
dataset/preprocessed/training-data/natural_language_inference/78,"Secondly , the explicit usage of compression enables improved interpretabilty since each alignment pair is compressed to a scalar and hence , can be easily visualised .",35,0,27
dataset/preprocessed/training-data/natural_language_inference/78,"Previous models such as ESIM use subtractive operations on alignment vectors , edging on the intuition that these vectors represent contradiction .",36,0,22
dataset/preprocessed/training-data/natural_language_inference/78,Our model is capable of visually demonstrating this phenomena .,37,0,10
dataset/preprocessed/training-data/natural_language_inference/78,"As such , our design choice enables a new way of deriving insight from neural NLI models .",38,0,18
dataset/preprocessed/training-data/natural_language_inference/78,"Thirdly , the alignment factorization layer is expressive and powerful , combining ideas from standard machine learning literature with modern neural NLI models .",39,0,24
dataset/preprocessed/training-data/natural_language_inference/78,"The factorization layer tries to decompose the alignment vector ( constructed from the variations of a ? b , ab and [ a ; b ] ) , learning higher - order feature interactions between each compared alignment .",40,0,39
dataset/preprocessed/training-data/natural_language_inference/78,"In other words , it models the second - order ( pairwise ) interactions between each feature in every alignment vector using factorized parameters , allowing more expressive comparison to be made over traditional fully - connected layers ( FC ) .",41,0,42
dataset/preprocessed/training-data/natural_language_inference/78,"Moreover , factorization - based models are also known to be able to model low - rank structure and reduce risks of overfitting .",42,0,24
dataset/preprocessed/training-data/natural_language_inference/78,The effectiveness of the factorization alignment over alternative baselines such as feed - forward neural networks is confirmed by early experiments .,43,0,22
dataset/preprocessed/training-data/natural_language_inference/78,The major contributions of this work are summarized as follows :,44,0,11
dataset/preprocessed/training-data/natural_language_inference/78,"We introduce a Compare , Compress and Propagate ( ComProp ) architecture for NLI .",45,0,15
dataset/preprocessed/training-data/natural_language_inference/78,The key idea is to use the myriad of generated comparison vectors for augmentation of the base word representation instead of simply aggregating them for prediction .,46,0,27
dataset/preprocessed/training-data/natural_language_inference/78,"Subsequently , a standard compositional encoder can then be used to learn representations from the augmented word representations .",47,0,19
dataset/preprocessed/training-data/natural_language_inference/78,We show that we are able to derive meaningful insight from visualizing these augmented features .,48,0,16
dataset/preprocessed/training-data/natural_language_inference/78,"For the first time , we adopt expressive factorization layers to model the relationships between soft - aligned sub- phrases of sentence pairs .",49,0,24
dataset/preprocessed/training-data/natural_language_inference/14,CODAH : An Adversarially - Authored Question Answering Dataset for Common Sense,2,1,12
dataset/preprocessed/training-data/natural_language_inference/14,"Commonsense reasoning is a critical AI capability , but it is difficult to construct challenging datasets that test commonsense .",4,1,20
dataset/preprocessed/training-data/natural_language_inference/14,"Recent neural question answering systems , based on large pre-trained models of language , have already achieved near - human - level performance on commonsense knowledge benchmarks .",5,0,28
dataset/preprocessed/training-data/natural_language_inference/14,"These systems do not possess human - level commonsense , but are able to exploit limitations of the datasets to achieve human - level scores .",6,0,26
dataset/preprocessed/training-data/natural_language_inference/14,"We introduce the CODAH dataset , an adversarially - constructed evaluation dataset for testing commonsense .",8,0,16
dataset/preprocessed/training-data/natural_language_inference/14,"CODAH forms a challenging extension to the recently - proposed SWAG dataset , which tests commonsense knowledge using sentence - completion questions that describe situations observed in video .",9,0,29
dataset/preprocessed/training-data/natural_language_inference/14,"To produce a more difficult dataset , we introduce a novel procedure for question acquisition in which workers author questions designed to target weaknesses of state - of the - art neural question answering systems .",10,0,36
dataset/preprocessed/training-data/natural_language_inference/14,Workers are rewarded for submissions that models fail to answer correctly both before and after fine - tuning ( in cross -validation ) .,11,0,24
dataset/preprocessed/training-data/natural_language_inference/14,We create 2.8 k questions via this procedure and evaluate the performance of multiple state - of - the - art question answering systems on our dataset .,12,0,28
dataset/preprocessed/training-data/natural_language_inference/14,"We observe a significant gap between human performance , which is 95.3 % , and the performance of the best baseline accuracy of 67.5 % by the BERT - Large model .",13,0,32
dataset/preprocessed/training-data/natural_language_inference/14,Enabling commonsense reasoning in machines is a longstanding challenge in AI .,15,0,12
dataset/preprocessed/training-data/natural_language_inference/14,The rise of datadriven methods has led to interest in developing large datasets for commonsense reasoning over text .,16,1,19
dataset/preprocessed/training-data/natural_language_inference/14,The Situations With Adversarial Generations ( SWAG ) dataset introduced a large - scale benchmark for commonsense question answering in the form of multiple choice sentence completion questions describing situations as observed in video .,17,0,35
dataset/preprocessed/training-data/natural_language_inference/14,"However , while SWAG was constructed to be resistant to certain baseline algorithms , powerful subsequent methods were able to perform very well on the dataset .",18,0,27
dataset/preprocessed/training-data/natural_language_inference/14,"In particular , the development of the transformer architecture has led to powerful pre-trained language model representations , including the OpenAI Transformer Language Model and the Bidirectional Encoder Representations from Transformers ( BERT ) model .",19,0,36
dataset/preprocessed/training-data/natural_language_inference/14,BERT achieved new state - of - the - art performance on SWAG that exceeded even that of a human expert .,20,0,22
dataset/preprocessed/training-data/natural_language_inference/14,"However , BERT does not possess human - level commonsense in general , as our experiments demonstrate .",21,0,18
dataset/preprocessed/training-data/natural_language_inference/14,It is instead able to exploit regularities in the SWAG dataset to score high .,22,0,15
dataset/preprocessed/training-data/natural_language_inference/14,"This motivates the construction of additional datasets that pose new challenges , and serve as more reliable benchmarks for commonsense reasoning systems .",23,0,23
dataset/preprocessed/training-data/natural_language_inference/14,"In this work , we introduce the COmmonsense Dataset Adversarially - authored by Humans ( CODAH ) for commonsense question answering in the style of SWAG multiple choice sentence completion .",24,1,31
dataset/preprocessed/training-data/natural_language_inference/14,"We propose a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",25,0,43
dataset/preprocessed/training-data/natural_language_inference/14,"Annotators are rewarded for submissions in which the model fails to identify the correct sentence completion both before and after fine - tuning on a sample of the submitted questions , encouraging the creation of questions that are not easily learnable .",26,0,42
dataset/preprocessed/training-data/natural_language_inference/14,"We experimentally demonstrate that CODAH 's generation procedure produces a dataset with a large gap between system performance and human expert accuracy , even when using state - of the - art pre-trained language models with and without fine - tuning on the large SWAG dataset .",27,0,47
dataset/preprocessed/training-data/natural_language_inference/14,"Using a model initially fine - tuned on SWAG , we find that the OpenAI GPT - 1 and BERT neural question answering models yield 65.5 % and 69.5 % accuracy , respectively , on the CODAH dataset in cross-validation .",28,0,41
dataset/preprocessed/training-data/natural_language_inference/14,"Thus , cross - validating on CO - DAH can form a challenging additional evaluation for SWAG - style commonsense QA systems .",29,0,23
dataset/preprocessed/training-data/natural_language_inference/14,"Human evaluators achieve 95.3 % accuracy , which is substantially higher than the 85.0 % and 87.7 % human performance on the SWAG and SNLI natural language inference tasks .",30,0,30
dataset/preprocessed/training-data/natural_language_inference/14,The high human performance suggests that answers to the CODAH questions are in fact commonsense knowledge .,31,0,17
dataset/preprocessed/training-data/natural_language_inference/14,"Finally , we also analyze differences in performance across questions that target different types of commonsense reasoning , including quantitative , negation , and object reference , showing consistency in performance for BERT and GPT on the proposed categories .",32,0,40
dataset/preprocessed/training-data/natural_language_inference/14,Prior work in question answering has largely focused on the development of reading comprehension - based question answering and resulted in the creation of several large datasets for factoid extraction such as SQuAD and the Google Natural Questions datasets .,34,0,40
dataset/preprocessed/training-data/natural_language_inference/14,"In these tasks , extraction of correct answers from the provided context requires little external world knowledge , understanding of intents , or other commonsense knowledge .",35,0,27
dataset/preprocessed/training-data/natural_language_inference/14,Earlier work has established multiple benchmarks for natural language inference and linguistic entailment with the release SNLI and MultiNLI datasets .,36,0,21
dataset/preprocessed/training-data/natural_language_inference/14,"In these tasks , systems must identify whether a hypothesis agrees with or contradicts a provided premise .",37,0,18
dataset/preprocessed/training-data/natural_language_inference/14,"In these datasets , determining entailment solely relies upon the provided premise and does not require a question answering system to utilize external knowledge .",38,0,25
dataset/preprocessed/training-data/natural_language_inference/14,"More recently , the SWAG dataset directly targets natural language inference that leverages commonsense knowledge .",39,0,16
dataset/preprocessed/training-data/natural_language_inference/14,SWAG multiple choice completion questions are constructed using a video caption as the ground truth with incorrect counterfactuals created using adversarially - filtered generations from an LSTM language model .,40,0,30
dataset/preprocessed/training-data/natural_language_inference/14,"State - of - the - art models for natural language inference have rapidly improved and approach human performance , which leaves little room for continued improvement on current benchmarks .",41,0,31
dataset/preprocessed/training-data/natural_language_inference/14,"Generation of adversarial examples has also been used to increase the robustness of NLP systems as part of the Build it , Break It , The Language Edition Workshop .",42,0,30
dataset/preprocessed/training-data/natural_language_inference/14,"In this workshop , builders designed systems for Sentiment Analysis and Question Answering Driven Semantic Role Labeling tasks and were evaluated on the accuracy of their models on adversarial test cases designed by breakers .",43,0,35
dataset/preprocessed/training-data/natural_language_inference/14,Whereas Build It Break,44,0,4
dataset/preprocessed/training-data/natural_language_inference/14,"It adversarial generation required submissions to match the format of a starter dataset and offered limited adversarial access to the target NLP systems , the CODAH construction procedure allows for entirely new questions and provide adversaries with a target model throughout the submission process , allowing workers to experiment .",45,0,50
dataset/preprocessed/training-data/natural_language_inference/14,The CODAH Dataset,46,0,3
dataset/preprocessed/training-data/natural_language_inference/14,Our dataset contains multiple choice sentence completion questions in the format of the SWAG dataset .,47,0,16
dataset/preprocessed/training-data/natural_language_inference/14,Examples of the questions are shown in .,48,0,8
dataset/preprocessed/training-data/natural_language_inference/14,"Each question consists of a prompt sentence , the subject of the subsequent sentence , and four candidate completions , such that exactly one candidate completion is consistent with commonsense .",49,0,31
dataset/preprocessed/training-data/natural_language_inference/17,Reinforced Mnemonic Reader for Machine Reading Comprehension,2,1,7
dataset/preprocessed/training-data/natural_language_inference/17,"In this paper , we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks , which enhances previous attentive readers in two aspects .",4,0,25
dataset/preprocessed/training-data/natural_language_inference/17,"First , a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions thatare temporally memorized in a multi-round alignment architecture , so as to avoid the problems of attention redundancy and attention deficiency .",5,0,39
dataset/preprocessed/training-data/natural_language_inference/17,"Second , a new optimization approach , called dynamic - critical reinforcement learning , is introduced to extend the standard supervised method .",6,0,23
dataset/preprocessed/training-data/natural_language_inference/17,It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms .,7,0,24
dataset/preprocessed/training-data/natural_language_inference/17,Extensive experiments on the Stanford Question Answering Dataset ( SQuAD ) show that our model achieves state - of - the - art results .,8,0,25
dataset/preprocessed/training-data/natural_language_inference/17,"Meanwhile , our model outperforms previous systems by over 6 % in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets .",9,0,26
dataset/preprocessed/training-data/natural_language_inference/17,Teaching machines to comprehend a given context paragraph and answer corresponding questions is one of the long - term goals of natural language processing and artificial intelligence .,11,0,28
dataset/preprocessed/training-data/natural_language_inference/17,gives an example of the machine reading comprehension ( MRC ) task .,12,0,13
dataset/preprocessed/training-data/natural_language_inference/17,"Benefiting from the rapid development of deep learning techniques and large - scale benchmark datasets , end - to - end neural networks have achieved promising results on this task .",13,0,31
dataset/preprocessed/training-data/natural_language_inference/17,"Despite of the advancements , we argue that there still exists two limitations :",14,0,14
dataset/preprocessed/training-data/natural_language_inference/17,"To capture complex interactions between the context and the question , a variety of neural attention , such as bi-attention , * Contribution during internship at Fudan University and Microsoft Research .",16,0,32
dataset/preprocessed/training-data/natural_language_inference/17,"coattention , are proposed in a single - round alignment architecture .",17,0,12
dataset/preprocessed/training-data/natural_language_inference/17,"In order to fully compose complete information of the inputs , multiround alignment architectures that compute attentions repeatedly have been proposed .",18,0,22
dataset/preprocessed/training-data/natural_language_inference/17,"However , in these approaches , the current attention is unaware of which parts of the context and question have been focused in earlier attentions , which results in two distinct but related issues , where multiple attentions 1 ) focuses on same texts , leading to attention redundancy and 2 ) fail to focus on some salient parts of the input , causing attention deficiency .",19,0,67
dataset/preprocessed/training-data/natural_language_inference/17,"To train the model , standard maximum - likelihood method is used for predicting exactly - matched ( EM ) answer spans .",21,0,23
dataset/preprocessed/training-data/natural_language_inference/17,"Recently , reinforcement learning algorithm , which measures the reward as word overlap between the predicted answer and the groung truth , is introduced to optimize towards the F1 metric instead of EM metric .",22,0,35
dataset/preprocessed/training-data/natural_language_inference/17,"Specifically , an estimated baseline is utilized to normalize the reward and reduce variances .",23,0,15
dataset/preprocessed/training-data/natural_language_inference/17,"However , the convergence can be suppressed when the baseline is better than the reward .",24,0,16
dataset/preprocessed/training-data/natural_language_inference/17,"This is harmful if the inferior reward is partially overlapped with the ground truth , as the normalized objective will discourage the prediction of ground truth positions .",25,0,28
dataset/preprocessed/training-data/natural_language_inference/17,We refer to this case as the convergence suppression problem .,26,0,11
dataset/preprocessed/training-data/natural_language_inference/17,"To address the first problem , we present a reattention mechanism that temporally memorizes past attentions and uses them to refine current attentions in a multi-round alignment architecture .",27,0,29
dataset/preprocessed/training-data/natural_language_inference/17,"The computation is based on the fact that two words should share similar semantics if their attentions about same texts are highly overlapped , and be less similar vice versa .",28,0,31
dataset/preprocessed/training-data/natural_language_inference/17,"Therefore , the reattention can be more concentrated if past attentions focus on same parts of the input , or be relatively more distracted so as to focus on new regions if past attentions are not overlapped at all .",29,0,40
dataset/preprocessed/training-data/natural_language_inference/17,"As for the second problem , we extend the traditional training method with a novel approach called dynamic - critical reinforcement learning .",30,0,23
dataset/preprocessed/training-data/natural_language_inference/17,"Unlike the traditional reinforcement learning algorithm where the reward and baseline are statically sampled , our approach dynamically decides the reward and the baseline according to two sampling strategies , Context : The American Football Conference ( AFC ) champion Denver Broncos defeated the National Football Conference ( NFC ) champion Carolina Panthers 24 - 10 to earn their third Super Bowl title .",31,0,64
dataset/preprocessed/training-data/natural_language_inference/17,Question : Which NFL team represented the AFC at Super Bowl 50 ?,32,0,13
dataset/preprocessed/training-data/natural_language_inference/17,Answer : Denver Broncos namely random inference and greedy inference .,33,0,11
dataset/preprocessed/training-data/natural_language_inference/17,The result with higher score is always set to be the reward while the other is the baseline .,34,0,19
dataset/preprocessed/training-data/natural_language_inference/17,"In this way , the normalized reward is ensured to be always positive so that no convergence suppression will be made .",35,0,22
dataset/preprocessed/training-data/natural_language_inference/17,All of the above innovations are integrated into a new end - to - end neural architecture called Reinforced Mnemonic Reader in .,36,0,23
dataset/preprocessed/training-data/natural_language_inference/17,We conducted extensive experiments on both the SQuAD dataset and two adversarial SQuAD datasets to evaluate the proposed model .,37,0,20
dataset/preprocessed/training-data/natural_language_inference/17,"On SQuAD , our single model obtains an exact match ( EM ) score of 79.5 % and F1 score of 86.6 % , while our ensemble model further boosts the result to 82.3 % and 88.5 % respectively .",38,0,40
dataset/preprocessed/training-data/natural_language_inference/17,"On adversarial SQuAD , our model surpasses existing approahces by more than 6 % on both AddSent and AddOneSent datasets .",39,0,21
dataset/preprocessed/training-data/natural_language_inference/17,MRC with Reattention,40,0,3
dataset/preprocessed/training-data/natural_language_inference/17,"For the MRC tasks , a question Q and a context C are given , our goal is to predict an answer A , which has different forms according to the specific task .",42,0,34
dataset/preprocessed/training-data/natural_language_inference/17,"In the SQuAD dataset , the answer A is constrained as a segment of text in the context C , nerual networks are designed to model the probability distribution p ( A |C , Q ) .",43,0,37
dataset/preprocessed/training-data/natural_language_inference/17,Alignment Architecture for MRC,44,0,4
dataset/preprocessed/training-data/natural_language_inference/17,"Among all state - of - the - art works for MRC , one of the key factors is the alignment architecture .",45,0,23
dataset/preprocessed/training-data/natural_language_inference/17,"That is , given the hidden representations of question and context , we align each context word with the entire question using attention mechanisms , and enhance the context representation with the attentive question information .",46,0,36
dataset/preprocessed/training-data/natural_language_inference/17,A detailed comparison of different alignment architectures is shown in .,47,0,11
dataset/preprocessed/training-data/natural_language_inference/17,"Early work for MRC , such as Match - LSTM , utilizes the attention mechanism stemmed from neural machine translation [ Dzmitry Bahdanau , 2015 ] serially , where the attention is computed inside the cell of recurrent neural networks .",48,0,41
dataset/preprocessed/training-data/natural_language_inference/17,"A more popular approach is to compute attentions in parallel , resulting in a similarity matrix .",49,0,17
dataset/preprocessed/training-data/natural_language_inference/81,COARSE - GRAIN FINE - GRAIN COATTENTION NET - WORK FOR MULTI - EVIDENCE QUESTION ANSWERING,2,1,16
dataset/preprocessed/training-data/natural_language_inference/81,"End - to - end neural models have made significant progress in question answering , however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document .",4,1,37
dataset/preprocessed/training-data/natural_language_inference/81,"In this work , we propose the Coarse - grain Fine - grain Coattention Network ( CFC ) , a new question answering model that combines information from evidence across multiple documents .",5,0,33
dataset/preprocessed/training-data/natural_language_inference/81,"The CFC consists of a coarse - grain module that interprets documents with respect to the query then finds a relevant answer , and a fine - grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query .",6,0,47
dataset/preprocessed/training-data/natural_language_inference/81,"We design these modules using hierarchies of coattention and selfattention , which learn to emphasize different parts of the input .",7,0,21
dataset/preprocessed/training-data/natural_language_inference/81,"On the Qangaroo WikiHop multi-evidence question answering task , the CFC obtains a new stateof - the - art result of 70.6 % on the blind test set , outperforming the previous best by 3 % accuracy despite not using pretrained contextual encoders .",8,0,44
dataset/preprocessed/training-data/natural_language_inference/81,"Figure 1 : The Coarse - grain Fine-grain Coattention Network . outputs from a traditional span extraction model ( Clark & Gardner , 2018 ) using the CFC improves exact match accuracy by 3.1 % and F1 by 3.0 % .",9,0,41
dataset/preprocessed/training-data/natural_language_inference/81,Our analysis shows that components in the attention hierarchies of the coarse and fine - grain modules learn to focus on distinct parts of the input .,10,0,27
dataset/preprocessed/training-data/natural_language_inference/81,This enables the CFC to more effectively represent a large collection of long documents .,11,0,15
dataset/preprocessed/training-data/natural_language_inference/81,"Finally , we outline common types of errors produced by CFC , caused by difficulty in aggregating large quantity of references , noise in distant supervision , and difficult relation types .",12,0,32
dataset/preprocessed/training-data/natural_language_inference/81,The coarse - grain module and fine - grain module of the CFC correspond to coarse - grain reasoning and fine - grain reasoning strategies .,13,0,26
dataset/preprocessed/training-data/natural_language_inference/81,"The coarse - grain module summarizes support documents without knowing the candidates : it builds codependent representations of support documents and the query using coattention , then produces a coarse - grain summary using self - attention .",14,0,38
dataset/preprocessed/training-data/natural_language_inference/81,"In contrast , the fine - grain module retrieves specific contexts in which each candidate occurs : it identifies coreferent mentions of the candidate , then uses coattention to build codependent representations between these mentions and the query .",15,0,39
dataset/preprocessed/training-data/natural_language_inference/81,"While low - level encodings of the inputs are shared between modules , we show that this division of labour allows the attention hierarchies in each module to focus on different parts of the input .",16,0,36
dataset/preprocessed/training-data/natural_language_inference/81,This enables the model to more effectively represent a large number of potentially long support documents .,17,0,17
dataset/preprocessed/training-data/natural_language_inference/81,"Suppose we are given a query , a set of N s support documents , and a set of N c candidates .",18,0,23
dataset/preprocessed/training-data/natural_language_inference/81,"Without loss of generality , let us consider the ith document and the jth candidate .",19,0,16
dataset/preprocessed/training-data/natural_language_inference/81,Let L q ?,20,0,4
dataset/preprocessed/training-data/natural_language_inference/81,"R Tqd emb , L s ?",21,0,7
dataset/preprocessed/training-data/natural_language_inference/81,"R Tsd emb , and L c ?",22,0,8
dataset/preprocessed/training-data/natural_language_inference/81,"Tcd emb respectively denote the word embeddings of the query , the ith support document , and the jth candidate answer .",24,0,22
dataset/preprocessed/training-data/natural_language_inference/81,"Here , T q , T s , and Tc are the number of words in the corresponding sequence .",25,0,20
dataset/preprocessed/training-data/natural_language_inference/81,d emb is the size of the word embedding .,26,0,10
dataset/preprocessed/training-data/natural_language_inference/81,"We begin by encoding each sequence using a bidirectional Gated Recurrent Units ( GRUs ) ( Cho et al. , 2014 ) .",27,0,23
dataset/preprocessed/training-data/natural_language_inference/81,"Nicola De Cao , Wilker Aziz , and Ivan Titov .",28,0,11
dataset/preprocessed/training-data/natural_language_inference/81,Question answering by reasoning across documents with graph convolutional networks .,29,0,11
dataset/preprocessed/training-data/natural_language_inference/81,"arXiv preprint ar Xiv : 1808.09920 , 2018 .",30,0,9
dataset/preprocessed/training-data/natural_language_inference/81,"Sang Keun Lee Deunsol Yoon , Dongbok Lee. Dynamic self - attention :",31,0,13
dataset/preprocessed/training-data/natural_language_inference/81,Computing attention over words dynamically for sentence embedding .,32,0,9
dataset/preprocessed/training-data/natural_language_inference/81,"arXiv preprint ar Xiv :1808.07383 , 2018 . models for reasoning over multiple mentions using coreference .",33,0,17
dataset/preprocessed/training-data/natural_language_inference/81,"arXiv preprint ar Xiv :1804.05922 , 2018 .",34,0,8
dataset/preprocessed/training-data/natural_language_inference/81,Li Dong and Mirella Lapata .,35,0,6
dataset/preprocessed/training-data/natural_language_inference/81,Coarse - to - fine decoding for neural semantic parsing .,36,0,11
dataset/preprocessed/training-data/natural_language_inference/81,"In ACL , 2018 .",37,0,5
dataset/preprocessed/training-data/natural_language_inference/81,"Surabhi Gupta , Ani Nenkova , and Dan Jurafsky .",38,0,10
dataset/preprocessed/training-data/natural_language_inference/81,Measuring importance and query relevance in topic - focused multi-document summarization .,39,0,12
dataset/preprocessed/training-data/natural_language_inference/81,"In ACL , 2007 .",40,0,5
dataset/preprocessed/training-data/natural_language_inference/81,A requirement of scalable and practical question answering ( QA ) systems is the ability to reason over multiple documents and combine their information to answer questions .,42,1,28
dataset/preprocessed/training-data/natural_language_inference/81,"Although existing datasets enabled the development of effective end - to - end neural question answering systems , they tend to focus on reasoning over localized sections of a single document .",43,1,32
dataset/preprocessed/training-data/natural_language_inference/81,"For example , find that 90 % of the questions in the Stanford Question Answering Dataset are answerable given 1 sentence in a document .",44,0,25
dataset/preprocessed/training-data/natural_language_inference/81,"In this work , we instead focus on multi-evidence QA , in which answering the question requires aggregating evidence from multiple documents .",45,0,23
dataset/preprocessed/training-data/natural_language_inference/81,"Our multi-evidence QA model , the Coarse - grain Fine - grain Coattention Network ( CFC ) , selects among a set of candidate answers given a set of support documents and a query .",46,0,35
dataset/preprocessed/training-data/natural_language_inference/81,The CFC is inspired by coarse - grain reasoning and fine - grain reasoning .,47,0,15
dataset/preprocessed/training-data/natural_language_inference/81,"In coarse - grain reasoning , the model builds a coarse summary of support documents conditioned on the query without knowing what candidates are available , then scores each candidate .",48,0,31
dataset/preprocessed/training-data/natural_language_inference/81,"In fine - grain reasoning , the model matches specific finegrain contexts in which the candidate is mentioned with the query in order to gauge the relevance of the candidate .",49,0,31
dataset/preprocessed/training-data/natural_language_inference/29,Product - Aware Answer Generation in E - Commerce Question - Answering,2,1,12
dataset/preprocessed/training-data/natural_language_inference/29,"In e-commerce portals , generating answers for product - related questions has become a crucial task .",4,0,17
dataset/preprocessed/training-data/natural_language_inference/29,"In this paper , we propose the task of product - aware answer generation , which tends to generate an accurate and complete answer from large - scale unlabeled e-commerce reviews and product attributes .",5,0,35
dataset/preprocessed/training-data/natural_language_inference/29,"Unlike existing question - answering problems , answer generation in e-commerce confronts three main challenges :",6,0,16
dataset/preprocessed/training-data/natural_language_inference/29,( 1 ) Reviews are informal and noisy ; ( 2 ) joint modeling of reviews and key - value product attributes is challenging ; ( 3 ) traditional methods easily generate meaningless answers .,7,0,35
dataset/preprocessed/training-data/natural_language_inference/29,"To tackle above challenges , we propose an adversarial learning based model , named PAAG , which is composed of three components : a questionaware review representation module , a key - value memory network encoding attributes , and a recurrent neural network as a sequence generator .",8,0,48
dataset/preprocessed/training-data/natural_language_inference/29,"Specifically , we employ a convolutional discriminator to distinguish whether our generated answer matches the facts .",9,0,17
dataset/preprocessed/training-data/natural_language_inference/29,"To extract the salience part of reviews , an attention - based review reader is proposed to capture the most relevant words given the question .",10,0,26
dataset/preprocessed/training-data/natural_language_inference/29,"Conducted on a large - scale real - world e-commerce dataset , our extensive experiments verify the effectiveness of each module in our proposed model .",11,0,26
dataset/preprocessed/training-data/natural_language_inference/29,"Moreover , our experiments show that our model achieves the state - of - the - art performance in terms of both automatic metrics and human evaluations .",12,0,28
dataset/preprocessed/training-data/natural_language_inference/29,"In recent years , the explosive popularity of question - answering ( QA ) is revitalizing the task of reading comprehension with promising results .",14,1,25
dataset/preprocessed/training-data/natural_language_inference/29,"Unlike traditional knowledge - based QA methods that require a structured knowledge graph as the input and output resource description framework ( RDF ) triples , most of reading comprehension approaches read context passages and extract text spans from input text as answers .",15,0,44
dataset/preprocessed/training-data/natural_language_inference/29,E-commerce are playing an increasingly important role in our daily life .,16,0,12
dataset/preprocessed/training-data/natural_language_inference/29,"As a convenience of users , more and more e-commerce portals provide community question - answering services that allow users to pose product - aware questions to other consumers who purchased the same product before .",17,0,36
dataset/preprocessed/training-data/natural_language_inference/29,"Unfortunately , many productaware questions lack of proposer answers .",18,0,10
dataset/preprocessed/training-data/natural_language_inference/29,"Under the circumstances , users have to read the product 's reviews to find the answer by themselves .",19,0,19
dataset/preprocessed/training-data/natural_language_inference/29,"Given product attributes and reviews , an answer is manually generated following a cascade procedure : ( 1 ) a user skims reviews and finds relevant sentences ; ( 2 ) she / he extracts useful semantic units ; and the user jointly combines these semantic units with attributes , and writes a proper answer .",20,0,56
dataset/preprocessed/training-data/natural_language_inference/29,"However , the information overload phenomenon makes this procedure an energy - draining process to pursue an answer from a rapidly increasing number of reviews .",21,0,26
dataset/preprocessed/training-data/natural_language_inference/29,"Consequently , automatic product - aware question - answering become more and more helpful in this scenario .",22,0,18
dataset/preprocessed/training-data/natural_language_inference/29,The task on which we focus is the product - aware answer generation given reviews and product attributes .,23,0,19
dataset/preprocessed/training-data/natural_language_inference/29,Our goal is to respond product - aware questions automatically given a large amount of reviews and attributes of a specific product .,24,0,23
dataset/preprocessed/training-data/natural_language_inference/29,"Unlike either a "" yes / no "" binary classification task or a review ranking task , product - aware answer generation provides a natural - sounding sentence as an answer .",25,0,32
dataset/preprocessed/training-data/natural_language_inference/29,The definition of our task is similar as the reading comprehension .,26,0,12
dataset/preprocessed/training-data/natural_language_inference/29,"However , most of existing reading comprehension solutions only extract text spans from contextual passages .",27,0,16
dataset/preprocessed/training-data/natural_language_inference/29,"Since the target of product - aware answer generation is to generate a naturalsounding answer instead of text spans , most of reading comprehension methods and datasets ( e.g. , SQuAD ) are not applicable .",28,0,36
dataset/preprocessed/training-data/natural_language_inference/29,"As far as we know , only few of reading comprehension approaches aim to generate a natural - sounding answers from extraction results .",29,0,24
dataset/preprocessed/training-data/natural_language_inference/29,"With a promising performance on MS - MARCO , S - Net framework proposed by Tan et al.",30,0,18
dataset/preprocessed/training-data/natural_language_inference/29,focuses on synthesizing answers from extraction results .,31,0,8
dataset/preprocessed/training-data/natural_language_inference/29,"However , S - Net requires a large amount of labeling data for extracting text spans , which is still unrealistic given a huge number of reviews .",32,0,28
dataset/preprocessed/training-data/natural_language_inference/29,"Moreover , product reviews from e-commerce website are informal and noisy , whereas in reading comprehension the given context passages are usually in a formal style .",33,0,27
dataset/preprocessed/training-data/natural_language_inference/29,"Generally , existing reading comprehension approaches confront three challenges when addressing product - aware question answering :",34,0,17
dataset/preprocessed/training-data/natural_language_inference/29,( 1 ) Review text is irrelevant and noisy .,35,0,10
dataset/preprocessed/training-data/natural_language_inference/29,( 2 ) It 's extremely expensive to label large amounts of explicit text spans from real - world e-commerce platforms .,36,0,22
dataset/preprocessed/training-data/natural_language_inference/29,"( 3 ) Traditional loss function calculation in reading comprehension tends to generate meaningless answers such as "" I do n't know "" .",37,0,24
dataset/preprocessed/training-data/natural_language_inference/29,"In this paper , we propose the product - aware answer generator ( PAAG ) , a product related question answering model which incorporates customer reviews with product attributes .",38,0,30
dataset/preprocessed/training-data/natural_language_inference/29,"Specifically , at the beginning we employ an attention mechanism to model interactions between a question and reviews .",39,0,19
dataset/preprocessed/training-data/natural_language_inference/29,"Simultaneously , we employ a key - value memory network to store the product attributes and extract the relevance values according to the question .",40,0,25
dataset/preprocessed/training-data/natural_language_inference/29,"Eventually , we propose a recurrent neural network ( RNN ) based decoder , which combines product - aware review representation and attributes to generate the answer .",41,0,28
dataset/preprocessed/training-data/natural_language_inference/29,"More importantly , to tackle the problem of meaningless answers , we propose an adversarial learning mechanism in the loss calculation for optimizing parameters .",42,0,25
dataset/preprocessed/training-data/natural_language_inference/29,"Conducted on a large - scale real - world e-commerce dataset , we evaluate the performance of PAAG using extensive experiments .",43,0,22
dataset/preprocessed/training-data/natural_language_inference/29,"Experimental results demonstrate that the PAAG model achieves significant improvement over other baselines , including the state - of - the - art reading comprehension model .",44,0,27
dataset/preprocessed/training-data/natural_language_inference/29,"Furthermore , we also examine the effectiveness of each module in PAAG .",45,0,13
dataset/preprocessed/training-data/natural_language_inference/29,Our experiments verify that adversarial learning is capable to significantly improve the denoising and facts extracting capacity of PAAG .,46,0,20
dataset/preprocessed/training-data/natural_language_inference/29,"To sum up , our contributions can be summarized as follows :",47,0,12
dataset/preprocessed/training-data/natural_language_inference/29,We propose a product - aware answer generation task .,48,0,10
dataset/preprocessed/training-data/natural_language_inference/29,"To tackle this task , we propose an end - to - end learning method to extract fact that is helpful for answering questions from reviews and attributes and then generate answer text .",49,0,34
dataset/preprocessed/training-data/natural_language_inference/24,Stochastic Answer Networks for Machine Reading Comprehension,2,1,7
dataset/preprocessed/training-data/natural_language_inference/24,We propose a simple yet robust stochastic answer network ( SAN ) that simulates multi-step reasoning in machine reading comprehension .,4,0,21
dataset/preprocessed/training-data/natural_language_inference/24,"Compared to previous work such as ReasoNet which used reinforcement learning to determine the number of steps , the unique feature is the use of a kind of stochastic prediction dropout on the answer module ( final layer ) of the neural network during the training .",5,0,47
dataset/preprocessed/training-data/natural_language_inference/24,"We show that this simple trick improves robustness and achieves results competitive to the state - of - the - art on the Stanford Question Answering Dataset ( SQuAD ) , the Adversarial SQuAD , and the Microsoft MAchine Reading COmprehension Dataset ( MS MARCO ) .",6,0,47
dataset/preprocessed/training-data/natural_language_inference/24,Machine reading comprehension ( MRC ) is a challenging task : the goal is to have machines read a text passage and then answer any question about the passage .,8,1,30
dataset/preprocessed/training-data/natural_language_inference/24,"This task is an useful benchmark to demonstrate natural language understanding , and also has important applications in e.g. conversational agents and customer service support .",9,0,26
dataset/preprocessed/training-data/natural_language_inference/24,It has been hypothesized that difficult MRC problems require some form of multi-step synthesis and reasoning .,10,1,17
dataset/preprocessed/training-data/natural_language_inference/24,"For instance , the following example from the MRC dataset SQuAD illustrates the need for synthesis of information across sentences and multiple steps of reasoning :",11,0,26
dataset/preprocessed/training-data/natural_language_inference/24,"To infer the answer ( the underlined portion of the passage P ) , the model needs to first perform coreference resolution so that it knows "" They "" refers "" V &A Theator "" , then extract the subspan in the direct object corresponding to the answer .",12,0,49
dataset/preprocessed/training-data/natural_language_inference/24,This kind of iterative process can be viewed as a form of multi-step reasoning .,13,0,15
dataset/preprocessed/training-data/natural_language_inference/24,"Several recent MRC models have embraced this kind of multistep strategy , where predictions are generated after making multiple passes through the same text and integrating intermediate information in the process .",14,0,32
dataset/preprocessed/training-data/natural_language_inference/24,The first models employed a predetermined fixed number of steps .,15,0,11
dataset/preprocessed/training-data/natural_language_inference/24,"Later , proposed using reinforcement learning to dynamically determine the number of steps based on the complexity of the question .",16,0,21
dataset/preprocessed/training-data/natural_language_inference/24,"Further , empirically showed that dynamic multi-step reasoning outperforms fixed multi-step reasoning , which in turn outperforms single - step reasoning on two distinct MRC datasets ( SQuAD and MS MARCO ) .",17,0,33
dataset/preprocessed/training-data/natural_language_inference/24,"In this work , we derive an alternative multi-step reasoning neural network for MRC .",18,0,15
dataset/preprocessed/training-data/natural_language_inference/24,"During training , we fix the number of reasoning steps , but perform stochastic dropout on the answer module ( final layer predictions ) .",19,0,25
dataset/preprocessed/training-data/natural_language_inference/24,"During decoding , we generate answers based on the average of predictions in all steps , rather than the final step .",20,0,22
dataset/preprocessed/training-data/natural_language_inference/24,"We call this a stochastic answer network ( SAN ) because the stochastic dropout is applied to the answer module ; albeit simple , this technique significantly improves the robustness and over all accuracy of the model .",21,0,38
dataset/preprocessed/training-data/natural_language_inference/24,"Intuitively this works because while the model successively refines its prediction over multiple steps , each step is still trained to generate the same answer ; we are performing a kind of stochastic ensemble over the model 's successive predic -s t - 1 st s t+1",22,0,47
dataset/preprocessed/training-data/natural_language_inference/24,"x : Illustration of "" stochastic prediction dropout "" in the answer module during training .",23,0,16
dataset/preprocessed/training-data/natural_language_inference/24,"At each reasoning step t , the model combines memory ( bottom row ) with hidden states s t?1 to generate a prediction ( multinomial distribution ) .",24,0,28
dataset/preprocessed/training-data/natural_language_inference/24,"Here , there are three steps and three predictions , but one prediction is dropped and the final result is an average of the remaining distributions .",25,0,27
dataset/preprocessed/training-data/natural_language_inference/24,tion refinements .,26,0,3
dataset/preprocessed/training-data/natural_language_inference/24,Stochastic prediction dropout is illustrated in .,27,0,7
dataset/preprocessed/training-data/natural_language_inference/24,Proposed model : SAN,28,0,4
dataset/preprocessed/training-data/natural_language_inference/24,"The machine reading comprehension ( MRC ) task as defined here involves a question Q = {q 0 , q 1 , ... , q m?1 } and a passage P = {p 0 , p 1 , ... , p n?1 } and aims to find an answer span A = { a start , a end } in P .",29,0,62
dataset/preprocessed/training-data/natural_language_inference/24,We assume that the answer exists in the passage P as a contiguous text string .,30,0,16
dataset/preprocessed/training-data/natural_language_inference/24,"Here , m and n denote the number of tokens in Q and P , respectively .",31,0,17
dataset/preprocessed/training-data/natural_language_inference/24,"The learning algorithm for reading comprehension is to learn a function f ( Q , P ) ?",32,0,18
dataset/preprocessed/training-data/natural_language_inference/24,"The training data is a set of the query , passage and answer tuples < Q , P , A >.",34,0,21
dataset/preprocessed/training-data/natural_language_inference/24,We now describe our model from the ground up .,35,0,10
dataset/preprocessed/training-data/natural_language_inference/24,"The main contribution of this work is the answer module , but in order to understand what goes into this module , we will start by describing how Q and P are processed by the lower layers .",36,0,38
dataset/preprocessed/training-data/natural_language_inference/24,Note the lower layers also have some novel variations thatare not used in previous work .,37,0,16
dataset/preprocessed/training-data/natural_language_inference/24,"As shown in Figure 2 , our model contains four different layers to capture different concept of representations .",38,0,19
dataset/preprocessed/training-data/natural_language_inference/24,The detailed description of our model is provided as follows .,39,0,11
dataset/preprocessed/training-data/natural_language_inference/24,Lexicon Encoding Layer .,40,0,4
dataset/preprocessed/training-data/natural_language_inference/24,The purpose of the first layer is to extract information from Q and Pat the word level and normalize for lexical vari - ants .,41,0,25
dataset/preprocessed/training-data/natural_language_inference/24,A typical technique to obtain lexicon embedding is concatenation of its word embedding with other linguistic embedding such as those derived from Part - Of - Speech ( POS ) tags .,42,0,32
dataset/preprocessed/training-data/natural_language_inference/24,"For word embeddings , we use the pre-trained 300 - dimensional GloVe vectors for the both Q and P .",43,0,20
dataset/preprocessed/training-data/natural_language_inference/24,"Following , we use three additional types of linguistic features for each token pi in the passage P :",44,0,19
dataset/preprocessed/training-data/natural_language_inference/24,9 - dimensional,45,0,3
dataset/preprocessed/training-data/natural_language_inference/24,POS tagging embedding for total 56 different types of the POS tags .,46,0,13
dataset/preprocessed/training-data/natural_language_inference/24,8 - dimensional named-entity recognizer ( NER ) embedding for total 18 different types of the NER tags .,47,0,19
dataset/preprocessed/training-data/natural_language_inference/24,We utilized small embedding sizes for POS and NER to reduce model size .,48,0,14
dataset/preprocessed/training-data/natural_language_inference/24,They mainly serve the role of coarse - grained word clusters .,49,0,12
dataset/preprocessed/training-data/natural_language_inference/42,A Fully Attention - Based Information Retriever,2,1,7
dataset/preprocessed/training-data/natural_language_inference/42,Recurrent neural networks are now the state - of the - art in natural language processing because they can build rich contextual representations and process texts of arbitrary length .,4,0,30
dataset/preprocessed/training-data/natural_language_inference/42,"However , recent developments on attention mechanisms have equipped feedforward networks with similar capabilities , hence enabling faster computations due to the increase in the number of operations that can be parallelized .",5,0,33
dataset/preprocessed/training-data/natural_language_inference/42,We explore this new type of architecture in the domain of question - answering and propose a novel approach that we call Fully Attention Based Information Retriever ( FABIR ) .,6,0,31
dataset/preprocessed/training-data/natural_language_inference/42,We show that FABIR achieves competitive results in the Stanford Question Answering Dataset ( SQuAD ) while having fewer parameters and being faster at both learning and inference than rival methods .,7,0,32
dataset/preprocessed/training-data/natural_language_inference/42,Question - answering ( QA ) systems that can answer queries expressed in natural language have been a perennial goal of the artificial intelligence community .,9,1,26
dataset/preprocessed/training-data/natural_language_inference/42,"An interesting strategy in the design of such systems is information extraction , where the answer is sought in a set of support documents .",10,0,25
dataset/preprocessed/training-data/natural_language_inference/42,"However , extracting information from large texts is still a challenging task , and most state - of - the - art models restrict themselves to single paragraphs .",11,0,29
dataset/preprocessed/training-data/natural_language_inference/42,"That is , in fact , the proposed focus of recent open - domain QA datasets , such as SQuAD .",12,1,21
dataset/preprocessed/training-data/natural_language_inference/42,"In SQuAD , each problem instance consists of a passage P and a question Q. A QA system must then provide an answer A by selecting a snippet from P .",13,1,31
dataset/preprocessed/training-data/natural_language_inference/42,"That format reduces the complexity of the task and also facilitates training , as one can learn a probability distribution over the words that compose the passage .",14,0,28
dataset/preprocessed/training-data/natural_language_inference/42,"Since its publication in 2016 , SQuAD has been targeted by many research groups , and the proposed models are gradually approaching ( even overcoming ) human - level performances .",15,0,31
dataset/preprocessed/training-data/natural_language_inference/42,"All but a few of these models rely on Recurrent Neural Networks ( RNNs ) , which currently dominate the stateof - the - art in most Natural Language Processing ( NLP ) tasks .",16,0,35
dataset/preprocessed/training-data/natural_language_inference/42,"However , RNNs do have some drawbacks , of which the most relevant to real - world applications is the high number of sequential operations , which increases the processing time of both learning and inference .",17,0,37
dataset/preprocessed/training-data/natural_language_inference/42,"To address these limitations , proposed the Transformer , a machine translation model that introduces a new deep learning architecture solely based on "" attention "" mechanisms .",18,0,28
dataset/preprocessed/training-data/natural_language_inference/42,We later clarify the meaning of attention in this context .,19,0,11
dataset/preprocessed/training-data/natural_language_inference/42,"Inspired by the positive results of Vaswani et al. in machine translation , we have applied a similar architecture to the domain of question - answering , a model that we have named Fully Attention - Based Information Retriever ( FABIR ) .",20,0,43
dataset/preprocessed/training-data/natural_language_inference/42,"Our goal then was to verify how much performance we can get exclusively from the attention mechanism , without combining it with several other techniques .",21,0,26
dataset/preprocessed/training-data/natural_language_inference/42,"We validated our model in the SQuAD dataset , which proved that FABIR not only achieves competitive results ( F1:77.6 % , EM : 67.7 % ) but also has fewer parameters and is faster at both training and testing times than competing methods .",22,0,45
dataset/preprocessed/training-data/natural_language_inference/42,"Besides the development of a new architecture , we identify three major contributions of our work that have made these results possible :",23,0,23
dataset/preprocessed/training-data/natural_language_inference/42,"Convolutional attention : a novel attention mechanism that encodes many - to - many relationships between words , enabling richer contextual representations .",24,0,23
dataset/preprocessed/training-data/natural_language_inference/42,Reduction layer : a new layer design that fits the pipeline proposed by Vaswani et al .,25,0,17
dataset/preprocessed/training-data/natural_language_inference/42,and compresses the input embedding size for subsequent layers ( this is especially beneficial when employing pre-trained embeddings ) .,26,0,20
dataset/preprocessed/training-data/natural_language_inference/42,Column - wise cross - attention : we modify the crossattention operation by and propose a new technique that is better suited to question - answering .,27,0,27
dataset/preprocessed/training-data/natural_language_inference/42,This article is organized as follows .,28,0,7
dataset/preprocessed/training-data/natural_language_inference/42,We first introduce some of the related work in question - answering and then present FABIR 's architecture and its basic design choices .,29,0,24
dataset/preprocessed/training-data/natural_language_inference/42,"Subsequently , we report and comment our results in the SQuAD dataset .",30,0,13
dataset/preprocessed/training-data/natural_language_inference/42,"Finally , we compare the performance of FABIR with RNN - based models and draw some conclusions , suggesting directions for future work .",31,0,24
dataset/preprocessed/training-data/natural_language_inference/42,The vast majority of papers that address the SQuAD dataset have adopted RNN - based models -.,34,0,17
dataset/preprocessed/training-data/natural_language_inference/42,"They all follow a similar pipeline , with pre-trained word - embeddings thatare processed by bidirectional RNNs .",35,0,18
dataset/preprocessed/training-data/natural_language_inference/42,"Question and passage are processed independently , and their interaction is modeled by attention mechanisms to produce an answer .",36,0,20
dataset/preprocessed/training-data/natural_language_inference/42,"There are slight differences in how each model employs attention , but they all calculate it over the hidden states of an RNN .",37,0,24
dataset/preprocessed/training-data/natural_language_inference/42,Vaswani et al.,38,0,3
dataset/preprocessed/training-data/natural_language_inference/42,"were the first to apply attention directly over the word - embeddings , and thus derived a new neural network architecture which , without any recurrence , achieved state - of the - art results in machine translation .",39,0,39
dataset/preprocessed/training-data/natural_language_inference/42,"In this section , we briefly discuss both types of attention models .",40,0,13
dataset/preprocessed/training-data/natural_language_inference/42,A. Traditional Attention Mechanisms,41,0,4
dataset/preprocessed/training-data/natural_language_inference/42,"In recent years , attention mechanisms have been used with success in a variety of NLP tasks , such as machine translation , and natural language inference , .",42,0,29
dataset/preprocessed/training-data/natural_language_inference/42,"Indeed , most models that target the SQuAD dataset use some form of attention to model the relationship between question and passage .",43,0,23
dataset/preprocessed/training-data/natural_language_inference/42,Attention can be defined as a mechanism that gives a score ?,44,0,12
dataset/preprocessed/training-data/natural_language_inference/42,"i to a vector pi from a set P = [ p 1 , ... , pm ] with respect to a vector q j from Q = [ q 1 , ... , q n ] .",45,0,38
dataset/preprocessed/training-data/natural_language_inference/42,This score is a function of both P and Q and is shown in its most general form in .,46,0,20
dataset/preprocessed/training-data/natural_language_inference/42,where s i and ?,47,0,5
dataset/preprocessed/training-data/natural_language_inference/42,i are scalars and f is a score function that measures the importance of pi relative to q j .,48,0,20
dataset/preprocessed/training-data/natural_language_inference/42,"Intuitively , a large weight ?",49,0,6
dataset/preprocessed/training-data/natural_language_inference/93,Gated Self - Matching Networks for Reading Comprehension and Question Answering,2,1,11
dataset/preprocessed/training-data/natural_language_inference/93,"In this paper , we present the gated selfmatching networks for reading comprehension style question answering , which aims to answer questions from a given passage .",4,1,27
dataset/preprocessed/training-data/natural_language_inference/93,We first match the question and passage with gated attention - based recurrent networks to obtain the question - aware passage representation .,5,0,23
dataset/preprocessed/training-data/natural_language_inference/93,"Then we propose a self - matching attention mechanism to refine the representation by matching the passage against itself , which effectively encodes information from the whole passage .",6,0,29
dataset/preprocessed/training-data/natural_language_inference/93,We finally employ the pointer networks to locate the positions of answers from the passages .,7,0,16
dataset/preprocessed/training-data/natural_language_inference/93,We conduct extensive experiments on the SQuAD dataset .,8,0,9
dataset/preprocessed/training-data/natural_language_inference/93,"The single model achieves 71.3 % on the evaluation metrics of exact match on the hidden test set , while the ensemble model further boosts the results to 75.9 % .",9,0,31
dataset/preprocessed/training-data/natural_language_inference/93,"At the time of submission of the paper , our model holds the first place on the SQuAD leaderboard for both single and ensemble model .",10,0,26
dataset/preprocessed/training-data/natural_language_inference/93,"In this paper , we focus on reading comprehension style question answering which aims to answer questions given a passage or document .",12,0,23
dataset/preprocessed/training-data/natural_language_inference/93,"We specifically focus on the Stanford Question Answering Dataset ( SQuAD ) , a largescale dataset for reading comprehension and question answering which is manually created through crowdsourcing .",13,0,29
dataset/preprocessed/training-data/natural_language_inference/93,"SQuAD constrains answers to the space of all possible spans within the reference passage , which is different from cloze - style reading comprehension datasets ( Hermann et al. , * Contribution during internship at Microsoft Research . Equal contribution .",14,0,41
dataset/preprocessed/training-data/natural_language_inference/93,2015 ; in which answers are single words or entities .,15,0,11
dataset/preprocessed/training-data/natural_language_inference/93,"Moreover , SQuAD requires different forms of logical reasoning to infer the answer .",16,0,14
dataset/preprocessed/training-data/natural_language_inference/93,Rapid progress has been made since the release of the SQuAD dataset .,17,0,13
dataset/preprocessed/training-data/natural_language_inference/93,"build question - aware passage representation with match - LSTM , and predict answer boundaries in the passage with pointer networks .",18,0,22
dataset/preprocessed/training-data/natural_language_inference/93,introduce bi-directional attention flow networks to model question - passage pairs at multiple levels of granularity .,19,0,17
dataset/preprocessed/training-data/natural_language_inference/93,propose dynamic co-attention networks which attend the question and passage simultaneously and iteratively refine answer predictions .,20,0,17
dataset/preprocessed/training-data/natural_language_inference/93,and predict answers by ranking continuous text spans within passages .,21,0,11
dataset/preprocessed/training-data/natural_language_inference/93,"Inspired by , we introduce a gated self - matching network , illustrated in , an end - to - end neural network model for reading comprehension and question answering .",22,0,31
dataset/preprocessed/training-data/natural_language_inference/93,Our model consists of four parts :,23,0,7
dataset/preprocessed/training-data/natural_language_inference/93,"1 ) the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",24,0,54
dataset/preprocessed/training-data/natural_language_inference/93,The key contributions of this work are three - fold .,25,0,11
dataset/preprocessed/training-data/natural_language_inference/93,"First , we propose a gated attention - based recurrent network , which adds an additional gate to the attention - based recurrent networks , to account for the fact that words in the passage are of different importance to answer a particular question for reading comprehension and question answering .",26,0,51
dataset/preprocessed/training-data/natural_language_inference/93,"In , words in a passage with their corresponding attention - weighted question context are en - coded together to produce question - aware passage representation .",27,0,27
dataset/preprocessed/training-data/natural_language_inference/93,"By introducing a gating mechanism , our gated attention - based recurrent network assigns different levels of importance to passage parts depending on their relevance to the question , masking out irrelevant passage parts and emphasizing the important ones .",28,0,40
dataset/preprocessed/training-data/natural_language_inference/93,"Second , we introduce a self - matching mechanism , which can effectively aggregate evidence from the whole passage to infer the answer .",29,0,24
dataset/preprocessed/training-data/natural_language_inference/93,"Through a gated matching layer , the resulting question - aware passage representation effectively encodes question information for each passage word .",30,0,22
dataset/preprocessed/training-data/natural_language_inference/93,"However , recurrent networks can only memorize limited passage context in practice despite its theoretical capability .",31,0,17
dataset/preprocessed/training-data/natural_language_inference/93,One answer candidate is often unaware of the clues in other parts of the passage .,32,0,16
dataset/preprocessed/training-data/natural_language_inference/93,"To address this problem , we propose a self - matching layer to dynamically refine passage representation with information from the whole passage .",33,0,24
dataset/preprocessed/training-data/natural_language_inference/93,"Based on question - aware passage representation , we employ gated attention - based recurrent networks on passage against passage itself , aggregating evidence relevant to the current passage word from every word in the passage .",34,0,37
dataset/preprocessed/training-data/natural_language_inference/93,"A gated attention - based recurrent network layer and self - matching layer dynamically enrich each passage representation with information aggregated from both question and passage , enabling subsequent network to better predict answers .",35,0,35
dataset/preprocessed/training-data/natural_language_inference/93,"Lastly , the proposed method yields state - of - theart results against strong baselines .",36,0,16
dataset/preprocessed/training-data/natural_language_inference/93,"Our single model achieves 71.3 % exact match accuracy on the hidden SQuAD test set , while the ensemble model further boosts the result to 75.9 % .",37,0,28
dataset/preprocessed/training-data/natural_language_inference/93,"At the time 1 of submission of this paper , our model holds the first place on the SQuAD leader board .",38,0,22
dataset/preprocessed/training-data/natural_language_inference/93,"For reading comprehension style question answering , a passage P and question Q are given , our task is to predict an answer A to question Q based on information found in P .",40,0,34
dataset/preprocessed/training-data/natural_language_inference/93,The SQuAD dataset further constrains answer A to be a continuous subspan of passage P. Answer A often includes nonentities and can be much longer phrases .,41,0,27
dataset/preprocessed/training-data/natural_language_inference/93,This setup challenges us to understand and reason about both the question and passage in order to infer the answer .,42,0,21
dataset/preprocessed/training-data/natural_language_inference/93,shows a simple example from the SQuAD dataset .,43,0,9
dataset/preprocessed/training-data/natural_language_inference/93,"1 On Feb. 6 , 2017",44,0,6
dataset/preprocessed/training-data/natural_language_inference/93,Passage : Tesla later approached Morgan to ask for more funds to build a more powerful transmitter .,45,0,18
dataset/preprocessed/training-data/natural_language_inference/93,"When asked where all the money had gone , Tesla responded by saying that he was affected by the Panic of 1901 , which he ( Morgan ) had caused .",46,0,31
dataset/preprocessed/training-data/natural_language_inference/93,Morgan was shocked by the reminder of his part in the stock market crash and by Tesla 's breach of contract by asking for more funds .,47,0,27
dataset/preprocessed/training-data/natural_language_inference/93,"Tesla wrote another plea to Morgan , but it was also fruitless .",48,0,13
dataset/preprocessed/training-data/natural_language_inference/93,"Morgan still owed Tesla money on the original agreement , and Tesla had been facing foreclosure even before construction of the tower began .",49,0,24
dataset/preprocessed/training-data/natural_language_inference/59,Neural Paraphrase Identification of Questions with Noisy Pretraining,2,1,8
dataset/preprocessed/training-data/natural_language_inference/59,We present a solution to the problem of paraphrase identification of questions .,4,1,13
dataset/preprocessed/training-data/natural_language_inference/59,"We focus on a recent dataset of question pairs annotated with binary paraphrase labels and show that a variant of the decomposable attention model ( Parikh et al. , 2016 ) results in accurate performance on this task , while being far simpler than many competing neural architectures .",5,0,49
dataset/preprocessed/training-data/natural_language_inference/59,"Furthermore , when the model is pretrained on a noisy dataset of automatically collected question paraphrases , it obtains the best reported performance on the dataset .",6,0,27
dataset/preprocessed/training-data/natural_language_inference/59,Question paraphrase identification is a widely useful NLP application .,8,1,10
dataset/preprocessed/training-data/natural_language_inference/59,"For example , in question - andanswer ( QA ) forums ubiquitous on the Web , there are vast numbers of duplicate questions .",9,0,24
dataset/preprocessed/training-data/natural_language_inference/59,Identifying these duplicates and consolidating their answers increases the efficiency of such QA forums .,10,0,15
dataset/preprocessed/training-data/natural_language_inference/59,"Moreover , identifying questions with the same semantic content could help Web-scale question answering systems thatare increasingly concentrating on retrieving focused answers to users ' queries .",11,0,27
dataset/preprocessed/training-data/natural_language_inference/59,"Here , we focus on a recent dataset published by the QA website Quora.com containing over 400 K annotated question pairs containing binary paraphrase labels .",12,0,26
dataset/preprocessed/training-data/natural_language_inference/59,"We believe that this dataset presents a great opportunity to the NLP research community and practitioners due to its scale and quality ; it can result in systems that accurately identify duplicate questions , thus increasing the quality of many QA forums .",13,0,43
dataset/preprocessed/training-data/natural_language_inference/59,"We examine a simple model family , the decomposable attention model of , that has shown promise in modeling natural language inference and has inspired recent work on similar tasks .",14,0,31
dataset/preprocessed/training-data/natural_language_inference/59,We present two contributions .,15,0,5
dataset/preprocessed/training-data/natural_language_inference/59,"First , to mitigate data sparsity , we modify the input representation of the decomposable attention model to use sums of character n-gram embeddings instead of word embeddings .",16,0,29
dataset/preprocessed/training-data/natural_language_inference/59,"We show that this model trained on the Quora dataset produces comparable or better results with respect to several complex neural architectures , all using pretrained word embeddings .",17,0,29
dataset/preprocessed/training-data/natural_language_inference/59,"Second , to significantly improve our model performance , we pretrain all our model parameters on the noisy , automatically collected question - paraphrase corpus Paralex , followed by fine - tuning the parameters on the Quora dataset .",18,0,39
dataset/preprocessed/training-data/natural_language_inference/59,"This two - stage training procedure achieves the best result on the Quora dataset to date , and is also significantly better than learning only the character n-gram embeddings during the pretraining stage .",19,0,34
dataset/preprocessed/training-data/natural_language_inference/59,Paraphrase identification is a well - studied task in NLP .,21,0,11
dataset/preprocessed/training-data/natural_language_inference/59,"Here , we focus on an instance , that of finding questions with identical meaning .",22,0,16
dataset/preprocessed/training-data/natural_language_inference/59,"consider a related task leveraging the AskUbuntu corpus , but it contains two orders of magnitude less annotations , thus limiting the quality of any model .",23,0,27
dataset/preprocessed/training-data/natural_language_inference/59,"Most relevant to this work is that of , who present the best results on the Quora dataset prior to this work .",24,0,23
dataset/preprocessed/training-data/natural_language_inference/59,The bilateral multi-perspective matching model ( BIMPM ) of Wang et al .,25,0,13
dataset/preprocessed/training-data/natural_language_inference/59,"uses a character - based LSTM at its input representation layer , a layer of bi - LSTMs for computing context information , four different types of multi-perspective matching layers , an additional bi - LSTM aggregation layer , followed by a two - layer feedforward network for prediction .",26,0,50
dataset/preprocessed/training-data/natural_language_inference/59,"In contrast , the decomposable attention model uses four simple feedforward networks to ( self - ) attend , compare and predict , leading to a more efficient architecture .",27,0,30
dataset/preprocessed/training-data/natural_language_inference/59,BIMPM falls short of our best performing model pretrained on noisy paraphrase data and uses more parameters than our best model .,28,0,22
dataset/preprocessed/training-data/natural_language_inference/59,Character - level modeling of text is a popular approach .,29,0,11
dataset/preprocessed/training-data/natural_language_inference/59,"While conceptually simple , character n-gram embeddings are a highly competitive representation .",30,0,13
dataset/preprocessed/training-data/natural_language_inference/59,More complex representations built directly from individual characters have also been proposed .,31,0,13
dataset/preprocessed/training-data/natural_language_inference/59,"These representations are robust to out - of - vocabulary items , often producing improved results .",32,0,17
dataset/preprocessed/training-data/natural_language_inference/59,"Our pretraining procedure is reminiscent of several recent papers , inter alia ) who aim for general purpose character n-gram embeddings .",33,0,22
dataset/preprocessed/training-data/natural_language_inference/59,"In contrast , we pretrain all model parameters on automatic but in - domain paraphrase data .",34,0,17
dataset/preprocessed/training-data/natural_language_inference/59,"We employ the same neural architecture as our end task , similar to prior work on multi-task learning inter alia ) , but use a simpler learning setup .",35,0,29
dataset/preprocessed/training-data/natural_language_inference/59,"Our starting point is the decomposable attention model , which despite its simplicity and efficiency has been shown to work remarkably well for the related task of natural language inference .",37,0,31
dataset/preprocessed/training-data/natural_language_inference/59,We extend this model with character n-gram embeddings and noisy pretraining for the task of question paraphrase identification .,38,0,19
dataset/preprocessed/training-data/natural_language_inference/59,"Leta = ( a 1 , . . . , a a ) and b = ( b 1 , . . . , b b ) be two input texts consisting of a and b tokens , respectively .",40,0,40
dataset/preprocessed/training-data/natural_language_inference/59,"We assume that each a i , b j ?",41,0,10
dataset/preprocessed/training-data/natural_language_inference/59,Rd is encoded in a vector of dimension d.,42,0,9
dataset/preprocessed/training-data/natural_language_inference/59,"A context window of size c is subsequently applied , such that the input to the model ( ? , b ) consists of partly overlap -",43,0,27
dataset/preprocessed/training-data/natural_language_inference/59,1 } is a binary label indicating whether a is a paraphrase of b or not .,44,0,17
dataset/preprocessed/training-data/natural_language_inference/59,"Our goal is to predict the correct label y given a pair of previously unseen texts ( a , b ) .",45,0,22
dataset/preprocessed/training-data/natural_language_inference/59,The Decomposable Attention Model,46,0,4
dataset/preprocessed/training-data/natural_language_inference/59,"The DECATT model divides the prediction into three steps : Attend , Compare and Aggregate .",47,0,16
dataset/preprocessed/training-data/natural_language_inference/59,"Due to lack of space , we only provide a brief outline below and refer to for further details on each of these steps .",48,0,25
dataset/preprocessed/training-data/natural_language_inference/80,DR- BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference,2,1,11
dataset/preprocessed/training-data/natural_language_inference/80,We present a novel deep learning architecture to address the natural language inference ( NLI ) task .,4,1,18
dataset/preprocessed/training-data/natural_language_inference/80,Existing approaches mostly rely on simple reading mechanisms for independent encoding of the premise and hypothesis .,5,0,17
dataset/preprocessed/training-data/natural_language_inference/80,"Instead , we propose a novel dependent reading bidirectional LSTM network ( DR - BiLSTM ) to efficiently model the relationship between a premise and a hypothesis during encoding and inference .",6,0,32
dataset/preprocessed/training-data/natural_language_inference/80,"We also introduce a sophisticated ensemble strategy to combine our proposed models , which noticeably improves final predictions .",7,0,19
dataset/preprocessed/training-data/natural_language_inference/80,"Finally , we demonstrate how the results can be improved further with an additional preprocessing step .",8,0,17
dataset/preprocessed/training-data/natural_language_inference/80,Our evaluation shows that DR - BiLSTM obtains the best single model and ensemble model results achieving the new state - of - the - art scores on the Stanford NLI dataset .,9,0,33
dataset/preprocessed/training-data/natural_language_inference/80,Natural Language Inference ( NLI ; a.k.a. is an important and challenging task for natural language understanding .,11,0,18
dataset/preprocessed/training-data/natural_language_inference/80,"The goal of NLI is to identify the logical relationship ( entailment , neutral , or contradiction ) between a premise and a corresponding hypothesis .",12,1,26
dataset/preprocessed/training-data/natural_language_inference/80,shows few example relationships from the Stanford Natural Language Inference ( SNLI ) dataset .,13,0,15
dataset/preprocessed/training-data/natural_language_inference/80,"Recently , NLI has received a lot of attention from the researchers , especially due to the availability of large annotated datasets like SNLI ( Bow - * This work was conducted as part of an internship program at Philips Research .",14,0,42
dataset/preprocessed/training-data/natural_language_inference/80,A senior is waiting at the Relationship window of a restaurant that serves sandwiches .,16,0,15
dataset/preprocessed/training-data/natural_language_inference/80,A person waits to be Entailment served his food .,18,0,10
dataset/preprocessed/training-data/natural_language_inference/80,A man is looking to order Neutral a grilled cheese sandwich .,19,0,12
dataset/preprocessed/training-data/natural_language_inference/80,"A man is waiting inline Contradiction for the bus. a P , Premise . b H , Hypothesis ..",20,0,19
dataset/preprocessed/training-data/natural_language_inference/80,Various deep learning models have been proposed that achieve successful results for this task .,21,0,15
dataset/preprocessed/training-data/natural_language_inference/80,Most of these existing NLI models use attention mechanism to jointly interpret and align the premise and hypothesis .,22,0,19
dataset/preprocessed/training-data/natural_language_inference/80,Such models use simple reading mechanisms to encode the premise and hypothesis independently .,23,0,14
dataset/preprocessed/training-data/natural_language_inference/80,"However , such a complex task require explicit modeling of dependency relationships between the premise and the hypothesis during the encoding and inference processes to prevent the network from the loss of relevant , contextual information .",24,0,37
dataset/preprocessed/training-data/natural_language_inference/80,"In this paper , we refer to such strategies as dependent reading .",25,0,13
dataset/preprocessed/training-data/natural_language_inference/80,There are some alternative reading mechanisms available in the literature that consider dependency aspects of the premise - hypothesis relationships .,26,0,21
dataset/preprocessed/training-data/natural_language_inference/80,"However , these mechanisms have two major limitations :",27,0,9
dataset/preprocessed/training-data/natural_language_inference/80,"So far , they have only explored dependency aspects during the encoding stage , while ignoring its benefit during inference .",28,0,21
dataset/preprocessed/training-data/natural_language_inference/80,"Such models only consider encoding a hypothesis depending on the premise , disre - arXiv : 1802.05577v2 [ cs.CL ] 11 Apr 2018 garding the dependency aspects in the opposite direction .",29,0,32
dataset/preprocessed/training-data/natural_language_inference/80,We propose a dependent reading bidirectional LSTM ( DR - BiLSTM ) model to address these limitations .,30,0,18
dataset/preprocessed/training-data/natural_language_inference/80,"Given a premise u and a hypothesis v , our model first encodes them considering dependency on each other .",31,0,20
dataset/preprocessed/training-data/natural_language_inference/80,"Next , the model employs a soft attention mechanism to extract relevant information from these encodings .",32,0,17
dataset/preprocessed/training-data/natural_language_inference/80,"The augmented sentence representations are then passed to the inference stage , which uses a similar dependent reading strategy in both directions , i.e. u ? v and v ? u .",33,0,32
dataset/preprocessed/training-data/natural_language_inference/80,"Finally , a decision is made through a multi - layer perceptron ( MLP ) based on the aggregated information .",34,0,21
dataset/preprocessed/training-data/natural_language_inference/80,"Our experiments on the SNLI dataset show that DR - BiLSTM achieves the best single model and ensemble model performance obtaining improvements of a considerable margin of 0.4 % and 0.3 % over the previous state - of - the - art single and ensemble models , respectively .",35,0,49
dataset/preprocessed/training-data/natural_language_inference/80,"Furthermore , we demonstrate the importance of a simple preprocessing step performed on the SNLI dataset .",36,0,17
dataset/preprocessed/training-data/natural_language_inference/80,Evaluation results show that such preprocessing allows our single model to achieve the same accuracy as the state - of - the - art ensemble model and improves our ensemble model to outperform the state - of - the - art ensemble model by a remarkable margin of 0.7 % .,37,0,51
dataset/preprocessed/training-data/natural_language_inference/80,"Finally , we perform an extensive analysis to clarify the strengths and weaknesses of our models .",38,0,17
dataset/preprocessed/training-data/natural_language_inference/80,Early studies use small datasets while leveraging lexical and syntactic features for NLI .,40,0,14
dataset/preprocessed/training-data/natural_language_inference/80,The recent availability of large - scale annotated datasets has enabled researchers to develop various deep learning - based architectures for NLI .,41,0,23
dataset/preprocessed/training-data/natural_language_inference/80,propose an attention - based model ) that decomposes the NLI task into sub-problems to solve them in parallel .,42,0,20
dataset/preprocessed/training-data/natural_language_inference/80,They further show the benefit of adding intra-sentence attention to input representations .,43,0,13
dataset/preprocessed/training-data/natural_language_inference/80,explore sequential inference models based on chain LSTMs with attentional input encoding and demonstrate the effectiveness of syntactic information .,44,0,20
dataset/preprocessed/training-data/natural_language_inference/80,We also use similar attention mechanisms .,45,0,7
dataset/preprocessed/training-data/natural_language_inference/80,"However , our model is distinct from these models as they do not benefit from dependent reading strategies .",46,0,19
dataset/preprocessed/training-data/natural_language_inference/80,use a word - by - word neural attention mechanism while propose re-read LSTM units by considering the dependency of a hypothesis on the information of its premise ( v |u ) to achieve promising results .,47,0,37
dataset/preprocessed/training-data/natural_language_inference/80,"However , these models suffer from weak inferencing methods by disregarding the dependency aspects from the opposite direction .",48,0,19
dataset/preprocessed/training-data/natural_language_inference/80,"Intuitively , when a human judges a premise - hypothesis relationship , s/ he might consider back - and - forth reading of both sentences before coming to a conclusion .",49,0,31
dataset/preprocessed/training-data/natural_language_inference/31,Simple and Effective Text Matching with Richer Alignment Features,2,1,9
dataset/preprocessed/training-data/natural_language_inference/31,"In this paper , we present a fast and strong neural approach for general purpose text matching applications .",4,0,19
dataset/preprocessed/training-data/natural_language_inference/31,"We explore what is sufficient to build a fast and well - performed text matching model and propose to keep three key features available for inter-sequence alignment : original point - wise features , previous aligned features , and contextual features while simplifying all the remaining components .",5,0,48
dataset/preprocessed/training-data/natural_language_inference/31,"We conduct experiments on four well - studied benchmark datasets across tasks of natural language inference , paraphrase identification and answer selection .",6,0,23
dataset/preprocessed/training-data/natural_language_inference/31,The performance of our model is on par with the state - of - the - art on all datasets with much fewer parameters and the inference speed is at least 6 times faster compared with similarly performed ones .,7,0,40
dataset/preprocessed/training-data/natural_language_inference/31,Text matching is a core research are a in natural language processing with along history .,9,0,16
dataset/preprocessed/training-data/natural_language_inference/31,"In text matching tasks , a model takes two text sequences as input and predicts a category or a scala value indicating their relationship .",10,0,25
dataset/preprocessed/training-data/natural_language_inference/31,"A wide range of tasks , including natural language inference ( also known as recognizing textual entailment ) , paraphrase identification , answer selection , and soon , can be seen as specific forms of text matching problems .",11,0,39
dataset/preprocessed/training-data/natural_language_inference/31,Research on general purpose text matching algorithm is beneficial to a large number of relevant applications .,12,0,17
dataset/preprocessed/training-data/natural_language_inference/31,Deep neural networks are the most popular choices for text matching nowadays .,13,0,13
dataset/preprocessed/training-data/natural_language_inference/31,Semantic alignment and comparison of two text sequences are the keys in neural text matching .,14,0,16
dataset/preprocessed/training-data/natural_language_inference/31,Many previous deep neural networks contain a single intersequence alignment layer .,15,0,12
dataset/preprocessed/training-data/natural_language_inference/31,"To make full use of this only alignment process , the model has to take rich external syntactic features or hand - designed align - ment features as additional inputs of the alignment layer , adopt a complicated alignment mechanism , or build avast amount of post-processing layers to analyze the alignment result .",16,0,54
dataset/preprocessed/training-data/natural_language_inference/31,More powerful models can be built with multiple inter-sequence alignment layers .,17,0,12
dataset/preprocessed/training-data/natural_language_inference/31,"Instead of making a prediction based on the comparison result of a single alignment process , a stacked model with multiple alignment layers maintains its intermediate states and gradually refines its predictions .",18,0,33
dataset/preprocessed/training-data/natural_language_inference/31,"However , suffering from inefficient propagation of lower - level features and vanishing gradients , these deeper architectures are harder to train .",19,0,23
dataset/preprocessed/training-data/natural_language_inference/31,"Recent works have come up with ways of connecting stacked building blocks including dense connection and recurrent neural networks , which strengthen the propagation of lower - level features and yield better results than those with a single alignment process .",20,0,41
dataset/preprocessed/training-data/natural_language_inference/31,"This paper presents RE2 , a fast and strong neural architecture with multiple alignment processes for general purpose text matching .",21,0,21
dataset/preprocessed/training-data/natural_language_inference/31,"We question the necessity of many slow components in text matching approaches presented in previous literature , including complicated multi-way alignment mechanisms , heavy distillations of alignment results , external syntactic features , or dense connections to connect stacked blocks when the model is going deep .",22,0,47
dataset/preprocessed/training-data/natural_language_inference/31,These design choices slowdown the model by a large amount and can be replaced by much more lightweight and equally effective ones .,23,0,23
dataset/preprocessed/training-data/natural_language_inference/31,"Meanwhile , we highlight three key components for an efficient text matching model .",24,0,14
dataset/preprocessed/training-data/natural_language_inference/31,"These components , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , original point - wise features ( Embedding vectors ) , and contextual features ( Encoded vectors ) .",25,0,37
dataset/preprocessed/training-data/natural_language_inference/31,The re-maining components can be as simple as possible to keep the model fast while still yielding strong performance .,26,0,20
dataset/preprocessed/training-data/natural_language_inference/31,The general architecture of RE2 is illustrated in .,27,0,9
dataset/preprocessed/training-data/natural_language_inference/31,An embedding layer first embeds discrete tokens .,28,0,8
dataset/preprocessed/training-data/natural_language_inference/31,"Several same - structured blocks consisting of encoding , alignment and fusion layers then process the sequences consecutively .",29,0,19
dataset/preprocessed/training-data/natural_language_inference/31,These blocks are connected by an augmented version of residual connections ( see section 2.1 ) .,30,0,17
dataset/preprocessed/training-data/natural_language_inference/31,A pooling layer aggregates sequential representations into vectors which are finally processed by a prediction layer to give the final prediction .,31,0,22
dataset/preprocessed/training-data/natural_language_inference/31,"The implementation of each layer is kept as simple as possible , and the whole model , as a well - organized combination , is quite powerful and lightweight at the same time .",32,0,34
dataset/preprocessed/training-data/natural_language_inference/31,"Our proposed method achieves the performance on par with the state - of - the - art on four benchmark datasets across three different tasks , namely SNLI and SciTail for natural language inference , Quora Question Pairs for paraphrase identification , and WikiQA for answer selection .",33,0,48
dataset/preprocessed/training-data/natural_language_inference/31,"Furthermore , our model has the least number of parameters and the fastest inference speed in all similarlyperformed models .",34,0,20
dataset/preprocessed/training-data/natural_language_inference/31,"We also conduct an ablation study to compare with alternative implementations of most components , perform robustness checks to see whether the model is robust to changes of structural hyperparameters , explore what roles the three key features in RE2 play by comparing their occlusion sensitivity and show the evolution of alignment results by a case study .",35,0,58
dataset/preprocessed/training-data/natural_language_inference/31,We release the source code 1 of our experiments for reproducibility and hope to facilitate future researches .,36,0,18
dataset/preprocessed/training-data/natural_language_inference/31,"In this section , we introduce our proposed approach RE2 for text matching .",38,0,14
dataset/preprocessed/training-data/natural_language_inference/31,gives an illustration of the over all architecture .,39,0,9
dataset/preprocessed/training-data/natural_language_inference/31,"Two text sequences are processed symmetrically before the prediction layer , and all parameters except those in the prediction layer are shared between the two sequences .",40,0,27
dataset/preprocessed/training-data/natural_language_inference/31,"For conciseness , we omit the part for the other sequence in the figure .",41,0,15
dataset/preprocessed/training-data/natural_language_inference/31,"In RE2 , tokens in each sequence are first embedded by the embedding layer and then processed consecutively by N same - structured blocks with independent parameters ( dashed boxes in : An overview of RE2 .",42,0,37
dataset/preprocessed/training-data/natural_language_inference/31,"There are three parts in the input of alignment and fusion layers : original pointwise features ( Embedding vectors , denoted by blank rectangles ) , previous aligned features ( Residual vectors , denoted by rectangles with diagonal stripes ) , and contextual features ( Encoded vectors , denoted by solid rectangles ) .",43,0,54
dataset/preprocessed/training-data/natural_language_inference/31,The architecture on the right is the same as the one on the left so it 's omitted for conciseness .,44,0,21
dataset/preprocessed/training-data/natural_language_inference/31,1 ) connected by augmented residual connections .,45,0,8
dataset/preprocessed/training-data/natural_language_inference/31,"Inside each block , a sequence encoder first computes contextual features of the sequence ( solid rectangles in ) .",46,0,20
dataset/preprocessed/training-data/natural_language_inference/31,The input and output of the encoder are concatenated and then fed into an alignment layer to model the alignment and interaction between the two sequences .,47,0,27
dataset/preprocessed/training-data/natural_language_inference/31,A fusion layer fuses the input and output of the alignment layer .,48,0,13
dataset/preprocessed/training-data/natural_language_inference/31,The output of the fusion layer is considered as the output of this block .,49,0,15
dataset/preprocessed/training-data/natural_language_inference/65,Attentive Pooling Networks,2,0,3
dataset/preprocessed/training-data/natural_language_inference/65,"In this work , we propose Attentive Pooling ( AP ) , a two - way attention mechanism for discriminative model training .",4,0,23
dataset/preprocessed/training-data/natural_language_inference/65,"In the context of pair - wise ranking or classification with neural networks , AP enables the pooling layer to be aware of the current input pair , in a way that information from the two input items can directly influence the computation of each other 's representations .",5,0,49
dataset/preprocessed/training-data/natural_language_inference/65,"Along with such representations of the paired inputs , AP jointly learns a similarity measure over projected segments ( e.g. trigrams ) of the pair , and subsequently , derives the corresponding attention vector for each input to guide the pooling .",6,0,42
dataset/preprocessed/training-data/natural_language_inference/65,"Our two - way attention mechanism is a general framework independent of the underlying representation learning , and it has been applied to both convolutional neural networks ( CNNs ) and recurrent neural networks ( RNNs ) in our studies .",7,0,41
dataset/preprocessed/training-data/natural_language_inference/65,"The empirical results , from three very different benchmark tasks of question answering / answer selection , demonstrate that our proposed models outperform a variety of strong baselines and achieve state - of - the - art performance in all the benchmarks .",8,0,43
dataset/preprocessed/training-data/natural_language_inference/65,"Neural networks ( NN ) with attention mechanisms have recently proven to be successful at different computer vision ( CV ) and natural language processing ( NLP ) tasks such as image captioning , machine translation and factoid question answering .",10,1,41
dataset/preprocessed/training-data/natural_language_inference/65,"However , most recent work on neural attention models have focused on one - way attention mechanisms based on recurrent neural networks designed . for generation tasks .",11,1,28
dataset/preprocessed/training-data/natural_language_inference/65,"Another important family of machine learning tasks are centered around pair - wise ranking or classification , which have a broad set of applications , including but not limited to , question answering , entailment , paraphrasing and any other pair - wise matching problems .",12,0,46
dataset/preprocessed/training-data/natural_language_inference/65,"The current state - of the - art models usually include NN - based representation for the input pair , followed by a discriminative ranking or classification models .",13,0,29
dataset/preprocessed/training-data/natural_language_inference/65,"For example , a convolution ( or a RNN ) and a max - pooling is used to independently construct distributed vector representations of the input pair , followed by a large - margin training .",14,0,36
dataset/preprocessed/training-data/natural_language_inference/65,"The key contribution of this work is that we propose Attentive Pooling ( AP ) , a two - way attention mechanism , that significantly improves such discriminative models ' performance on pair - wise ranking or classification , by enabling a joint learning of the representations of both inputs as well as their similarity measurement .",15,0,57
dataset/preprocessed/training-data/natural_language_inference/65,"Specifically , AP enables the pooling layer to be aware of the current input pair , in a way that information from the two input items can directly influence the computation of each other 's representations .",16,0,37
dataset/preprocessed/training-data/natural_language_inference/65,"The main idea in AP consists of learning a similarity measure over projected segments ( e.g. trigrams ) of the two items in the input pair , and using the similarity scores between the segments to compute attention vectors in both directions .",17,0,43
dataset/preprocessed/training-data/natural_language_inference/65,"Next , the attention vectors are used to perform pooling .",18,0,11
dataset/preprocessed/training-data/natural_language_inference/65,There are a few key benefits of our model .,19,0,10
dataset/preprocessed/training-data/natural_language_inference/65,"Thanks to the two - way attention , our model projects the paired inputs , even though they may not be always semantically comparable for some applications ( e.g. , questions and answers in question answering ) , into a common representation space that they can be compared in a more plausible way .",20,0,54
dataset/preprocessed/training-data/natural_language_inference/65,Our model is effective in matching pairs of inputs with significant length variations .,21,0,14
dataset/preprocessed/training-data/natural_language_inference/65,The two - way attention mechanism is independent of the underlying representation learning .,22,0,14
dataset/preprocessed/training-data/natural_language_inference/65,"For example , AP can be applied to both CNNs and RNNs , which is in contrast to the one - way attention used in the generation models mostly based on recurrent nets .",23,0,34
dataset/preprocessed/training-data/natural_language_inference/65,"In this work , we perform an extensive number of experiments on applying attentive pooling CNNs ( AP - CNN ) and biLSTMs ( AP - biLSTM ) for the answer selection task .",24,0,34
dataset/preprocessed/training-data/natural_language_inference/65,"In this task , given a question q and an candidate answer pool P = {a 1 , a 2 , , a p } for this question , the goal is to search for and select the candidate answer a ?",25,0,42
dataset/preprocessed/training-data/natural_language_inference/65,P that correctly answers q .,26,0,6
dataset/preprocessed/training-data/natural_language_inference/65,"We perform experiments with three publicly available benchmark datasets , which vary in data scale , complexity and length ratios between question and answers : InsuranceQA , TREC - QA and Wiki QA .",27,0,34
dataset/preprocessed/training-data/natural_language_inference/65,"For the three datasets , AP - CNN and AP - biLSTM respectively outperform the CNN and the biLSTM that do not use attention .",28,0,25
dataset/preprocessed/training-data/natural_language_inference/65,"Additionally , AP - CNN achieves state - of - the - art results for the three datasets .",29,0,19
dataset/preprocessed/training-data/natural_language_inference/65,Our experimental results also demonstrate that attentive pooling makes the CNN more robust to large input texts .,30,0,18
dataset/preprocessed/training-data/natural_language_inference/65,"This is an important finding , since recent work have demonstrated that , in the context of semantically equivalent question retrieval , CNN based representations do not scale well with the size of the input text ( dos .",31,0,39
dataset/preprocessed/training-data/natural_language_inference/65,"Additionally , as AP - CNN does not rely only on the final vector representation to capture interactions between the input question and answer , it requires much less convolutional filters than the regular CNN .",32,0,36
dataset/preprocessed/training-data/natural_language_inference/65,"It means that AP - CNN - based representations are more compact , which can help to speedup the training process .",33,0,22
dataset/preprocessed/training-data/natural_language_inference/65,"Although we demonstrate experimental results for NLP tasks only , AP is a general method that can be also applied to different types of NNs that perform matching of two inputs .",34,0,32
dataset/preprocessed/training-data/natural_language_inference/65,"Therefore , we believe that AP can be useful for different applications , such as computer vision and bioinformatics .",35,0,20
dataset/preprocessed/training-data/natural_language_inference/65,This paper is organized as follows .,36,0,7
dataset/preprocessed/training-data/natural_language_inference/65,"In Section 2 , we describe two NN architectures for answer selection that have been recently proposed in the literature .",37,0,21
dataset/preprocessed/training-data/natural_language_inference/65,"In Section 3 , we detail the attentive pooling approach .",38,0,11
dataset/preprocessed/training-data/natural_language_inference/65,"In Section 4 , we discuss some related work .",39,0,10
dataset/preprocessed/training-data/natural_language_inference/65,"Sections 5 and 6 detail our experimental setup and results , respectively .",40,0,13
dataset/preprocessed/training-data/natural_language_inference/65,In Section 7 we present our final remarks .,41,0,9
dataset/preprocessed/training-data/natural_language_inference/65,Neural Networks for Answer Selection,42,0,5
dataset/preprocessed/training-data/natural_language_inference/65,Different neural network architectures have been recently proposed to perform matching of semantically related text segments .,43,0,17
dataset/preprocessed/training-data/natural_language_inference/65,In this section we briefly review two NN architectures that have previously been applied to the answer selection task : QA - CNN and QA - biLSTM .,44,0,28
dataset/preprocessed/training-data/natural_language_inference/65,"Given a pair ( q , a ) consisting of a question q and a candidate answer a , both networks score the pair by first computing fixed - length independent continuous vector representations r q and r a , and then computing the cosine similarity between these two vectors .",45,0,51
dataset/preprocessed/training-data/natural_language_inference/65,In we present a joint illustration of these two neural networks .,46,0,12
dataset/preprocessed/training-data/natural_language_inference/65,The first layer in both QA - CNN and QA - biLSTM transforms each input word w into a fixed - size real - valued word embedding r w ?,47,0,30
dataset/preprocessed/training-data/natural_language_inference/65,Word embeddings ( WEs ) are encoded by column vectors in an embedding matrix W 0 ?,49,0,17
dataset/preprocessed/training-data/natural_language_inference/92,A Discrete Hard EM Approach for Weakly Supervised Question Answering,2,1,10
dataset/preprocessed/training-data/natural_language_inference/92,Many question answering ( QA ) tasks only provide weak supervision for how the answer should be computed .,4,1,19
dataset/preprocessed/training-data/natural_language_inference/92,"For example , TRIVIAQA answers are entities that can be mentioned multiple times in supporting documents , while DROP answers can be computed by deriving many different equations from numbers in the reference text .",5,0,35
dataset/preprocessed/training-data/natural_language_inference/92,"In this paper , we show it is possible to convert such tasks into discrete latent variable learning problems with a precomputed , task - specific set of possible solutions ( e.g. different mentions or equations ) that contains one correct option .",6,0,43
dataset/preprocessed/training-data/natural_language_inference/92,We then develop a hard EM learning scheme that computes gradients relative to the most likely solution at each update .,7,0,21
dataset/preprocessed/training-data/natural_language_inference/92,"Despite its simplicity , we show that this approach significantly outperforms previous methods on six QA tasks , including absolute gains of 2 - 10 % , and achieves the stateof - the - art on five of them .",8,1,40
dataset/preprocessed/training-data/natural_language_inference/92,"Using hard updates instead of maximizing marginal likelihood is key to these results as it encourages the model to find the one correct answer , which we show through detailed qualitative analysis .",9,0,33
dataset/preprocessed/training-data/natural_language_inference/92,A natural setting in many question answering ( QA ) tasks is to provide weak supervision to determine how the question should be answered given the evidence text .,12,0,29
dataset/preprocessed/training-data/natural_language_inference/92,"For example , as seen in 1 , TRIVIAQA answers are entities that can be mentioned multiple times in supporting documents , while DROP answers can be computed by deriving many different equations from numbers in the reference text .",13,0,40
dataset/preprocessed/training-data/natural_language_inference/92,"Such weak supervision is attractive because it is relatively easy to gather , allowing for large datasets , but complicates learning because there are many different spurious ways to derive the correct answer .",14,0,34
dataset/preprocessed/training-data/natural_language_inference/92,"It is natural to The answer text is mentioned five times in the given document , however , only the fourth span actually answers the question .",15,0,27
dataset/preprocessed/training-data/natural_language_inference/92,( Bottom ) Reading comprehension with discrete reasoning .,16,0,9
dataset/preprocessed/training-data/natural_language_inference/92,"There are many potential equations which execute the answer ( ' 4 ' ) , but only one of them is the correct equation and the others are false positives .",17,0,31
dataset/preprocessed/training-data/natural_language_inference/92,"model such ambiguities with a latent variable during learning , but most prior work on reading comprehension has rather focused on the model architecture and used heuristics to map the weak signal to full supervision ( e.g. by selecting the first answer span in TRIVIAQA ) .",18,0,47
dataset/preprocessed/training-data/natural_language_inference/92,"Some models are trained with maximum marginal likelihood ( MML ) , but it is unclear if it gives a meaningful improvement over the heuristics .",19,0,26
dataset/preprocessed/training-data/natural_language_inference/92,"In this paper , we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent - variable learning problems .",20,0,27
dataset/preprocessed/training-data/natural_language_inference/92,"First , we define a solution to be a particular derivation of a model to predict the answer ( e.g. a span in : Six QA datasets in three different categories used in this paper ( detailed in Section 5 ) along with the size of each dataset .",21,0,49
dataset/preprocessed/training-data/natural_language_inference/92,An average and median of the size of precomputed solution sets ( denoted by Z ) are also reported .,22,0,20
dataset/preprocessed/training-data/natural_language_inference/92,Details on how to obtain Z are given in Section 4 .,23,0,12
dataset/preprocessed/training-data/natural_language_inference/92,the document or an equation to compute the answer ) .,24,0,11
dataset/preprocessed/training-data/natural_language_inference/92,"We demonstrate that for many recently introduced tasks , which we group into three categories as given in , it is relatively easy to precompute a discrete , task - specific set of possible solutions that contains the correct solution along with a modest number of spurious options .",25,0,49
dataset/preprocessed/training-data/natural_language_inference/92,"The learning challenge is then to determine which solution in the set is the correct one , while estimating a complete QA model .",26,0,24
dataset/preprocessed/training-data/natural_language_inference/92,"We model the set of possible solutions as a discrete latent variable , and develop a learning strategy that uses hard - EM - style parameter updates .",27,0,28
dataset/preprocessed/training-data/natural_language_inference/92,"This algorithm repeatedly ( i ) predicts the most likely solution according to the current model from the precomputed set , and ( ii ) updates the model parameters to further encourage its own prediction .",28,0,36
dataset/preprocessed/training-data/natural_language_inference/92,"Intuitively , these hard updates more strongly enforce our prior beliefs that there is a single correct solution .",29,0,19
dataset/preprocessed/training-data/natural_language_inference/92,This method can be applied to any problem that fits our weak supervision assumptions and can be used with any model architecture .,30,0,23
dataset/preprocessed/training-data/natural_language_inference/92,We experiment on six different datasets ( Table 1 ) using strong task - specific model architectures .,31,0,18
dataset/preprocessed/training-data/natural_language_inference/92,"Our learning approach significantly outperforms previous methods which use heuristic supervision and MML updates , including absolute gains of 2 - 10 % , and achives the state - of - the - art on five datasets .",32,0,38
dataset/preprocessed/training-data/natural_language_inference/92,"It outperforms recent state - of - the - art reward - based semantic parsing algorithms by 13 % absolute percentage on WIKISQL , strongly suggesting that having a small precomputed set of possible solutions is a key ingredient .",33,0,40
dataset/preprocessed/training-data/natural_language_inference/92,"Finally , we present a detailed analysis showing that , in practice , the introduction of hard updates encourages models to assign much higher probability to the correct solution .",34,0,30
dataset/preprocessed/training-data/natural_language_inference/92,Reading Comprehension .,36,0,3
dataset/preprocessed/training-data/natural_language_inference/92,Large - scale reading comprehension ( RC ) tasks that provide full supervision for answer spans have seen significant progress recently .,37,0,22
dataset/preprocessed/training-data/natural_language_inference/92,"More recently , the community has moved towards more challenging tasks such as distantly supervised , RC with free - form human generated answers and RC requiring discrete or multi-hop reasoning .",38,0,32
dataset/preprocessed/training-data/natural_language_inference/92,These tasks introduce new learning challenges since the gold solution that is required to answer the question ( e.g. a span or an equation ) is not given .,39,0,29
dataset/preprocessed/training-data/natural_language_inference/92,"Nevertheless , not much work has been done for this particular learning challenge .",40,0,14
dataset/preprocessed/training-data/natural_language_inference/92,"Most work on RC focuses on the model architecture and simply chooses the first span or a random span from the document , rather than modeling this uncertainty as a latent choice .",41,0,33
dataset/preprocessed/training-data/natural_language_inference/92,"Others maximize the sum of the likelihood of multiple spans , but it is unclear if it gives a meaningful improvement .",42,0,22
dataset/preprocessed/training-data/natural_language_inference/92,"In this paper , we highlight the learning challenge and show that our learning method , independent of the model architecture , can give a significant gain .",43,0,28
dataset/preprocessed/training-data/natural_language_inference/92,"Specifically , we assume that one of mentions are related to the question and others are false positives because ( i ) this happens for most cases , as the first example in , and ( ii ) even in the case where multiple mentions contribute to the answer , there is often a single span which fits the question the best .",44,0,63
dataset/preprocessed/training-data/natural_language_inference/92,Semantic Parsing .,45,0,3
dataset/preprocessed/training-data/natural_language_inference/92,Latent - variable learning has been extensively studied in the literature of semantic parsing .,46,0,15
dataset/preprocessed/training-data/natural_language_inference/92,"For example , a question and an answer pair ( x , y) is given but the logical form that is used to compute the answer is not .",47,0,29
dataset/preprocessed/training-data/natural_language_inference/92,Two common learning paradigms are maximum marginal likelihood ( MML ) and rewardbased methods .,48,0,15
dataset/preprocessed/training-data/natural_language_inference/92,"In MML , the objective maximizes z?? P ( z | x ) , where ?",49,0,16
dataset/preprocessed/training-data/natural_language_inference/67,End - to - End Answer Chunk Extraction and Ranking for Reading Comprehension,2,1,13
dataset/preprocessed/training-data/natural_language_inference/67,"This paper proposes dynamic chunk reader ( DCR ) , an end - toend neural reading comprehension ( RC ) model that is able to extract and rank a set of answer candidates from a given document to answer questions .",4,1,41
dataset/preprocessed/training-data/natural_language_inference/67,"DCR is able to predict answers of variable lengths , whereas previous neural RC models primarily focused on predicting single tokens or entities .",5,0,24
dataset/preprocessed/training-data/natural_language_inference/67,"DCR encodes a document and an input question with recurrent neural networks , and then applies a word - by - word attention mechanism to acquire question - aware representations for the document , followed by the generation of chunk representations and a ranking module to propose the top - ranked chunk as the answer .",6,0,56
dataset/preprocessed/training-data/natural_language_inference/67,Experimental results show that DCR achieves stateof - the - art exact match and F 1 scores on the SQuAD dataset ( Rajpurkar et al. 2016 ) .,7,0,28
dataset/preprocessed/training-data/natural_language_inference/67,Reading comprehension - based question answering ( RCQA ) is the task of answering a question with a chunk of text taken from related document ( s ) .,9,1,29
dataset/preprocessed/training-data/natural_language_inference/67,A variety of neural models have been proposed recently either for extracting a single entity or a single token as an answer from a given text ( Hermann et al .,10,0,31
dataset/preprocessed/training-data/natural_language_inference/67,"In both cases , an answer boundary is either easy to determine or already given .",11,0,16
dataset/preprocessed/training-data/natural_language_inference/67,"Different from the above two assumptions for RCQA , in the real - world QA scenario , people may ask questions about both entities ( factoid ) and non-entities such as explanations and reasons ( non -factoid ) ( see for examples ) .",12,1,44
dataset/preprocessed/training-data/natural_language_inference/67,"In this regard , RCQA has the potential to complement other QA approaches that leverage structured data ( e.g. , knowledge bases ) for both the above question types .",13,0,30
dataset/preprocessed/training-data/natural_language_inference/67,"This is because RCQA can exploit the textual evidences to ensure increased answer coverage , which is particularly helpful for non-factoid answers .",14,0,23
dataset/preprocessed/training-data/natural_language_inference/67,"However , it is also challenging for RCQA to identify answer in arbitrary position in the passage with arbitrary length , especially for nonfactoid answers which might be clauses or sentences .",15,0,32
dataset/preprocessed/training-data/natural_language_inference/67,"As a result , apart from a few exceptions , this research direction has not been fully explored yet .",16,0,20
dataset/preprocessed/training-data/natural_language_inference/67,"Compared to the relatively easier RC task of predicting single tokens / entities 1 , predicting answers of arbitrary lengths and positions significantly increase the search space complexity : the number of possible candidates to consider is in the order of O ( n 2 ) , where n is the number of passage words .",17,0,56
dataset/preprocessed/training-data/natural_language_inference/67,"In contrast , for previous works in which answers are single tokens / entities or from candidate lists , the complexity is in O ( n ) or the size of candidate lists l ( usually l ? 5 ) , respectively .",18,0,43
dataset/preprocessed/training-data/natural_language_inference/67,"To address the above complexity , Rajpurkar et al .",19,0,10
dataset/preprocessed/training-data/natural_language_inference/67,"used a two - step chunk - and - rank approach that employs a rule - based algorithm to extract answer candidates from a passage , followed by a ranking approach with hand - crafted features to select the best answer .",20,0,42
dataset/preprocessed/training-data/natural_language_inference/67,The rule - based chunking approach suffered from low coverage ( ?,21,0,12
dataset/preprocessed/training-data/natural_language_inference/67,70 % recall of answer chunks ) that can not be improved during training ; and candidate ranking performance depends greatly on the quality of the hand - crafted features .,22,0,31
dataset/preprocessed/training-data/natural_language_inference/67,"More recently , proposed two endto - end neural network models , one of which chunks a candidate answer by predicting the answer 's two boundary indices and the other classifies each passage word into answer / notanswer .",23,0,39
dataset/preprocessed/training-data/natural_language_inference/67,Both models improved significantly over the method proposed by Rajpurkar et al ..,24,0,13
dataset/preprocessed/training-data/natural_language_inference/67,"Our proposed model , called dynamic chunk reader ( DCR ) , not only significantly differs from both the above systems in the way that answer candidates are generated and ranked , but also shares merits with both works .",25,0,40
dataset/preprocessed/training-data/natural_language_inference/67,"First , our model uses deep networks to learn better representations for candidate answer chunks , instead of using fixed feature representations as in .",26,0,25
dataset/preprocessed/training-data/natural_language_inference/67,"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",27,0,42
dataset/preprocessed/training-data/natural_language_inference/67,The contributions of this paper are three - fold .,28,0,10
dataset/preprocessed/training-data/natural_language_inference/67,"( 1 ) We pro - We also propose several simple but effective features to strengthen the attention mechanism , which fundamentally improves candidate ranking , with the by -product of higher exact boundary match accuracy .",29,0,37
dataset/preprocessed/training-data/natural_language_inference/67,"The experiments on the Stanford Question Answering Dataset ( SQuAD ) , which contains a variety of human - generated factoid and non-factoid questions , have shown the effectiveness of above three contributions .",30,0,34
dataset/preprocessed/training-data/natural_language_inference/67,Our paper is organized as follows .,31,0,7
dataset/preprocessed/training-data/natural_language_inference/67,We formally define the RCQA problem first .,32,0,8
dataset/preprocessed/training-data/natural_language_inference/67,"Next , we describe our baseline with a neural network component .",33,0,12
dataset/preprocessed/training-data/natural_language_inference/67,We present the end - to - end dynamic chunk reader model next .,34,0,14
dataset/preprocessed/training-data/natural_language_inference/67,"Finally , we analyze our experimental results and discuss the related work .",35,0,13
dataset/preprocessed/training-data/natural_language_inference/67,"shows an example of our RC setting where the goal is to answer a question Q i , factoid ( Q1 ) or non-factoid ( Q2 and Q3 ) , based on a supporting passage P i , by selecting a continuous sequence of text A i ?",36,0,48
dataset/preprocessed/training-data/natural_language_inference/67,P i as answer .,37,0,5
dataset/preprocessed/training-data/natural_language_inference/67,"Q i , P i , and A i are all word sequences , where each word is drawn from a vocabulary , V .",38,0,25
dataset/preprocessed/training-data/natural_language_inference/67,"The i - th instance in the training set is a triple in the form of ( P i , Q i , A i ) , where P i = ( p i 1 , . . . , p i| Pi| ) , Q i = ( q i 1 , . . . , q i| Qi| ) , and A i = ( a i 1 , . . . , a i | Ai | ) ( p i , q i , a i ? V ) .",39,0,94
dataset/preprocessed/training-data/natural_language_inference/67,"Owing to the dis agreement among annotators , there could be more than one correct answer for the same question ; and the k - th answer to Q i is denoted by",40,0,33
dataset/preprocessed/training-data/natural_language_inference/67,"An answer candidate for the i - th training example is defined as c m , n i , a sub-sequence in P i , that spans from position m ton ( 1 ? m ? n ? | P i | ) .",42,0,44
dataset/preprocessed/training-data/natural_language_inference/67,"The ground truth answer A i could be included in the set of all candidates Ci = { c m ,n i |? m , n ? N + , subj ( m , n , P i ) and 1 ? m ? n ? | P i |} , where subj ( m , n , P i ) is the constraint put on the candidate chunk for P i , such as , "" c m , n i can have at most 10 tokens "" , or "" c m , n i must have a pre-defined POS pattern "" .",43,0,105
dataset/preprocessed/training-data/natural_language_inference/67,"To evaluate a system 's performance , it s top answer to a question is matched against the corresponding gold standard answer ( s ) .",44,0,26
dataset/preprocessed/training-data/natural_language_inference/67,Remark : Categories of RC Tasks,45,0,6
dataset/preprocessed/training-data/natural_language_inference/67,Other simpler variants of the aforementioned RC task were explored in the past .,46,0,14
dataset/preprocessed/training-data/natural_language_inference/67,"For example , quiz - style datasets ( e.g. , MCTest ( Richardson , Burges , and Renshaw 2013 ) , Movie QA ) have multiple - choice questions with answer options .",47,0,33
dataset/preprocessed/training-data/natural_language_inference/67,"Cloze - style datesets , usually automatically generated , have factoid "" question "" s created by replacing the answer in a sentence from the text with blank .",48,0,29
dataset/preprocessed/training-data/natural_language_inference/67,"For the answer selection task this paper focuses on , several datasets exist , e.g.",49,0,15
dataset/preprocessed/training-data/natural_language_inference/37,Simple and Effective Multi - Paragraph Reading Comprehension,2,1,8
dataset/preprocessed/training-data/natural_language_inference/37,We consider the problem of adapting neural paragraph - level question answering models to the case where entire documents are given as input .,4,1,24
dataset/preprocessed/training-data/natural_language_inference/37,Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs .,5,0,18
dataset/preprocessed/training-data/natural_language_inference/37,"We sample multiple paragraphs from the documents during training , and use a sharednormalization training objective that encourages the model to produce globally correct output .",6,0,26
dataset/preprocessed/training-data/natural_language_inference/37,We combine this method with a stateof - the - art pipeline for training models on document QA data .,7,0,20
dataset/preprocessed/training-data/natural_language_inference/37,Experiments demonstrate strong performance on several document QA datasets .,8,0,10
dataset/preprocessed/training-data/natural_language_inference/37,"Overall , we are able to achieve a score of 71.3 F1 on the web portion of Triv - iaQA , a large improvement from the 56.7 F1 of the previous best system .",9,0,34
dataset/preprocessed/training-data/natural_language_inference/37,Teaching machines to answer arbitrary usergenerated questions is a long - term goal of natural language processing .,11,0,18
dataset/preprocessed/training-data/natural_language_inference/37,"For a wide range of questions , existing information retrieval methods are capable of locating documents thatare likely to contain the answer .",12,0,23
dataset/preprocessed/training-data/natural_language_inference/37,"However , automatically extracting the answer from those texts remains an open challenge .",13,0,14
dataset/preprocessed/training-data/natural_language_inference/37,The recent success of neural models at answering questions given a related paragraph suggests neural models have the potential to be a key part of a solution to this problem .,14,1,31
dataset/preprocessed/training-data/natural_language_inference/37,"Training and testing neural models that take entire documents as input is extremely computationally expensive , so typically this requires adapting a paragraph - level model to process document - level input .",15,0,33
dataset/preprocessed/training-data/natural_language_inference/37,There are two basic approaches to this task .,16,0,9
dataset/preprocessed/training-data/natural_language_inference/37,"Pipelined approaches select a single paragraph * Work completed while interning at the Allen Institute for Artificial Intelligence from the input documents , which is then passed to the paragraph model to extract an answer .",17,0,36
dataset/preprocessed/training-data/natural_language_inference/37,Confidence based methods apply the model to multiple paragraphs and returns the answer with the highest confidence .,18,0,18
dataset/preprocessed/training-data/natural_language_inference/37,"Confidence methods have the advantage of being robust to errors in the ( usually less sophisticated ) paragraph selection step , however they require a model that can produce accurate confidence scores for each paragraph .",19,0,36
dataset/preprocessed/training-data/natural_language_inference/37,"As we shall show , naively trained models often struggle to meet this requirement .",20,0,15
dataset/preprocessed/training-data/natural_language_inference/37,In this paper we start by proposing an improved pipelined method which achieves state - of - the - art results .,21,0,22
dataset/preprocessed/training-data/natural_language_inference/37,"Then we introduce a method for training models to produce accurate per-paragraph confidence scores , and we show how combining this method with multiple paragraph selection further increases performance .",22,0,30
dataset/preprocessed/training-data/natural_language_inference/37,Our pipelined method focuses on addressing the challenges that come with training on documentlevel data .,23,0,16
dataset/preprocessed/training-data/natural_language_inference/37,We propose a TF - IDF heuristic to select which paragraphs to train and test on .,24,0,17
dataset/preprocessed/training-data/natural_language_inference/37,"Since annotating entire documents is very expensive , data of this sort is typically distantly supervised , meaning only the answer text , not the answer spans , are known .",25,0,31
dataset/preprocessed/training-data/natural_language_inference/37,"To handle the noise this creates , we use a summed objective function that marginalizes the model 's output over all locations the answer text occurs .",26,0,27
dataset/preprocessed/training-data/natural_language_inference/37,"We apply this approach with a model design that integrates some recent ideas in reading comprehension models , including selfattention and bi-directional attention .",27,0,24
dataset/preprocessed/training-data/natural_language_inference/37,Our confidence method extends this approach to better handle the multi-paragraph setting .,28,0,13
dataset/preprocessed/training-data/natural_language_inference/37,Previous approaches trained the model on questions paired with paragraphs thatare known a priori to contain the answer .,29,0,19
dataset/preprocessed/training-data/natural_language_inference/37,"This has several downsides : the model is not trained to produce low confidence scores for paragraphs that do not contain an answer , and the training objective does not require confidence scores to be comparable between paragraphs .",30,0,39
dataset/preprocessed/training-data/natural_language_inference/37,"We resolve these problems by sampling paragraphs from the context documents , including paragraphs that do not contain an answer , to train on .",31,0,25
dataset/preprocessed/training-data/natural_language_inference/37,"We then use a shared - normalization objective where paragraphs are processed independently , but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document .",32,0,32
dataset/preprocessed/training-data/natural_language_inference/37,This requires the model to produce globally correct output even though each paragraph is processed independently .,33,0,17
dataset/preprocessed/training-data/natural_language_inference/37,"We evaluate our work on Trivia QA web , a dataset of questions paired with web documents that contain the answer .",34,0,22
dataset/preprocessed/training-data/natural_language_inference/37,"We achieve 71.3 F1 on the test set , a 15 point absolute gain over prior work .",35,0,18
dataset/preprocessed/training-data/natural_language_inference/37,"We additionally perform an ablation study on our pipelined method , and we show the effectiveness of our multi-paragraph methods on TriviaQA unfiltered and a modified version of SQuAD where only the correct document , not the correct paragraph , is known .",36,0,43
dataset/preprocessed/training-data/natural_language_inference/37,We also build a demonstration of our method by combining our model with a reimplementation of the retrieval mechanism used in TriviaQA to build a prototype end - to - end general question answering system 1 .,37,0,37
dataset/preprocessed/training-data/natural_language_inference/37,We release our code 2 to facilitate future work in this field .,38,0,13
dataset/preprocessed/training-data/natural_language_inference/37,"In this section we propose an approach to training pipelined question answering systems , where a single paragraph is heuristically extracted from the context document ( s ) and passed to a paragraphlevel QA model .",40,0,36
dataset/preprocessed/training-data/natural_language_inference/37,We suggest using a TF - IDF based paragraph selection method and argue that a summed objective function should be used to handle noisy supervision .,41,0,26
dataset/preprocessed/training-data/natural_language_inference/37,We also propose a refined model that incorporates some recent modeling ideas for reading comprehension systems .,42,0,17
dataset/preprocessed/training-data/natural_language_inference/37,Our paragraph selection method chooses the paragraph that has the smallest TF - IDF cosine distance with the question .,44,0,20
dataset/preprocessed/training-data/natural_language_inference/37,"Document frequencies are computed using just the paragraphs within the relevant documents , not the entire corpus .",45,0,18
dataset/preprocessed/training-data/natural_language_inference/37,"The advantage of this approach is that if a question word is prevalent in the context , for example if 1 documentqa.allenai.org 2 github.com/allenai / document - qa",46,0,28
dataset/preprocessed/training-data/natural_language_inference/37,"the word "" tiger "" is prevalent in the document ( s ) for the question "" What is the largest living subspecies of the tiger ? "" , greater weight will be given to question words thatare less common , such as "" largest "" or "" sub -species "" .",47,0,52
dataset/preprocessed/training-data/natural_language_inference/37,"Relative to selecting the first paragraph in the document , this improves the chance of the selected paragraph containing the correct answer from 83.1 % to 85.1 % on Triv - ia QA web .",48,0,35
dataset/preprocessed/training-data/natural_language_inference/37,We also expect this approach to do a better job of selecting paragraphs that relate to the question since it is explicitly selecting paragraphs that contain question words .,49,0,29
dataset/preprocessed/training-data/natural_language_inference/33,Published as a conference paper at ICLR 2018 LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS VIA LARGE SCALE MULTI - TASK LEARNING,2,1,23
dataset/preprocessed/training-data/natural_language_inference/33,A lot of the recent success in natural language processing ( NLP ) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner .,4,1,33
dataset/preprocessed/training-data/natural_language_inference/33,These representations are typically used as general purpose features for words across a range of NLP problems .,5,0,18
dataset/preprocessed/training-data/natural_language_inference/33,"However , extending this success to learning representations of sequences of words , such as sentences , remains an open problem .",6,1,22
dataset/preprocessed/training-data/natural_language_inference/33,Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed - length sentence representations .,7,0,25
dataset/preprocessed/training-data/natural_language_inference/33,"In this work , we present a simple , effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model .",8,0,30
dataset/preprocessed/training-data/natural_language_inference/33,We train this model on several data sources with multiple training objectives on over 100 million sentences .,9,0,18
dataset/preprocessed/training-data/natural_language_inference/33,Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods .,10,0,22
dataset/preprocessed/training-data/natural_language_inference/33,We present substantial improvements in the context of transfer learning and low - resource settings using our learned general - purpose representations .,11,0,23
dataset/preprocessed/training-data/natural_language_inference/33,Transfer learning has driven a number of recent successes in computer vision and NLP .,14,0,15
dataset/preprocessed/training-data/natural_language_inference/33,"Computer vision tasks like image captioning and visual question answering typically use CNNs pretrained on ImageNet to extract representations of the image , while several natural language tasks such as reading comprehension and sequence labeling have benefited from pretrained word embeddings thatare either fine - tuned for a specific task or held fixed .",15,0,54
dataset/preprocessed/training-data/natural_language_inference/33,"Many neural NLP systems are initialized with pretrained word embeddings but learn their representations of words in context from scratch , in a task - specific manner from supervised learning signals .",16,0,32
dataset/preprocessed/training-data/natural_language_inference/33,"However , learning these representations reliably from scratch is not always feasible , especially in low - resource settings , where we believe that using general purpose sentence representations will be beneficial .",17,0,33
dataset/preprocessed/training-data/natural_language_inference/33,Some recent work has addressed this by learning general - purpose sentence representations .,18,1,14
dataset/preprocessed/training-data/natural_language_inference/33,"However , there exists no clear consensus yet on what training objective or methodology is best suited to this goal .",19,0,21
dataset/preprocessed/training-data/natural_language_inference/33,Understanding the inductive biases of distinct neural models is important for guiding progress in representation learning .,20,0,17
dataset/preprocessed/training-data/natural_language_inference/33,and demonstrate that neural ma -chine translation ( NMT ) systems appear to capture morphology and some syntactic properties .,21,0,20
dataset/preprocessed/training-data/natural_language_inference/33,also present evidence that sequence - to - sequence parsers more strongly encode source language syntax .,22,0,17
dataset/preprocessed/training-data/natural_language_inference/33,"Similarly , probe representations extracted by sequence autoencoders , word embedding averages , and skip - thought vectors with a multi - layer perceptron ( MLP ) classifier to study whether sentence characteristics such as length , word content and word order are encoded .",23,0,45
dataset/preprocessed/training-data/natural_language_inference/33,"To generalize across a diverse set of tasks , it is important to build representations that encode several aspects of a sentence .",24,0,23
dataset/preprocessed/training-data/natural_language_inference/33,"Neural approaches to tasks such as skip - thoughts , machine translation , natural language inference , and constituency parsing likely have different inductive biases .",25,0,26
dataset/preprocessed/training-data/natural_language_inference/33,"Our work exploits this in the context of a simple one - to - many multi -task learning ( MTL ) framework , wherein a single recurrent sentence encoder is shared across multiple tasks .",26,0,35
dataset/preprocessed/training-data/natural_language_inference/33,"We hypothesize that sentence representations learned by training on a reasonably large number of weakly related tasks will generalize better to novel tasks unseen during training , since this process encodes the inductive biases of multiple models .",27,0,38
dataset/preprocessed/training-data/natural_language_inference/33,This hypothesis is based on the theoretical work of .,28,0,10
dataset/preprocessed/training-data/natural_language_inference/33,"While our work aims at learning fixed - length distributed sentence representations , it is not always practical to assume that the entire "" meaning "" of a sentence can be encoded into a fixed - length vector .",29,0,39
dataset/preprocessed/training-data/natural_language_inference/33,We merely hope to capture some of its characteristics that could be of use in a variety of tasks .,30,0,20
dataset/preprocessed/training-data/natural_language_inference/33,The primary contribution of our work is to combine the benefits of diverse sentence - representation learning objectives into a single multi-task framework .,31,0,24
dataset/preprocessed/training-data/natural_language_inference/33,"To the best of our knowledge , this is the first large - scale reusable sentence representation model obtained by combining a set of training objectives with the level of diversity explored here , i.e. multi-lingual NMT , natural language inference , constituency parsing and skip - thought vectors .",32,0,50
dataset/preprocessed/training-data/natural_language_inference/33,We demonstrate through extensive experimentation that representations learned in this way lead to improved performance across a diverse set of novel tasks not used in the learning of our representations .,33,0,31
dataset/preprocessed/training-data/natural_language_inference/33,Such representations facilitate low - resource learning as exhibited by significant improvements to model performance for new tasks in the low labelled data regime - achieving comparable performance to a few models trained from scratch using only 6 % of the available training set on the Quora duplicate question dataset .,34,0,51
dataset/preprocessed/training-data/natural_language_inference/33,The problem of learning distributed representations of phrases and sentences dates back over a decade .,36,0,16
dataset/preprocessed/training-data/natural_language_inference/33,"For example , present an additive and multiplicative linear composition function of the distributed representations of individual words .",37,0,19
dataset/preprocessed/training-data/natural_language_inference/33,combine symbolic and distributed representations of words using tensor products .,38,0,11
dataset/preprocessed/training-data/natural_language_inference/33,Advances in learning better distributed representations of words combined with deep learning have made it possible to learn complex non-linear composition functions of an arbitrary number of word embeddings using convolutional or recurrent neural networks ( RNNs ) .,39,0,39
dataset/preprocessed/training-data/natural_language_inference/33,"A network 's representation of the last element in a sequence , which is a non-linear composition of all inputs , is typically assumed to contain a squashed "" summary "" of the sentence .",40,0,35
dataset/preprocessed/training-data/natural_language_inference/33,Most work in supervised learning for NLP builds task - specific representations of sentences rather than general - purpose ones .,41,0,21
dataset/preprocessed/training-data/natural_language_inference/33,"Notably , skip - thought vectors , an extension of the skip - gram model for word embeddings to sentences , learn re-usable sentence representations from weakly labeled data .",42,0,30
dataset/preprocessed/training-data/natural_language_inference/33,"Unfortunately , these models take weeks or often months to train .",43,0,12
dataset/preprocessed/training-data/natural_language_inference/33,"address this by considering faster alternatives such as sequential denoising autoencoders and shallow log - linear models. , however , demonstrate that simple word embedding averages are comparable to more complicated models like skip - thoughts .",44,0,37
dataset/preprocessed/training-data/natural_language_inference/33,"More recently , show that a completely supervised approach to learning sentence representations from natural language inference data outperforms all previous approaches on transfer learning benchmarks .",45,0,27
dataset/preprocessed/training-data/natural_language_inference/33,"Here we use the terms "" transfer learning performance "" on "" transfer tasks "" to mean the performance of sentence representations evaluated on tasks unseen during training .",46,0,29
dataset/preprocessed/training-data/natural_language_inference/33,demonstrated that representations learned by state - of - the - art large - scale NMT systems also generalize well to other tasks .,47,0,24
dataset/preprocessed/training-data/natural_language_inference/33,"However , their use of an attention mechanism prevents the learning of a single fixedlength vector representation of a sentence .",48,0,21
dataset/preprocessed/training-data/natural_language_inference/33,"As a result , they present a bi-attentive classification network that composes information present in all of the model 's hidden states to achieve improvements over a corresponding model trained from scratch .",49,0,33
dataset/preprocessed/training-data/natural_language_inference/94,Commonsense for Generative Multi - Hop Question Answering Tasks,2,1,9
dataset/preprocessed/training-data/natural_language_inference/94,"Reading comprehension QA tasks have seen a recent surge in popularity , yet most works have focused on fact - finding extractive QA .",4,1,24
dataset/preprocessed/training-data/natural_language_inference/94,"We instead focus on a more challenging multihop generative task ( NarrativeQA ) , which requires the model to reason , gather , and synthesize disjoint pieces of information within the context to generate an answer .",5,0,37
dataset/preprocessed/training-data/natural_language_inference/94,"This type of multi-step reasoning also often requires understanding implicit relations , which humans resolve via external , background commonsense knowledge .",6,0,22
dataset/preprocessed/training-data/natural_language_inference/94,We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer - generator decoder to synthesize the answer .,7,0,29
dataset/preprocessed/training-data/natural_language_inference/94,"This model performs substantially better than previous generative models , and is competitive with current state - of - theart span prediction models .",8,0,24
dataset/preprocessed/training-data/natural_language_inference/94,We next introduce a novel system for selecting grounded multi-hop relational commonsense information from Concept Net via a pointwise mutual information and term - frequency based scoring function .,9,0,29
dataset/preprocessed/training-data/natural_language_inference/94,"Finally , we effectively use this extracted commonsense information to fill in gaps of reasoning between context hops , using a selectively - gated attention mechanism .",10,0,27
dataset/preprocessed/training-data/natural_language_inference/94,"This boosts the model 's performance significantly ( also verified via human evaluation ) , establishing a new state - of - the - art for the task .",11,0,29
dataset/preprocessed/training-data/natural_language_inference/94,"We also show promising initial results of the generalizability of our background knowledge enhancements by demonstrating some improvement on QAngaroo - WikiHop , another multihop reasoning dataset .",12,0,28
dataset/preprocessed/training-data/natural_language_inference/94,"Reading comprehension QA tasks have seen a recent surge in popularity , yet most works have focused on fact - finding extractive QA .",14,1,24
dataset/preprocessed/training-data/natural_language_inference/94,"We instead focus on a more challenging multihop generative task ( NarrativeQA ) , which requires the model to reason , gather , and synthesize disjoint pieces of information within the context to generate an answer .",15,0,37
dataset/preprocessed/training-data/natural_language_inference/94,"This type of multi-step reasoning also often requires understanding implicit relations , which humans resolve via external , background commonsense knowledge .",16,0,22
dataset/preprocessed/training-data/natural_language_inference/94,We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer - generator decoder to synthesize the answer .,17,0,29
dataset/preprocessed/training-data/natural_language_inference/94,"This model performs substantially better than previous generative models , and is competitive with current state - of - theart span prediction models .",18,0,24
dataset/preprocessed/training-data/natural_language_inference/94,We next introduce a novel system for selecting grounded multi-hop relational commonsense information from Concept Net via a pointwise mutual information and term - frequency based scoring function .,19,0,29
dataset/preprocessed/training-data/natural_language_inference/94,"Finally , we effectively use this extracted commonsense information to fill in gaps of reasoning between context hops , using a selectively - gated attention mechanism .",20,0,27
dataset/preprocessed/training-data/natural_language_inference/94,"This boosts the model 's performance significantly ( also verified via human evaluation ) , establishing a new state - of - the - art for the task .",21,0,29
dataset/preprocessed/training-data/natural_language_inference/94,"We also show promising initial results of the generalizability of our background knowledge enhancements by demonstrating some improvement on QAngaroo - WikiHop , another multihop reasoning dataset .",22,0,28
dataset/preprocessed/training-data/natural_language_inference/94,"In this paper , we explore the task of machine reading comprehension ( MRC ) based QA .",24,1,18
dataset/preprocessed/training-data/natural_language_inference/94,This * Equal contribution .,25,0,5
dataset/preprocessed/training-data/natural_language_inference/94,"We publicly release all our code , models , and data at :",26,0,13
dataset/preprocessed/training-data/natural_language_inference/94,https://github.com/yicheng-w/CommonSenseMultiHopQA task tests a model 's natural language understanding capabilities by asking it to answer a question based on a passage of relevant content .,27,0,25
dataset/preprocessed/training-data/natural_language_inference/94,"Much progress has been made in reasoning - based MRC - QA on the bAbI dataset , which contains questions that require the combination of multiple disjoint pieces of evidence in the context .",28,1,34
dataset/preprocessed/training-data/natural_language_inference/94,"However , due to its synthetic nature , bAb I evidences have smaller lexicons and simpler passage structures when compared to humangenerated text .",29,0,24
dataset/preprocessed/training-data/natural_language_inference/94,There also have been several attempts at the MRC - QA task on human - generated text .,30,0,18
dataset/preprocessed/training-data/natural_language_inference/94,Large scale datasets such as CNN / DM and have made the training of end - to - end neural models possible .,31,0,23
dataset/preprocessed/training-data/natural_language_inference/94,"However , these datasets are fact - based and do not place heavy emphasis on multi-hop reasoning capabilities .",32,0,19
dataset/preprocessed/training-data/natural_language_inference/94,More recent datasets such as QAngaroo have prompted a strong focus on multi-hop reasoning in very long texts .,33,0,19
dataset/preprocessed/training-data/natural_language_inference/94,"However , QAngaroo is an extractive dataset where answers are guaranteed to be spans within the context ; hence , this is more focused on fact finding and linking , and does not require models to synthesize and generate new information .",34,0,42
dataset/preprocessed/training-data/natural_language_inference/94,"We focus on the recently published Narra - tive QA generative dataset that contains questions requiring multi-hop reasoning for long , complex stories and other narratives , which requires the model to go beyond fact linking and to synthesize non-span answers .",35,0,42
dataset/preprocessed/training-data/natural_language_inference/94,"Hence , models that perform well on previous reasoning tasks have had limited success on this dataset .",36,0,18
dataset/preprocessed/training-data/natural_language_inference/94,"In this paper , we first propose the Multi - Hop Pointer - Generator Model ( MHPGM ) , a strong baseline model that uses multiple hops of bidirectional attention , self - attention , and a pointer - generator decoder to effectively read and reason within along passage and synthesize a coherent response .",37,0,55
dataset/preprocessed/training-data/natural_language_inference/94,"Our model achieves 41.49 Rouge - L and 17.33 METEOR on the summary subtask of Narrative QA , substantially better than the performance of previous generative models .",38,0,28
dataset/preprocessed/training-data/natural_language_inference/94,"Next , to address the issue that understanding human - generated text and performing longdistance reasoning on it often involves intermittent access to missing hops of external commonsense ( background ) knowledge , we present an algorithm for selecting useful , grounded multi-hop relational knowledge paths from ConceptNet ) via a pointwise mutual information ( PMI ) and term - frequency - based scoring function .",39,0,66
dataset/preprocessed/training-data/natural_language_inference/94,"We then present a novel method of inserting these selected commonsense paths between the hops of document - context reasoning within our model , via the Necessary and Optional Information Cell ( NOIC ) , which employs a selectivelygated attention mechanism that utilizes commonsense information to effectively fill in gaps of inference .",40,0,53
dataset/preprocessed/training-data/natural_language_inference/94,"With these additions , we further improve performance on the Narrative QA dataset , achieving 44.16 Rouge - L and 19.03 METEOR ( also verified via human evaluation ) .",41,0,30
dataset/preprocessed/training-data/natural_language_inference/94,We also provide manual analysis on the effectiveness of our commonsense selection algorithm .,42,0,14
dataset/preprocessed/training-data/natural_language_inference/94,"Finally , to show the generalizability of our multi-hop reasoning and commonsense methods , we show some promising initial results via the addition of commonsense information over the baseline on , an extractive dataset for multi-hop reasoning from a different domain .",43,0,42
dataset/preprocessed/training-data/natural_language_inference/94,Machine Reading Comprehension :,45,0,4
dataset/preprocessed/training-data/natural_language_inference/94,MRC has long been a task used to assess a model 's ability to understand and reason about language .,46,0,20
dataset/preprocessed/training-data/natural_language_inference/94,"Large scale datasets such as CNN / Daily Mail and have encouraged the development of many advanced , high performing attention - based neural models .",47,0,26
dataset/preprocessed/training-data/natural_language_inference/94,"Concurrently , datasets such as bAbI have focused specifically on multi-step reasoning by requiring the model to reason with disjoint pieces of information .",48,0,24
dataset/preprocessed/training-data/natural_language_inference/94,"On this task , it has been shown that iteratively updating the query representation with information from the context can effectively emulate multi-step reasoning .",49,0,25
dataset/preprocessed/training-data/natural_language_inference/63,A Multi - Stage Memory Augmented Neural Network for Machine Reading Comprehension,2,1,12
dataset/preprocessed/training-data/natural_language_inference/63,Reading Comprehension ( RC ) of text is one of the fundamental tasks in natural language processing .,4,1,18
dataset/preprocessed/training-data/natural_language_inference/63,"In recent years , several end - to - end neural network models have been proposed to solve RC tasks .",5,1,21
dataset/preprocessed/training-data/natural_language_inference/63,"However , most of these models suffer in reasoning overlong documents .",6,0,12
dataset/preprocessed/training-data/natural_language_inference/63,"In this work , we propose a novel Memory Augmented Machine Comprehension Network ( MAMCN ) to address long - range dependencies present in machine reading comprehension .",7,0,28
dataset/preprocessed/training-data/natural_language_inference/63,"We perform extensive experiments to evaluate proposed method with the renowned benchmark datasets such as SQuAD , QUASAR - T , and Trivia QA .",8,0,25
dataset/preprocessed/training-data/natural_language_inference/63,"We achieve the state of the art performance on both the document - level ( QUASAR - T , TriviaQA ) and paragraph - level ( SQuAD ) datasets compared to all the previously published approaches .",9,0,37
dataset/preprocessed/training-data/natural_language_inference/63,Reading Comprehension ( RC ) is essential for understanding human knowledge written in text form .,11,0,16
dataset/preprocessed/training-data/natural_language_inference/63,"One possible way of measuring RC is by formulating it as answer span prediction style Question Answering ( QA ) task , which is finding an answer to the question based on the given document ( s ) .",12,1,39
dataset/preprocessed/training-data/natural_language_inference/63,"Recently , influential deep learning approaches have been proposed to solve this QA task . ; propose the attention mechanism between question and context for question - aware contextual representation .",13,1,31
dataset/preprocessed/training-data/natural_language_inference/63,refine these contextual representations by using self - attention to improve the performance .,14,0,14
dataset/preprocessed/training-data/natural_language_inference/63,Even further performance improvement is gained by using contextualized word representations for query and context .,15,0,16
dataset/preprocessed/training-data/natural_language_inference/63,"Based on those approaches , several methods have successfully made progress towards reaching human - level performance on SQuAD .",16,0,20
dataset/preprocessed/training-data/natural_language_inference/63,Each training example in the SQuAD only has the relevant paragraph with the corresponding answer .,17,0,16
dataset/preprocessed/training-data/natural_language_inference/63,"However , most of the documents present in the real - world are long , containing relevant and irrelevant paragraphs , and do not guarantee answer presence .",18,0,28
dataset/preprocessed/training-data/natural_language_inference/63,Therefore the models proposed to solve SQuAD have difficulty in applying to real - world documents .,19,0,17
dataset/preprocessed/training-data/natural_language_inference/63,"Recently , QUASAR - T and Trivia",20,0,7
dataset/preprocessed/training-data/natural_language_inference/63,QA datasets have been proposed to resemble real - world document .,21,0,12
dataset/preprocessed/training-data/natural_language_inference/63,"These datasets use document - level evidence as training example instead of using only the relevant paragraph and evidence does not guarantee answer presence , which makes them more realistic .",22,0,31
dataset/preprocessed/training-data/natural_language_inference/63,"To effectively comprehend long documents present in the QUASAR - T and TriviaQA datasets , the QA models have to resolve long - range dependencies present in these documents .",23,0,30
dataset/preprocessed/training-data/natural_language_inference/63,"In this work , we build a QA model that can understand long documents by utilizing Memory Augmented Neural Networks ( MANNs ) .",24,0,24
dataset/preprocessed/training-data/natural_language_inference/63,This type of neural networks decouples the memory capacity from the number of model parameters .,25,0,16
dataset/preprocessed/training-data/natural_language_inference/63,"While there have been several attempts to use MANNs in managing long - range dependencies , applications are limited to only toy datasets .",26,0,24
dataset/preprocessed/training-data/natural_language_inference/63,"Compared to the previous approaches , we mainly focus on the document - level QA task on QUASAR - T and Trivia QA .",27,0,24
dataset/preprocessed/training-data/natural_language_inference/63,We also apply our model to SQuAD to show that our model even works well on the paragraph - level .,28,0,21
dataset/preprocessed/training-data/natural_language_inference/63,Our contributions in this work are as follows :,29,0,9
dataset/preprocessed/training-data/natural_language_inference/63,Many neural networks have been proposed to solve answer span QA task .,32,0,13
dataset/preprocessed/training-data/natural_language_inference/63,Ranking continuous text spans within a passage was proposed by and .,33,0,12
dataset/preprocessed/training-data/natural_language_inference/63,"combine match - LSTM , originally introduced in and pointer networks to produce the boundary of the answer .",34,0,19
dataset/preprocessed/training-data/natural_language_inference/63,"Since then , most of the models adopted pointer networks as a prediction layer and then focused on improving other layers .",35,0,22
dataset/preprocessed/training-data/natural_language_inference/63,"Some methods focused on devising more accurate attention method ; ; ; employ attention mechanism to match the question context mutually ; In addition , apply multi-layer attention and expand to multi-level attention to get more enriched attention information .",36,0,40
dataset/preprocessed/training-data/natural_language_inference/63,Other approaches use contextualized word representations to further improve the performance .,37,0,12
dataset/preprocessed/training-data/natural_language_inference/63,utilize embedding from pre-trained language model as an additional feature and select machine translation model instead .,38,0,17
dataset/preprocessed/training-data/natural_language_inference/63,"Also , there are few attempts at augmenting memory capacity of the model .",39,0,14
dataset/preprocessed/training-data/natural_language_inference/63,"refine the contextual representation with multi-hops , and simply use the encoded query representations as a memory vector for refining the answer prediction , which are not meant to handle long - range dependency that we consider in this work .",40,0,41
dataset/preprocessed/training-data/natural_language_inference/63,We propose a memory augmented reader for answer - span style QA task .,42,0,14
dataset/preprocessed/training-data/natural_language_inference/63,Answer - span style QA task is defined as follows .,43,0,11
dataset/preprocessed/training-data/natural_language_inference/63,"Question and document can be represented by sequence of words q = {w q i } m i = 1 , d = {w d j } n j=1 respectively .",44,0,31
dataset/preprocessed/training-data/natural_language_inference/63,"Answerspan a = {w d k } e k=s ( where , 1 ? s ? e ? n) should be returned , given q and d .",45,0,28
dataset/preprocessed/training-data/natural_language_inference/63,We embed the sequence of words in q and d to get contextual representations .,46,0,15
dataset/preprocessed/training-data/natural_language_inference/63,These contextual representations are used for calculating question - aware context representation which is controlled by external memory unit and used for predicting answer - span .,47,0,27
dataset/preprocessed/training-data/natural_language_inference/63,We describe more details of each layer in following sections and depict over all architecture of proposed model in .,48,0,20
dataset/preprocessed/training-data/natural_language_inference/0,Gated - Attention Readers for Text Comprehension,2,1,7
dataset/preprocessed/training-data/natural_language_inference/0,In this paper we study the problem of answering cloze - style questions over documents .,4,0,16
dataset/preprocessed/training-data/natural_language_inference/0,"Our model , the Gated - Attention ( GA ) Reader 1 , integrates a multi-hop architecture with a novel attention mechanism , which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader .",5,0,45
dataset/preprocessed/training-data/natural_language_inference/0,This enables the reader to build query - specific representations of tokens in the document for accurate answer selection .,6,0,20
dataset/preprocessed/training-data/natural_language_inference/0,The GA Reader obtains state - of - the - art results on three benchmarks for this task - the CNN & Daily Mail news stories and the Who Did What dataset .,7,0,33
dataset/preprocessed/training-data/natural_language_inference/0,"The effectiveness of multiplicative interaction is demonstrated by an ablation study , and by comparing to alternative compositional operators for implementing the gated - attention .",8,0,26
dataset/preprocessed/training-data/natural_language_inference/0,* BD and HL contributed equally to this work .,9,0,10
dataset/preprocessed/training-data/natural_language_inference/0,Source code is available on github : https:// github.com/bdhingra/ga-reader,11,0,9
dataset/preprocessed/training-data/natural_language_inference/0,A recent trend to measure progress towards machine reading is to test a system 's ability to answer questions about a document it has to comprehend .,13,1,27
dataset/preprocessed/training-data/natural_language_inference/0,"Towards this end , several large - scale datasets of cloze - style questions over a context document have been introduced recently , which allow the training of supervised machine learning systems .",14,0,33
dataset/preprocessed/training-data/natural_language_inference/0,Such datasets can be easily constructed automatically and the unambiguous nature of their queries provides an objective benchmark to measure a system 's performance at text comprehension .,15,0,28
dataset/preprocessed/training-data/natural_language_inference/0,Deep learning models have been shown to outperform traditional shallow approaches on text comprehension tasks .,16,0,16
dataset/preprocessed/training-data/natural_language_inference/0,The success of many recent models can be attributed primarily to two factors :,17,0,14
dataset/preprocessed/training-data/natural_language_inference/0,"( 1 ) Multi-hop architectures , allow a model to scan the document and the question iteratively for multiple passes .",18,0,21
dataset/preprocessed/training-data/natural_language_inference/0,"( 2 ) Attention mechanisms , borrowed from the machine translation literature , allow the model to focus on appropriate subparts of the context document .",19,0,26
dataset/preprocessed/training-data/natural_language_inference/0,"Intuitively , the multi-hop architecture allows the reader to incrementally refine token representations , and the attention mechanism re-weights different parts in the document according to their relevance to the query .",20,0,32
dataset/preprocessed/training-data/natural_language_inference/0,The effectiveness of multi-hop reasoning and attentions have been explored orthogonally so far in the literature .,21,0,17
dataset/preprocessed/training-data/natural_language_inference/0,"In this paper , we focus on combining both in a complementary manner , by designing a novel attention mechanism which gates the evolving token representations across hops .",22,0,29
dataset/preprocessed/training-data/natural_language_inference/0,"More specifically , unlike existing models where the query attention is applied either token - wise or sentence - wise to allow weighted aggregation , the Gated - Attention ( GA ) module proposed in this work allows the query to directly interact with each dimension of the token embeddings at the semantic - level , and is applied layer - wise as information filters during the multi-hop representation learning process .",23,0,72
dataset/preprocessed/training-data/natural_language_inference/0,"Such a fine - grained attention enables our model to learn conditional token representations w.r.t. the given question , leading to accurate answer selections .",24,0,25
dataset/preprocessed/training-data/natural_language_inference/0,"We show in our experiments that the proposed GA reader , despite its relative simplicity , consis-tently improves over a variety of strong baselines on three benchmark datasets .",25,0,29
dataset/preprocessed/training-data/natural_language_inference/0,"Our key contribution , the GA module , provides a significant improvement for large datasets .",26,0,16
dataset/preprocessed/training-data/natural_language_inference/0,"Qualitatively , visualization of the attentions at intermediate layers of the GA reader shows that in each layer the GA reader attends to distinct salient aspects of the query which help in determining the answer .",27,0,36
dataset/preprocessed/training-data/natural_language_inference/0,"The cloze - style QA task involves tuples of the form ( d , q , a , C ) , where dis a document ( context ) , q is a query over the contents of d , in which a phrase is replaced with a placeholder , and a is the answer to q , which comes from a set of candidates C .",29,0,66
dataset/preprocessed/training-data/natural_language_inference/0,In this work we consider datasets where each candidate c ?,30,0,11
dataset/preprocessed/training-data/natural_language_inference/0,C has at least one token which also appears in the document .,31,0,13
dataset/preprocessed/training-data/natural_language_inference/0,"The task can then be described as : given a document - query pair ( d , q ) , find a ?",32,0,23
dataset/preprocessed/training-data/natural_language_inference/0,C which answers q .,33,0,5
dataset/preprocessed/training-data/natural_language_inference/0,Below we provide an overview of representative neural network architectures which have been applied to this problem .,34,0,18
dataset/preprocessed/training-data/natural_language_inference/0,"LSTMs with Attention : Several architectures introduced in employ LSTM units to compute a combined document - query representation g ( d , q ) , which is used to rank the candidate answers .",35,0,35
dataset/preprocessed/training-data/natural_language_inference/0,"These include the DeepLSTM Reader which performs a single forward pass through the concatenated ( document , query ) pair to obtain g ( d , q ) ; the Attentive Reader which first computes a document vector d ( q ) by a weighted aggregation of words according to attentions based on q , and then combines d ( q ) and q to obtain their joint representation g ( d ( q ) , q ) ; and the Impatient Reader where the document representation is built incrementally .",36,0,91
dataset/preprocessed/training-data/natural_language_inference/0,"The architecture of the Attentive Reader has been simplified recently in Stanford Attentive Reader , where shallower recurrent units were used with a bilinear form for the query - document attention .",37,0,32
dataset/preprocessed/training-data/natural_language_inference/0,Attention Sum : The Attention - Sum ( AS ) Reader uses two bidirectional GRU networks to encode both d and q into vectors .,38,0,25
dataset/preprocessed/training-data/natural_language_inference/0,A probability distribution over the entities ind is obtained by computing dot products between q and the entity embeddings and taking a softmax .,39,0,24
dataset/preprocessed/training-data/natural_language_inference/0,"Then , an aggregation scheme named pointer - sum attention is further applied to sum the probabilities of the same entity , so that frequent entities the document will be favored compared to rare ones .",40,0,36
dataset/preprocessed/training-data/natural_language_inference/0,"Building on the AS Reader , the Attention - over - Attention ( AoA ) Reader introduces a two - way attention mechanism where the query and the document are mutually attentive to each other .",41,0,36
dataset/preprocessed/training-data/natural_language_inference/0,"Mulit - hop Architectures : Memory Networks ( MemNets ) were proposed in , where each sentence in the document is encoded to a memory by aggregating nearby words .",42,0,30
dataset/preprocessed/training-data/natural_language_inference/0,"Attention over the memory slots given the query is used to compute an over all memory and to renew the query representation over multiple iterations , allowing certain types of reasoning over the salient facts in the memory and the query .",43,0,42
dataset/preprocessed/training-data/natural_language_inference/0,Neural Semantic Encoders ( NSE ) extended MemNets by introducing a write operation which can evolve the memory overtime during the course of reading .,44,0,25
dataset/preprocessed/training-data/natural_language_inference/0,"Iterative reasoning has been found effective in several more recent models , including the Iterative Attentive Reader and ReasoNet .",45,0,20
dataset/preprocessed/training-data/natural_language_inference/0,The latter allows dynamic reasoning steps and is trained with reinforcement learning .,46,0,13
dataset/preprocessed/training-data/natural_language_inference/0,"Other related works include Dynamic Entity Representation network ( DER ) , which builds dynamic representations of the candidate answers while reading the document , and accumulates the information about an entity by max - pooling ; EpiReader consists of two networks , where one proposes a small set of candidate answers , and the other reranks the proposed candidates conditioned on the query and the context ; Bi- Directional Attention Flow network ( BiDAF ) adopts a multi-stage hierarchical architecture along with a flow - based attention mechanism ; showed a 10 % improvement on the CBT corpus by training the AS Reader on an augmented training set of about 14 million examples , making a case for the community to exploit data abundance .",47,0,126
dataset/preprocessed/training-data/natural_language_inference/0,"The focus of this paper , however , is on designing models which exploit the available data efficiently .",48,0,19
dataset/preprocessed/training-data/natural_language_inference/0,Gated - Attention Reader,49,0,4
dataset/preprocessed/training-data/natural_language_inference/66,Learning Natural Language Inference with LSTM,2,1,6
dataset/preprocessed/training-data/natural_language_inference/66,Natural language inference ( NLI ) is a fundamentally important task in natural language processing that has many applications .,4,1,20
dataset/preprocessed/training-data/natural_language_inference/66,The recently released Stanford Natural Language Inference ( SNLI ) corpus has made it possible to develop and evaluate learning - centered methods such as deep neural networks for natural language inference ( NLI ) .,5,0,36
dataset/preprocessed/training-data/natural_language_inference/66,"In this paper , we propose a special long short - term memory ( LSTM ) architecture for NLI .",6,1,20
dataset/preprocessed/training-data/natural_language_inference/66,Our model builds on top of a recently proposed neural attention model for NLI but is based on a significantly different idea .,7,0,23
dataset/preprocessed/training-data/natural_language_inference/66,"Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification , our solution uses a match - LSTM to perform wordby - word matching of the hypothesis with the premise .",8,0,37
dataset/preprocessed/training-data/natural_language_inference/66,This LSTM is able to place more emphasis on important word - level matching results .,9,0,16
dataset/preprocessed/training-data/natural_language_inference/66,"In particular , we observe that this LSTM remembers important mismatches thatare critical for predicting the contradiction or the neutral relationship label .",10,0,23
dataset/preprocessed/training-data/natural_language_inference/66,"On the SNLI corpus , our model achieves an accuracy of 86.1 % , outperforming the state of the art .",11,0,21
dataset/preprocessed/training-data/natural_language_inference/66,A more recent work by Rocktschel et al. ( 2016 ) improved the performance by applying a neural attention model .,12,0,21
dataset/preprocessed/training-data/natural_language_inference/66,"While their basic architecture is still based on sentence embeddings for the premise and the hypothesis , a key difference is that the embedding of the premise takes into consideration the alignment between the premise and the hypothesis .",13,0,39
dataset/preprocessed/training-data/natural_language_inference/66,This so - called attention - weighted representation of the premise was shown to help push the accuracy to,14,0,19
dataset/preprocessed/training-data/natural_language_inference/66,Natural language inference ( NLI ) is the problem of determining whether from a premise sentence P one can infer another hypothesis sentence H ) .,16,0,26
dataset/preprocessed/training-data/natural_language_inference/66,"NLI is a fundamentally important problem that has applications in many tasks including question answering , semantic search and automatic text summarization .",17,0,23
dataset/preprocessed/training-data/natural_language_inference/66,"There has been much interest in NLI in the past decade , especially surround - ing the PASCAL Recognizing Textual Entailment ( RTE ) Challenge .",18,0,26
dataset/preprocessed/training-data/natural_language_inference/66,"Existing solutions to NLI range from shallow approaches based on lexical similarities to advanced methods that consider syntax , perform explicit sentence alignment or use formal logic ) .",19,0,29
dataset/preprocessed/training-data/natural_language_inference/66,"Recently , released the Stanford Natural Language Inference ( SNLI ) corpus for the purpose of encouraging more learning - centered approaches to NLI .",20,0,25
dataset/preprocessed/training-data/natural_language_inference/66,"This corpus contains around 570K sentence pairs with three labels : entailment , contradiction and neutral .",21,0,17
dataset/preprocessed/training-data/natural_language_inference/66,"The size of the corpus makes it now feasible to train deep neural network models , which typically require a large amount of training data .",22,0,26
dataset/preprocessed/training-data/natural_language_inference/66,tested a straightforward architecture of deep neural networks for NLI .,23,0,11
dataset/preprocessed/training-data/natural_language_inference/66,"In their architecture , the premise and the hypothesis are each represented by a sentence embedding vector .",24,0,18
dataset/preprocessed/training-data/natural_language_inference/66,The two vectors are then fed into a multi - layer neural network to train a classifier .,25,0,18
dataset/preprocessed/training-data/natural_language_inference/66,achieved an accuracy of 77.6 % when long short - term memory ( LSTM ) networks were used to obtain the sentence embeddings .,26,0,24
dataset/preprocessed/training-data/natural_language_inference/66,83.5 % on the SNLI corpus .,27,0,7
dataset/preprocessed/training-data/natural_language_inference/66,"A limitation of the aforementioned two models is that they reduce both the premise and the hypothesis to a single embedding vector before matching them ; i.e. , in the end , they use two embedding vectors to perform sentence - level matching .",28,0,44
dataset/preprocessed/training-data/natural_language_inference/66,"However , not all word or phrase - level matching results are equally important .",29,0,15
dataset/preprocessed/training-data/natural_language_inference/66,"For example , the matching between stop words in the two sentences is not likely to contribute much to the final prediction .",30,0,23
dataset/preprocessed/training-data/natural_language_inference/66,"Also , for a hypothesis to contradict a premise , a single word or phrase - level mismatch ( e.g. , a mismatch of the subjects of the two sentences ) maybe sufficient and other matching results are less important , but this intuition is hard to be captured if we directly match two sentence embeddings .",31,0,57
dataset/preprocessed/training-data/natural_language_inference/66,"In this paper , we propose a new LSTM - based architecture for learning natural language inference .",32,0,18
dataset/preprocessed/training-data/natural_language_inference/66,"Different from previous models , our prediction is not based on whole sentence embeddings of the premise and the hypothesis .",33,0,21
dataset/preprocessed/training-data/natural_language_inference/66,"Instead , we use an LSTM to perform word - by - word matching of the hypothesis with the premise .",34,0,21
dataset/preprocessed/training-data/natural_language_inference/66,"Our LSTM sequentially processes the hypothesis , and at each position , it tries to match the current word in the hypothesis with an attention - weighted representation of the premise .",35,0,32
dataset/preprocessed/training-data/natural_language_inference/66,"Matching results thatare critical for the final prediction will be "" remembered "" by the LSTM while less important matching results will be "" forgotten . """,36,0,27
dataset/preprocessed/training-data/natural_language_inference/66,"We refer to this architecture a match - LSTM , or m LSTM for short .",37,0,16
dataset/preprocessed/training-data/natural_language_inference/66,"Experiments show that our m LSTM model achieves an accuracy of 86.1 % on the SNLI corpus , outperforming the state of the art .",38,0,25
dataset/preprocessed/training-data/natural_language_inference/66,"Furthermore , through further analyses of the learned parameters , we show that the m LSTM architecture can indeed pickup the more important word - level matching results that need to be remembered for the final prediction .",39,0,38
dataset/preprocessed/training-data/natural_language_inference/66,"In particular , we observe that good wordlevel matching results are generally "" forgotten "" but important mismatches , which often indicate a contradiction or a neutral relationship , tend to be "" remembered . """,40,0,36
dataset/preprocessed/training-data/natural_language_inference/66,Our code is available online 1 .,41,0,7
dataset/preprocessed/training-data/natural_language_inference/66,"In this section , we first review LSTM .",45,0,9
dataset/preprocessed/training-data/natural_language_inference/66,"We then review the word - by - word attention model by , which is their best performing model .",46,0,20
dataset/preprocessed/training-data/natural_language_inference/66,Finally we present our mLSTM architecture for natural language inference .,47,0,11
dataset/preprocessed/training-data/natural_language_inference/66,LSTM : Let us first briefly review LSTM .,49,0,9
dataset/preprocessed/training-data/natural_language_inference/50,Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,2,1,10
dataset/preprocessed/training-data/natural_language_inference/50,The dominant neural architectures in question answer retrieval are based on recurrent or convolutional encoders configured with complex word matching layers .,4,0,22
dataset/preprocessed/training-data/natural_language_inference/50,"Given that recent architectural innovations are mostly new word interaction layers or attentionbased matching mechanisms , it seems to be a well - established fact that these components are mandatory for good performance .",5,0,34
dataset/preprocessed/training-data/natural_language_inference/50,"Unfortunately , the memory and computation cost incurred by these complex mechanisms are undesirable for practical applications .",6,0,18
dataset/preprocessed/training-data/natural_language_inference/50,"As such , this paper tackles the question of whether it is possible to achieve competitive performance with simple neural architectures .",7,0,22
dataset/preprocessed/training-data/natural_language_inference/50,We propose a simple but novel deep learning architecture for fast and efficient question - answer ranking and retrieval .,8,0,20
dataset/preprocessed/training-data/natural_language_inference/50,"More specifically , our proposed model , HyperQA , is a parameter efficient neural network that outperforms other parameter intensive models such as Attentive Pooling BiLSTMs and Multi - Perspective CNNs on multiple QA benchmarks .",9,0,36
dataset/preprocessed/training-data/natural_language_inference/50,The novelty behind Hyper QA is a pairwise ranking objective that models the relationship between question and answer embeddings in Hyperbolic space instead of Euclidean space .,10,0,27
dataset/preprocessed/training-data/natural_language_inference/50,This empowers our model with a self - organizing ability and enables automatic discovery of latent hierarchies while learning embeddings of questions and answers .,11,0,25
dataset/preprocessed/training-data/natural_language_inference/50,"Our model requires no feature engineering , no similarity matrix matching , no complicated attention mechanisms nor over-parameterized layers and yet outperforms and remains competitive to many models that have these functionalities on multiple benchmarks .",12,0,36
dataset/preprocessed/training-data/natural_language_inference/50,are as follows :,14,0,4
dataset/preprocessed/training-data/natural_language_inference/50,"Firstly , representations of questions and answers are first learned via a neural encoder such as the long short - term memory ( LSTM ) network or convolutional neural network ( CNN ) .",15,0,34
dataset/preprocessed/training-data/natural_language_inference/50,"Secondly , these representations of questions and answers are composed by an interaction function to produce an over all matching score .",16,0,22
dataset/preprocessed/training-data/natural_language_inference/50,The design of the interaction function between question and answer representations lives at the heart of deep learning QA research .,17,0,21
dataset/preprocessed/training-data/natural_language_inference/50,"While it is simply possible to combine QA representations with simple feed forward neural networks or other composition functions , a huge bulk of recent work is concerned with designing novel word interaction layers that model the relationship between the words in the QA pairs .",18,0,46
dataset/preprocessed/training-data/natural_language_inference/50,"For example , similarity matrix based matching , soft attention alignment and attentive pooling are highly popular techniques for improving the performance of neural ranking models .",19,0,27
dataset/preprocessed/training-data/natural_language_inference/50,"Apparently , it seems to be well - established that grid - based matching is essential to good performance .",20,0,20
dataset/preprocessed/training-data/natural_language_inference/50,"Notably , these new innovations come with trade - offs such as huge computational cost that lead to significantly longer training times and also a larger memory footprint .",21,0,29
dataset/preprocessed/training-data/natural_language_inference/50,"Additionally , it is good to consider that the base neural encoder employed also contributes to the computational cost of these neural ranking models , e.g. , LSTM networks are known to be over-parameterized and also incur a parameter and runtime cost of quadratic scale .",22,0,46
dataset/preprocessed/training-data/natural_language_inference/50,"It also seems to be a well - established fact that a neural encoder ( such as the LSTM , Gated Recurrent Unit ( GRU ) , CNN , etc. ) must be first selected for learning individual representations of questions and answers and is generally treated as mandatory for good performance .",23,0,53
dataset/preprocessed/training-data/natural_language_inference/50,"In this paper , we propose an extremely simple neural ranking model for question answering that achieves highly competitive results on several benchmarks with only a fraction of the runtime and only 40K - 90 K parameters ( as opposed to millions ) .",24,0,44
dataset/preprocessed/training-data/natural_language_inference/50,Our neural ranking models the relationships between QA pairs in Hyperbolic space instead of Euclidean space .,25,0,17
dataset/preprocessed/training-data/natural_language_inference/50,Hyperbolic space is an embedding space with a constant negative curvature in which the distance towards the border is increasing exponentially .,26,0,22
dataset/preprocessed/training-data/natural_language_inference/50,"Intuitively , this makes it suitable for learning embeddings that reflect a natural hierarchy ( e.g. , networks , text , etc. ) which we believe might benefit neural ranking models for QA .",27,0,34
dataset/preprocessed/training-data/natural_language_inference/50,"Notably , our work is inspired by the recently incepted Poincar embeddings which demonstrates the effectiveness of inducing a structural ( hierarchical ) bias in the embedding space for improved generalization .",28,0,32
dataset/preprocessed/training-data/natural_language_inference/50,"In our early empirical experiments , we discovered that a simple feed forward neural network trained in Hyperbolic space is capable of outperforming more sophisticated models on several standard benchmark datasets .",29,0,32
dataset/preprocessed/training-data/natural_language_inference/50,We believe that this can be attributed to two reasons .,30,0,11
dataset/preprocessed/training-data/natural_language_inference/50,"Firstly , latent hierarchies are prominent in QA .",31,0,9
dataset/preprocessed/training-data/natural_language_inference/50,"Aside from the natural hierarchy of questions and answers , conceptual hierarchies also exist .",32,0,15
dataset/preprocessed/training-data/natural_language_inference/50,"Secondly , natural language is inherently hierarchical which can be traced to power law distributions such as Zipf 's law .",33,0,21
dataset/preprocessed/training-data/natural_language_inference/50,The key contributions in this paper are as follows :,34,0,10
dataset/preprocessed/training-data/natural_language_inference/50,We propose a new neural ranking model for ranking question answer pairs .,35,0,13
dataset/preprocessed/training-data/natural_language_inference/50,"For the first time , our proposed model , HyperQA , performs matching of questions and answers in Hyperbolic space .",36,0,21
dataset/preprocessed/training-data/natural_language_inference/50,"To the best of our knowledge , we are the first to model QA pairs in Hyperbolic space .",37,0,19
dataset/preprocessed/training-data/natural_language_inference/50,"While hyperbolic geometry and embeddings have been explored in the domains of complex networks or graphs , our work is the first to investigate the suitability of this metric space for question answering .",38,0,34
dataset/preprocessed/training-data/natural_language_inference/50,"QA is an extremely fast and parameter efficient model that achieves very competitive results on multiple QA benchmarks such as TrecQA , WikiQA and YahooCQA .",40,0,26
dataset/preprocessed/training-data/natural_language_inference/50,The efficiency and speed of Hyper QA are attributed by the fact that we do not use any sophisticated neural encoder and have no complicated word interaction layer .,41,0,29
dataset/preprocessed/training-data/natural_language_inference/50,"In fact , Hyper QA is a mere single layered neural network with only 90K parameters .",42,0,17
dataset/preprocessed/training-data/natural_language_inference/50,"Very surprisingly , Hyper QA actually outperforms many state - of - the - art models such as Attentive Pooling BiLSTMs and Multi - Perspective CNNs .",43,0,27
dataset/preprocessed/training-data/natural_language_inference/50,We believe that this allows us to reconsider whether many of these complex word interaction layers are really necessary for good performance .,44,0,23
dataset/preprocessed/training-data/natural_language_inference/50,We conduct extensive qualitative analysis of both the learned QA embeddings and word embeddings .,45,0,15
dataset/preprocessed/training-data/natural_language_inference/50,We discover several interesting properties of QA embeddings in Hyperbolic space .,46,0,12
dataset/preprocessed/training-data/natural_language_inference/50,"Due to its compositional nature , we find that our model learns to self - organize not only at the QA level but also at the word - level .",47,0,30
dataset/preprocessed/training-data/natural_language_inference/50,Our qualitative studies enable us to gain a better intuition pertaining to the good performance of our model .,48,0,19
dataset/preprocessed/training-data/natural_language_inference/72,CliCR : A Dataset of Clinical Case Reports for Machine Reading Comprehension *,2,1,13
dataset/preprocessed/training-data/natural_language_inference/72,We present a new dataset for machine comprehension in the medical domain .,4,1,13
dataset/preprocessed/training-data/natural_language_inference/72,"Our dataset uses clinical case reports with around 100,000 gap - filling queries about these cases .",5,0,17
dataset/preprocessed/training-data/natural_language_inference/72,"We apply several baselines and state - of - the - art neural readers to the dataset , and observe a considerable gap in performance ( 20 % F1 ) between the best human and machine readers .",6,0,38
dataset/preprocessed/training-data/natural_language_inference/72,We analyze the skills required for successful answering and show how reader performance varies depending on the applicable skills .,7,0,20
dataset/preprocessed/training-data/natural_language_inference/72,"We find that inferences using domain knowledge and object tracking are the most frequently required skills , and that recognizing omitted information and spatio- temporal reasoning are the most difficult for the machines .",8,0,34
dataset/preprocessed/training-data/natural_language_inference/72,Machine comprehension is a task in which a system reads a text passage and then answers questions about it .,10,0,20
dataset/preprocessed/training-data/natural_language_inference/72,"The progress in machine comprehension heavily depends on the introduction of new datasets , which encourages the development of new algorithms and deepens our understanding of the ( linguistic ) challenges that can or can not be tackled well by these algorithms .",11,0,43
dataset/preprocessed/training-data/natural_language_inference/72,"Recently , a number of reading comprehension datasets have been proposed ( 2 ) , differing in various aspects such as mode of construction , answer - query formulation and required understanding skills .",12,0,34
dataset/preprocessed/training-data/natural_language_inference/72,"Most are open - domain datasets built from news , fiction and Wikipedia texts .",13,0,15
dataset/preprocessed/training-data/natural_language_inference/72,"For specialized domains , however , large machine comprehension datasets are extremely scarce , and * We provide the information about accessing the dataset , as well as the code for the experiments , at http://github.",14,0,36
dataset/preprocessed/training-data/natural_language_inference/72,passage : [. . . ],16,0,6
dataset/preprocessed/training-data/natural_language_inference/72,A gradual improvement in clinical and laboratory status was achieved within 20 days of antituberculous treatment .,17,0,17
dataset/preprocessed/training-data/natural_language_inference/72,The patient was then subjected to a thoracic CT scan that also showed significant radiological improvement .,18,0,17
dataset/preprocessed/training-data/natural_language_inference/72,"Thereafter , tapering of corticosteroids was initiated with no clinical relapse .",19,0,12
dataset/preprocessed/training-data/natural_language_inference/72,The patient was discharged after being treated for a total of 30 days and continued receiving antituberculous therapy with no reported problems for a total of 6 months under the supervision of his hometown physicians .,20,0,36
dataset/preprocessed/training-data/natural_language_inference/72,"If steroids are used , great caution should be exercised on their gradual tapering to avoid .",22,0,17
dataset/preprocessed/training-data/natural_language_inference/72,"answer : relapse ( sem type =problem , cui=C0035020 ) :",23,0,11
dataset/preprocessed/training-data/natural_language_inference/72,"An example from the dataset , with the passage sentence relevant for answering italicized .",24,0,15
dataset/preprocessed/training-data/natural_language_inference/72,The passage has been shortened for clarity .,25,0,8
dataset/preprocessed/training-data/natural_language_inference/72,the required comprehension skills poorly understood .,26,0,7
dataset/preprocessed/training-data/natural_language_inference/72,"With our work we hope to narrow this gap by proposing a new resource for reading comprehension in the clinical domain , and by analyzing the different types of comprehension skills thatare triggered while answering .",27,0,36
dataset/preprocessed/training-data/natural_language_inference/72,"Machine comprehension for healthcare and medicine has received little attention so far , although it offers great potential for practical use .",28,0,22
dataset/preprocessed/training-data/natural_language_inference/72,"A typical application would be clinical decision support , where given a massive amount of text , a clinician asks questions about either external , medical knowledge ( reading literature ) or about particular patients ( reading electronic health records ) .",29,0,42
dataset/preprocessed/training-data/natural_language_inference/72,"Currently , patient - specific questions are tackled by manually browsing or searching those records .",30,0,16
dataset/preprocessed/training-data/natural_language_inference/72,"This task can be facilitated by summarization and QA systems , and we believe , by fine - grained machine reading .",31,0,22
dataset/preprocessed/training-data/natural_language_inference/72,"Reading comprehension systems that perform on a finer level could play an important role especially when combined with document retrieval to perform machine reading at scale , such as in the models of and for the general domain .",32,0,39
dataset/preprocessed/training-data/natural_language_inference/72,"For our dataset , we construct queries , answers and supporting passages from BMJ Case Reports , the largest online repository of such documents .",33,0,25
dataset/preprocessed/training-data/natural_language_inference/72,"A case report is a detailed description of a clinical case that focuses on rare diseases , unusual presentation of common conditions and novel treatment methods .",34,0,27
dataset/preprocessed/training-data/natural_language_inference/72,"Each report contains a Learning points section , summarizing the key pieces of information from that report .",35,0,18
dataset/preprocessed/training-data/natural_language_inference/72,The learning points are typically paraphrased portions of passage text and do not match passage sentences exactly .,36,0,18
dataset/preprocessed/training-data/natural_language_inference/72,We use these learning points to create queries by blanking out a medical entity .,37,0,15
dataset/preprocessed/training-data/natural_language_inference/72,"To counteract potential errors and inconsistencies due to automated dataset creation , we perform several checks to improve the quality of the dataset ( 3 ) .",38,0,27
dataset/preprocessed/training-data/natural_language_inference/72,"Our dataset contains around 100,000 queries on 12,000 case reports , has long support passages ( around 1,500 tokens on average ) and includes answers which are single - or multiword medical entities .",39,0,34
dataset/preprocessed/training-data/natural_language_inference/72,We show an example from the dataset in .,40,0,9
dataset/preprocessed/training-data/natural_language_inference/72,We examine the performance on the dataset in two ways .,41,0,11
dataset/preprocessed/training-data/natural_language_inference/72,"First , we report machine performance for several baselines and neural readers .",42,0,13
dataset/preprocessed/training-data/natural_language_inference/72,"To enable a more flexible answer evaluation , we expand the answers with their respective synonyms from a medical knowledge base , and additionally supplement the standard evaluation metrics with BLEU and embedding - based methods .",43,0,37
dataset/preprocessed/training-data/natural_language_inference/72,We investigate different ways of representing medical entities in the text and how this affects the neural readers .,44,0,19
dataset/preprocessed/training-data/natural_language_inference/72,"We obtain the best results with a recurrent neural network ( RNN ) with gated attention , but a simple approach based on embedding similarity proves to be a strong baseline as well .",45,0,34
dataset/preprocessed/training-data/natural_language_inference/72,"Second , we look at how well humans perform on this task , by asking both a medical expert and a novice to answer a portion of the validation set .",46,0,31
dataset/preprocessed/training-data/natural_language_inference/72,"When categorizing the skills necessary to find the right answer , we observe that a large number of comprehension skills get activated and that prior knowledge in the form of the ability to perform lexico - grammatical inferences matters the most .",47,0,42
dataset/preprocessed/training-data/natural_language_inference/72,"This suggests that for our dataset and possibly for domain - specific datasets more generally , more background knowledge should be incorporated in machine comprehension models .",48,0,27
dataset/preprocessed/training-data/natural_language_inference/72,The current gap between the best machine and the best human performance is nearly :,49,0,15
dataset/preprocessed/training-data/natural_language_inference/45,Published as a conference paper at ICLR 2017 WORDS OR CHARACTERS ? FINE - GRAINED GATING FOR READING COMPREHENSION,2,1,19
dataset/preprocessed/training-data/natural_language_inference/45,"Previous work combines word - level and character - level representations using concatenation or scalar weighting , which is suboptimal for high - level tasks like reading comprehension .",4,0,29
dataset/preprocessed/training-data/natural_language_inference/45,We present a fine - grained gating mechanism to dynamically combine word - level and character - level representations based on properties of the words .,5,0,26
dataset/preprocessed/training-data/natural_language_inference/45,We also extend the idea of fine - grained gating to modeling the interaction between questions and paragraphs for reading comprehension .,6,0,22
dataset/preprocessed/training-data/natural_language_inference/45,"Experiments show that our approach can improve the performance on reading comprehension tasks , achieving new state - of - the - art results on the Children 's Book Test and Who Did What datasets .",7,0,36
dataset/preprocessed/training-data/natural_language_inference/45,"To demonstrate the generality of our gating mechanism , we also show improved results on asocial media tag prediction task .",8,0,21
dataset/preprocessed/training-data/natural_language_inference/45,Finding semantically meaningful representations of the words ( also called tokens ) in a document is necessary for strong performance in Natural Language Processing tasks .,11,0,26
dataset/preprocessed/training-data/natural_language_inference/45,"In neural networks , tokens are mainly represented in two ways , either using word - level representations or character - level representations .",12,0,24
dataset/preprocessed/training-data/natural_language_inference/45,"Word - level representations are obtained from a lookup table , where each unique token is represented as a vector .",13,0,21
dataset/preprocessed/training-data/natural_language_inference/45,"Character - level representations are usually obtained by applying recurrent neural networks ( RNNs ) or convolutional neural networks ( CNNs ) on the character sequence of the token , and their hidden states are combined to form the representation .",14,0,41
dataset/preprocessed/training-data/natural_language_inference/45,Word - level representations are good at memorizing the semantics of the tokens while character - level representations are more suitable for modeling sub - word morphologies .,15,0,28
dataset/preprocessed/training-data/natural_language_inference/45,"For example , considering "" cat "" and "" cats "" , word - level representations can only learn the similarities between the two tokens by training on a large amount of training data , while character - level representations , by design , can easily capture the similarities .",16,0,50
dataset/preprocessed/training-data/natural_language_inference/45,Character - level representations are also used to alleviate the difficulties of modeling out - of - vocabulary ( OOV ) tokens .,17,0,23
dataset/preprocessed/training-data/natural_language_inference/45,Hybrid word - character models have been proposed to leverage the advantages of both word - level and character - level representations .,18,0,23
dataset/preprocessed/training-data/natural_language_inference/45,The most commonly used method is to concatenate these two representations .,19,0,12
dataset/preprocessed/training-data/natural_language_inference/45,"However , concatenating word - level and character - level representations is technically problematic .",20,0,15
dataset/preprocessed/training-data/natural_language_inference/45,"For frequent tokens , the word - level representations are usually accurately estimated during the training process , and thus introducing character - level representations can potentially bias the entire representations .",21,0,32
dataset/preprocessed/training-data/natural_language_inference/45,"For infrequent tokens , the estimation of wordlevel representations have high variance , which will have negative effects when combined with the character - level representations .",22,0,27
dataset/preprocessed/training-data/natural_language_inference/45,"To address this issue , recently introduced a scalar gate conditioned on the word - level representations to control the ratio of the two representations .",23,0,26
dataset/preprocessed/training-data/natural_language_inference/45,"However , for the task of reading comprehension , preliminary experiments showed that this method was notable to improve the performance over concatenation .",24,0,24
dataset/preprocessed/training-data/natural_language_inference/45,There are two possible reasons .,25,0,6
dataset/preprocessed/training-data/natural_language_inference/45,"First , word - level representations might not contain sufficient information to support the decisions of selecting between the two representations .",26,0,22
dataset/preprocessed/training-data/natural_language_inference/45,"Second , using a scalar gate means applying the same ratio for each of the dimensions , which can be suboptimal .",27,0,22
dataset/preprocessed/training-data/natural_language_inference/45,"In this work , we present a fine - grained gating mechanism to combine the word - level and characterlevel representations .",28,0,22
dataset/preprocessed/training-data/natural_language_inference/45,We compute a vector gate as a linear projection of the token features followed 1 Code is available at https://github.com/kimiyoung/fg-gating 1 ar Xiv: 1611.01724v2 [ cs.CL ] 11 Sep 2017,29,0,30
dataset/preprocessed/training-data/natural_language_inference/45,Published as a conference paper at ICLR 2017 by a sigmoid activation .,30,0,13
dataset/preprocessed/training-data/natural_language_inference/45,We then multiplicatively apply the gate to the character - level and wordlevel representations .,31,0,15
dataset/preprocessed/training-data/natural_language_inference/45,Each dimension of the gate controls how much information is flowed from the word - level and character - level representations respectively .,32,0,23
dataset/preprocessed/training-data/natural_language_inference/45,"We use named entity tags , part - ofspeech tags , document frequencies , and word - level representations as the features for token properties which determine the gate .",33,0,30
dataset/preprocessed/training-data/natural_language_inference/45,"More generally , our fine - grained gating mechanism can be used to model multiple levels of structure in language , including words , characters , phrases , sentences and paragraphs .",34,0,32
dataset/preprocessed/training-data/natural_language_inference/45,In this work we focus on studying the effects on word - character gating .,35,0,15
dataset/preprocessed/training-data/natural_language_inference/45,"To better tackle the problem of reading comprehension , we also extend the idea of fine - grained gating for modeling the interaction between documents and queries .",36,0,28
dataset/preprocessed/training-data/natural_language_inference/45,Previous work has shown the importance of modeling interactions between document and query tokens by introducing various attention architectures for the task .,37,0,23
dataset/preprocessed/training-data/natural_language_inference/45,Most of these use an inner product between the two representations to compute the relative importance of document tokens .,38,0,20
dataset/preprocessed/training-data/natural_language_inference/45,The Gated - Attention Reader showed improved performance by replacing the inner-product with an element - wise product to allow for better matching at the semantic level .,39,0,28
dataset/preprocessed/training-data/natural_language_inference/45,"However , they use aggregated representations of the query which may lead to loss of information .",40,0,17
dataset/preprocessed/training-data/natural_language_inference/45,In this work we use a fine - grained gating mechanism for each token in the paragraph and each token in the query .,41,0,24
dataset/preprocessed/training-data/natural_language_inference/45,The fine - grained gating mechanism applies an element - wise multiplication of the two representations .,42,0,17
dataset/preprocessed/training-data/natural_language_inference/45,"We show improved performance on reading comprehension datasets , including Children 's Book Test ( CBT ) , Who Did What , and SQuAD .",43,0,25
dataset/preprocessed/training-data/natural_language_inference/45,"On CBT , our approach achieves new state - of - the - art results without using an ensemble .",44,0,20
dataset/preprocessed/training-data/natural_language_inference/45,Our model also improves over state - of - the - art results on the Who Did What dataset .,45,0,20
dataset/preprocessed/training-data/natural_language_inference/45,"To demonstrate the generality of our method , we apply our word - character finegrained gating mechanism to asocial media tag prediction task and show improved performance over previous methods .",46,0,31
dataset/preprocessed/training-data/natural_language_inference/45,Our contributions are two - fold .,47,0,7
dataset/preprocessed/training-data/natural_language_inference/45,"First , we present a fine - grained word - character gating mechanism and show improved performance on a variety of tasks including reading comprehension .",48,0,26
dataset/preprocessed/training-data/natural_language_inference/45,"Second , to better tackle the reading comprehension tasks , we extend our fine - grained gating approach to modeling the interaction between documents and queries .",49,0,27
dataset/preprocessed/training-data/natural_language_inference/70,KeLP at SemEval-2016 Task 3 : Learning Semantic Relations between Questions and Answers,2,0,13
dataset/preprocessed/training-data/natural_language_inference/70,This paper describes the KeLP system participating in the SemEval - 2016 Community Question Answering ( c QA ) task .,4,1,21
dataset/preprocessed/training-data/natural_language_inference/70,The challenge tasks are modeled as binary classification problems : kernel - based classifiers are trained on the SemEval datasets and their scores are used to sort the instances and produce the final ranking .,5,0,35
dataset/preprocessed/training-data/natural_language_inference/70,All classifiers and kernels have been implemented within the Kernel - based Learning Platform called KeLP .,6,0,17
dataset/preprocessed/training-data/natural_language_inference/70,"Our primary submission ranked first in Subtask A , third in Subtask B and second in Subtask C .",7,0,19
dataset/preprocessed/training-data/natural_language_inference/70,"These ranks are based on MAP , which is the referring challenge system score .",8,0,15
dataset/preprocessed/training-data/natural_language_inference/70,Our approach outperforms all the other systems with respect to all the other challenge metrics .,9,0,16
dataset/preprocessed/training-data/natural_language_inference/70,This paper describes the KeLP system participating in the SemEval - 2016 cQA challenge .,13,0,15
dataset/preprocessed/training-data/natural_language_inference/70,"In this task , participants are asked to automatically provide good answers in a c QA setting .",14,1,18
dataset/preprocessed/training-data/natural_language_inference/70,"In particular , the main task is : given a new question and a large collection of question - comment threads created by a user community , rank the most useful comments on the top .",15,0,36
dataset/preprocessed/training-data/natural_language_inference/70,"We participated in all English subtasks : the datasets were extracted from Qatar Living 1 , a web forum where people pose questions about multiple aspects of their daily life in Qatar .",16,0,33
dataset/preprocessed/training-data/natural_language_inference/70,"Three subtasks are associated with the English challenge : Subtask A : Given a question q and its first 10 comments c 1 , . . . , c 10 in its question thread , re-rank these 10 comments according to their relevance with respect to the question , i.e. , the good comments have to be ranked above potential or bad comments .",17,0,64
dataset/preprocessed/training-data/natural_language_inference/70,"Subtask B : Given a new question o and the set of the first 10 related questions q 1 , . . . , q 10 ( retrieved by a search engine ) , re-rank the related questions according to their similarity with respect too , i.e. , the perfect match and relevant questions should be ranked above the irrelevant ones .",18,0,62
dataset/preprocessed/training-data/natural_language_inference/70,"Subtask C : Given a new question o , and the set of the first 10 related questions , q 1 , . . . , q 10 , ( retrieved by a search engine ) , each one associated with its first 10 comments , c q 1 , . . . , c q 10 , appearing in its thread , re-rank the 100 comments according to their relevance with respect too , i.e. , the good comments are to be ranked above potential or bad comments .",19,0,90
dataset/preprocessed/training-data/natural_language_inference/70,All the above subtasks have been modeled as binary classification problems : kernel - based classifiers are trained and the classification score is used to sort the instances and produce the final ranking .,20,0,34
dataset/preprocessed/training-data/natural_language_inference/70,"All classifiers and kernels have been implemented within the Kernel - based Learning Platform 2 ( KeLP ) , thus determining the team 's name .",21,0,26
dataset/preprocessed/training-data/natural_language_inference/70,"The proposed solution provides three main contributions : ( i ) we employ the approach proposed in , which applies tree kernels directly to question and answer texts modeled as pairs of linked syntactic trees .",22,0,36
dataset/preprocessed/training-data/natural_language_inference/70,We further improve the methods using the kernels proposed in .,23,0,11
dataset/preprocessed/training-data/natural_language_inference/70,"( ii ) we extended the features developed in , by adopting several features ( also derived from Word Embeddings ) .",24,0,22
dataset/preprocessed/training-data/natural_language_inference/70,( iii ) we propose a stacking schema so that classifiers for Subtask B and C exploit the inferences obtained in the previous subtasks .,25,0,25
dataset/preprocessed/training-data/natural_language_inference/70,"Our primary submission ranked first in Subtask A , third in Subtask B and second in Subtask C , demonstrating that the proposed method is very accurate and adaptable to different learning problems .",26,0,34
dataset/preprocessed/training-data/natural_language_inference/70,These ranks are based on the MAP metric .,27,0,9
dataset/preprocessed/training-data/natural_language_inference/70,"However , if we consider the other metrics also adopted in the challenge ( e.g. , F 1 or Accuracy ) our approach outperforms all the remaining systems .",28,0,29
dataset/preprocessed/training-data/natural_language_inference/70,"In the remaining , Section 2 introduces the system , Sections 3 and 4 describe the feature and kernel modeling , while Section 5 reports official results .",29,0,28
dataset/preprocessed/training-data/natural_language_inference/70,The KeLP system : an overview,30,0,6
dataset/preprocessed/training-data/natural_language_inference/70,"In the three subtasks , the underlying problem is to understand if two texts are related .",31,0,17
dataset/preprocessed/training-data/natural_language_inference/70,"Thus , in subtasks A and C , each pair , ( question , comment ) , generates a training instance for a binary Support Vector Machine ( SVM ) , where the positive label is associated with a good comment and the negative label includes the potential and bad comments .",32,0,52
dataset/preprocessed/training-data/natural_language_inference/70,"In Subtask B , we evaluated the similarity between two questions .",33,0,12
dataset/preprocessed/training-data/natural_language_inference/70,"Each pair generates a training instance for SVM , where the positive label is associated with the perfect match or relevant classes and the negative label is associated with the irrelevant ; the resulting classification score is used to rank the question pairs .",34,0,44
dataset/preprocessed/training-data/natural_language_inference/70,"In KeLP , the SVM learning algorithm operates on a linear combination of kernel functions , each one applied over a specific representation of the targeted examples : ( i ) feature vectors containing linguistic similarities between the texts in a pair ; ( ii ) shallow syntactic trees that encode the lexical and morphosyntactic information shared between text pairs ; ( iii ) feature vectors capturing task - specific information ; ( iv ) in subtasks B and C , feature vectors encoding stacked information derived by applying the classifiers obtained in the previous subtasks .",35,0,97
dataset/preprocessed/training-data/natural_language_inference/70,"While ( i ) and ( ii ) use linguistic information that can be applied in any semantic processing task defined over text pairs ( see Sec. 3 ) , the information derived via ( iii ) and ( iv ) is task specific ( see Sec. 4 ) .",36,0,50
dataset/preprocessed/training-data/natural_language_inference/70,Learning from Text Pairs with Kernels,37,0,6
dataset/preprocessed/training-data/natural_language_inference/70,"The problem of deciding whether two questions are related or whether a comment answers a question , can be somehow connected to the problems of recognizing textual entailment , and paraphrase identification .",38,0,33
dataset/preprocessed/training-data/natural_language_inference/70,"From a machine learning perspective , in these tasks , an example is a pair of texts , instead of a single entity .",39,0,24
dataset/preprocessed/training-data/natural_language_inference/70,Conventional approaches convert input text pairs into feature vectors where each feature represents a score corresponding to a certain type of shared information or similarity between the elements within a pair .,40,0,32
dataset/preprocessed/training-data/natural_language_inference/70,"These intra-pair similarity approaches can not capture complex relational pattern between the elements in the pair , such as a rewriting rule characterizing a valid paraphrase , or a questionanswer pattern .",41,0,32
dataset/preprocessed/training-data/natural_language_inference/70,"Such information might be manually encoded into specific features , but it would require a complex feature engineering and a deep knowledge of the linguistic involved phenomena .",42,0,28
dataset/preprocessed/training-data/natural_language_inference/70,"To automatize relational learning between pairs of texts , e.g. , in case of QA , one of the early works is .",43,0,23
dataset/preprocessed/training-data/natural_language_inference/70,"This approach was improved in several subsequent researches , exploiting relational tags and linked open data .",44,0,17
dataset/preprocessed/training-data/natural_language_inference/70,"In particular , in , we propose new inter-pair methods to directly employ text pairs into a kernel - based learning framework .",45,0,23
dataset/preprocessed/training-data/natural_language_inference/70,"In the proposed approach , we integrate the information derived from simple intra-pair similarity functions ( Section 3.1 ) and from the structural analogies ( Section 3.2 ) .",46,0,29
dataset/preprocessed/training-data/natural_language_inference/70,"In subtasks A and C , a good comment is likely to share similar terms with the question .",48,0,19
dataset/preprocessed/training-data/natural_language_inference/70,In subtask Ba question that is relevant to another probably shows common words .,49,0,14
dataset/preprocessed/training-data/natural_language_inference/79,Distributed Representations of Sentences and Documents,2,1,6
dataset/preprocessed/training-data/natural_language_inference/79,Many machine learning algorithms require the input to be represented as a fixed - length feature vector .,4,0,18
dataset/preprocessed/training-data/natural_language_inference/79,"When it comes to texts , one of the most common fixed - length features is bag - of - words .",5,0,22
dataset/preprocessed/training-data/natural_language_inference/79,"Despite their popularity , bag - of - words features have two major weaknesses : they lose the ordering of the words and they also ignore semantics of the words .",6,0,31
dataset/preprocessed/training-data/natural_language_inference/79,"For example , "" powerful , "" "" strong "" and "" Paris "" are equally distant .",7,0,18
dataset/preprocessed/training-data/natural_language_inference/79,"In this paper , we propose Paragraph Vector , an unsupervised algorithm that learns fixed - length feature representations from variable - length pieces of texts , such as sentences , paragraphs , and documents .",8,0,36
dataset/preprocessed/training-data/natural_language_inference/79,Our algorithm represents each document by a dense vector which is trained to predict words in the document .,9,0,19
dataset/preprocessed/training-data/natural_language_inference/79,Its construction gives our algorithm the potential to overcome the weaknesses of bag - ofwords models .,10,0,17
dataset/preprocessed/training-data/natural_language_inference/79,Empirical results show that Paragraph Vectors outperform bag - of - words models as well as other techniques for text representations .,11,0,22
dataset/preprocessed/training-data/natural_language_inference/79,"Finally , we achieve new state - of - the - art results on several text classification and sentiment analysis tasks .",12,0,22
dataset/preprocessed/training-data/natural_language_inference/79,"Text classification and clustering play an important role in many applications , e.g , document retrieval , web search , spam filtering .",14,0,23
dataset/preprocessed/training-data/natural_language_inference/79,At the heart of these applications is machine learning algorithms such as logistic regression or Kmeans .,15,0,17
dataset/preprocessed/training-data/natural_language_inference/79,These algorithms typically require the text input to be represented as a fixed - length vector .,16,0,17
dataset/preprocessed/training-data/natural_language_inference/79,"Perhaps the most common fixed - length vector representation for texts is the bag - of - words or bag - of - n- grams due to its simplicity , efficiency and often surprising accuracy .",17,0,36
dataset/preprocessed/training-data/natural_language_inference/79,"However , the bag - of - words ( BOW ) has many dis advan - tages .",18,0,18
dataset/preprocessed/training-data/natural_language_inference/79,"The word order is lost , and thus different sentences can have exactly the same representation , as long as the same words are used .",19,0,26
dataset/preprocessed/training-data/natural_language_inference/79,"Even though bag - of - n- grams considers the word order in short context , it suffers from data sparsity and high dimensionality .",20,0,25
dataset/preprocessed/training-data/natural_language_inference/79,Bag - of - words and bagof - n- grams have very little sense about the semantics of the words or more formally the distances between the words .,21,0,29
dataset/preprocessed/training-data/natural_language_inference/79,"This means that words "" powerful , "" "" strong "" and "" Paris "" are equally distant despite the fact that semantically , "" powerful "" should be closer to "" strong "" than "" Paris . """,22,0,39
dataset/preprocessed/training-data/natural_language_inference/79,"In this paper , we propose Paragraph Vector , an unsupervised framework that learns continuous distributed vector representations for pieces of texts .",23,0,23
dataset/preprocessed/training-data/natural_language_inference/79,"The texts can be of variable - length , ranging from sentences to documents .",24,0,15
dataset/preprocessed/training-data/natural_language_inference/79,"The name Paragraph Vector is to emphasize the fact that the method can be applied to variable - length pieces of texts , anything from a phrase or sentence to a large document .",25,0,34
dataset/preprocessed/training-data/natural_language_inference/79,"In our model , the vector representation is trained to be useful for predicting words in a paragraph .",26,0,19
dataset/preprocessed/training-data/natural_language_inference/79,"More precisely , we concatenate the paragraph vector with several word vectors from a paragraph and predict the following word in the given context .",27,0,25
dataset/preprocessed/training-data/natural_language_inference/79,Both word vectors and paragraph vectors are trained by the stochastic gradient descent and backpropagation .,28,0,16
dataset/preprocessed/training-data/natural_language_inference/79,"While paragraph vectors are unique among paragraphs , the word vectors are shared .",29,0,14
dataset/preprocessed/training-data/natural_language_inference/79,"At prediction time , the paragraph vectors are inferred by fixing the word vectors and training the new paragraph vector until convergence .",30,0,23
dataset/preprocessed/training-data/natural_language_inference/79,Our technique is inspired by the recent work in learning vector representations of words using neural networks .,31,0,18
dataset/preprocessed/training-data/natural_language_inference/79,"In their formulation , each word is represented by a vector which is concatenated or averaged with other word vectors in a context , and the resulting vector is used to predict other words in the context .",32,0,38
dataset/preprocessed/training-data/natural_language_inference/79,"For example , the neural network language model proposed in uses the concatenation of several previous word vectors to form the input of a neural network , and tries to predict the next word .",33,0,35
dataset/preprocessed/training-data/natural_language_inference/79,"The outcome is that after the model is trained , the word vectors are mapped into a vector space such that semantically similar words have similar vector representations ( e.g. , "" strong "" is close to "" powerful "" ) .",34,0,42
dataset/preprocessed/training-data/natural_language_inference/79,"Following these successful techniques , researchers have tried to extend the models to go beyond word level to achieve phrase - level or sentence - level representations .",35,0,28
dataset/preprocessed/training-data/natural_language_inference/79,"For instance , a simple approach is using a weighted average of all the words in the document .",36,0,19
dataset/preprocessed/training-data/natural_language_inference/79,"A more sophisticated approach is combining the word vectors in an order given by a parse tree of a sentence , using matrix - vector operations .",37,0,27
dataset/preprocessed/training-data/natural_language_inference/79,Both approaches have weaknesses .,38,0,5
dataset/preprocessed/training-data/natural_language_inference/79,"The first approach , weighted averaging of word vectors , loses the word order in the same way as the standard bag - of - words models do .",39,0,29
dataset/preprocessed/training-data/natural_language_inference/79,"The second approach , using a parse tree to combine word vectors , has been shown to work for only sentences because it relies on parsing .",40,0,27
dataset/preprocessed/training-data/natural_language_inference/79,Paragraph Vector is capable of constructing representations of input sequences of variable length .,41,0,14
dataset/preprocessed/training-data/natural_language_inference/79,"Unlike some of the previous approaches , it is general and applicable to texts of any length : sentences , paragraphs , and documents .",42,0,25
dataset/preprocessed/training-data/natural_language_inference/79,It does not require task - specific tuning of the word weighting function nor does it rely on the parse trees .,43,0,22
dataset/preprocessed/training-data/natural_language_inference/79,"Further in the paper , we will present experiments on several benchmark datasets that demonstrate the advantages of Paragraph Vector .",44,0,21
dataset/preprocessed/training-data/natural_language_inference/79,"For example , on sentiment analysis task , we achieve new stateof - the - art results , better than complex methods , yielding a relative improvement of more than 16 % in terms of error rate .",45,0,38
dataset/preprocessed/training-data/natural_language_inference/79,"On a text classification task , our method convincingly beats bag - of - words models , giving a relative improvement of about 30 % .",46,0,26
dataset/preprocessed/training-data/natural_language_inference/79,We start by discussing previous methods for learning word vectors .,48,0,11
dataset/preprocessed/training-data/natural_language_inference/79,These methods are the inspiration for our Paragraph Vector methods .,49,0,11
dataset/preprocessed/training-data/natural_language_inference/95,Multi - Passage Machine Reading Comprehension with Cross - Passage Answer Verification,2,1,12
dataset/preprocessed/training-data/natural_language_inference/95,Machine reading comprehension ( MRC ) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine .,4,1,27
dataset/preprocessed/training-data/natural_language_inference/95,"Compared with MRC on a single passage , multi-passage MRC is more challenging , since we are likely to get multiple confusing answer candidates from different passages .",5,1,28
dataset/preprocessed/training-data/natural_language_inference/95,"To address this problem , we propose an end - to - end neural model that enables those answer candidates from different passages to verify each other based on their content representations .",6,0,33
dataset/preprocessed/training-data/natural_language_inference/95,"Specifically , we jointly train three modules that can predict the final answer based on three factors : the answer boundary , the answer content and the cross - passage answer verification .",7,0,33
dataset/preprocessed/training-data/natural_language_inference/95,"The experimental results show that our method outperforms the baseline by a large margin and achieves the state - of - the - art performance on the English MS - MARCO dataset and the Chinese DuReader dataset , both of which are designed for MRC in real - world settings .",8,0,51
dataset/preprocessed/training-data/natural_language_inference/95,"Machine reading comprehension ( MRC ) , empowering computers with the ability to acquire knowledge and answer questions from textual data , is believed to be a crucial step in building a general intelligent agent .",10,0,36
dataset/preprocessed/training-data/natural_language_inference/95,Recent years have seen rapid growth in the MRC community .,11,0,11
dataset/preprocessed/training-data/natural_language_inference/95,"With the release of various datasets , the MRC task has evolved from the early cloze - style test to answer extraction from a single passage and to the latest more complex question answering on web data .",12,0,38
dataset/preprocessed/training-data/natural_language_inference/95,"Great efforts have also been made to develop models for these MRC tasks , especially for the answer extraction on single passage .",13,0,23
dataset/preprocessed/training-data/natural_language_inference/95,A significant milestone is that several MRC models have exceeded the performance of human annotators on the SQuAD dataset 1 ) .,14,0,22
dataset/preprocessed/training-data/natural_language_inference/95,"However , this success on single Wikipedia passage is still not adequate , considering the ultimate goal of reading the whole web .",15,0,23
dataset/preprocessed/training-data/natural_language_inference/95,"Therefore , several latest datasets attempt to design the MRC tasks in more realistic settings by involving search engines .",16,0,20
dataset/preprocessed/training-data/natural_language_inference/95,"For each question , they use the search engine to retrieve multiple passages and the MRC models are required to read these passages in order to give the final answer .",17,0,31
dataset/preprocessed/training-data/natural_language_inference/95,"One of the intrinsic challenges for such multipassage MRC is that since all the passages are question - related but usually independently written , it 's probable that multiple confusing answer candidates ( correct or incorrect ) exist .",18,0,39
dataset/preprocessed/training-data/natural_language_inference/95,shows an example from MS - MARCO .,19,0,8
dataset/preprocessed/training-data/natural_language_inference/95,We can see that all the answer candidates have semantic matching with the question while they are literally different and some of them are even incorrect .,20,0,27
dataset/preprocessed/training-data/natural_language_inference/95,"As is shown by , these confusing answer candidates could be quite difficult for MRC models to distinguish .",21,0,19
dataset/preprocessed/training-data/natural_language_inference/95,"Therefore , special consideration is required for such multi -passage MRC problem .",22,0,13
dataset/preprocessed/training-data/natural_language_inference/95,"In this paper , we propose to leverage the answer candidates from different passages to verify the final correct answer and rule out the noisy incorrect answers .",23,0,28
dataset/preprocessed/training-data/natural_language_inference/95,Our hypothesis is that the cor - 1 https://rajpurkar.github.io/SQuAD-explorer/ ar Xiv:1805.02220v2 [ cs.CL ] 10 May 2018,24,0,17
dataset/preprocessed/training-data/natural_language_inference/95,What is the difference between a mixed and pure culture ?,26,0,11
dataset/preprocessed/training-data/natural_language_inference/95,[ 1 ] A culture is a society 's total way of living and a society is a group that live in a defined territory and participate in common culture .,28,0,31
dataset/preprocessed/training-data/natural_language_inference/95,"While the answer given is in essence true , societies originally form for the express purpose to enhance . . . . . .",29,0,24
dataset/preprocessed/training-data/natural_language_inference/95,There has been resurgence in the economic system known as capitalism during the past two decades .,30,0,17
dataset/preprocessed/training-data/natural_language_inference/95,4 . The mixed economy is a balance between socialism and capitalism .,31,0,13
dataset/preprocessed/training-data/natural_language_inference/95,"As a result , some institutions are owned and maintained by . . .",32,0,14
dataset/preprocessed/training-data/natural_language_inference/95,[ 3 ] A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies .,33,0,30
dataset/preprocessed/training-data/natural_language_inference/95,"Culture on the other hand , is the lifestyle that the people in the country . . .",34,0,18
dataset/preprocessed/training-data/natural_language_inference/95,[ 4 ] Best Answer :,35,0,6
dataset/preprocessed/training-data/natural_language_inference/95,A pure culture comprises a single species or strains .,36,0,10
dataset/preprocessed/training-data/natural_language_inference/95,A mixed culture is taken from a source and may contain multiple strains or species .,37,0,16
dataset/preprocessed/training-data/natural_language_inference/95,A contaminated culture contains organisms that derived from someplace . . .,38,0,12
dataset/preprocessed/training-data/natural_language_inference/95,[ 5 ] . . .,39,0,6
dataset/preprocessed/training-data/natural_language_inference/95,It will beat that time when we can truly obtain a pure culture .,40,0,14
dataset/preprocessed/training-data/natural_language_inference/95,A pure culture is a culture consisting of only one strain .,41,0,12
dataset/preprocessed/training-data/natural_language_inference/95,You can obtain a pure culture by picking out a small portion of the mixed culture . . .,42,0,19
dataset/preprocessed/training-data/natural_language_inference/95,[ 6 ] A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies .,43,0,30
dataset/preprocessed/training-data/natural_language_inference/95,A pure culture is a culture consisting of only one strain .,44,0,12
dataset/preprocessed/training-data/natural_language_inference/95,. . . Reference Answer :,45,0,6
dataset/preprocessed/training-data/natural_language_inference/95,A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies .,46,0,27
dataset/preprocessed/training-data/natural_language_inference/95,"rect answers could occur more frequently in those passages and usually share some commonalities , while incorrect answers are usually different from one another .",47,0,25
dataset/preprocessed/training-data/natural_language_inference/95,The example in demonstrates this phenomenon .,48,0,7
dataset/preprocessed/training-data/natural_language_inference/95,"We can see that the answer candidates extracted from the last four passages are all valid answers to the question and they are semantically similar to each other , while the answer candidates from the other two passages are incorrect and there is no supportive information from other passages .",49,0,50
dataset/preprocessed/training-data/natural_language_inference/12,Shortcut - Stacked Sentence Encoders for Multi- Domain Inference,2,1,9
dataset/preprocessed/training-data/natural_language_inference/12,We present a simple sequential sentence encoder for multi-domain natural language inference .,4,1,13
dataset/preprocessed/training-data/natural_language_inference/12,Our encoder is based on stacked bidirectional LSTM - RNNs with shortcut connections and fine - tuning of word embeddings .,5,0,21
dataset/preprocessed/training-data/natural_language_inference/12,"The over all supervised model uses the above encoder to encode two input sentences into two vectors , and then uses a classifier over the vector combination to label the relationship between these two sentences as that of entailment , contradiction , or neural .",6,0,45
dataset/preprocessed/training-data/natural_language_inference/12,"Our Shortcut - Stacked sentence encoders achieve strong improvements over existing encoders on matched and mismatched multi-domain natural language inference ( top non-ensemble single - model result in the EMNLP RepEval 2017 Shared Task ( Nangia et al. , 2017 ) ) .",7,0,43
dataset/preprocessed/training-data/natural_language_inference/12,"Moreover , they achieve the new state - of - theart encoding result on the original SNLI dataset ( Bowman et al. , 2015 ) .",8,0,26
dataset/preprocessed/training-data/natural_language_inference/12,Introduction and Background,9,0,3
dataset/preprocessed/training-data/natural_language_inference/12,Natural language inference ( NLI ) or recognizing textual entailment ( RTE ) is a fundamental semantic task in the field of natural language processing .,10,1,26
dataset/preprocessed/training-data/natural_language_inference/12,The problem is to determine whether a given hypothesis sentence can be logically inferred from a given premise sentence .,11,0,20
dataset/preprocessed/training-data/natural_language_inference/12,"Recently released datasets such as the Stanford Natural Language Inference Corpus ( SNLI ) and the Multi - Genre Natural Language Inference Corpus ) ( Multi - NLI ) have not only encouraged several end - to - end neural network approaches to NLI , but have also served as an evaluation resource for general representation learning of natural language .",12,0,61
dataset/preprocessed/training-data/natural_language_inference/12,"Depending on whether a model will first encode a sentence into a fixed - length vector without any incorporating information from the other sentence , the several proposed models can be categorized into two groups : ( 1 ) encoding - based models ( or sentence encoders ) , such as Tree - based CNN encoders ( TBCNN ) in or in , and ( 2 ) joint , pairwise models that use cross -features between the two sentences to encode them , such as the Enhanced Sequential Inference Model ( ESIM ) in or the bilateral multiperspective matching ( BiMPM ) model .",13,0,104
dataset/preprocessed/training-data/natural_language_inference/12,"Moreover , common sentence encoders can again be classified into tree - based encoders such as which we mentioned before , or sequential encoders such as the biLSTM model by .",14,0,31
dataset/preprocessed/training-data/natural_language_inference/12,"In this paper , we follow the former approach of encoding - based models , and propose a novel yet simple sequential sentence encoder for the Multi - NLI problem .",15,0,31
dataset/preprocessed/training-data/natural_language_inference/12,Our encoder does not require any syntactic information of the sentence .,16,0,12
dataset/preprocessed/training-data/natural_language_inference/12,It also does not contain any attention or memory structure .,17,0,11
dataset/preprocessed/training-data/natural_language_inference/12,It is basically a stacked ( multi-layered ) bidirectional LSTM - RNN with shortcut connections ( feeding all previous layers ' outputs and word embeddings to each layer ) and word embedding fine - tuning .,18,0,36
dataset/preprocessed/training-data/natural_language_inference/12,"The over all supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors , and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment , contradiction , or neural ( similar to the classifier setup of and ) .",19,0,55
dataset/preprocessed/training-data/natural_language_inference/12,"Our simple shortcut - stacked encoders achieve strong improvements over existing encoders due to its multi-layered and shortcutconnected properties , on both matched and mis- matched evaluation settings for multi-domain natural language inference , as well as on the original SNLI dataset .",20,0,43
dataset/preprocessed/training-data/natural_language_inference/12,"It is the top single - model ( nonensemble ) result in the EMNLP RepEval 2017 Multi - NLI Shared Task , and the new state - of - the - art for encoding - based results on the SNLI dataset .",21,0,42
dataset/preprocessed/training-data/natural_language_inference/12,Github Code Link : https://github.com/ easonnie/multiNLI_encoder,22,0,6
dataset/preprocessed/training-data/natural_language_inference/12,"Our model mainly consists of two separate components , a sentence encoder and an entailment classifier .",24,0,17
dataset/preprocessed/training-data/natural_language_inference/12,The sentence encoder compresses each source sentence into a vector representation and the classifier makes a three - way classification based on the two vectors of the two source sentences .,25,0,31
dataset/preprocessed/training-data/natural_language_inference/12,"The model follows the ' encoding - based rule ' , i.e. , the encoder will encode each source sentence into a fixed length vector without any information or function based on the other sentence ( e.g. , cross - attention or memory comparing the two sentences ) .",26,0,49
dataset/preprocessed/training-data/natural_language_inference/12,"In order to fully explore the generalization of the sentence encoder , the same encoder is applied to both the premise and the hypothesis with shared parameters projecting them into the same space .",27,0,34
dataset/preprocessed/training-data/natural_language_inference/12,This setting follows the idea of Siamese Networks in .,28,0,10
dataset/preprocessed/training-data/natural_language_inference/12,shows the overview of our encoding model ( the standard classifier setup is not shown here ; see and for that ) .,29,0,23
dataset/preprocessed/training-data/natural_language_inference/12,Our sentence encoder is simply composed of multiple stacked bidirectional LSTM ( biLSTM ) layers with shortcut connections followed by a max pooling layer .,31,0,25
dataset/preprocessed/training-data/natural_language_inference/12,"Let bilstm i represent the ith biLSTM layer , which is defined as :",32,0,14
dataset/preprocessed/training-data/natural_language_inference/12,"where hi t is the output of the ith biLSTM at time t over input sequence ( x i 1 , x i 2 , ... , xi n ) .",33,0,31
dataset/preprocessed/training-data/natural_language_inference/12,"In a typical stacked biLSTM structure , the input of the next LSTM - RNN layer is simply the output sequence of the previous LSTM - RNN layer .",34,0,29
dataset/preprocessed/training-data/natural_language_inference/12,"In our settings , the input sequences for the ith biLSTM layer are the concatenated outputs of all the previous layers , plus the original word embedding sequence .",35,0,29
dataset/preprocessed/training-data/natural_language_inference/12,"This gives a shortcut connection style setup , related to the widely used idea of residual connections in CNNs for computer vision , highway networks for RNNs in speech processing , and shortcut connections in hierarchical multitasking learning ; but in our case we feed in all the previous layers ' output se-quences as well as the word embedding sequence to every layer .",36,0,64
dataset/preprocessed/training-data/natural_language_inference/12,"Let W = ( w 1 , w 2 , ... , w n ) represent words in the source sentence .",37,0,22
dataset/preprocessed/training-data/natural_language_inference/12,We assume w i ?,38,0,5
dataset/preprocessed/training-data/natural_language_inference/12,Rd is a word embedding vector which are initialized using some pre-trained vector embeddings ( and is then fine - tuned end - to - end via the NLI supervision ) .,39,0,32
dataset/preprocessed/training-data/natural_language_inference/12,"Then , the input of ith biLSTM layer at time t is defined as :",40,0,15
dataset/preprocessed/training-data/natural_language_inference/12,"Then , assuming we have m layers of biLSTM , the final vector representation will be obtained by applying row - max - pool over the output of the last biLSTM layer , similar to .",41,0,36
dataset/preprocessed/training-data/natural_language_inference/12,The final layer is defined as :,42,0,7
dataset/preprocessed/training-data/natural_language_inference/12,", d m is the dimension of the hidden state of the last forward and backward LSTM layers , and v is the final vector representation for the source sentence ( which is later fed to the NLI classifier ) .",43,0,41
dataset/preprocessed/training-data/natural_language_inference/12,"The closest encoder architecture to ours is that of , whose model consists of a single - layer biLSTM with a max - pooling layer , which we treat as our starting point .",44,0,34
dataset/preprocessed/training-data/natural_language_inference/12,Our experiments ( Section 4 ) demonstrate that our enhancements of the stacked - bi RNN with shortcut connections provide significant gains on top of this baseline ( for both SNLI and Multi - NLI ) .,45,0,37
dataset/preprocessed/training-data/natural_language_inference/12,"After we obtain the vector representation for the premise and hypothesis sentence , we apply three matching methods to the two vectors ( i ) concatenation ( ii ) element - wise distance and ( iii ) elementwise product for these two vectors and then concatenate these three match vectors ( based on the heuristic matching presented in ) .",47,0,60
dataset/preprocessed/training-data/natural_language_inference/12,"Let v p and v h be the vector representations for premise and hypothesis , respectively .",48,0,17
dataset/preprocessed/training-data/natural_language_inference/12,The matching vector is then defined as :,49,0,8
dataset/preprocessed/training-data/natural_language_inference/2,A Question - Focused Multi- Factor Attention Network for Question Answering,2,1,11
dataset/preprocessed/training-data/natural_language_inference/2,Neural network models recently proposed for question answering ( QA ) primarily focus on capturing the passagequestion relation .,4,1,19
dataset/preprocessed/training-data/natural_language_inference/2,"However , they have minimal capability to link relevant facts distributed across multiple sentences which is crucial in achieving deeper understanding , such as performing multi-sentence reasoning , co-reference resolution , etc .",5,0,33
dataset/preprocessed/training-data/natural_language_inference/2,They also do not explicitly focus on the question and answer type which often plays a critical role in QA .,6,1,21
dataset/preprocessed/training-data/natural_language_inference/2,"In this paper , we propose a novel end - to - end question - focused multi-factor attention network for answer extraction .",7,0,23
dataset/preprocessed/training-data/natural_language_inference/2,Multi-factor attentive encoding using tensor - based transformation aggregates meaningful facts even when they are located in multiple sentences .,8,0,20
dataset/preprocessed/training-data/natural_language_inference/2,"To implicitly infer the answer type , we also propose a max-attentional question aggregation mechanism to encode a question vector based on the important words in a question .",9,0,29
dataset/preprocessed/training-data/natural_language_inference/2,"During prediction , we incorporate sequence - level encoding of the first wh-word and its immediately following word as an additional source of question type information .",10,0,27
dataset/preprocessed/training-data/natural_language_inference/2,"Our proposed model achieves significant improvements over the best prior state - of - the - art results on three large - scale challenging QA datasets , namely NewsQA , TriviaQA , and Search QA .",11,0,36
dataset/preprocessed/training-data/natural_language_inference/2,"In machine comprehension - based ( MC ) question answering ( QA ) , a machine is expected to provide an answer for a given question by understanding texts .",13,1,30
dataset/preprocessed/training-data/natural_language_inference/2,"In recent years , several MC datasets have been released .",14,0,11
dataset/preprocessed/training-data/natural_language_inference/2,released a multiple - choice question answering dataset .,15,0,9
dataset/preprocessed/training-data/natural_language_inference/2,created a large cloze - style dataset using CNN and Daily Mail news articles .,16,0,15
dataset/preprocessed/training-data/natural_language_inference/2,"Several models ) based on neural attentional and pointer networks ( Vinyals , Fortunato , and Jaitly 2015 ) have been proposed since then .",17,0,25
dataset/preprocessed/training-data/natural_language_inference/2,released the SQuAD dataset where the answers are freeform unlike in the previous MC datasets .,18,0,16
dataset/preprocessed/training-data/natural_language_inference/2,"Most of the previously released datasets are closed - world , i.e. , the questions and answers are formulated given the text passages .",19,0,24
dataset/preprocessed/training-data/natural_language_inference/2,"As such , the answer spans can often be extracted by simple word and context matching .",20,0,17
dataset/preprocessed/training-data/natural_language_inference/2,"Copyright c 2018 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",21,0,15
dataset/preprocessed/training-data/natural_language_inference/2,All rights reserved .,22,0,4
dataset/preprocessed/training-data/natural_language_inference/2,attempted to alleviate this issue by proposing the News QA dataset where the questions are formed only using the CNN article summaries without accessing the full text .,23,0,28
dataset/preprocessed/training-data/natural_language_inference/2,"As a result , a significant proportion of questions require reasoning beyond simple word matching .",24,0,16
dataset/preprocessed/training-data/natural_language_inference/2,"Two even more challenging open - world QA datasets , TriviaQA and SearchQA , have recently been released .",25,0,19
dataset/preprocessed/training-data/natural_language_inference/2,QA consists of question - answer pairs authored by trivia enthusiasts and independently gathered evidence documents from Wikipedia as well as Bing Web search .,27,0,25
dataset/preprocessed/training-data/natural_language_inference/2,"In SearchQA , the question - answer pairs are crawled from the Jeopardy archive and are augmented with text snippets retrieved from Google search .",28,0,25
dataset/preprocessed/training-data/natural_language_inference/2,"Recently , many neural models have been proposed , which mostly focus on passage - question interaction to capture the context similarity for extracting a text span as the answer .",29,0,31
dataset/preprocessed/training-data/natural_language_inference/2,"However , most of the models do not focus on synthesizing evidence from multiple sentences and fail to perform well on challenging open - world QA tasks such as New s QA and Triv - ia QA .",30,0,38
dataset/preprocessed/training-data/natural_language_inference/2,"Moreover , none of the models explicitly focus on question / answer type information for predicting the answer .",31,0,19
dataset/preprocessed/training-data/natural_language_inference/2,"In practice , fine - grained understanding of question / answer type plays an important role in QA .",32,0,19
dataset/preprocessed/training-data/natural_language_inference/2,"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",33,0,49
dataset/preprocessed/training-data/natural_language_inference/2,"Intuitively , AMANDA extracts the answer not only by synthesizing relevant facts from the passage but also by implicitly determining the suitable answer type during prediction .",34,0,27
dataset/preprocessed/training-data/natural_language_inference/2,The key contributions of this paper are :,35,0,8
dataset/preprocessed/training-data/natural_language_inference/2,We propose a multi-factor attentive encoding approach based on tensor transformation to synthesize meaningful evidence across multiple sentences .,36,0,19
dataset/preprocessed/training-data/natural_language_inference/2,"It is particularly effective when answering a question requires deeper understanding such as multi-sentence reasoning , co-reference resolution , etc .",37,0,21
dataset/preprocessed/training-data/natural_language_inference/2,"To subsume fine - grained answer type information , we propose a max- attentional question aggregation mecha - nism which learns to identify the meaningful portions of a question .",38,0,30
dataset/preprocessed/training-data/natural_language_inference/2,We also incorporate sequence - level representations of the first wh-word and its immediately following word in a question as an additional source of question type information .,39,0,28
dataset/preprocessed/training-data/natural_language_inference/2,"Given a pair of passage and question , an MC system needs to extract a text span from the passage as the answer .",41,0,24
dataset/preprocessed/training-data/natural_language_inference/2,"We formulate the answer as two pointers in the passage , which represent the beginning and ending tokens of the answer .",42,0,22
dataset/preprocessed/training-data/natural_language_inference/2,"Let P be a passage with tokens ( P 1 , P 2 , . . . , PT ) and Q be a question with tokens ( Q 1 , Q 2 , . . . , Q U ) , where T and U are the length of the passage and question respectively .",43,0,56
dataset/preprocessed/training-data/natural_language_inference/2,"To answer the question , a system needs to determine two pointers in the passage , band e , such that 1 ? b ? e ? T .",44,0,29
dataset/preprocessed/training-data/natural_language_inference/2,"The resulting answer tokens will be ( P b , P b+1 , . . . , P e ) .",45,0,21
dataset/preprocessed/training-data/natural_language_inference/2,The architecture of the proposed question - focused multifactor attention network 1 is given in .,47,0,16
dataset/preprocessed/training-data/natural_language_inference/2,Word - level Embedding,48,0,4
dataset/preprocessed/training-data/natural_language_inference/2,"Word - level embeddings are formed by two components : pre-trained word embedding vectors from GloVe ( Pennington , Socher , and Manning 2014 ) and convolutional neural network - based ( CNN ) character embeddings .",49,0,37
dataset/preprocessed/training-data/natural_language_inference/75,Key - Value Memory Networks for Directly Reading Documents,2,0,9
dataset/preprocessed/training-data/natural_language_inference/75,Directly reading documents and being able to answer questions from them is an unsolved challenge .,4,1,16
dataset/preprocessed/training-data/natural_language_inference/75,"To avoid its inherent difficulty , question answering ( QA ) has been directed towards using Knowledge Bases ( KBs ) instead , which has proven effective .",5,1,28
dataset/preprocessed/training-data/natural_language_inference/75,"Unfortunately KBs often suffer from being too restrictive , as the schema can not support certain types of answers , and too sparse , e.g. Wikipedia contains much more information than Freebase .",6,0,33
dataset/preprocessed/training-data/natural_language_inference/75,"In this work we introduce a new method , Key - Value Memory Networks , that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation .",7,0,37
dataset/preprocessed/training-data/natural_language_inference/75,"To compare using KBs , information extraction or Wikipedia documents directly in a single framework we construct an analysis tool , WIKIMOVIES , a QA dataset that contains raw text alongside a preprocessed KB , in the domain of movies .",8,1,41
dataset/preprocessed/training-data/natural_language_inference/75,Our method reduces the gap between all three settings .,9,0,10
dataset/preprocessed/training-data/natural_language_inference/75,It also achieves state - of - the - art results on the existing WIKIQA benchmark .,10,0,17
dataset/preprocessed/training-data/natural_language_inference/75,"Question answering ( QA ) has been along standing research problem in natural language processing , with the first systems attempting to answer questions by directly reading documents .",12,0,29
dataset/preprocessed/training-data/natural_language_inference/75,"The development of large - scale Knowledge Bases ( KBs ) such as Freebase helped organize information into structured forms , prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such data bases .",13,0,45
dataset/preprocessed/training-data/natural_language_inference/75,"Unfortunately , KBs have intrinsic limitations such as their inevitable incompleteness and fixed schemas that can not support all varieties of answers .",14,0,23
dataset/preprocessed/training-data/natural_language_inference/75,"Since information extraction ( IE ) , intended to fill in missing information in KBs , is neither accurate nor reliable enough , collections of raw textual resources and documents such as Wikipedia will always contain more information .",15,0,39
dataset/preprocessed/training-data/natural_language_inference/75,"As a result , even if KBs can be satisfactory for closed - domain problems , they are unlikely to scale up to answer general questions on any topic .",16,0,30
dataset/preprocessed/training-data/natural_language_inference/75,"Starting from this observation , in this work we study the problem of answering by directly reading documents .",17,0,19
dataset/preprocessed/training-data/natural_language_inference/75,"Retrieving answers directly from text is harder than from KBs because information is far less structured , is indirectly and ambiguously expressed , and is usually scattered across multiple documents .",18,0,31
dataset/preprocessed/training-data/natural_language_inference/75,This explains why using a satisfactory KB - typically only available in closed domains - is preferred over raw text .,19,0,21
dataset/preprocessed/training-data/natural_language_inference/75,"We postulate that before trying to provide answers thatare not in KBs , document - based QA systems should first reach KB - based systems ' performance in such closed domains , where clear comparison and evaluation is possible .",20,0,40
dataset/preprocessed/training-data/natural_language_inference/75,"To this end , this paper introduces WIKIMOVIES , a new analysis tool that allows for measuring the performance of QA systems when the knowledge source is switched from a KB to unstructured documents .",21,0,35
dataset/preprocessed/training-data/natural_language_inference/75,WIKIMOVIES contains ?,22,0,3
dataset/preprocessed/training-data/natural_language_inference/75,"100 k questions in the movie domain , and was designed to be answerable by using either a perfect KB ( based on OMDb 1 ) , Wikipedia pages or an imper- fect KB obtained through running an engineered IE pipeline on those pages .",23,0,45
dataset/preprocessed/training-data/natural_language_inference/75,"To bridge the gap between using a KB and reading documents directly , we still lack appropriate machine learning algorithms .",24,0,21
dataset/preprocessed/training-data/natural_language_inference/75,"In this work we propose the Key - Value Memory Network ( KV - MemNN ) , a new neural network architecture that generalizes the original Memory Network and can work with either knowledge source .",25,0,36
dataset/preprocessed/training-data/natural_language_inference/75,The KV - MemNN performs QA by first storing facts in a key - value structured memory before reasoning on them in order to predict an answer .,26,0,28
dataset/preprocessed/training-data/natural_language_inference/75,"The memory is designed so that the model learns to use keys to address relevant memories with respect to the question , whose corresponding values are subsequently returned .",27,0,29
dataset/preprocessed/training-data/natural_language_inference/75,"This structure allows the model to encode prior knowledge for the considered task and to leverage possibly complex transforms between keys and values , while still being trained using standard backpropagation via stochastic gradient descent .",28,0,36
dataset/preprocessed/training-data/natural_language_inference/75,"Our experiments on WIKIMOVIES indicate that , thanks to its key - value memory , the KV - Mem NN consistently outperforms the original Memory Network , and reduces the gap between answering from a human - annotated KB , from an automatically extracted KB or from directly reading Wikipedia .",29,0,51
dataset/preprocessed/training-data/natural_language_inference/75,"We confirm our findings on WIKIQA , another Wikipedia - based QA benchmark where no KB is available , where we demonstrate that KV - Mem NN can reach state - of - the - art resultssurpassing the most recent attention - based neural network models .",30,0,47
dataset/preprocessed/training-data/natural_language_inference/75,"Early QA systems were based on information retrieval and were designed to return snippets of text containing an answer , with limitations in terms of question complexity and response coverage .",32,0,31
dataset/preprocessed/training-data/natural_language_inference/75,The creation of large - scale KBs have led to the development of a new class of QA methods based on semantic parsing that can return precise answers to complicated compositional questions .,33,0,33
dataset/preprocessed/training-data/natural_language_inference/75,"Due to the sparsity of KB data , however , the main challenge shifts from finding answers to developing efficient information extraction methods to populate KBs auto - matically not an easy problem .",34,0,34
dataset/preprocessed/training-data/natural_language_inference/75,"For this reason , recent initiatives are returning to the original setting of directly answering from text using datasets like TRECQA , which is based on classical TREC resources , and WIKIQA , which is extracted from Wikipedia .",35,0,39
dataset/preprocessed/training-data/natural_language_inference/75,"Both benchmarks are organized around the task of answer sentence selection , where a system must identify the sentence containing the correct answer in a collection of documents , but need not return the actual answer as a KB - based system would do .",36,0,45
dataset/preprocessed/training-data/natural_language_inference/75,"Unfortunately , these datasets are very small ( hundreds of examples ) and , because of their answer selection setting , do not offer the option to directly compare answering from a KB against answering from pure text .",37,0,39
dataset/preprocessed/training-data/natural_language_inference/75,"Using similar resources as the dialog dataset of , our new benchmark WIKIMOVIES addresses both deficiencies by providing a substantial corpus of questionanswer pairs that can be answered by either using a KB or a corresponding set of documents .",38,0,40
dataset/preprocessed/training-data/natural_language_inference/75,"Even though standard pipeline QA systems like AskMR have been recently revisited , the best published results on TRECQA and WIKIQA have been obtained by either convolutional neural networks or recurrent neural networksboth usually with attention mechanisms inspired by .",39,0,40
dataset/preprocessed/training-data/natural_language_inference/75,"In this work , we introduce KV - MemNNs , a Memory Network model that operates a symbolic memory structured as ( key , value ) pairs .",40,0,28
dataset/preprocessed/training-data/natural_language_inference/75,Such structured memory is not employed in any existing attention - based neural network architecture for QA .,41,0,18
dataset/preprocessed/training-data/natural_language_inference/75,"As we will show , it gives the model greater flexibility for encoding knowledge sources and helps shrink the gap between directly reading documents and answering from a KB .",42,0,30
dataset/preprocessed/training-data/natural_language_inference/75,Key - Value Memory Networks,43,0,5
dataset/preprocessed/training-data/natural_language_inference/75,"The Key - Value Memory Network model is based on the Memory Network ( Mem NNs ) model which has proven useful for a variety of document reading and question answering tasks : for reading children 's books and answering questions about them , for complex reasoning over sim - ulated stories and for utilizing KBs to answer questions .",44,0,60
dataset/preprocessed/training-data/natural_language_inference/75,Key - value paired memories are a generalization of the way context ( e.g. knowledge bases or documents to be read ) are stored in memory .,45,0,27
dataset/preprocessed/training-data/natural_language_inference/75,The lookup ( addressing ) stage is based on the key memory while the reading stage ( giving the returned result ) uses the value memory .,46,0,27
dataset/preprocessed/training-data/natural_language_inference/75,This gives both ( i ) greater flexibility for the practitioner to encode prior knowledge about their task ; and ( ii ) more effective power in the model via nontrivial transforms between key and value .,47,0,37
dataset/preprocessed/training-data/natural_language_inference/75,"The key should be designed with features to help match it to the question , while the value should be designed with features to help match it to the response ( answer ) .",48,0,34
dataset/preprocessed/training-data/natural_language_inference/75,An important property of the model is that the entire model can be trained with key - value transforms while still using standard backpropagation via stochastic gradient descent .,49,0,29
dataset/preprocessed/training-data/natural_language_inference/51,Neural Stored - program Memory,2,1,5
dataset/preprocessed/training-data/natural_language_inference/51,Neural networks powered with external memory simulate computer behaviors .,4,0,10
dataset/preprocessed/training-data/natural_language_inference/51,"These models , which use the memory to store data for a neural controller , can learn algorithms and other complex tasks .",5,0,23
dataset/preprocessed/training-data/natural_language_inference/51,"In this paper , we introduce a new memory to store weights for the controller , analogous to the stored - program memory in modern computer architectures .",6,1,28
dataset/preprocessed/training-data/natural_language_inference/51,"The proposed model , dubbed Neural Stored - program Memory , augments current memory - augmented neural networks , creating differentiable machines that can switch programs through time , adapt to variable contexts and thus resemble the Universal Turing Machine .",7,0,41
dataset/preprocessed/training-data/natural_language_inference/51,"A wide range of experiments demonstrate that the resulting machines not only excel in classical algorithmic problems , but also have potential for compositional , continual , few - shot learning and question - answering tasks .",8,0,37
dataset/preprocessed/training-data/natural_language_inference/51,Recurrent Neural Networks ( RNNs ) are .,10,0,8
dataset/preprocessed/training-data/natural_language_inference/51,"However , in practice RNNs struggle to learn simple procedures as they lack explicit memory .",11,0,16
dataset/preprocessed/training-data/natural_language_inference/51,"These findings have sparked a new research direction called Memory Augmented Neural Networks ( MANNs ) that emulate modern computer behavior by detaching memorization from computation via memory and controller network , respectively .",12,0,34
dataset/preprocessed/training-data/natural_language_inference/51,MANNs have demonstrated significant improvements over memory - less RNNs in various sequential learning tasks .,13,0,16
dataset/preprocessed/training-data/natural_language_inference/51,"Nonetheless , MANNs have barely simulated general - purpose computers .",14,0,11
dataset/preprocessed/training-data/natural_language_inference/51,Current MANNs miss a key concept in computer design : stored - program memory .,15,0,15
dataset/preprocessed/training-data/natural_language_inference/51,The concept has emerged from the idea of Universal Turing Machine ( UTM ) and further developed in Harvard Architecture .,16,0,21
dataset/preprocessed/training-data/natural_language_inference/51,"In UTM , both data and programs that manipulate the data are stored in memory .",17,0,16
dataset/preprocessed/training-data/natural_language_inference/51,A control unit then reads the programs from the memory and executes them with the data .,18,0,17
dataset/preprocessed/training-data/natural_language_inference/51,This mechanism allows flexibility to perform universal computations .,19,0,9
dataset/preprocessed/training-data/natural_language_inference/51,"Unfortunately , current MANNs such as Neural Turing Machine ( NTM ) , Differentiable Neural Computer ( DNC ) and Least Recently Used Access ( LRUA ) only support memory for data and embed a single program into the controller network , which goes against the stored - program memory principle .",20,0,52
dataset/preprocessed/training-data/natural_language_inference/51,Our goal is to advance a step further towards UTM by coupling a MANN with an external program memory .,21,0,20
dataset/preprocessed/training-data/natural_language_inference/51,"The program memory co-exists with the data memory in the MANN , providing more flexibility , reuseability and modularity in learning complicated tasks .",22,0,24
dataset/preprocessed/training-data/natural_language_inference/51,"The program memory stores the weights of the MANN 's controller network , which are retrieved quickly via a key - value attention mechanism across timesteps yet updated slowly via backpropagation .",23,0,32
dataset/preprocessed/training-data/natural_language_inference/51,"By introducing a meta network to moderate the operations of the program memory , our model , henceforth referred to as Neural Stored - program Memory ( NSM ) , can learn to switch the programs / weights in the controller network appropriately , adapting to different functionalities aligning with different parts of a sequential task , or different tasks in continual and few - shot learning .",24,0,68
dataset/preprocessed/training-data/natural_language_inference/51,"To validate our proposal , the NTM armed with NSM , namely Neural Universal Turing Machine ( NUTM ) , is tested on a variety of synthetic tasks including algorithmic tasks from , composition of algorithmic tasks and continual procedure learning .",25,0,42
dataset/preprocessed/training-data/natural_language_inference/51,"For these algorithmic problems , we demonstrate clear improvements of NUTM over NTM .",26,0,14
dataset/preprocessed/training-data/natural_language_inference/51,"Further , we investigate NUTM in few - shot learning by using LRUA as the MANN and achieve notably better results .",27,0,22
dataset/preprocessed/training-data/natural_language_inference/51,"Finally , we expand NUTM application to linguistic problems by equipping NUTM with DNC core and achieve competitive performances against stateof - the - arts in the bAbI task .",28,0,30
dataset/preprocessed/training-data/natural_language_inference/51,"Taken together , our study advances neural network simulation of Turing Machines to neural architecture for Universal Turing Machines .",29,0,20
dataset/preprocessed/training-data/natural_language_inference/51,"This develops a new class of MANNs that can store and query both the weights and data of their own controllers , thereby following the stored - program principle .",30,0,30
dataset/preprocessed/training-data/natural_language_inference/51,A set of five diverse experiments demonstrate the computational universality of the approach .,31,0,14
dataset/preprocessed/training-data/natural_language_inference/51,"In this section , we briefly review MANN and its relations to Turing Machines .",33,0,15
dataset/preprocessed/training-data/natural_language_inference/51,A MANN consists of a controller network and an external memory M ?,34,0,13
dataset/preprocessed/training-data/natural_language_inference/51,"RN M , which is a collection of NM - dimensional vectors .",35,0,13
dataset/preprocessed/training-data/natural_language_inference/51,"The controller network is responsible for accessing the memory , updating its state and optionally producing output at each timestep .",36,0,21
dataset/preprocessed/training-data/natural_language_inference/51,"The first two functions are executed by an interface network and a state network 1 , respectively .",37,0,18
dataset/preprocessed/training-data/natural_language_inference/51,"Usually , the interface network is a Feedforward neural network whose input is ct - the output of the state network implemented as RNNs .",38,0,25
dataset/preprocessed/training-data/natural_language_inference/51,"Let W c denote the weight of the interface network , then the state update and memory control are as follows ,",39,0,22
dataset/preprocessed/training-data/natural_language_inference/51,"where x t and r t?1 are data from current input and the previous memory read , respectively .",40,0,19
dataset/preprocessed/training-data/natural_language_inference/51,The interface vector ?,41,0,4
dataset/preprocessed/training-data/natural_language_inference/51,t then is used to read from and write to the memory M.,42,0,13
dataset/preprocessed/training-data/natural_language_inference/51,"We use a generic notation memory (? t , M ) to represent these memory operations that either update or retrieve read valuer t from the memory .",43,0,28
dataset/preprocessed/training-data/natural_language_inference/51,"To support multiple memory accesses per step , the interface network may produce multiple interfaces , also known as control heads .",44,0,22
dataset/preprocessed/training-data/natural_language_inference/51,Readers are referred to App. F and ; for details of memory read / write examples .,45,0,17
dataset/preprocessed/training-data/natural_language_inference/51,"A deterministic one - tape Turing Machine can be defined by 4 - tuple ( Q , ? , ? , q 0 ) , in which Q is finite set of states , q 0 ?",46,0,37
dataset/preprocessed/training-data/natural_language_inference/51,"Q is an initial state , ?",47,0,7
dataset/preprocessed/training-data/natural_language_inference/51,is finite set of symbol stored in the tape ( the data ) and ?,48,0,15
dataset/preprocessed/training-data/natural_language_inference/51,"is the transition function ( the program ) , ? : Q ? ? ? {? 1 , 1 } Q . At each step , the machine performs the transition function , which takes the current state and the read value from the tape as inputs and outputs actions including writing new values , moving tape head to new location ( left / right ) and jumping to another state .",49,0,72
dataset/preprocessed/training-data/natural_language_inference/4,DCN + : MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING,2,1,12
dataset/preprocessed/training-data/natural_language_inference/4,"Traditional models for question answering optimize using cross entropy loss , which encourages exact answers at the cost of penalizing nearby or overlapping answers thatare sometimes equally accurate .",4,0,29
dataset/preprocessed/training-data/natural_language_inference/4,We propose a mixed objective that combines cross entropy loss with self - critical policy learning .,5,0,17
dataset/preprocessed/training-data/natural_language_inference/4,The objective uses rewards derived from word overlap to solve the mis alignment between evaluation metric and optimization objective .,6,0,20
dataset/preprocessed/training-data/natural_language_inference/4,"In addition to the mixed objective , we improve dynamic coattention networks ( DCN ) with a deep residual coattention encoder that is inspired by recent work in deep self - attention and residual networks .",7,0,36
dataset/preprocessed/training-data/natural_language_inference/4,"Our proposals improve model performance across question types and input lengths , especially for long questions that requires the ability to capture long - term dependencies .",8,0,27
dataset/preprocessed/training-data/natural_language_inference/4,"On the Stanford Question Answering Dataset , our model achieves state - of - the - art results with 75.1 % exact match accuracy and 83.1 % F1 , while the ensemble obtains 78.9 % exact match accuracy and 86.0 % F1 .",9,0,43
dataset/preprocessed/training-data/natural_language_inference/4,Existing state - of - the - art question answering models are trained to produce exact answer spans for a question and a document .,11,0,25
dataset/preprocessed/training-data/natural_language_inference/4,"In this setting , aground truth answer used to supervise the model is defined as a start and an end position within the document .",12,0,25
dataset/preprocessed/training-data/natural_language_inference/4,Existing training approaches optimize using cross entropy loss over the two positions .,13,0,13
dataset/preprocessed/training-data/natural_language_inference/4,"However , this suffers from a fundamental disconnect between the optimization , which is tied to the position of a particular ground truth answer span , and the evaluation , which is based on the textual content of the answer .",14,0,41
dataset/preprocessed/training-data/natural_language_inference/4,"This disconnect is especially harmful in cases where answers thatare textually similar to , but distinct in positions from , the ground truth are penalized in the same fashion as answers thatare textually dissimilar .",15,0,35
dataset/preprocessed/training-data/natural_language_inference/4,"For example , suppose we are given the sentence "" Some believe that the Golden State Warriors team of 2017 is one of the greatest teams in NBA history "" , the question "" which team is considered to be one of the greatest teams in NBA history "" , and aground truth answer of "" the Golden State Warriors team of 2017 "" .",16,0,65
dataset/preprocessed/training-data/natural_language_inference/4,"The span "" Warriors "" is also a correct answer , but from the perspective of traditional cross entropy based training it is no better than the span "" history "" .",17,0,32
dataset/preprocessed/training-data/natural_language_inference/4,"To address this problem , we propose a mixed objective that combines traditional cross entropy loss over positions with a measure of word overlap trained with reinforcement learning .",18,0,29
dataset/preprocessed/training-data/natural_language_inference/4,We obtain the latter objective using self - critical policy learning in which the reward is based on word overlap between the proposed answer and the ground truth answer .,19,0,30
dataset/preprocessed/training-data/natural_language_inference/4,Our mixed objective brings two benefits : ( i ) the reinforcement learning objective encourages answers thatare textually similar to the ground truth answer and discourages those thatare not ; ( ii ) the cross entropy objective significantly facilitates policy learning by encouraging trajectories thatare known to be correct .,20,0,50
dataset/preprocessed/training-data/natural_language_inference/4,The resulting objective is one that is both faithful to the evaluation metric and converges quickly in practice .,21,0,19
dataset/preprocessed/training-data/natural_language_inference/4,"In addition to our mixed training objective , we extend the Dynamic Coattention Network ( DCN ) by with a deep residual coattention encoder .",22,0,25
dataset/preprocessed/training-data/natural_language_inference/4,This allows the network to build richer representations of the input by enabling each input sequence to attend to previous attention contexts .,23,0,23
dataset/preprocessed/training-data/natural_language_inference/4,show that the stacking of attention layers helps model long - range BiLSTM1 BiLSTM1,24,0,14
dataset/preprocessed/training-data/natural_language_inference/4,Output BiLSTM dependencies .,27,0,4
dataset/preprocessed/training-data/natural_language_inference/4,We merge coattention outputs from each layer by means of residual connections to reduce the length of signal paths .,28,0,20
dataset/preprocessed/training-data/natural_language_inference/4,show that skip layer connections facilitate signal propagation and alleviate gradient degradation .,29,0,13
dataset/preprocessed/training-data/natural_language_inference/4,"The combination of the deep residual coattention encoder and the mixed objective leads to higher performance across question types , question lengths , and answer lengths on the Stanford Question Answering Dataset ( SQuAD ) compared to our DCN baseline .",30,0,41
dataset/preprocessed/training-data/natural_language_inference/4,"The improvement is especially apparent on long questions , which require the model to capture long - range dependencies between the document and the question .",31,0,26
dataset/preprocessed/training-data/natural_language_inference/4,"Our model , which we call DCN + , achieves state - of - the - art results on SQuAD , with 75.1 % exact match accuracy and 83.1 % F1 .",32,0,32
dataset/preprocessed/training-data/natural_language_inference/4,"When ensembled , the DCN + obtains 78.9 % exact match accuracy and 86.0 % F1 .",33,0,17
dataset/preprocessed/training-data/natural_language_inference/4,"We consider the question answering task in which we are given a document and a question , and are asked to find the answer in the document .",35,0,28
dataset/preprocessed/training-data/natural_language_inference/4,"Our model is based on the DCN by , which consists of a coattention encoder and a dynamic decoder .",36,0,20
dataset/preprocessed/training-data/natural_language_inference/4,"The encoder first encodes the question and the document separately , then builds a codependent representation through coattention .",37,0,19
dataset/preprocessed/training-data/natural_language_inference/4,The decoder then produces a start and endpoint estimate given the coattention .,38,0,13
dataset/preprocessed/training-data/natural_language_inference/4,"The DCN decoder is dynamic in the sense that it iteratively estimates the start and end positions , stopping when estimates between iterations converge to the same positions or when a predefined maximum number of iterations is reached .",39,0,39
dataset/preprocessed/training-data/natural_language_inference/4,We make two significant changes to the DCN by introducing a deep residual coattention encoder and a mixed training objective that combines cross entropy loss from maximum likelihood estimation and reinforcement learning rewards from self - critical policy learning .,40,0,40
dataset/preprocessed/training-data/natural_language_inference/4,DEEP RESIDUAL COATTENTION ENCODER,41,0,4
dataset/preprocessed/training-data/natural_language_inference/4,"Because it only has a single - layer coattention encoder , the DCN is limited in its ability to compose complex input representations .",42,0,24
dataset/preprocessed/training-data/natural_language_inference/4,proposed stacked self - attention modules to facilitate signal traversal .,43,0,11
dataset/preprocessed/training-data/natural_language_inference/4,They also showed that the network 's ability to model long - range dependencies can be improved by reducing the length of signal paths .,44,0,25
dataset/preprocessed/training-data/natural_language_inference/4,We propose two modifications to the coattention encoder to leverage these findings .,45,0,13
dataset/preprocessed/training-data/natural_language_inference/4,"First , we extend the coattention encoder with self - attention by stacking coattention layers .",46,0,16
dataset/preprocessed/training-data/natural_language_inference/4,This allows the network to build richer representations over the input .,47,0,12
dataset/preprocessed/training-data/natural_language_inference/4,"Second , we merge coattention outputs from each layer with residual connections .",48,0,13
dataset/preprocessed/training-data/natural_language_inference/4,This reduces the length of signal paths .,49,0,8
dataset/preprocessed/training-data/natural_language_inference/13,Multi-range Reasoning for Machine Comprehension,2,1,5
dataset/preprocessed/training-data/natural_language_inference/13,"We propose MRU ( Multi - Range Reasoning Units ) , a new fast compositional encoder for machine comprehension ( MC ) .",4,1,23
dataset/preprocessed/training-data/natural_language_inference/13,"Our proposed MRU encoders are characterized by multi-ranged gating , executing a series of parameterized contractand - expand layers for learning gating vectors that benefit from long and short - term dependencies .",5,0,33
dataset/preprocessed/training-data/natural_language_inference/13,"The aims of our approach are as follows : ( 1 ) learning representations thatare concurrently aware of long and short - term context , ( 2 ) modeling relationships between intra-document blocks and ( 3 ) fast and efficient sequence encoding .",6,0,43
dataset/preprocessed/training-data/natural_language_inference/13,We show that our proposed encoder demonstrates promising results both as a standalone encoder and as well as a complementary building block .,7,0,23
dataset/preprocessed/training-data/natural_language_inference/13,"We conduct extensive experiments on three challenging MC datasets , namely RACE , Search QA and Narrative QA , achieving highly competitive performance on all .",8,0,26
dataset/preprocessed/training-data/natural_language_inference/13,"On the RACE benchmark , our model outperforms DFN ( Dynamic Fusion Networks ) by 1.5 % ?",9,0,18
dataset/preprocessed/training-data/natural_language_inference/13,6 % without using any recurrent or convolution layers .,10,0,10
dataset/preprocessed/training-data/natural_language_inference/13,"Similarly , we achieve competitive performance relative to AMANDA [ 17 ] on the SearchQA benchmark and BiDAF [ 23 ] on the Narrative QA benchmark without using any LSTM / GRU layers .",11,0,34
dataset/preprocessed/training-data/natural_language_inference/13,"Finally , incorporating MRU encoders with standard BiLSTM architectures further improves performance , achieving state - of - the - art results .",12,0,23
dataset/preprocessed/training-data/natural_language_inference/13,"Teaching machines to read , comprehend and reason lives at the heart of machine comprehension ( MC ) tasks .",14,0,20
dataset/preprocessed/training-data/natural_language_inference/13,"In these tasks , the goal is to answer questions based on a given passage , effectively testing the learner 's capability to understand natural language .",15,0,27
dataset/preprocessed/training-data/natural_language_inference/13,"This has been an extremely productive are a of research in the recent years , giving rise to many highly advanced neural network architectures .",16,0,25
dataset/preprocessed/training-data/natural_language_inference/13,"A common denominator in many of these models is the compositional encoder , i.e. , usually a bidirectional recurrent - based ( LSTM or GRU ) encoder that sequentially parses the text sequence word - by - word .",17,0,39
dataset/preprocessed/training-data/natural_language_inference/13,"This helps to model compositionality of words , capturing rich and complex linguistic and syntactic structure in language .",18,0,19
dataset/preprocessed/training-data/natural_language_inference/13,"While the usage of recurrent encoder is often regarded as indispensable in highly complex MC tasks , there are still several challenges and problems pertaining to it 's usage in modern MC tasks .",19,1,34
dataset/preprocessed/training-data/natural_language_inference/13,"Firstly , documents can be extremely long to the point where running a BiRNN model across along document is computationally prohibitive .",20,0,22
dataset/preprocessed/training-data/natural_language_inference/13,This is aggravated since MC tasks can be easily extended to reasoning over multiple long documents .,21,0,17
dataset/preprocessed/training-data/natural_language_inference/13,"Secondly , recurrent encoders have limited access to long term context since each word is sequentially parsed .",22,0,18
dataset/preprocessed/training-data/natural_language_inference/13,This restricts any form of multi-sentence and intra-document reasoning from happening within compositional encoder layer .,23,0,16
dataset/preprocessed/training-data/natural_language_inference/13,"To this end , we propose a new compositional encoder that can either be used in - place of standard RNN encoders or serve as a new module that is complementary to existing neural architectures .",24,0,36
dataset/preprocessed/training-data/natural_language_inference/13,Our proposed MRU encoders learns gating vectors via multiple contract - and - expand layers at multiple dilated resolutions .,25,0,20
dataset/preprocessed/training-data/natural_language_inference/13,"Specifically , we compress the input document an arbitrary k times at multi-ranges ( e.g. , 1 , 2 , 4 , 10 , 25 ) into a neural bag - of - words ( summed ) representation .",26,0,39
dataset/preprocessed/training-data/natural_language_inference/13,The compact sequence is then passed through affine transformation layers and then re-expanded to the original sequence length .,27,0,19
dataset/preprocessed/training-data/natural_language_inference/13,The k document representations ( at multiple ranges and n-gram blocks ) are then combined and modeled with fully connected layers to form the final compositional gate which are applied onto the original input document .,28,0,36
dataset/preprocessed/training-data/natural_language_inference/13,"This can be interpreted as compositional gating by exploiting information at multiple - ranges , modeling relationships across different granularities and hierarchies .",29,0,23
dataset/preprocessed/training-data/natural_language_inference/13,"Intuitively , this is because 1 - gram blocks are compared with 2 - gram blocks and 10 - gram blocks and soon .",30,0,24
dataset/preprocessed/training-data/natural_language_inference/13,This has several advantages .,31,0,5
dataset/preprocessed/training-data/natural_language_inference/13,"Firstly , we enable a major speedup by avoiding either costly step - by - step gate construction while still maintaining interactions between neighboring words .",32,0,26
dataset/preprocessed/training-data/natural_language_inference/13,"As such , our model belongs to a class of architectures which is inspired by QRNNs and SRUs .",33,0,19
dataset/preprocessed/training-data/natural_language_inference/13,The key difference is that our gates are not constructed by convolution layers but explicit block - based matching across multiple ranges .,34,0,23
dataset/preprocessed/training-data/natural_language_inference/13,"Secondly , modeling at along range ( e.g. , 25 or 50 ) enables our model to look further ahead as opposed to only one step forward .",35,0,28
dataset/preprocessed/training-data/natural_language_inference/13,"As such , the learned gastes possess not only information about nearby words but also a larger overview of the context .",36,0,22
dataset/preprocessed/training-data/natural_language_inference/13,"This is in similar spirit to self - attention , albeit executing within the encoder .",37,0,16
dataset/preprocessed/training-data/natural_language_inference/13,"Thirdly , the final gates are formed by modeling relationships between multi-range projections ( n - gram blocks ) , allowing for fine - grained intra-document relationships to be captured .",38,0,31
dataset/preprocessed/training-data/natural_language_inference/13,The over all contributions of our work is as follows :,39,0,11
dataset/preprocessed/training-data/natural_language_inference/13,"We propose MRU ( Multi-range Reasoning Units ) , a new compositional encoder which construct gates from a novel contract - and - expand operation .",40,0,26
dataset/preprocessed/training-data/natural_language_inference/13,We propose an over all architecture that utilizes MRU within a bi-attentive framework for both multiple choice and span prediction MC tasks .,41,0,23
dataset/preprocessed/training-data/natural_language_inference/13,"MRU can be used as a standalone ( without RNNs ) for fast reading and / or together with RNN models ( i.e. , MRU - LSTM ) for more expressive reading .",42,0,33
dataset/preprocessed/training-data/natural_language_inference/13,"We conduct extensive experiments on three large - scale and challenging machine comprehension datasets - RACE , Search QA and Narrative QA .",43,0,23
dataset/preprocessed/training-data/natural_language_inference/13,"Our model is lightweight , fast and efficient , achieving state - of - the - art or highly competitive performance on all benchmarked datasets .",44,0,26
dataset/preprocessed/training-data/natural_language_inference/13,"Since MC datasets often require a considerable amount of reasoning and natural language understanding , we believe that they serve as good testbeds for benchmarking encoders .",45,0,27
dataset/preprocessed/training-data/natural_language_inference/13,"On RACE , our model outperforms Dynamic Fusion Networks ( DFN ) , a highly complex model .",46,0,18
dataset/preprocessed/training-data/natural_language_inference/13,Our Proposed MRU Encoder,47,0,4
dataset/preprocessed/training-data/natural_language_inference/13,"In this section , we describe our proposed MRU encoder .",48,0,11
dataset/preprocessed/training-data/natural_language_inference/13,"The inputs to the MRU encoder is an input document {w 1 , w 2 w } , and list of ranges {r 1 , r 2 r k } where k is the number of times the contract and expand operation is executed .",49,0,45
dataset/preprocessed/training-data/natural_language_inference/20,Sentence Embeddings in NLI with Iterative Refinement Encoders,2,1,8
dataset/preprocessed/training-data/natural_language_inference/20,Sentence - level representations are necessary for various NLP tasks .,4,1,11
dataset/preprocessed/training-data/natural_language_inference/20,Recurrent neural networks have proven to be very effective in learning distributed representations and can be trained efficiently on natural language inference tasks .,5,0,24
dataset/preprocessed/training-data/natural_language_inference/20,We build on top of one such model and propose a hierarchy of BiLSTM and max pooling layers that implements an iterative refinement strategy and yields state of the art results on the SciTail dataset as well as strong results for SNLI and MultiNLI .,6,0,45
dataset/preprocessed/training-data/natural_language_inference/20,"We can show that the sentence embeddings learned in this way can be utilized in a wide variety of transfer learning tasks , outperforming InferSent on 7 out of 10 and SkipThought on 8 out of 9 SentEval sentence embedding evaluation tasks .",7,0,43
dataset/preprocessed/training-data/natural_language_inference/20,"Furthermore , our model beats the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings ' ability to capture some of the important linguistic properties of sentences .",8,0,36
dataset/preprocessed/training-data/natural_language_inference/20,Neural networks have been shown to provide a powerful tool for building representations of natural languages on multiple levels of linguistic abstraction .,10,0,23
dataset/preprocessed/training-data/natural_language_inference/20,Perhaps the most widely used representations in natural language processing are word embeddings .,11,0,14
dataset/preprocessed/training-data/natural_language_inference/20,Recently there has been a growing interest in models for sentencelevel representations using a range of different neural network architectures .,12,0,21
dataset/preprocessed/training-data/natural_language_inference/20,"Such sentence embeddings have been generated using unsupervised learning approaches , and supervised learning .",13,0,15
dataset/preprocessed/training-data/natural_language_inference/20,Supervision typically comes in the form of an underlying semantic task with labeled data to train the model .,14,0,19
dataset/preprocessed/training-data/natural_language_inference/20,The most prominent task for that purpose is natural language inference ( NLI ) that tries to model the inferential relationship between two or more given sentences .,15,0,28
dataset/preprocessed/training-data/natural_language_inference/20,"In particular , given two sentences - the premise p and the hypothesis h - the task is to determine whether h is entailed by p , whether the sentences are in contradiction with each other or whether there is no inferential relationship between the sentences ( neutral ) .",16,0,50
dataset/preprocessed/training-data/natural_language_inference/20,There are two main neural approaches ar Xiv : 1808.08762v2 [ cs. CL ] 3 Jun 2019 to NLI .,17,0,20
dataset/preprocessed/training-data/natural_language_inference/20,Sentence encoding - based models focus on building separate embeddings for the premises and the hypothesis and then combine those using a classifier .,18,0,24
dataset/preprocessed/training-data/natural_language_inference/20,Other approaches do not treat the two sentences separately but utilize e.g. cross - sentence attention .,19,0,17
dataset/preprocessed/training-data/natural_language_inference/20,"With the goal of obtaining general - purpose sentence representations in mind , we opt for the sentence encoding approach .",20,0,21
dataset/preprocessed/training-data/natural_language_inference/20,Motivated by the success of the InferSent architecture we extend their architecture with a hierarchylike structure of bidirectional LSTM ( BiLSTM ) layers with max pooling .,21,0,27
dataset/preprocessed/training-data/natural_language_inference/20,"All in all , our model improves the previous state of the art for SciTail and achieves strong results for the SNLI and Multi - Genre Natural Language Inference corpus ( MultiNLI ; .",22,0,34
dataset/preprocessed/training-data/natural_language_inference/20,"In order to demonstrate the semantic abstractions achieved by our approach , we also apply our model to a number of transfer learning tasks using the SentEval testing library , and show that it outperforms the InferSent model on 7 out of 10 and SkipThought on 8 out of 9 tasks , comparing to the scores reported by .",23,0,59
dataset/preprocessed/training-data/natural_language_inference/20,"Moreover , our model outperforms the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings ' ability to capture some of the important linguistic properties of sentences .",24,0,36
dataset/preprocessed/training-data/natural_language_inference/20,"This highlights the generalization capability of the proposed model , confirming that its architecture is able to learn sentence representations with strong performance across a wide variety of different NLP tasks .",25,0,32
dataset/preprocessed/training-data/natural_language_inference/20,There is a wide variety of approaches to sentence - level representations that can be used in natural language inference .,27,0,21
dataset/preprocessed/training-data/natural_language_inference/20,"and explore RNN and LSTM architectures , convolutional neural networks and GRUs , to name a few .",28,0,18
dataset/preprocessed/training-data/natural_language_inference/20,The basic idea behind these approaches is to encode the premise and hypothesis sentences separately and then combine those using a neural network classifier .,29,0,25
dataset/preprocessed/training-data/natural_language_inference/20,"explore multiple different sentence embedding architectures ranging from LSTM , BiLSTM and intra-attention to convolution neural networks and the performance of these architectures on NLI tasks .",30,0,27
dataset/preprocessed/training-data/natural_language_inference/20,"They show that , out of these models , BiLSTM with max pooling achieves the strongest results not only in NLI but also in many other NLP tasks requiring sentence level meaning representations .",31,0,34
dataset/preprocessed/training-data/natural_language_inference/20,They also show that their model trained on NLI data achieves strong performance on various transfer learning tasks .,32,0,19
dataset/preprocessed/training-data/natural_language_inference/20,"Although sentence embedding approaches have proven their effectiveness in NLI , there are multiple studies showing that treating the hypothesis and premise sentences together and focusing on the relationship between those sentences yields better results .",33,0,36
dataset/preprocessed/training-data/natural_language_inference/20,These methods are focused on the inference relations rather than the internal semantics of the sentences .,34,0,17
dataset/preprocessed/training-data/natural_language_inference/20,"Therefore , they do not offer similar insights about the sentence level semantics , as individual sentence embeddings do , and they can not straightforwardly be used outside of the NLI context .",35,0,33
dataset/preprocessed/training-data/natural_language_inference/20,Our proposed architecture follows a sentence embedding - based approach for NLI introduced by .,37,0,15
dataset/preprocessed/training-data/natural_language_inference/20,"The model illustrated in contains sentence embeddings for the two input sentences , where the output of the sentence embeddings are combined using a heuristic introduced by , putting together the concatenation ( u , v ) , absolute element - wise difference |u ? v| , and element - wise product u * v.",38,0,55
dataset/preprocessed/training-data/natural_language_inference/20,The combined vector is then passed onto a 3 layered multi - layer perceptron ( MLP ) with a 3 - way softmax classifier .,39,0,25
dataset/preprocessed/training-data/natural_language_inference/20,The first two layers of the MLP both utilize dropout and a ReLU activation function .,40,0,16
dataset/preprocessed/training-data/natural_language_inference/20,"We use a variant of ReLU called Leaky ReLU , defined by :",41,0,13
dataset/preprocessed/training-data/natural_language_inference/20,where we set y = 0.01 as the negative slope for x < 0 . This prevents the gradient from dying when x < 0 .,42,0,26
dataset/preprocessed/training-data/natural_language_inference/20,1 . Overall NLI Architecture,44,0,5
dataset/preprocessed/training-data/natural_language_inference/20,For the sentence representations we first embed the individual words with pretrained word embeddings .,45,0,15
dataset/preprocessed/training-data/natural_language_inference/20,The sequence of the embedded words is then passed onto the sentence encoder which utilizes BiLSTM with max pooling .,46,0,20
dataset/preprocessed/training-data/natural_language_inference/20,"Given a sequence T of words ( w 1 . . . , w T ) , the output of the bi-directional LSTM is a set of vectors ( h 1 , . . . , h T ) , where each ht ? ( h 1 , . . . , h T ) is the concatenation",47,0,58
dataset/preprocessed/training-data/natural_language_inference/20,of a forward and backward LSTMs,48,0,6
dataset/preprocessed/training-data/natural_language_inference/20,"The max pooling layer produces a vector of the same dimensionality ash t , returning , for each dimension , its maximum value over the hidden units ( h 1 , . . . , h T ) .",49,0,39
dataset/preprocessed/training-data/topic_models/0,Learning document embeddings along with their uncertainties,2,1,7
dataset/preprocessed/training-data/topic_models/0,Majority of the text modelling techniques yield only point - estimates of document embeddings and lack in capturing the uncertainty of the estimates .,4,0,24
dataset/preprocessed/training-data/topic_models/0,These uncertainties give a notion of how well the embeddings represent a document .,5,0,14
dataset/preprocessed/training-data/topic_models/0,"We present Bayesian subspace multinomial model ( Bayesian SMM ) , a generative log - linear model that learns to represent documents in the form of Gaussian distributions , thereby encoding the uncertainty in its covariance .",6,0,37
dataset/preprocessed/training-data/topic_models/0,"Additionally , in the proposed Bayesian SMM , we address a commonly encountered problem of intractability that appears during variational inference in mixed - logit models .",7,0,27
dataset/preprocessed/training-data/topic_models/0,We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .,8,1,19
dataset/preprocessed/training-data/topic_models/0,Our intrinsic evaluation using perplexity measure shows that the proposed Bayesian SMM fits the data better as compared to the state - of - the - art neural variational document model on ( Fisher ) speech and ( 20 Newsgroups ) text corpora .,9,0,44
dataset/preprocessed/training-data/topic_models/0,Our topic identification experiments show that the proposed systems are robust to over-fitting on unseen test data .,10,0,18
dataset/preprocessed/training-data/topic_models/0,The topic ID results show that the proposed model is outperforms state - of - the - art unsupervised topic models and achieve comparable results to the state - of - the - art fully supervised discriminative models .,11,0,39
dataset/preprocessed/training-data/topic_models/0,"Learning document embeddings along with their uncertainties Santosh Kesiraju , Old?ich Plchot , Luk Burget , and Suryakanth V Gangashetty",13,0,20
dataset/preprocessed/training-data/topic_models/0,Abstract - Majority of the text modelling techniques yield only point - estimates of document embeddings and lack in capturing the uncertainty of the estimates .,14,0,26
dataset/preprocessed/training-data/topic_models/0,These uncertainties give a notion of how well the embeddings represent a document .,15,0,14
dataset/preprocessed/training-data/topic_models/0,"We present Bayesian subspace multinomial model ( Bayesian SMM ) , a generative log - linear model that learns to represent documents in the form of Gaussian distributions , thereby encoding the uncertainty in its covariance .",16,0,37
dataset/preprocessed/training-data/topic_models/0,"Additionally , in the proposed Bayesian SMM , we address a commonly encountered problem of intractability that appears during variational inference in mixed - logit models .",17,0,27
dataset/preprocessed/training-data/topic_models/0,We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .,18,1,19
dataset/preprocessed/training-data/topic_models/0,Our intrinsic evaluation using perplexity measure shows that the proposed Bayesian SMM fits the data better as compared to the state - of - the - art neural variational document model on ( Fisher ) speech and ( 20 Newsgroups ) text corpora .,19,0,44
dataset/preprocessed/training-data/topic_models/0,Our topic identification experiments show that the proposed systems are robust to over-fitting on unseen test data .,20,0,18
dataset/preprocessed/training-data/topic_models/0,The topic ID results show that the proposed model is outperforms state - of - the - art unsupervised topic models and achieve comparable results to the state - of - the - art fully supervised discriminative models .,21,0,39
dataset/preprocessed/training-data/topic_models/0,"Index Terms - Bayesian methods , embeddings , topic identification",22,0,10
dataset/preprocessed/training-data/topic_models/0,"L EARNING word and document embeddings have proven to be useful in wide range of information retrieval , speech and natural language processing applications -.",24,1,25
dataset/preprocessed/training-data/topic_models/0,These embeddings elicit the latent semantic relations present among the co-occurring words in a sentence or bag - of - words from a document .,25,0,25
dataset/preprocessed/training-data/topic_models/0,"Majority of the techniques for learning these embeddings are based on two complementary ideologies , ( i ) topic modelling , and ( ii ) word prediction .",26,0,28
dataset/preprocessed/training-data/topic_models/0,The former methods are primarily built on top of bag - of - words model and tend to capture higher level semantics such as topics .,27,0,26
dataset/preprocessed/training-data/topic_models/0,The latter techniques capture lower level semantics by exploiting the contextual information of words in a sequence -.,28,0,18
dataset/preprocessed/training-data/topic_models/0,"On the other hand , there is a growing interest towards developing pre-trained language models , , thatare then finetuned for specific tasks such as document classification , question answering , named entity recognition , etc .",29,0,37
dataset/preprocessed/training-data/topic_models/0,Although these models achieve state - of - the - art results in several NLP tasks ; they require enormous computational resources to train .,30,0,25
dataset/preprocessed/training-data/topic_models/0,Latent variable models are a popular choice in unsupervised learning ; where the observed data is assumed to be S. generated through the latent variables according to a stochastic process .,31,0,31
dataset/preprocessed/training-data/topic_models/0,"The goal is then to estimate the model parameters , and also the latent variables .",32,0,16
dataset/preprocessed/training-data/topic_models/0,"In probabilistic topic models ( PTMs ) the latent variables are attributed to topics , and the generative process assumes that every topic is a sample from a distribution over words in the vocabulary and documents are generated from the distribution of ( latent ) topics .",33,0,47
dataset/preprocessed/training-data/topic_models/0,"Recent works showed that auto - encoders can also be seen as generative models for images and text , .",34,0,20
dataset/preprocessed/training-data/topic_models/0,"Generative models allows us to incorporate prior information about the latent variables , and with the help of variational Bayes ( VB ) techniques , , , one can infer posterior distribution over the latent variables instead of just point - estimates .",35,0,43
dataset/preprocessed/training-data/topic_models/0,The posterior distribution captures uncertainty of the latent variable estimates while trying to explain ( fit ) the observed data and our prior belief .,36,0,25
dataset/preprocessed/training-data/topic_models/0,"In the context of text modelling , these latent variables are seen as embeddings .",37,0,15
dataset/preprocessed/training-data/topic_models/0,"In this paper , we present Bayesian subspace multinomial model ( Bayesian SMM ) as a generative model for bag - ofwords representation of documents .",38,0,26
dataset/preprocessed/training-data/topic_models/0,"We show that our model can learn to represent each document in the form of a Gaussian distribution , thereby encoding the uncertainty in its covariance .",39,0,27
dataset/preprocessed/training-data/topic_models/0,"Further , we propose a generative Gaussian classifier that exploits this uncertainty for topic identification ( ID ) .",40,0,19
dataset/preprocessed/training-data/topic_models/0,"The proposed VB framework can be extended in a straightforward way for subspace n-gram model , that can model n-gram distribution of words in sentences .",41,0,26
dataset/preprocessed/training-data/topic_models/0,"Earlier , ( non-Bayesian ) SMM was used for learning document embeddings in an unsupervised fashion .",42,0,17
dataset/preprocessed/training-data/topic_models/0,"They were then used for training linear classifiers for topic ID from spoken and textual documents , .",43,0,18
dataset/preprocessed/training-data/topic_models/0,"However , one of the limitations was that the learned document embeddings ( also termed as document i-vectors ) were only point - estimates and were prone to over-fitting , especially for shorter documents .",44,0,35
dataset/preprocessed/training-data/topic_models/0,Our proposed model can overcome this problem by capturing the uncertainty of the embeddings in the form of posterior distributions .,45,0,21
dataset/preprocessed/training-data/topic_models/0,"Given the significant prior research in PTMs and related algorithms for learning representations , it is important to draw precise relations between the presented model and former works .",46,0,29
dataset/preprocessed/training-data/topic_models/0,"We do this from the following viewpoints : ( a ) Graphical models illustrating the dependency of random and observed variables , ( b ) assumptions of distributions over random variables and their limitations , and ( c ) approximations made during the inference and their consequences .",47,0,48
dataset/preprocessed/training-data/topic_models/0,"The contributions of this paper are as follows : ( a ) we present Bayesian subspace multinomial model and analyse its relation to popular models such as latent Dirichlet allocation ( LDA ) , correlated topic model ( CTM ) , paragraph vector ( PV - DBOW ) and neural variational document model ( NVDM ) , ( b ) we adapt tricks from for faster and efficient variational inference of the proposed model , ( c ) we combine optimization techniques from , and use them to train the proposed model , ( d ) we propose a generative Gaussian classifier that exploits uncertainty in the posterior distribution of document embeddings , ( e ) we provide experimental results on both text and speech data showing that the proposed document representations achieve state - of - theart perplexity scores , and ( f ) with our proposed classification systems , we illustrate robustness of the model to over-fitting and at the same time obtain superior classification results when compared systems based on state - of - the - art unsupervised models .",48,0,183
dataset/preprocessed/training-data/topic_models/0,"We begin with the description of Bayesian SMM in Section II , followed by VB for the model in Section III .",49,0,22
dataset/preprocessed/training-data/text_summarization/7,Cutting - off Redundant Repeating Generations for Neural Abstractive Summarization,2,1,10
dataset/preprocessed/training-data/text_summarization/7,This paper tackles the reduction of redundant repeating generation that is often observed in RNN - based encoder - decoder models .,4,0,22
dataset/preprocessed/training-data/text_summarization/7,Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder .,5,0,30
dataset/preprocessed/training-data/text_summarization/7,Our method shows significant improvement over a strong RNN - based encoder - decoder baseline and achieved its best results on an abstractive summarization benchmark .,6,0,26
dataset/preprocessed/training-data/text_summarization/7,"The RNN - based encoder - decoder ( EncDec ) approach has recently been providing significant progress in various natural language generation ( NLG ) tasks , i.e. , machine translation ( MT ) and abstractive summarization ( ABS ) .",8,1,41
dataset/preprocessed/training-data/text_summarization/7,"Since a scheme in this approach can be interpreted as a conditional language model , it is suitable for NLG tasks .",9,0,22
dataset/preprocessed/training-data/text_summarization/7,"However , one potential weakness is that it sometimes repeatedly generates the same phrase ( or word ) .",10,0,19
dataset/preprocessed/training-data/text_summarization/7,This issue has been discussed in the neural MT ( NMT ) literature as apart of a coverage problem .,11,0,20
dataset/preprocessed/training-data/text_summarization/7,Such repeating generation behavior can become more severe in some NLG tasks than in MT .,12,0,16
dataset/preprocessed/training-data/text_summarization/7,"The very short ABS task in is a typical example because it requires the generation of a summary in a pre-defined limited output space , such as ten words or 75 bytes .",13,0,33
dataset/preprocessed/training-data/text_summarization/7,"Thus , the repeated output consumes precious limited output space .",14,0,11
dataset/preprocessed/training-data/text_summarization/7,"Unfortunately , the coverage approach can not be directly applied to ABS tasks since they require us to optimally find salient ideas from the input in a lossy compression manner , and thus the summary ( output ) length hardly depends on the input length ; an MT task is mainly loss - less generation and nearly one - to - one correspondence between input and output .",15,0,68
dataset/preprocessed/training-data/text_summarization/7,"From this background , this paper tackles this issue and proposes a method to overcome it in ABS tasks .",16,0,20
dataset/preprocessed/training-data/text_summarization/7,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step .,17,0,41
dataset/preprocessed/training-data/text_summarization/7,We refer to our additional component as a wordfrequency estimation ( WFE ) sub-model .,18,0,15
dataset/preprocessed/training-data/text_summarization/7,The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process .,19,0,27
dataset/preprocessed/training-data/text_summarization/7,"Thus , we expect to decisively prohibit excessive generation .",20,0,10
dataset/preprocessed/training-data/text_summarization/7,"Finally , we evaluate the effectiveness of our method on well - studied ABS benchmark data provided by , and evaluated in .",21,0,23
dataset/preprocessed/training-data/text_summarization/7,Baseline RNN - based EncDec Model,22,0,6
dataset/preprocessed/training-data/text_summarization/7,The baseline of our proposal is an RNN - based EncDec model with an attention mechanism .,23,0,17
dataset/preprocessed/training-data/text_summarization/7,"In fact , this model has already been used as a strong baseline for ABS tasks as well as in the NMT literature .",24,0,24
dataset/preprocessed/training-data/text_summarization/7,"More specifically , as a case study we employ a 2 - layer bidirectional LSTM encoder and a 2 - layer LSTM decoder with a global attention .",25,0,28
dataset/preprocessed/training-data/text_summarization/7,We omit a detailed review of the descriptions due to space limitations .,26,0,13
dataset/preprocessed/training-data/text_summarization/7,The following are the necessary parts for explaining our proposed method .,27,0,12
dataset/preprocessed/training-data/text_summarization/7,"Let X = ( x i ) I i=1 and Y = ( y j ) J j= 1 be input and output sequences , respectively , where xi and y j are one - hot vectors , which correspond to the i - th word in the input and the j - th word in the output .",28,0,59
dataset/preprocessed/training-data/text_summarization/7,Vt denote the vocabulary ( set of words ) of output .,30,0,12
dataset/preprocessed/training-data/text_summarization/7,"For simplification , this paper uses the following four notation rules : ( 1 ) ( x i ) I i= 1 is a short notation for representing a list of ( column ) vectors , i.e. , ( x 1 , . . . ,",31,0,46
dataset/preprocessed/training-data/text_summarization/7,"( 2 ) v ( a , D ) represents a D-dimensional ( column ) vector whose elements are all a , i.e. , v ( 1 , 3 ) = ( 1 , 1 , 1 ) .",32,0,39
dataset/preprocessed/training-data/text_summarization/7,Encoder : Let ? s ( ) denote the over all process of our 2 - layer bidirectional LSTM encoder .,33,0,21
dataset/preprocessed/training-data/text_summarization/7,The encoder receives input X and returns a list of final hidden states H s = ( h s i ) I i = 1 :,34,0,26
dataset/preprocessed/training-data/text_summarization/7,We employ a K - best beam - search decoder to find the ( approximated ) best output ?,36,0,19
dataset/preprocessed/training-data/text_summarization/7,given input X. shows a typical Kbest beam search algorithm used in the decoder of EncDec approach .,37,0,18
dataset/preprocessed/training-data/text_summarization/7,"We define the ( minimal ) required information h shown in for the jth decoding process is the following triplet , h = ( s j?1 , ? j?1 , H t j?1 ) , where s j ?1 is the cumulative log - likelihood from step 0 to j ? 1 , ?",38,0,54
dataset/preprocessed/training-data/text_summarization/7,"j?1 is a ( candidate of ) output word sequence generated so far from step 0 to j ? 1 , that is , ? j?1 = ( y 0 , . . . , y j?1 ) and H t j ?1 is the all the hidden states for calculating the j - th decoding process .",39,0,58
dataset/preprocessed/training-data/text_summarization/7,"Then , the function calcLL in Line 8 can be written as follows :",40,0,14
dataset/preprocessed/training-data/text_summarization/7,where Softmax ( ) is the softmax function for a given vector and ? t ( ) represents the over all process of a single decoding step .,41,0,28
dataset/preprocessed/training-data/text_summarization/7,", that is calculated using the k - th candidate in Q w at the ( j ? 1 ) - th step .",42,0,24
dataset/preprocessed/training-data/text_summarization/7,"In Line 12 , the function makeTriplet constructs a set of triplets based on the information of index ( m , k ) .",43,0,24
dataset/preprocessed/training-data/text_summarization/7,"Then , in Line 13 , the function select TopK selects the top - K candidates from union of a set of generated triplets at current step {h z } K?C z=1 and a set of triplets of complete sentences in Q c  .",44,0,44
dataset/preprocessed/training-data/text_summarization/7,"Finally , the function sepComp in Line 13 divides a set of triplets Q in two distinct sets whether they are complete sentences , Q c , or not , Q w .",45,0,33
dataset/preprocessed/training-data/text_summarization/7,"If the elements in Q are all complete sentences , namely , Q c = Q and Q w = ? , then the algorithm stops according to the evaluation of Line 15 .",46,0,34
dataset/preprocessed/training-data/text_summarization/7,Word Frequency Estimation,47,0,3
dataset/preprocessed/training-data/text_summarization/7,"This section describes our proposed method , which roughly consists of two parts : ( 1 ) a submodel that estimates the upper-bound frequencies of the target vocabulary words in the output , and ( 2 ) architecture for controlling the output words in the decoder using estimations .",48,0,49
dataset/preprocessed/training-data/text_summarization/3,Concept Pointer Network for Abstractive Summarization,2,1,6
dataset/preprocessed/training-data/text_summarization/3,A quality abstractive summary should not only copy salient source texts as summaries but should also tend to generate new conceptual words to express concrete details .,4,0,27
dataset/preprocessed/training-data/text_summarization/3,"Inspired by the popular pointer generator sequence - tosequence model , this paper presents a concept pointer network for improving these aspects of abstractive summarization .",5,0,26
dataset/preprocessed/training-data/text_summarization/3,"The network leverages knowledge - based , context - aware conceptualizations to derive an extended set of candidate concepts .",6,0,20
dataset/preprocessed/training-data/text_summarization/3,The model then points to the most appropriate choice using both the concept set and original source text .,7,0,19
dataset/preprocessed/training-data/text_summarization/3,This joint approach generates abstractive summaries with higher - level semantic concepts .,8,0,13
dataset/preprocessed/training-data/text_summarization/3,"The training model is also optimized in a way that adapts to different data , which is based on a novel method of distantly - supervised learning guided by reference summaries and testing set .",9,0,35
dataset/preprocessed/training-data/text_summarization/3,"Overall , the proposed approach provides statistically significant improvements over several state - of - the - art models on both the DUC - 2004 and Gigaword datasets .",10,0,29
dataset/preprocessed/training-data/text_summarization/3,A human evaluation of the model 's abstractive abilities also supports the quality of the summaries produced within this framework .,11,0,21
dataset/preprocessed/training-data/text_summarization/3,"* Corresponding author Figure 1 : "" summary1 "" only copies keyword from the source text , while "" summary2 "" generates new concepts to convey the meaning .",12,0,29
dataset/preprocessed/training-data/text_summarization/3,Abstractive summarization ( ABS ) has gained overwhelming success owing to a tremendous development of sequence - to - sequence ( seq2seq ) model and its variants .,14,1,28
dataset/preprocessed/training-data/text_summarization/3,"In tandem with seq2seq models , pointer generator was developed by as a solution to tackle the rare words and out - of - vocabulary ( OOV ) problem associated with generative - based models .",15,0,36
dataset/preprocessed/training-data/text_summarization/3,The idea behind is to use attention as a pointer to determine the probability of generating a word from both a vocab - ulary distribution and the source text .,16,0,30
dataset/preprocessed/training-data/text_summarization/3,"Pointer generator networks have also been extensively accepted by the ABS community due to their efficacy with long document summaries , title summarization , etc .",17,0,26
dataset/preprocessed/training-data/text_summarization/3,"However , the current power of abstractive summarization falls short of their potential .",18,0,14
dataset/preprocessed/training-data/text_summarization/3,"As the example in shows , a seq2seq model with a pointer mechanism ( marked as the direct pointer ) is likely to merely copy parts of the original text to form a summary using keywords and phrases , such as "" 317 athletes "" .",19,0,46
dataset/preprocessed/training-data/text_summarization/3,"Conversely , a more humanlike summary would be based on one 's own understanding of the detail in the words , expressed as higher - level concepts drawn from world knowledge - like using the word "" group "" to replace "" athletes and officials "" .",20,0,47
dataset/preprocessed/training-data/text_summarization/3,"This indicates that a good summary should not simply copy original material , it should also generate new and even abstract concepts that reflect high - level semantics .",21,0,29
dataset/preprocessed/training-data/text_summarization/3,"Therefore , a pointer generator network that solely considers the source material to generate a summary does not adequately satisfy the needs of high - quality abstractive summarization .",22,0,29
dataset/preprocessed/training-data/text_summarization/3,We argue that concepts have a greater ability to express deeper meanings than verbatim words .,23,0,16
dataset/preprocessed/training-data/text_summarization/3,"As such , it is essential to explore the potential of us - ing concepts from world knowledge to assist with abstractive summarization .",24,0,24
dataset/preprocessed/training-data/text_summarization/3,Our developed model not only points to informative source texts but also leverages conceptual words from human knowledge in the summaries it generates .,25,0,24
dataset/preprocessed/training-data/text_summarization/3,"Hence , in this paper , we propose a novel model based on a concept pointer generator that encourages the generation of conceptual and abstract words .",26,0,27
dataset/preprocessed/training-data/text_summarization/3,"As a hidden benefit , the model also alleviates the OOV problems .",27,0,13
dataset/preprocessed/training-data/text_summarization/3,"Our model uses pointer network to capture the salient information from a source text , and then employs another pointer to generalize the detailed words according to their upper level of expressions .",28,0,33
dataset/preprocessed/training-data/text_summarization/3,"Finally , the output is also consistent with language model by the seq2seq generator .",29,0,15
dataset/preprocessed/training-data/text_summarization/3,Unique to our concept pointer is a set of concept candidates particular for a word that is drawn from a huge knowledge base .,30,0,24
dataset/preprocessed/training-data/text_summarization/3,"The set of candidates adheres to a concept distribution , where the probability of each concept being generated is linked to how strongly the candidate represents each word .",31,0,29
dataset/preprocessed/training-data/text_summarization/3,"Moreover , the concept distribution is iteratively updated to better explain the target word given the context of the source material and inherent semantics in the texts .",32,0,28
dataset/preprocessed/training-data/text_summarization/3,"Hence , the learned concept pointer points to the most suitable and expressive concepts or words .",33,0,17
dataset/preprocessed/training-data/text_summarization/3,The optimization function is adaptive so as to cater for different datasets with distantly - supervised training .,34,0,18
dataset/preprocessed/training-data/text_summarization/3,"The network is then optimized end - to - end using reinforcement learning , with the distant - supervision strategy as a complement to further improve the summary .",35,0,29
dataset/preprocessed/training-data/text_summarization/3,"Overall , the contributions of this paper are : 1 ) a novel concept pointer generator network that leverages context - aware conceptualization and a concept pointer , both of which are jointly integrated into the generator to deliver informative and abstract - oriented summaries ; 2 ) a novel distant supervision training strategy that favors model adaptation and generalization , which results in performance that outperforms the wellaccepted evaluation - based reinforcement learning optimization on a test - only dataset in terms of ROUGE metrics ; 3 ) a statistical analysis of quantitative results and human evaluations from comparative experiments with several state - of - the - art models that shows the proposed method provides promising performance .",36,0,120
dataset/preprocessed/training-data/text_summarization/3,"Abstractive summarization supposedly digests and understands the source content and , consequently , the generated summaries are typically a reorganization of the wording that sometimes form new sentences .",38,0,29
dataset/preprocessed/training-data/text_summarization/3,"Historically , abstractive summarization has been performed through rule - based sentence selection , key information extraction ( Genest and Lapalme , 2011 ) , syntactic parsing and soon .",39,0,30
dataset/preprocessed/training-data/text_summarization/3,"However , more recently , seq2seq models with attention have played a more dominant role in generating abstractive summaries .",40,0,20
dataset/preprocessed/training-data/text_summarization/3,Extensions to the seq2seq approach include an intra-decoder attention and coverage vectors to decrease repetition in phrasing .,41,0,18
dataset/preprocessed/training-data/text_summarization/3,Copy mechanism has been integrated into these models to tackle OOV problem .,42,0,13
dataset/preprocessed/training-data/text_summarization/3,went onto propose SeqCopyNet which copies complete sequences from an input sentence to further maintain the readability of the generated summary .,43,0,22
dataset/preprocessed/training-data/text_summarization/3,"Pointer mechanism has drawn much attention in text summarization , because this technique not only provides a potential solution for rare words and OOV but also extends abstractive summarization in a flexible way .",44,0,34
dataset/preprocessed/training-data/text_summarization/3,"Further , pointer generator models can effectively adaptive to both extractor and abstractor networks , and summaries can be generated by incorporating a pointer - generator and multiple relevant tasks , such as question or entailment generation , or multiple source texts .",45,0,43
dataset/preprocessed/training-data/text_summarization/3,"However , work particularly targets the problem of the abstraction is rare .",46,0,13
dataset/preprocessed/training-data/text_summarization/3,"Abstract Meaning Representation ( AMR ) is used to transform a sentence into a concept graph , then merge those similar concept nodes to form a new summary graph .",47,0,30
dataset/preprocessed/training-data/text_summarization/3,Concepts are also incorporated as auxiliary features .,48,0,8
dataset/preprocessed/training-data/text_summarization/3,and define the number of new n-grams as the primary criteria of abstractiveness .,49,0,14
dataset/preprocessed/training-data/text_summarization/10,Soft Layer - Specific Multi - Task Summarization with Entailment and Question Generation,2,1,13
dataset/preprocessed/training-data/text_summarization/10,An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document .,4,0,23
dataset/preprocessed/training-data/text_summarization/10,"We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation , where the former teaches the summarization model how to look for salient questioning - worthy details , and the latter teaches the model how to rewrite a summary which is a directed - logical subset of the input document .",5,1,62
dataset/preprocessed/training-data/text_summarization/10,"We also propose novel multitask architectures with high - level ( semantic ) layer - specific sharing across multiple encoder and decoder layers of the three tasks , as well as soft - sharing mechanisms ( and show performance ablations and analysis examples of each contribution ) .",6,0,48
dataset/preprocessed/training-data/text_summarization/10,"Overall , we achieve statistically significant improvements over the state - of the - art on both the CNN / DailyMail and Gigaword datasets , as well as on the DUC - 2002 transfer setup .",7,0,36
dataset/preprocessed/training-data/text_summarization/10,We also present several quantitative and qualitative analysis studies of our model 's learned saliency and entailment skills .,8,0,19
dataset/preprocessed/training-data/text_summarization/10,"Abstractive summarization is the challenging NLG task of compressing and rewriting a document into a short , relevant , salient , and coherent summary .",10,0,25
dataset/preprocessed/training-data/text_summarization/10,"It has numerous applications such as summarizing storylines , event understanding , etc .",11,0,14
dataset/preprocessed/training-data/text_summarization/10,"As compared to extractive or compressive summarization , abstractive summaries are based on rewriting as opposed to selecting .",12,0,19
dataset/preprocessed/training-data/text_summarization/10,"Recent end - to - end , neural sequence - tosequence models and larger datasets have allowed substantial progress on the abstractive task , with ideas ranging from copy - pointer mechanism and redundancy coverage , to metric reward based reinforcement learning .",13,0,43
dataset/preprocessed/training-data/text_summarization/10,"Despite these strong recent advancements , there is still a lot of scope for improving the summary quality generated by these models .",14,0,23
dataset/preprocessed/training-data/text_summarization/10,"A good rewritten summary is one that contains all the salient information from the document , is logically followed ( entailed ) by it , and avoids redundant information .",15,0,30
dataset/preprocessed/training-data/text_summarization/10,"The redundancy aspect was addressed by coverage models , but we still need to teach these models about how to better detect salient information from the input document , as well as about better logicallydirected natural language inference skills .",16,0,40
dataset/preprocessed/training-data/text_summarization/10,"In this work , we improve abstractive text summarization via soft , high - level ( semantic ) layerspecific multi-task learning with two relevant auxiliary tasks .",17,1,27
dataset/preprocessed/training-data/text_summarization/10,"The first is that of document - toquestion generation , which teaches the summarization model about what are the right questions to ask , which in turn is directly related to what the salient information in the input document is .",18,0,41
dataset/preprocessed/training-data/text_summarization/10,"The second auxiliary task is a premise - to - entailment generation task to teach it how to rewrite a summary which is a directed - logical subset of ( i.e. , logically follows from ) the input document , and contains no contradictory or unrelated information .",19,0,48
dataset/preprocessed/training-data/text_summarization/10,"For the question generation task , we use the SQuAD dataset , where we learn to generate a question given a sentence containing the answer , similar to the recent work by .",20,0,33
dataset/preprocessed/training-data/text_summarization/10,"Our entailment generation task is based on the recent SNLI classification dataset and task , converted to a generation task .",21,0,21
dataset/preprocessed/training-data/text_summarization/10,"Further , we also present novel multi-task learning architectures based on multi-layered encoder and decoder models , where we empirically show that it is substantially better to share the higherlevel semantic layers between the three aforementioned tasks , while keeping the lower - level ( lexico- syntactic ) layers unshared .",22,0,51
dataset/preprocessed/training-data/text_summarization/10,We also explore different ways to optimize the shared parameters and show that ' soft ' parameter sharing achieves higher performance than hard sharing .,23,0,25
dataset/preprocessed/training-data/text_summarization/10,"Empirically , our soft , layer - specific sharing model with the question and entailment generation auxiliary tasks achieves statistically significant improvements over the state - of - the - art on both the CNN / DailyMail and Gigaword datasets .",24,0,41
dataset/preprocessed/training-data/text_summarization/10,"It also performs significantly better on the DUC - 2002 transfer setup , demonstrating its strong generalizability as well as the importance of auxiliary knowledge in low - resource scenarios .",25,0,31
dataset/preprocessed/training-data/text_summarization/10,We also report improvements on our auxiliary question and entailment generation tasks over their respective previous state - of - the - art .,26,0,24
dataset/preprocessed/training-data/text_summarization/10,"Moreover , we significantly decrease the training time of the multitask models by initializing the individual tasks from their pretrained baseline models .",27,0,23
dataset/preprocessed/training-data/text_summarization/10,"Finally , we present human evaluation studies as well as detailed quantitative and qualitative analysis studies of the improved saliency detection and logical inference skills learned by our multi-task model .",28,0,31
dataset/preprocessed/training-data/text_summarization/10,"Automatic text summarization has been progressively improving over the time , initially more focused on extractive and compressive models , and moving more towards compressive and abstractive summarization based on graphs and concept maps and discourse trees , syntactic parse trees , and Abstract Meaning Representations ( AMR ) .",30,0,50
dataset/preprocessed/training-data/text_summarization/10,"Recent work has also adopted machine translation inspired neural seq2seq models for abstractive summarization with advances in hierarchical , distractive , saliency , and graphattention modeling .",31,0,27
dataset/preprocessed/training-data/text_summarization/10,and incorporated recent advances from reinforcement learning .,32,0,8
dataset/preprocessed/training-data/text_summarization/10,"Also , further improved results via pointercopy mechanism and addressed the redundancy with coverage mechanism .",33,0,16
dataset/preprocessed/training-data/text_summarization/10,Multi - task learning ( MTL ) is a useful paradigm to improve the generalization performance of a task with related tasks while sharing some common parameters / representations .,34,0,30
dataset/preprocessed/training-data/text_summarization/10,Several recent works have adopted MTL in neural models .,35,0,10
dataset/preprocessed/training-data/text_summarization/10,"Moreover , some of the above works have investigated the use of shared vs unshared sets of parameters .",36,0,19
dataset/preprocessed/training-data/text_summarization/10,"On the other hand , we investigate the importance of soft parameter sharing and highlevel versus low - level layer - specific sharing .",37,0,24
dataset/preprocessed/training-data/text_summarization/10,Our previous workshop paper presented some preliminary results for multi-task learning of textual summarization with entailment generation .,38,0,18
dataset/preprocessed/training-data/text_summarization/10,This current paper has several major differences :,39,0,8
dataset/preprocessed/training-data/text_summarization/10,( 1 ) We present question generation as an additional effective auxiliary task to enhance the important complementary aspect of saliency detection ; ( 2 ) Our new high - level layer - specific sharing approach is significantly better than alternative layer - sharing approaches ( including the decoder - only sharing by ) ; ( 3 ) Our new soft sharing parameter approach gives stat .,40,0,67
dataset/preprocessed/training-data/text_summarization/10,"significant improvements over hard sharing ; ( 4 ) We propose a useful idea of starting multi-task models from their pretrained baselines , which significantly speeds up our experiment cycle 1 ; ( 5 ) For evaluation , we show diverse improvements of our soft , layer - specific MTL model ( over state - of - the - art pointer + coverage baselines ) on the CNN / DailyMail , Gigaword , as well as DUC datasets ; we also report human evaluation plus analysis examples of learned saliency and entailment skills ; we also report improvements on the auxiliary question and entailment generation tasks over their respective previous state - of - the - art .",41,0,118
dataset/preprocessed/training-data/text_summarization/10,"In our work , we use a question generation task to improve the saliency of abstractive summarization in a multi -task setting .",42,0,23
dataset/preprocessed/training-data/text_summarization/10,"Using the SQuAD dataset , we learn to generate a question given the sentence containing the answer span in the comprehension ( similar to ) .",43,0,26
dataset/preprocessed/training-data/text_summarization/10,"For the second auxiliary task of entailment generation , we use the generation version of the RTE classification task .",44,0,20
dataset/preprocessed/training-data/text_summarization/10,"Some previous work has explored the use of RTE for redundancy detection in summarization by modeling graph - based relationships between sentences to select the most non-redundant sentences , whereas our approach is based on multi-task learning .",45,0,38
dataset/preprocessed/training-data/text_summarization/10,Average Entailment Probability Baseline 0.907 Multi- Task ( EG ) 0.912 : Entailment classification results of our baseline vs. EG - multi - task model ( p < 0.001 ) .,47,0,31
dataset/preprocessed/training-data/text_summarization/10,these alternate sharing methods in all metrics with statistical significance ( p < 0.05 ) .,48,0,16
dataset/preprocessed/training-data/text_summarization/9,Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,2,1,8
dataset/preprocessed/training-data/text_summarization/9,Abstractive Sentence Summarization generates a shorter version of a given sentence while attempting to preserve its meaning .,4,0,18
dataset/preprocessed/training-data/text_summarization/9,We introduce a conditional recurrent neural network ( RNN ) which generates a summary of an input sentence .,5,0,19
dataset/preprocessed/training-data/text_summarization/9,The conditioning is provided by a novel convolutional attention - based encoder which ensures that the decoder focuses on the appropriate input words at each step of generation .,6,0,29
dataset/preprocessed/training-data/text_summarization/9,Our model relies only on learned features and is easy to train in an end - to - end fashion on large data sets .,7,0,25
dataset/preprocessed/training-data/text_summarization/9,Our experiments show that the model significantly outperforms the recently proposed state - of - the - art method on the Gigaword corpus while performing competitively on the DUC - 2004 shared task .,8,0,34
dataset/preprocessed/training-data/text_summarization/9,Generating a condensed version of a passage while preserving its meaning is known as text summarization .,10,1,17
dataset/preprocessed/training-data/text_summarization/9,Tackling this task is an important step towards natural language understanding .,11,0,12
dataset/preprocessed/training-data/text_summarization/9,Summarization systems can be broadly classified into two categories .,12,0,10
dataset/preprocessed/training-data/text_summarization/9,Extractive models generate summaries by cropping important segments from the original text and putting them together to form a coherent summary .,13,0,22
dataset/preprocessed/training-data/text_summarization/9,Abstractive models generate summaries from scratch without being constrained to reuse phrases from the original text .,14,0,17
dataset/preprocessed/training-data/text_summarization/9,In this paper we propose a novel recurrent neural network for the problem of abstractive sentence summarization .,15,0,18
dataset/preprocessed/training-data/text_summarization/9,"Inspired by the recently proposed architectures for machine translation , our model consists of a conditional recurrent neural network , which acts as a decoder to generate the summary of an input sentence , much like a standard recurrent language model .",16,0,42
dataset/preprocessed/training-data/text_summarization/9,"In addition , at every time step the decoder also takes a conditioning input which is the output of an encoder module .",17,0,23
dataset/preprocessed/training-data/text_summarization/9,"Depending on the current state of the RNN , the encoder computes scores over the words in the input sentence .",18,0,21
dataset/preprocessed/training-data/text_summarization/9,"These scores can be interpreted as a soft alignment over the input text , informing the decoder which part of the input sentence it should focus onto generate the next word .",19,0,32
dataset/preprocessed/training-data/text_summarization/9,Both the decoder and encoder are jointly trained on a data set consisting of sentence - summary pairs .,20,0,19
dataset/preprocessed/training-data/text_summarization/9,Our model can be seen as an extension of the recently proposed model for the same problem by .,21,0,19
dataset/preprocessed/training-data/text_summarization/9,"While they use a feed - forward neural language model for generation , we use a recurrent neural network .",22,0,20
dataset/preprocessed/training-data/text_summarization/9,"Furthermore , our encoder is more sophisticated , in that it explicitly encodes the position information of the input words .",23,0,21
dataset/preprocessed/training-data/text_summarization/9,"Lastly , our encoder uses a convolutional network to encode input words .",24,0,13
dataset/preprocessed/training-data/text_summarization/9,These extensions result in improved performance .,25,0,7
dataset/preprocessed/training-data/text_summarization/9,The main contribution of this paper is a novel convolutional attention - based conditional recurrent neural network model for the problem of abstractive sentence summarization .,26,0,26
dataset/preprocessed/training-data/text_summarization/9,Empirically we show that our model beats the state - of - the - art systems of on multiple data sets .,27,0,22
dataset/preprocessed/training-data/text_summarization/9,"Particularly notable is the fact that even with a simple generation module , which does not use any extractive feature tuning , our model manages to significantly outperform their ABS + system on the Gigaword data set and is comparable on the DUC - 2004 task .",28,0,47
dataset/preprocessed/training-data/text_summarization/9,"While there is a large body of work for generating extractive summaries of sentences , there has been much less research on abstractive summarization .",30,0,25
dataset/preprocessed/training-data/text_summarization/9,A count - based noisy - channel machine translation model was proposed for the problem in .,31,0,17
dataset/preprocessed/training-data/text_summarization/9,"The task of abstractive sentence summarization was later formalized around the , where the TOP - IARY system was the state - of the - art .",32,0,27
dataset/preprocessed/training-data/text_summarization/9,More recently and later proposed systems which made heavy use of the syntactic features of the sentence - summary pairs .,33,0,21
dataset/preprocessed/training-data/text_summarization/9,"Later , along the lines of , MOSES was used directly as a method for text simplification by .",34,0,19
dataset/preprocessed/training-data/text_summarization/9,Other works which have recently been proposed for the problem of sentence summarization include .,35,0,15
dataset/preprocessed/training-data/text_summarization/9,Very recently proposed a neural attention model for this problem using a new data set for training and showing state - of - the - art performance on the DUC tasks .,36,0,32
dataset/preprocessed/training-data/text_summarization/9,Our model can be seen as an extension of their model .,37,0,12
dataset/preprocessed/training-data/text_summarization/9,Attentive Recurrent Architecture,38,0,3
dataset/preprocessed/training-data/text_summarization/9,"Let x denote the input sentence consisting of a sequence of M words x = [x 1 , . . . , x M ] , where each word xi is part of vocabulary V , of size | V | = V .",39,0,44
dataset/preprocessed/training-data/text_summarization/9,"Our task is to generate a target sequence y = [ y 1 , . . . , y N ] , of N words , where N < M , such that the meaning of x is preserved : y = argmax y P ( y|x ) , where y is a random variable denoting a sequence of N words .",40,0,62
dataset/preprocessed/training-data/text_summarization/9,Typically the conditional probability is modeled by a parametric function with parameters ?: P ( y|x ) = P ( y |x ; ?) .,41,0,25
dataset/preprocessed/training-data/text_summarization/9,Training involves finding the ?,42,0,5
dataset/preprocessed/training-data/text_summarization/9,which maximizes the conditional probability of sentence - summary pairs in the training corpus .,43,0,15
dataset/preprocessed/training-data/text_summarization/9,"If the model is trained to generate the next word of the summary , given the previous words , then the above conditional can be factorized into a product of indi-vidual conditional probabilities :",44,0,34
dataset/preprocessed/training-data/text_summarization/9,"In this work we model this conditional probability using an RNN Encoder - Decoder architecture , inspired by and subsequently extended in .",45,0,23
dataset/preprocessed/training-data/text_summarization/9,We call our model RAS ( Recurrent Attentive Summarizer ) .,46,0,11
dataset/preprocessed/training-data/text_summarization/9,The above conditional is modeled using an RNN :,48,0,9
dataset/preprocessed/training-data/text_summarization/9,where ht is the hidden state of the RNN :,49,0,10
dataset/preprocessed/training-data/text_summarization/1,Mixture Content Selection for Diverse Sequence Generation,2,1,7
dataset/preprocessed/training-data/text_summarization/1,Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one - to - many relationships between source and the target sequences .,4,1,31
dataset/preprocessed/training-data/text_summarization/1,We present a method to explicitly separate diversification from generation using a general plug - and - play module ( called SELECTOR ) that wraps around and guides an existing encoder - decoder model .,5,0,35
dataset/preprocessed/training-data/text_summarization/1,The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection .,6,0,22
dataset/preprocessed/training-data/text_summarization/1,The generation stage uses a standard encoder - decoder model given each selected content from the source sequence .,7,0,19
dataset/preprocessed/training-data/text_summarization/1,"Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask , we leverage a proxy for ground - truth mask and adopt stochastic hard - EM for training .",8,0,37
dataset/preprocessed/training-data/text_summarization/1,"In question generation ( SQuAD ) and abstractive summarization ( CNN - DM ) , our method demonstrates significant improvements in accuracy , diversity and training efficiency , including state - of - the - art top - 1 accuracy in both datasets , 6 % gain in top - 5 accuracy , and 3.7 times faster training over a state - of - the - art model .",9,0,69
dataset/preprocessed/training-data/text_summarization/1,Our code is publicly available at https://github.com/ clovaai/FocusSeq2Seq.,10,0,8
dataset/preprocessed/training-data/text_summarization/1,Generating target sequences given a source sequence has applications in a wide range of problems in NLP with different types of relationships between the source and target sequences .,12,1,29
dataset/preprocessed/training-data/text_summarization/1,"For instance , paraphrasing or machine translation exhibit a one - to - one relationship because the source and the target should carry the same meaning .",13,0,27
dataset/preprocessed/training-data/text_summarization/1,"On the other hand , summarization or question generation exhibit one - to - many relationships because * Most work done during internship at Clova AI .",14,0,27
dataset/preprocessed/training-data/text_summarization/1,"Source Passage : in december 1878 , tesla left graz and severed all relations with his family to hide the fact that he dropped out of school .",15,0,28
dataset/preprocessed/training-data/text_summarization/1,Target : what did tesla do in december 1878 ?,16,0,10
dataset/preprocessed/training-data/text_summarization/1,"Focus 1 : in december 1878 , tesla left graz and severed all relations with his family to hide the fact that he dropped out of school .",17,0,28
dataset/preprocessed/training-data/text_summarization/1,( Ours ) ?,18,0,4
dataset/preprocessed/training-data/text_summarization/1,what did tesla do ?,19,0,5
dataset/preprocessed/training-data/text_summarization/1,"Focus 2 : in december 1878 , tesla left graz and severed all relations with his family to hide the fact that he dropped out of school .",20,0,28
dataset/preprocessed/training-data/text_summarization/1,( Ours ) ?,21,0,4
dataset/preprocessed/training-data/text_summarization/1,what did tesla do in december 1878 ?,22,0,8
dataset/preprocessed/training-data/text_summarization/1,"Focus 3 : in december 1878 , tesla left graz and severed all relations with his family to hide the fact that he dropped out of school .",23,0,28
dataset/preprocessed/training-data/text_summarization/1,( Ours ) ?,24,0,4
dataset/preprocessed/training-data/text_summarization/1,what did tesla do to hide he dropped out of school ? :,25,0,13
dataset/preprocessed/training-data/text_summarization/1,Sample questions produced by our method from given passage - answer pair ( answer is underlined ) .,26,0,18
dataset/preprocessed/training-data/text_summarization/1,"Our method generates diverse questions , by selecting different tokens to focus ( colored ) in contrast to 3 mixture decoder that generates 3 identical questions : "" what did tesla do to hide the fact that he dropped out of school ? "" .",27,0,45
dataset/preprocessed/training-data/text_summarization/1,a single source often results in diverse target sequences with different semantics .,28,0,13
dataset/preprocessed/training-data/text_summarization/1,shows different questions that can be generated from a given passage .,29,0,12
dataset/preprocessed/training-data/text_summarization/1,"Encoder - decoder models are widely used for sequence generation , most notably in machine translation where neural models are now often almost as good as human translators in some language pairs .",30,1,33
dataset/preprocessed/training-data/text_summarization/1,"However , a standard encoder - decoder often shows a poor performance when it attempts to produce multiple , diverse outputs .",31,0,22
dataset/preprocessed/training-data/text_summarization/1,Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms or mixture of decoders .,32,0,20
dataset/preprocessed/training-data/text_summarization/1,"These methods promote diversity at the decoding step , while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences .",33,0,30
dataset/preprocessed/training-data/text_summarization/1,"In this paper , we present a method for diverse generation that separates diversification and generation stages .",34,0,18
dataset/preprocessed/training-data/text_summarization/1,"The diversification stage leverages content selection to map the source to multiple sequences , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .",35,0,34
dataset/preprocessed/training-data/text_summarization/1,The generation stage uses a standard encoder - decoder model to generate a target sequence given each selected content from the source ( one - to - one mapping ) .,36,0,31
dataset/preprocessed/training-data/text_summarization/1,We present a generic module called SELECTOR that is specialized for diversification .,37,0,13
dataset/preprocessed/training-data/text_summarization/1,This module can be used as a plug - and - play to an arbitrary encoder - decoder model for generation without architecture change .,38,0,25
dataset/preprocessed/training-data/text_summarization/1,The SELECTOR module leverages a mixture of experts to identify diverse key contents to focus on during generation .,39,0,19
dataset/preprocessed/training-data/text_summarization/1,Each mixture samples a sequential latent variable modeled as a binary mask on every source sequence token .,40,0,18
dataset/preprocessed/training-data/text_summarization/1,Then an encoder - decoder model generates multiple target sequences given these binary masks along with the original source tokens .,41,0,21
dataset/preprocessed/training-data/text_summarization/1,"Due to the non-differentiable nature of discrete sampling , we adopt stochastic hard - EM for training SELECTOR .",42,0,19
dataset/preprocessed/training-data/text_summarization/1,"To mitigate the lack of ground truth annotation for the mask ( content selection ) , we use the overlap between the source and target sequences as a simple proxy for the ground - truth mask .",43,0,37
dataset/preprocessed/training-data/text_summarization/1,We experiment on question generation and abstractive summarization tasks and show that our method achieves the best trade - off between accuracy and diversity over previous models on SQuAD and datasets .,44,0,32
dataset/preprocessed/training-data/text_summarization/1,"In particular , compared to the recently - introduced mixture decoder that also aims to diversify outputs by creating multiple decoders , our modular method not only demonstrates better accuracy and diversity , but also trains 3.7 times faster .",45,0,40
dataset/preprocessed/training-data/text_summarization/1,Diverse Search Algorithms,47,0,3
dataset/preprocessed/training-data/text_summarization/1,"Beam search , the most commonly used search algorithm for decoding , is known to produce samples thatare short , contain repetitive phrases , and share majority of their tokens .",48,0,31
dataset/preprocessed/training-data/text_summarization/1,Hence several methods are intro-duced to diversify search algorithms for decoding . ; tune temperature hyperparameter in softmax function . ; penalize similar samples during beam search in order to obtain diverse set of samples .,49,0,36
dataset/preprocessed/training-data/text_summarization/5,"Retrieve , Rerank and Rewrite : Soft Template Based Neural Summarization",2,1,11
dataset/preprocessed/training-data/text_summarization/5,"Most previous seq2seq summarization systems purely depend on the source text to generate summaries , which tends to work unstably .",4,0,21
dataset/preprocessed/training-data/text_summarization/5,"Inspired by the traditional template - based summarization approaches , this paper proposes to use existing summaries as soft templates to guide the seq2seq model .",5,0,26
dataset/preprocessed/training-data/text_summarization/5,"To this end , we use a popular IR platform to Retrieve proper summaries as candidate templates .",6,0,18
dataset/preprocessed/training-data/text_summarization/5,"Then , we extend the seq2seq framework to jointly conduct template Reranking and templateaware summary generation ( Rewriting ) .",7,0,20
dataset/preprocessed/training-data/text_summarization/5,"Experiments show that , in terms of informativeness , our model significantly outperforms the state - of - the - art methods , and even soft templates themselves demonstrate high competitiveness .",8,0,32
dataset/preprocessed/training-data/text_summarization/5,"In addition , the import of high - quality external summaries improves the stability and readability of generated summaries .",9,0,20
dataset/preprocessed/training-data/text_summarization/5,The exponentially growing online information has necessitated the development of effective automatic summarization systems .,11,0,15
dataset/preprocessed/training-data/text_summarization/5,"In this paper , we focus on an increasingly intriguing task , i.e. , abstractive sentence summarization , which generates a shorter version of a given sentence while attempting to preserve its original meaning .",12,1,35
dataset/preprocessed/training-data/text_summarization/5,It can be used to design or refine appealing headlines .,13,0,11
dataset/preprocessed/training-data/text_summarization/5,"Recently , the application of the attentional sequence - to - sequence ( seq2seq ) framework has attracted growing attention and achieved state - of - the - art performance on this task .",14,0,34
dataset/preprocessed/training-data/text_summarization/5,Most previous seq2seq models purely depend on the source text to generate summaries .,15,0,14
dataset/preprocessed/training-data/text_summarization/5,"However , as reported in many studies , the performance of a seq2seq model deteriorates quickly with the increase of the length of generation .",16,0,25
dataset/preprocessed/training-data/text_summarization/5,"Our experiments also show that seq2seq models tend to "" lose control "" sometimes .",17,0,15
dataset/preprocessed/training-data/text_summarization/5,"For example , 3 % of summaries contain less than 3 words , while there are 4 summaries repeating a word for even 99 times .",18,0,26
dataset/preprocessed/training-data/text_summarization/5,These results largely reduce the informativeness and readability of the generated summaries .,19,0,13
dataset/preprocessed/training-data/text_summarization/5,"In addition , we find seq2seq models usually focus on copying source words in order , without any actual "" summarization "" .",20,0,23
dataset/preprocessed/training-data/text_summarization/5,"Therefore , we argue that , the free generation based on the source sentence is not enough for a seq2seq model .",21,0,22
dataset/preprocessed/training-data/text_summarization/5,"Template based summarization ( e.g. , ) is a traditional approach to abstractive summarization .",22,0,15
dataset/preprocessed/training-data/text_summarization/5,"In general , a template is an incomplete sentence which can be filled with the input text using the manually defined rules .",23,0,23
dataset/preprocessed/training-data/text_summarization/5,"For instance , a concise template to conclude the stock market quotation is : shares [ open / close ] [ NUMBER ] percent [ lower / higher ] , e.g. , "" hong kong shares close # . # percent lower "" .",24,0,44
dataset/preprocessed/training-data/text_summarization/5,"Since the templates are written by humans , the produced summaries are usually fluent and informative .",25,0,17
dataset/preprocessed/training-data/text_summarization/5,"However , the construction of templates is extremely time - consuming and requires aplenty of domain knowledge .",26,0,18
dataset/preprocessed/training-data/text_summarization/5,"Moreover , it is impossible to develop all templates for summaries in various domains .",27,0,15
dataset/preprocessed/training-data/text_summarization/5,"Inspired by retrieve - based conversation systems , we assume the golden summaries of the similar sentences can provide a reference point to guide the input sentence summarization process .",28,0,30
dataset/preprocessed/training-data/text_summarization/5,We call these existing summaries soft templates since no actual rules are nee-ded to build new summaries from them .,29,0,20
dataset/preprocessed/training-data/text_summarization/5,"Due to the strong rewriting ability of the seq2seq framework , in this paper , we propose to combine the seq2seq and template based summarization approaches .",30,0,27
dataset/preprocessed/training-data/text_summarization/5,"We call our summarization system Re 3 Sum , which consists of three modules : Retrieve , Rerank and Rewrite .",31,0,21
dataset/preprocessed/training-data/text_summarization/5,We utilize a widely - used Information Retrieval ( IR ) platform to find out candidate soft templates from the training corpus .,32,0,23
dataset/preprocessed/training-data/text_summarization/5,"Then , we extend the seq2seq model to jointly learn template saliency measurement ( Rerank ) and final summary generation ( Rewrite ) .",33,0,24
dataset/preprocessed/training-data/text_summarization/5,"Specifically , a Recurrent Neural Network ( RNN ) encoder is applied to convert the input sentence and each candidate template into hidden states .",34,0,25
dataset/preprocessed/training-data/text_summarization/5,"In Rerank , we measure the informativeness of a candidate template according to its hidden state relevance to the input sentence .",35,0,22
dataset/preprocessed/training-data/text_summarization/5,The candidate template with the highest predicted informativeness is regarded as the actual soft template .,36,0,16
dataset/preprocessed/training-data/text_summarization/5,"In Rewrite , the summary is generated according to the hidden states of both the sentence and template .",37,0,19
dataset/preprocessed/training-data/text_summarization/5,We conduct extensive experiments on the popular Gigaword dataset .,38,0,10
dataset/preprocessed/training-data/text_summarization/5,"Experiments show that , in terms of informativeness , Re 3 Sum significantly outperforms the state - of the - art seq2seq models , and even soft templates themselves demonstrate high competitiveness .",39,0,33
dataset/preprocessed/training-data/text_summarization/5,"In addition , the import of high - quality external summaries improves the stability and readability of generated summaries .",40,0,20
dataset/preprocessed/training-data/text_summarization/5,The contributions of this work are summarized as follows :,41,0,10
dataset/preprocessed/training-data/text_summarization/5,We propose to introduce soft templates as additional input to improve the readability and stability of seq2seq summarization systems .,42,0,20
dataset/preprocessed/training-data/text_summarization/5,Code and results can be found at http://www4.comp.polyu.edu.hk/cszqcao/,43,0,8
dataset/preprocessed/training-data/text_summarization/5,We extend the seq2seq framework to conduct template reranking and template - aware summary generation simultaneously .,44,0,17
dataset/preprocessed/training-data/text_summarization/5,"We fuse the popular IR - based and seq2seqbased summarization systems , which fully utilize the supervisions from both sides .",45,0,21
dataset/preprocessed/training-data/text_summarization/5,"As shown in we choose the one with the maximal actual saliency score in C , which speeds up convergence and shows no obvious side effect in the experiments .",47,0,30
dataset/preprocessed/training-data/text_summarization/5,"Then , we jointly conduct reranking and rewriting through a shared encoder .",48,0,13
dataset/preprocessed/training-data/text_summarization/5,"Specifically , both the sentence x and the soft template rare converted into hidden states with a RNN encoder .",49,0,20
dataset/preprocessed/training-data/text_summarization/8,Bottom - Up Abstractive Summarization,2,1,5
dataset/preprocessed/training-data/text_summarization/8,"Neural network - based methods for abstractive summarization produce outputs that are more fluent than other techniques , but perform poorly at content selection .",4,0,25
dataset/preprocessed/training-data/text_summarization/8,This work proposes a simple technique for addressing this issue : use a data- efficient content selector to over - determine phrases in a source document that should be part of the summary .,5,0,34
dataset/preprocessed/training-data/text_summarization/8,We use this selector as a bottom - up attention step to constrain the model to likely phrases .,6,0,19
dataset/preprocessed/training-data/text_summarization/8,"We show that this approach improves the ability to compress text , while still generating fluent summaries .",7,0,18
dataset/preprocessed/training-data/text_summarization/8,"This two - step process is both simpler and higher performing than other end - toend content selection models , leading to significant improvements on ROUGE for both the CNN - DM and NYT corpus .",8,0,36
dataset/preprocessed/training-data/text_summarization/8,"Furthermore , the content selector can be trained with as little as 1,000 sentences , making it easy to transfer a trained summarizer to a new domain .",9,0,28
dataset/preprocessed/training-data/text_summarization/8,Text summarization systems aim to generate natural language summaries that compress the information in a longer text .,11,1,18
dataset/preprocessed/training-data/text_summarization/8,Approaches using neural networks have shown promising results on this task with end - to - end models that encode a source document and then decode it into an abstractive summary .,12,0,32
dataset/preprocessed/training-data/text_summarization/8,Current state - of - the - art neural abstractive summarization models combine extractive and abstractive techniques by using pointergenerator style models which can copy words from the source document .,13,1,31
dataset/preprocessed/training-data/text_summarization/8,"These end - to - end models produce fluent abstractive summaries but have had mixed success in content selection , i.e. deciding what to summarize , compared to fully extractive models .",14,0,32
dataset/preprocessed/training-data/text_summarization/8,"There is an appeal to end - to - end models from a modeling perspective ; however , there is evidence that when summarizing people follow a two - step Source Document german chancellor angela merkel [ did ] not too pleased about the weather during her easter holiday as britain in and temperatures of up to 21 c , mrs merkel and her husband [ , chemistry professor joachim sauer , ] had to settle for a measly 12 degrees .",15,0,82
dataset/preprocessed/training-data/text_summarization/8,"the chancellor and her have been spending easter on the small island of ischia , near naples in the mediterranean for over a [ not so sunny : ] angela merkel her husband are spotted on their easter trip to the island of ischia near naples [ .",16,0,48
dataset/preprocessed/training-data/text_summarization/8,"the ] couple spend their holiday at the fivestar miramare spa hotel on the south of the island [ , which comes ] with it s own private beach [ , and balconies overlooking the ] ocean ...",17,0,38
dataset/preprocessed/training-data/text_summarization/8,Reference angela merkel and husband spotted while on italian island holiday .,18,0,12
dataset/preprocessed/training-data/text_summarization/8,. . .,19,0,3
dataset/preprocessed/training-data/text_summarization/8,"angela merkel and her husband , chemistry professor joachim sauer , are spotted on their annual easter trip to the island of ischia , near naples .",21,0,27
dataset/preprocessed/training-data/text_summarization/8,. . .,22,0,3
dataset/preprocessed/training-data/text_summarization/8,Bottom - Up Summarization,23,0,4
dataset/preprocessed/training-data/text_summarization/8,"angela merkel and her husband are spotted on their easter trip to the island of ischia , near naples .",24,0,20
dataset/preprocessed/training-data/text_summarization/8,. . . approach of first selecting important phrases and then paraphrasing them .,25,0,14
dataset/preprocessed/training-data/text_summarization/8,A similar argument has been made for image captioning .,26,0,10
dataset/preprocessed/training-data/text_summarization/8,develop a state - of - the - art model with a two - step approach that first pre-computes bounding boxes of segmented objects and then applies attention to these regions .,27,0,32
dataset/preprocessed/training-data/text_summarization/8,This so - called bottom - up attention is inspired by neuroscience research describing attention based on properties in - herent to a stimulus .,28,0,25
dataset/preprocessed/training-data/text_summarization/8,"Motivated by this approach , we consider bottom - up attention for neural abstractive summarization .",29,0,16
dataset/preprocessed/training-data/text_summarization/8,Our approach first selects a selection mask for the source document and then constrains a standard neural model by this mask .,30,0,22
dataset/preprocessed/training-data/text_summarization/8,"This approach can better decide which phrases a model should include in a summary , without sacrificing the fluency advantages of neural abstractive summarizers .",31,0,25
dataset/preprocessed/training-data/text_summarization/8,"Furthermore , it requires much fewer data to train , which makes it more adaptable to new domains .",32,0,19
dataset/preprocessed/training-data/text_summarization/8,Our full model incorporates a separate content selection system to decide on relevant aspects of the source document .,33,0,19
dataset/preprocessed/training-data/text_summarization/8,"We frame this selection task as a sequence - tagging problem , with the objective of identifying tokens from a document that are part of its summary .",34,0,28
dataset/preprocessed/training-data/text_summarization/8,"We show that a content selection model that builds on contextual word embeddings can identify correct tokens with a recall of over 60 % , and a precision of over 50 % .",35,0,33
dataset/preprocessed/training-data/text_summarization/8,"To incorporate bottom - up attention into abstractive summarization models , we employ masking to constrain copying words to the selected parts of the text , which produces grammatical outputs .",36,0,31
dataset/preprocessed/training-data/text_summarization/8,"We additionally experiment with multiple methods to incorporate similar constraints into the training process of more complex end - to - end abstractive summarization models , either through multi-task learning or through directly incorporating a fully differentiable mask .",37,0,39
dataset/preprocessed/training-data/text_summarization/8,Our experiments compare bottom - up attention with several other state - of - the - art abstractive systems .,38,0,20
dataset/preprocessed/training-data/text_summarization/8,Compared to our baseline models of bottom - up attention leads to an improvement in ROUGE - L score on the CNN - Daily Mail ( CNN - DM ) corpus from 36.4 to 38.3 while being simpler to train .,39,0,41
dataset/preprocessed/training-data/text_summarization/8,We also see comparable or better results than recent reinforcement - learning based methods with our MLE trained system .,40,0,20
dataset/preprocessed/training-data/text_summarization/8,"Furthermore , we find that the content selection model is very data - efficient and can be trained with less than 1 % of the original training data .",41,0,29
dataset/preprocessed/training-data/text_summarization/8,This provides opportunities for domain - transfer and lowresource summarization .,42,0,11
dataset/preprocessed/training-data/text_summarization/8,"We show that a summarization model trained on CNN - DM and evaluated on the NYT corpus can be improved by over 5 points in ROUGE - L with a content selector trained on only 1,000 in - domain sentences .",43,0,41
dataset/preprocessed/training-data/text_summarization/8,There is a tension in document summarization between staying close to the source document and allowing compressive or abstractive modification .,45,0,21
dataset/preprocessed/training-data/text_summarization/8,Many non-neural systems take a select and compress approach .,46,0,10
dataset/preprocessed/training-data/text_summarization/8,"For example , introduced a system that first extracts noun and verb phrases from the first sentence of a news article and uses an iterative shortening algorithm to compress it .",47,0,31
dataset/preprocessed/training-data/text_summarization/8,Recent systems such as also learn a model to select sentences and then compress them .,48,0,16
dataset/preprocessed/training-data/text_summarization/8,"In contrast , recent work in neural network based data - driven extractive summarization has focused on extracting and ordering full sentences .",49,0,23
dataset/preprocessed/training-data/text_summarization/6,Deep Recurrent Generative Decoder for Abstractive Text Summarization,2,1,8
dataset/preprocessed/training-data/text_summarization/6,We propose a new framework for abstractive text summarization based on a sequence - to - sequence oriented encoderdecoder model equipped with a deep recurrent generative decoder ( DRGN ) .,5,0,31
dataset/preprocessed/training-data/text_summarization/6,Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality .,6,0,23
dataset/preprocessed/training-data/text_summarization/6,Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables .,7,0,17
dataset/preprocessed/training-data/text_summarization/6,Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states .,8,0,17
dataset/preprocessed/training-data/text_summarization/6,Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state - of the - art methods .,9,0,24
dataset/preprocessed/training-data/text_summarization/6,Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document .,11,1,22
dataset/preprocessed/training-data/text_summarization/6,"Different from the common extraction - based and compression - based methods , abstraction - based methods aim at constructing new sentences as summaries , thus they require a deeper understanding of the text and the capability of generating new sentences , which provide an obvious advantage in improving the focus of a summary , reducing the redundancy , and keeping a good compression rate . *",12,0,67
dataset/preprocessed/training-data/text_summarization/6,"The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region , China ( Project Code : 14203414 ) .",13,0,31
dataset/preprocessed/training-data/text_summarization/6,Some previous research works show that human - written summaries are more abstractive .,14,0,14
dataset/preprocessed/training-data/text_summarization/6,"Moreover , our investigation reveals that people may naturally follow some inherent structures when they write the abstractive summaries .",15,0,20
dataset/preprocessed/training-data/text_summarization/6,"To illustrate this observation , we show some examples in , which are some top story summaries or headlines from the channel "" Technology "" of CNN .",16,0,28
dataset/preprocessed/training-data/text_summarization/6,"After analyzing the summaries carefully , we can find some common structures from them , such as "" What "" , "" What - Happened "" , "" Who Action What "" , etc .",17,0,35
dataset/preprocessed/training-data/text_summarization/6,"For example , the summary "" Apple sues Qualcomm for nearly $ 1 billion "" can be structuralized as "" Who ( Apple ) Action ( sues ) What ( Qualcomm ) "" .",18,0,34
dataset/preprocessed/training-data/text_summarization/6,"Similarly , the summaries "" [ fixes ] [ botched @POTUS account transfer ] "" , "" [ to pay ] [ $ 20 million ] for misleading drivers "" , and "" [ Bipartis an bill ] aims to [ H - 1B vis a system ] "" also follow the structure of "" Who Action What "" .",19,0,60
dataset/preprocessed/training-data/text_summarization/6,"The summary "" The emergence of the ' cyber cold war "" ' matches with the structure of "" What "" , and the summary "" St. Louis ' public library computers hacked "" follows the structure of "" What - Happened "" .",20,0,44
dataset/preprocessed/training-data/text_summarization/6,"Intuitively , if we can incorporate the latent structure information of summaries into the abstractive summarization model , it will improve the quality of the generated summaries .",21,0,28
dataset/preprocessed/training-data/text_summarization/6,"However , very few existing works specifically consider the latent structure information of summaries in their summarization models .",22,0,19
dataset/preprocessed/training-data/text_summarization/6,"Although a very popular neural network based sequence - to - sequence ( seq2seq ) framework has been proposed to tackle the abstractive summarization problem , the calculation of the internal decoding states is entirely deterministic .",23,0,37
dataset/preprocessed/training-data/text_summarization/6,The deterministic transformations in these discriminative models lead to limitations on the representation ability of the latent structure information .,24,0,20
dataset/preprocessed/training-data/text_summarization/6,"extended the seq2seq framework and proposed a generative model to capture the latent summary information , but they did not consider the recurrent dependencies in their generative model leading to limited representation ability .",25,0,34
dataset/preprocessed/training-data/text_summarization/6,"To tackle the above mentioned problems , we design a new framework based on sequence to - sequence oriented encoder - decoder model equipped with a latent structure modeling component .",26,0,31
dataset/preprocessed/training-data/text_summarization/6,We employ Variational Auto - Encoders ( VAEs ) as the base model for our generative framework which can handle the inference problem associated with complex generative modeling .,27,0,29
dataset/preprocessed/training-data/text_summarization/6,"However , the standard framework of VAEs is not designed for sequence modeling related tasks .",28,0,16
dataset/preprocessed/training-data/text_summarization/6,"Inspired by , we add historical dependencies on the latent variables of VAEs and propose a deep recurrent generative decoder ( DRGD ) for latent structure modeling .",29,0,28
dataset/preprocessed/training-data/text_summarization/6,Then the standard discriminative deterministic decoder and the recurrent generative decoder are integrated into a unified decoding framework .,30,0,19
dataset/preprocessed/training-data/text_summarization/6,The target summaries will be decoded based on both the discriminative deterministic variables and the generative latent structural information .,31,0,20
dataset/preprocessed/training-data/text_summarization/6,All the neural parameters are learned by back - propagation in an end - to - end training paradigm .,32,0,20
dataset/preprocessed/training-data/text_summarization/6,The main contributions of our framework are summarized as follows :,33,0,11
dataset/preprocessed/training-data/text_summarization/6,( 1 ) We propose a sequence - to - sequence oriented encoder - decoder model equipped with a deep recurrent generative decoder ( DRGD ) to model and learn the latent structure information implied in the target summaries of the training data .,34,0,44
dataset/preprocessed/training-data/text_summarization/6,Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables .,35,0,17
dataset/preprocessed/training-data/text_summarization/6,( 2 ) Both the generative latent structural information and the discriminative deterministic variables are jointly considered in the generation process of the abstractive summaries .,36,0,26
dataset/preprocessed/training-data/text_summarization/6,( 3 ) Experimental results on some benchmark datasets in different languages show that our framework achieves better performance than the state - of - the - art models .,37,0,30
dataset/preprocessed/training-data/text_summarization/6,Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document .,39,1,22
dataset/preprocessed/training-data/text_summarization/6,"Traditionally , the summarization methods can be classified into three categories : extraction - based methods , compression - based methods , and abstraction - based methods .",40,0,28
dataset/preprocessed/training-data/text_summarization/6,"In fact , previous investigations show that human - written summaries are more abstractive .",41,0,15
dataset/preprocessed/training-data/text_summarization/6,Abstraction - based approaches can generate new sentences based on the facts from different source sentences .,42,0,17
dataset/preprocessed/training-data/text_summarization/6,employed sentence fusion to generate a new sentence .,43,0,9
dataset/preprocessed/training-data/text_summarization/6,"proposed a more fine - grained fusion framework , where new sentences are generated by selecting and merging salient phrases .",44,0,21
dataset/preprocessed/training-data/text_summarization/6,"These methods can be regarded as a kind of indirect abstractive summarization , and complicated constraints are used to guarantee the linguistic quality .",45,0,24
dataset/preprocessed/training-data/text_summarization/6,"Recently , some researchers employ neural network based framework to tackle the abstractive summarization problem .",46,0,16
dataset/preprocessed/training-data/text_summarization/6,"proposed a neural network based model with local attention modeling , which is trained on the Gigaword corpus , but combined with an additional loglinear extractive summarization model with handcrafted features .",47,0,32
dataset/preprocessed/training-data/text_summarization/6,integrated a copying mechanism into a seq2seq framework to improve the quality of the generated summaries .,48,0,17
dataset/preprocessed/training-data/text_summarization/6,"proposed a new attention mechanism that not only considers the important source segments , but also distracts them in the decoding step in order to better grasp the over all meaning of input documents .",49,0,35
dataset/preprocessed/training-data/text_summarization/11,Global Encoding for Abstractive Summarization,2,1,5
dataset/preprocessed/training-data/text_summarization/11,"In neural abstractive summarization , the conventional sequence - to - sequence ( seq2seq ) model often suffers from repetition and semantic irrelevance .",4,0,24
dataset/preprocessed/training-data/text_summarization/11,"To tackle the problem , we propose a global encoding framework , which controls the information flow from the encoder to the decoder based on the global information of the source context .",5,0,33
dataset/preprocessed/training-data/text_summarization/11,It consists of a convolutional gated unit to perform global encoding to improve the representations of the source - side information .,6,0,22
dataset/preprocessed/training-data/text_summarization/11,"Evaluations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models , and the analysis shows that our model is capable of generating summary of higher quality and reducing repetition 1 .",7,0,38
dataset/preprocessed/training-data/text_summarization/11,Abstractive summarization can be regarded as a sequence mapping task that the source text should be mapped to the target summary .,9,0,22
dataset/preprocessed/training-data/text_summarization/11,"Therefore , sequence - to - sequence learning can be applied to neural abstractive summarization , whose model consists of an encoder and a decoder .",10,1,26
dataset/preprocessed/training-data/text_summarization/11,Attention mechanism has been broadly used in seq2seq models where the decoder extracts information from the encoder based on the attention scores on the source - side information .,11,0,29
dataset/preprocessed/training-data/text_summarization/11,"Many attention - based seq2seq models have been proposed for abstractive summarization , which outperformed the conventional statistical methods .",12,0,20
dataset/preprocessed/training-data/text_summarization/11,"Text : the mainstream fatah movement on monday officially chose mahmoud abbas , chairman of the palestine liberation organization ( plo ) , as it s candidate to run for the presidential election due on jan .",13,0,37
dataset/preprocessed/training-data/text_summarization/11,"# , # ### , the official wafa news agency reported .",14,0,12
dataset/preprocessed/training-data/text_summarization/11,seq2seq : fatah officially officially elects abbas as candidate for candidate .,15,0,12
dataset/preprocessed/training-data/text_summarization/11,Gold : fatah officially elects abbas as candidate for presidential election :,16,0,12
dataset/preprocessed/training-data/text_summarization/11,An example of the summary of the conventional attention - based seq2seq model on the Gigaword dataset .,17,0,18
dataset/preprocessed/training-data/text_summarization/11,"The text highlighted indicates repetition , "" # "" refers to masked number .",18,0,14
dataset/preprocessed/training-data/text_summarization/11,"However , recent studies show that there are salient problems in the attention mechanism .",19,0,15
dataset/preprocessed/training-data/text_summarization/11,"pointed out that there is no obvious alignment relationship between the source text and the target summary , and the encoder outputs contain noise for the attention .",20,0,28
dataset/preprocessed/training-data/text_summarization/11,"For example , in the summary generated by the seq2seq in , "" officially "" is followed by the same word , as the attention mechanism still attends to the word with high attention score .",21,0,36
dataset/preprocessed/training-data/text_summarization/11,"Attention - based seq2seq model for abstractive summarization can suffer from repetition and semantic irrelevance , causing grammatical errors and insufficient reflection of the main idea of the source text .",22,0,31
dataset/preprocessed/training-data/text_summarization/11,"To tackle this problem , we propose a model of global encoding for abstractive summarization .",23,0,16
dataset/preprocessed/training-data/text_summarization/11,We set a convolutional gated unit to perform global encoding on the source context .,24,0,15
dataset/preprocessed/training-data/text_summarization/11,"The gate based on convolutional neural network ( CNN ) filters each encoder output based on the global context due to the parameter sharing , so that the representations at each time step are refined with consideration of the global context .",25,0,42
dataset/preprocessed/training-data/text_summarization/11,"We conduct experiments on LCSTS and Gigaword , two benchmark datasets for sentence summarization , which shows that our model outperforms the state - of - theart methods with ROUGE - 2 F1 score 26.8 and 17.8 respectively .",26,0,39
dataset/preprocessed/training-data/text_summarization/11,"Moreover , the analysis shows : Structure of our proposed Convolutional Gated Unit .",27,0,14
dataset/preprocessed/training-data/text_summarization/11,"We implement 1 - dimensional convolution with a structure similar to the Inception over the outputs of the RNN encoder , where k refers to the kernel size .",28,0,29
dataset/preprocessed/training-data/text_summarization/11,that our model is capable of reducing repetition compared with the seq2seq model .,29,0,14
dataset/preprocessed/training-data/text_summarization/11,Our model is based on the seq2seq model with attention .,31,0,11
dataset/preprocessed/training-data/text_summarization/11,"For the encoder , we set a convolutional gated unit for global encoding .",32,0,14
dataset/preprocessed/training-data/text_summarization/11,"Based on the outputs from the RNN encoder , the global encoding refines the representation of the source context with a CNN to improve the connection of the word representation with the global context .",33,0,35
dataset/preprocessed/training-data/text_summarization/11,"In the following , the techniques are introduced in detail .",34,0,11
dataset/preprocessed/training-data/text_summarization/11,Attention - based seq2seq,35,0,4
dataset/preprocessed/training-data/text_summarization/11,The RNN encoder receives the word embedding of each word from the source text sequentially .,36,0,16
dataset/preprocessed/training-data/text_summarization/11,The final hidden state with the information of the whole source text becomes the initial hidden state of the decoder .,37,0,21
dataset/preprocessed/training-data/text_summarization/11,"Here our encoder is a bidirectional LSTM encoder , where the encoder outputs from both directions at each time step are concatenated",38,0,22
dataset/preprocessed/training-data/text_summarization/11,"We implement a unidirectional LSTM decoder to read the input words and generate summary word byword , with a fixed target vocabulary embedded in a high - dimensional space Y ? R | Y | dim .",40,0,37
dataset/preprocessed/training-data/text_summarization/11,"At each time step , the decoder generates a summary wordy t by sampling from a distribution of the target vocabulary P vocab until sampling the token representing the end of sentence .",41,0,33
dataset/preprocessed/training-data/text_summarization/11,The hidden state of the decoder st and the en-coders output hi at each time step i of the encoding process are computed with a weight matrix W a to obtain the global attention ?,42,0,35
dataset/preprocessed/training-data/text_summarization/11,"t , i and the context vector ct .",43,0,9
dataset/preprocessed/training-data/text_summarization/11,It is described below :,44,0,5
dataset/preprocessed/training-data/text_summarization/11,"where C refers to the cell state in the LSTM , and g ( ) refers to a non-linear function .",45,0,21
dataset/preprocessed/training-data/text_summarization/11,Convolutional Gated Unit,46,0,3
dataset/preprocessed/training-data/text_summarization/11,Abstractive summarization requires the core information at each encoding time step .,47,0,12
dataset/preprocessed/training-data/text_summarization/11,"To reach this goal , we implement a gated unit on top of the encoder outputs at each time step , which is a CNN that convolves all the encoder outputs .",48,0,32
dataset/preprocessed/training-data/text_summarization/11,"The parameter sharing of the convolutional kernels enables the model to extract certain types of features , specifically n-gram features .",49,0,21
dataset/preprocessed/training-data/text_summarization/14,Ensure the Correctness of the Summary : Incorporate Entailment Knowledge into Abstractive Sentence Summarization,2,1,14
dataset/preprocessed/training-data/text_summarization/14,"In this paper , we investigate the sentence summarization task that produces a summary from a source sentence .",4,1,19
dataset/preprocessed/training-data/text_summarization/14,"Neural sequence - to - sequence models have gained considerable success for this task , while most existing approaches only focus on improving word overlap between the generated summary and the reference , which ignore the correctness , i.e. , the summary should not contain error messages with respect to the source sentence .",5,0,54
dataset/preprocessed/training-data/text_summarization/14,We argue that correctness is an essential requirement for summarization systems .,6,0,12
dataset/preprocessed/training-data/text_summarization/14,"Considering a correct summary is semantically entailed by the source sentence , we incorporate entailment knowledge into abstractive summarization models .",7,0,21
dataset/preprocessed/training-data/text_summarization/14,"We propose an entailment - aware encoder under multi-task framework ( i.e. , summarization generation and entailment recognition ) and an entailment - aware decoder by entailment Reward Augmented Maximum Likelihood ( RAML ) training .",8,0,36
dataset/preprocessed/training-data/text_summarization/14,Experimental results demonstrate that our models significantly outperform baselines from the aspects of informativeness and correctness .,9,0,17
dataset/preprocessed/training-data/text_summarization/14,This work is licensed under a Creative Commons Attribution 4.0 International License .,10,0,13
dataset/preprocessed/training-data/text_summarization/14,License details : http:// creativecommons.org/licenses/by/4.0/.,11,0,5
dataset/preprocessed/training-data/text_summarization/14,Sentence summarization is a well - studied task that creates a condensed version of along source sentence .,13,0,18
dataset/preprocessed/training-data/text_summarization/14,Sequence - to - sequence ( seq2seq ) model that encodes a source sequence into a latent representation and outputs another sequence is the dominating framework for sentence summarization .,14,0,30
dataset/preprocessed/training-data/text_summarization/14,"Despite substantial improvements on this task , most of the existing researches typically aim to improve word overlap between the generated summary and the references , which is measured by n-gram matching metrics ( e.g. , ROUGE ) .",15,0,39
dataset/preprocessed/training-data/text_summarization/14,"Hence , it can not guarantee the semantic correctness of the summary as a whole .",16,0,16
dataset/preprocessed/training-data/text_summarization/14,"Therefore , in some cases , the summary giving high matching scores may contain critical error messages , which makes the summary fail to capture the correct information with respect to the source sentence .",17,0,35
dataset/preprocessed/training-data/text_summarization/14,Previous study shows that about 30 % of the summaries generated by state - of - the - art seq2seq system are subject to this problem .,18,0,27
dataset/preprocessed/training-data/text_summarization/14,"Here is an example ( the digits are replaced by "" # "" ) :",19,0,15
dataset/preprocessed/training-data/text_summarization/14,Source sentence : franch won the gold medal at women 's epee team event of the fie #### world championships by beating china ## -# # .,20,0,27
dataset/preprocessed/training-data/text_summarization/14,Reference : france beats china for women 's epee team gold State - of - the - art seq2seq model : canada wins women 's epee team event,21,0,28
dataset/preprocessed/training-data/text_summarization/14,"For the example shown above , the seq2seq system produces a fluent summary which contains an obvious mistake .",22,0,19
dataset/preprocessed/training-data/text_summarization/14,"The true winner of the "" women 's epee team event "" is "" france "" , while the summarization model wrongly generates "" canada "" , which is probably due to similar word representations for country names .",23,0,39
dataset/preprocessed/training-data/text_summarization/14,"Though the word overlap between the generated summary and the reference is considerable , leading to high ROUGE scores , the summary is invalid .",24,0,25
dataset/preprocessed/training-data/text_summarization/14,"We argue that correctness is an essential requirement for summarization systems , while most existing systems ignore it .",25,0,19
dataset/preprocessed/training-data/text_summarization/14,"Generally , a correct summary is semantically entailed by the source sentence , thus we believe entailment 1 knowledge is beneficial to avoid producing contradictory or unrelated information in the summary .",26,0,32
dataset/preprocessed/training-data/text_summarization/14,"To incorporate entailment knowledge into abstractive summarization models , we propose in this work an entailment - aware encoder and an entailment - aware decoder .",27,0,26
dataset/preprocessed/training-data/text_summarization/14,"We share the encoder of the summarization generation system with the entailment recognition system , so that the encoder can grasp both the gist of the source sentence and be aware of entailment relationships .",28,0,35
dataset/preprocessed/training-data/text_summarization/14,"Furthermore , we propose an entailment Reward Augmented Maximum Likelihood ( RAML ) training that encourages the decoder of the summarization system to produce summary entailed by the source .",29,0,30
dataset/preprocessed/training-data/text_summarization/14,Experimental results demonstrate that our models significantly outperform some solid baselines on objective evaluation for informativeness and manual evaluation for correctness .,30,0,22
dataset/preprocessed/training-data/text_summarization/14,Further analysis suggests that our summarization model is aware of entailment knowledge .,31,0,13
dataset/preprocessed/training-data/text_summarization/14,Our main contributions are as follows :,32,0,7
dataset/preprocessed/training-data/text_summarization/14,We incorporate entailment knowledge into summarization models to avoid producing unrelated information with respect to the source sentence .,33,0,19
dataset/preprocessed/training-data/text_summarization/14,We propose an entailment - aware encoder by jointly modeling summarization generation and entailment recognition .,34,0,16
dataset/preprocessed/training-data/text_summarization/14,We introduce an entailment - aware decoder via entailment RAML training .,35,0,12
dataset/preprocessed/training-data/text_summarization/14,Background : Seq2seq Learning,36,0,4
dataset/preprocessed/training-data/text_summarization/14,"In this section , we describe the basic seq2seq learning framework .",37,0,12
dataset/preprocessed/training-data/text_summarization/14,"Given a dataset of input-output pairs ,",38,0,7
dataset/preprocessed/training-data/text_summarization/14,", the seq2seq model maximizes the conditional probability of a target sequence y * : p ( y * | x ) .",39,0,23
dataset/preprocessed/training-data/text_summarization/14,Recurrent Neural Networks ( RNN ) encoder reads and converts a variablelength input sequence x into a context representation c as follows :,40,0,23
dataset/preprocessed/training-data/text_summarization/14,where ht ?,41,0,3
dataset/preprocessed/training-data/text_summarization/14,"Rn is a hidden state at time t , and ct is a context vector generated from the sequence of the hidden states .",42,0,24
dataset/preprocessed/training-data/text_summarization/14,f enc and f care nonlinear activation functions .,43,0,9
dataset/preprocessed/training-data/text_summarization/14,The decoder generates wordy t given the context vector ct and the previously generated words :,44,0,16
dataset/preprocessed/training-data/text_summarization/14,where st is the hidden state of the decoder and f dec is a nonlinear activation function .,45,0,18
dataset/preprocessed/training-data/text_summarization/14,The maximum likelihood ( ML ) framework tries to minimize negative log - likelihood of the parameters as follows :,46,0,20
dataset/preprocessed/training-data/text_summarization/14,3 Our Proposed Model,47,0,4
dataset/preprocessed/training-data/text_summarization/14,"In order to avoid generating unrelated summary with respect to the source sentence , we propose two strategies to incorporate entailment knowledge into seq2seq summarization model .",49,0,27
dataset/preprocessed/training-data/text_summarization/0,Abstractive Text Summarization by Incorporating Reader Comments,2,1,7
dataset/preprocessed/training-data/text_summarization/0,"In neural abstractive summarization field , conventional sequence - to - sequence based models often suffer from summarizing the wrong aspect of the document with respect to the main aspect .",4,1,31
dataset/preprocessed/training-data/text_summarization/0,"To tackle this problem , we propose the task of reader - aware abstractive summary generation , which utilizes the reader comments to help the model produce better summary about the main aspect .",5,1,34
dataset/preprocessed/training-data/text_summarization/0,"Unlike traditional abstractive summarization task , reader - aware summarization confronts two main challenges :",6,1,15
dataset/preprocessed/training-data/text_summarization/0,( 1 ) Comments are informal and noisy ;,7,0,9
dataset/preprocessed/training-data/text_summarization/0,( 2 ) jointly modeling the news document and the reader comments is challenging .,8,0,15
dataset/preprocessed/training-data/text_summarization/0,"To tackle the above challenges , we design an adversarial learning model named reader - aware summary generator ( RASG ) , which consists of four components :",9,0,28
dataset/preprocessed/training-data/text_summarization/0,( 1 ) a sequence - to - sequence based summary generator ; ( 2 ) a reader attention module capturing the reader focused aspects ;,10,0,26
dataset/preprocessed/training-data/text_summarization/0,( 3 ) a supervisor modeling the semantic gap between the generated summary and reader focused aspects ; ( 4 ) a goal tracker producing the goal for each generation step .,11,0,32
dataset/preprocessed/training-data/text_summarization/0,The supervisor and the goal tacker are used to guide the training of our framework in an adversarial manner .,12,0,20
dataset/preprocessed/training-data/text_summarization/0,"Extensive experiments are conducted on our large - scale real - world text summarization dataset , and the results show that RASG achieves the stateof - the - art performance in terms of both automatic metrics and human evaluations .",13,0,40
dataset/preprocessed/training-data/text_summarization/0,The experimental results also demonstrate the effectiveness of each module in our framework .,14,0,14
dataset/preprocessed/training-data/text_summarization/0,We release our large - scale dataset for further research 1 .,15,0,12
dataset/preprocessed/training-data/text_summarization/0,"Abstractive summarization can be regarded as a sequence mapping task that the source text is mapped to the target summary , and has drawn much attention since the deep neural networks are widely applied in natural language processing field .",17,0,40
dataset/preprocessed/training-data/text_summarization/0,"Recently , sequence - to - sequence ( seq2seq ) framework has been proved effective for the task of abstractive summarization and other text generation tasks .",18,0,27
dataset/preprocessed/training-data/text_summarization/0,"In this paper , we use "" aspect "" to denote the topic described in a specific paragraph or a sentence of a news document , and use "" main aspect "" to denote the central :",19,0,37
dataset/preprocessed/training-data/text_summarization/0,Examples of the text summarization .,20,0,6
dataset/preprocessed/training-data/text_summarization/0,"The text in red denotes the focused aspect by the good summary , while the text in blue is described by the bad summary .",21,0,25
dataset/preprocessed/training-data/text_summarization/0,The text with underline is the focused aspect by reader comments .,22,0,12
dataset/preprocessed/training-data/text_summarization/0,"On August 28 , according to a person familiar with the matter , Toyota Motor Corporation will invest 500 million U.S. dollars into the Uber , a taxi service company , with a valuation of up to 72 billion U.S. dollars .",23,0,42
dataset/preprocessed/training-data/text_summarization/0,The investment will focus on driverless car technology .,24,0,9
dataset/preprocessed/training-data/text_summarization/0,"However , its development path is not smooth .",25,0,9
dataset/preprocessed/training-data/text_summarization/0,"In March of this year , a Uber driverless car hit a woman and caused her death .",26,0,18
dataset/preprocessed/training-data/text_summarization/0,"In last year , Softbank also invested into Uber with a valuation of $ 48 billion .",27,0,17
dataset/preprocessed/training-data/text_summarization/0,comments Toyota 's investment in Uber is a wise choice .,28,0,11
dataset/preprocessed/training-data/text_summarization/0,$ 500 million investment is really a lot of money !,29,0,11
dataset/preprocessed/training-data/text_summarization/0,good summary Toyota invests $ 500 million into Uber with a valuation of $ 72 billion bad summary An Uber driverless car hits a passerby to death topic which the author tends to convey to the readers .,30,0,38
dataset/preprocessed/training-data/text_summarization/0,"Although a document may describe an event in many different aspects , the summary of this document should always focus on the main aspect .",31,0,25
dataset/preprocessed/training-data/text_summarization/0,"As shown in , the good summary describes the main aspect and the bad summary describes another trivial aspect that is not the main point of the document .",32,0,29
dataset/preprocessed/training-data/text_summarization/0,"To focus on the main aspect , some summarization methods first select several sentences about the main aspect and then generate the summary .",33,0,24
dataset/preprocessed/training-data/text_summarization/0,"However , it is very challenging to discover which is the main aspect of the news document .",34,0,18
dataset/preprocessed/training-data/text_summarization/0,"Nowadays , a great number of news comments are generated by readers to express their opinions about the event .",35,0,20
dataset/preprocessed/training-data/text_summarization/0,Some comments may mention the main aspect of the document for several times .,36,0,14
dataset/preprocessed/training-data/text_summarization/0,"Take the casein as an example , the focused aspect of the reader is "" investment of Toyota "" which is also the main aspect of this document .",37,0,29
dataset/preprocessed/training-data/text_summarization/0,"To be specific , we define "" reader focused aspect "" to denote the focused aspect by a reader through the comments .",38,0,23
dataset/preprocessed/training-data/text_summarization/0,"Intuitively , these reader comments may help the summary generator capture the main aspect of document , thereby improving the quality of the generated summary .",39,0,26
dataset/preprocessed/training-data/text_summarization/0,"Therefore , in this paper , we investigate a new problem setting of the task of abstractive text summarization .",40,0,20
dataset/preprocessed/training-data/text_summarization/0,We name such paradigm of extension as reader - aware abstractive text summarization .,41,0,14
dataset/preprocessed/training-data/text_summarization/0,The effect of comments or social contexts in document summarization have been explored by several previous works .,42,0,18
dataset/preprocessed/training-data/text_summarization/0,"Unlike these approaches that directly extract sentences from the original document , we aim to generate a natural - sounding summary from scratch instead of extracting words from the document .",43,0,31
dataset/preprocessed/training-data/text_summarization/0,"Generally , existing text summarization approaches confront two challenges when addressing reader - aware summarization task .",44,0,17
dataset/preprocessed/training-data/text_summarization/0,The first challenge is that reader comments are very noisy and informative .,45,0,13
dataset/preprocessed/training-data/text_summarization/0,Not all the information provided by the comments is useful when modeling the reader focused aspects .,46,0,17
dataset/preprocessed/training-data/text_summarization/0,"Therefore , it is crucial to make the model own the ability of capturing main aspect and filtering noisy information when incorporating reader comments .",47,0,25
dataset/preprocessed/training-data/text_summarization/0,The second challenge is how to generate summaries by jointly modeling the main aspect of document and the reader focused aspect revealed by comments .,48,0,25
dataset/preprocessed/training-data/text_summarization/0,"Meanwhile , the model should not be sensitive to the diverse unimportant aspects introduced by some reader comments .",49,0,19
dataset/preprocessed/training-data/text_summarization/12,Selective Encoding for Abstractive Sentence Summarization,2,1,6
dataset/preprocessed/training-data/text_summarization/12,We propose a selective encoding model to extend the sequence - to - sequence framework for abstractive sentence summarization .,4,0,20
dataset/preprocessed/training-data/text_summarization/12,"It consists of a sentence encoder , a selective gate network , and an attention equipped decoder .",5,0,18
dataset/preprocessed/training-data/text_summarization/12,The sentence encoder and decoder are built with recurrent neural networks .,6,0,12
dataset/preprocessed/training-data/text_summarization/12,The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder .,7,0,20
dataset/preprocessed/training-data/text_summarization/12,"The second level representation is tailored for sentence summarization task , which leads to better performance .",8,1,17
dataset/preprocessed/training-data/text_summarization/12,"We evaluate our model on the English Gigaword , DUC 2004 and MSR abstractive sentence summarization datasets .",9,0,18
dataset/preprocessed/training-data/text_summarization/12,The experimental results show that the proposed selective encoding model outperforms the state - of the - art baseline models .,10,0,21
dataset/preprocessed/training-data/text_summarization/12,Sentence summarization aims to shorten a given sentence and produce a brief summary of it .,12,0,16
dataset/preprocessed/training-data/text_summarization/12,"This is different from document level summarization task since it is hard to apply existing techniques in extractive methods , such as extracting sentence level features and ranking sentences .",13,0,30
dataset/preprocessed/training-data/text_summarization/12,"Early works propose using rule - based methods , syntactic tree pruning methods , statistical machine translation techniques and soon for this task .",14,0,24
dataset/preprocessed/training-data/text_summarization/12,We focus on abstractive sentence summarization task in this paper .,15,0,11
dataset/preprocessed/training-data/text_summarization/12,"Recently , neural network models have been applied in this task .",16,0,12
dataset/preprocessed/training-data/text_summarization/12,use autoconstructed sentence - headline pairs to train a neu - * Contribution during internship at Microsoft Research .,17,0,19
dataset/preprocessed/training-data/text_summarization/12,ral network summarization model .,18,0,5
dataset/preprocessed/training-data/text_summarization/12,They use a Convolutional Neural Network ( CNN ) encoder and feed - forward neural network language model decoder for this task .,19,0,23
dataset/preprocessed/training-data/text_summarization/12,extend their work by replacing the decoder with Recurrent Neural Network ( RNN ) .,20,0,15
dataset/preprocessed/training-data/text_summarization/12,follow this line and change the encoder to RNN to make it a full RNN based sequence - tosequence model .,21,0,21
dataset/preprocessed/training-data/text_summarization/12,the sri lankan government on wednesday announced the closure of government schools with immediate effect as a military campaign against tamil separatists escalated in the north of the country .,22,0,30
dataset/preprocessed/training-data/text_summarization/12,sri lanka closes schools as war escalates :,23,0,8
dataset/preprocessed/training-data/text_summarization/12,An abstractive sentence summarization system may produce the output summary by distilling the salient information from the highlight to generate a fluent sentence .,24,0,24
dataset/preprocessed/training-data/text_summarization/12,We model the distilling process with selective encoding .,25,0,9
dataset/preprocessed/training-data/text_summarization/12,"All the above works fall into the encodingdecoding paradigm , which first encodes the input sentence to an abstract representation and then decodes the intended output sentence based on the encoded information .",26,0,33
dataset/preprocessed/training-data/text_summarization/12,"As an extension of the encoding - decoding framework , attentionbased approach has been broadly used : the encoder produces a list of vectors for all tokens in the input , and the decoder uses an attention mechanism to dynamically extract encoded information and align with the output tokens .",27,0,50
dataset/preprocessed/training-data/text_summarization/12,"This approach achieves huge success in tasks like machine translation , where alignment between all parts of the input and output are required .",28,0,24
dataset/preprocessed/training-data/text_summarization/12,"However , in abstractive sentence summarization , there is no explicit alignment relationship between the input sentence and the summary ex-cept for the extracted common words .",29,0,27
dataset/preprocessed/training-data/text_summarization/12,"The challenge here is not to infer the alignment , but to select the highlights while filtering out secondary information in the input .",30,0,24
dataset/preprocessed/training-data/text_summarization/12,"A desired work - flow for abstractive sentence summarization is encoding , selection , and decoding .",31,0,17
dataset/preprocessed/training-data/text_summarization/12,"After selecting the important information from an encoded sentence , the decoder produces the output summary using the selected information .",32,0,21
dataset/preprocessed/training-data/text_summarization/12,"For example , in 1 , given the input sentence , the summarization system first selects the important information , and then rephrases or paraphrases to produce a well - organized summary .",33,0,33
dataset/preprocessed/training-data/text_summarization/12,"Although this is implicitly modeled in the encoding - decoding framework , we argue that abstractive sentence summarization shall benefit from explicitly modeling this selection process .",34,0,27
dataset/preprocessed/training-data/text_summarization/12,In this paper we propose Selective Encoding for Abstractive Sentence Summarization ( SEASS ) .,35,0,15
dataset/preprocessed/training-data/text_summarization/12,"We treat the sentence summarization as a threephase task : encoding , selection , and decoding .",36,0,17
dataset/preprocessed/training-data/text_summarization/12,"It consists of a sentence encoder , a selective gate network , and a summary decoder .",37,0,17
dataset/preprocessed/training-data/text_summarization/12,"First , the sentence encoder reads the input words through an RNN unit to construct the first level sentence representation .",38,0,21
dataset/preprocessed/training-data/text_summarization/12,Then the selective gate network selects the encoded information to construct the second level sentence representation .,39,0,17
dataset/preprocessed/training-data/text_summarization/12,"The selective mechanism controls the information flow from encoder to decoder by applying a gate network according to the sentence information , which helps improve encoding effectiveness and release the burden of the decoder .",40,0,35
dataset/preprocessed/training-data/text_summarization/12,"Finally , the attention - equipped decoder generates the summary using the second level sentence representation .",41,0,17
dataset/preprocessed/training-data/text_summarization/12,"We conduct experiments on English Gigaword , DUC 2004 and Microsoft Research Abstractive Text Compression test sets .",42,0,18
dataset/preprocessed/training-data/text_summarization/12,"Our SEASS model achieves 17.54 ROUGE - 2 F1 , 9.56 ROUGE - 2 recall and 10.63 ROUGE - 2 F1 on these test sets respectively , which improves performance compared to the state - of - the - art methods .",43,0,42
dataset/preprocessed/training-data/text_summarization/12,"Abstractive sentence summarization , also known as sentence compression and similar to headline generation , is used to help compressor fuse the selected sentences in extractive document summarization systems since they may inadvertently include unnecessary information .",45,0,37
dataset/preprocessed/training-data/text_summarization/12,The sentence summarization task has been long connected to the headline generation task .,46,0,14
dataset/preprocessed/training-data/text_summarization/12,"There are some previous methods to solve this task , such as the linguistic rule - based method .",47,0,19
dataset/preprocessed/training-data/text_summarization/12,"As for the statistical machine learning based methods , apply statistical machine translation techniques by modeling headline generation as a translation task and use 8000 article - headline pairs to train the system .",48,0,34
dataset/preprocessed/training-data/text_summarization/12,propose leveraging news data in Annotated English Gigaword corpus to construct large scale parallel data for sentence summarization task .,49,0,20
dataset/preprocessed/training-data/text_summarization/2,Structure - Infused Copy Mechanisms for Abstractive Summarization,2,1,8
dataset/preprocessed/training-data/text_summarization/2,Seq2seq learning has produced promising results on summarization .,4,1,9
dataset/preprocessed/training-data/text_summarization/2,"However , in many cases , system summaries still struggle to keep the meaning of the original intact .",5,0,19
dataset/preprocessed/training-data/text_summarization/2,They may miss out important words or relations that play critical roles in the syntactic structure of source sentences .,6,0,20
dataset/preprocessed/training-data/text_summarization/2,"In this paper , we present structure - infused copy mechanisms to facilitate copying important words and relations from the source sentence to summary sentence .",7,0,26
dataset/preprocessed/training-data/text_summarization/2,The approach naturally combines source dependency structure with the copy mechanism of an abstractive sentence summarizer .,8,0,17
dataset/preprocessed/training-data/text_summarization/2,"Experimental results demonstrate the effectiveness of incorporating source - side syntactic information in the system , and our proposed approach compares favorably to state - of - the - art methods .",9,0,32
dataset/preprocessed/training-data/text_summarization/2,": Example source sentences , reference and system summaries produced by a neural attentive seq - to - seq model .",11,0,21
dataset/preprocessed/training-data/text_summarization/2,"System summaries fail to preserve summary - worthy content of the source ( e.g. , main verbs ) despite their syntactic importance .",12,0,23
dataset/preprocessed/training-data/text_summarization/2,The sequence - to - sequence learning paradigm has achieved remarkable success on abstractive summarization .,13,0,16
dataset/preprocessed/training-data/text_summarization/2,"While the results are impressive , individual system summaries can appear unreliable and fail to preserve the meaning of the source texts .",14,0,23
dataset/preprocessed/training-data/text_summarization/2,presents two examples .,15,0,4
dataset/preprocessed/training-data/text_summarization/2,"In these cases , the syntactic structure of source sentences is relatively rare but perfectly normal .",16,0,17
dataset/preprocessed/training-data/text_summarization/2,"The first sentence contains two appositional phrases ( "" suspect of murdering Jorge Microsse , "" "" director of Maputo central prison "" ) and the second sentence has a relative clause ( "" who was too drunk to drive "" ) , both located between the subject and the main verb .",17,0,53
dataset/preprocessed/training-data/text_summarization/2,"The system , however , fails to identify the main verb in both cases ; it instead chooses to focus on the first few words of the source sentences .",18,0,30
dataset/preprocessed/training-data/text_summarization/2,"We observe that rare syntactic constructions of the source can pose problems for neural summarization systems , possibly for two reasons .",19,0,22
dataset/preprocessed/training-data/text_summarization/2,"First , similar to rare words , certain syntactic constructions do not occur frequently enough in the training data to allow the system to learn the patterns .",20,0,28
dataset/preprocessed/training-data/text_summarization/2,"Second , neural summarization systems are not explicitly informed of the syntactic structure of the source sentences and they tend to bias towards sequential recency .",21,0,26
dataset/preprocessed/training-data/text_summarization/2,A father who was too drunk to drive had his 11 - year - old son take the wheel .:,22,0,20
dataset/preprocessed/training-data/text_summarization/2,An example dependency parse tree created for the source sentence in .,23,0,12
dataset/preprocessed/training-data/text_summarization/2,"If important dependency edges such as "" father ? had "" can be preserved in the summary , the system summary is likely to preserve the meaning of the original .",24,0,31
dataset/preprocessed/training-data/text_summarization/2,In this paper we seek to address this problem by incorporating source syntactic structure in neural sentence summarization to help the system identify summary - worthy content and compose summaries that preserve the important meaning of the source texts .,25,0,40
dataset/preprocessed/training-data/text_summarization/2,We present structure - infused copy mechanisms to facilitate copying source words and relations to the summary based on their semantic and structural importance in the source sentences .,26,0,29
dataset/preprocessed/training-data/text_summarization/2,"For example , if important parts of the source syntactic structure , such as a dependency edge from the main verb to the subject ( "" father "" ?",27,0,29
dataset/preprocessed/training-data/text_summarization/2,""" had , "" shown in ) , can be preserved in the summary , the "" missing verb "" issue in can be effectively alleviated .",28,0,27
dataset/preprocessed/training-data/text_summarization/2,Our model therefore learns to recognize important source words and source dependency relations and strives to preserve them in the summaries .,29,0,22
dataset/preprocessed/training-data/text_summarization/2,Our research contributions include the following :,30,0,7
dataset/preprocessed/training-data/text_summarization/2,we introduce novel neural architectures that encourage salient source words / relations to be preserved in summaries .,31,0,18
dataset/preprocessed/training-data/text_summarization/2,The framework naturally combines the dependency parse tree structure with the copy mechanism of an abstractive summarization system .,32,0,19
dataset/preprocessed/training-data/text_summarization/2,"To the best of our knowledge , this is the first attempt at comparing various neural architectures for this purpose ; we study the effectiveness of several important components , including the vocabulary size , a coveragebased regularizer , and a beam search with reference mechanism ; through extensive experiments we demonstrate that incorporating syntactic information in neural sentence summarization is effective .",33,0,63
dataset/preprocessed/training-data/text_summarization/2,Our approach surpasses state - of - the - art published systems on the benchmark dataset .,34,0,17
dataset/preprocessed/training-data/text_summarization/2,"Prior to the deep learning era , sentence syntactic structure has been utilized to generate summaries with an "" extract - and - compress "" framework .",37,0,27
dataset/preprocessed/training-data/text_summarization/2,"Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents , or a pipeline approach that combines generic sentence compression with a sentence pre-selection or post-selection process .",38,0,34
dataset/preprocessed/training-data/text_summarization/2,"Although syntactic information is helpful for summarization , there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems .",39,0,29
dataset/preprocessed/training-data/text_summarization/2,Existing neural summarization systems handle syntactic structure only implicitly .,40,0,10
dataset/preprocessed/training-data/text_summarization/2,"Most systems adopt a "" cut-andstitch "" scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent language model .",41,0,29
dataset/preprocessed/training-data/text_summarization/2,"However , there lacks a mechanism to ensure structurally salient words and relations in source sentences are preserved in the summaries .",42,0,22
dataset/preprocessed/training-data/text_summarization/2,"The resulting summary sentences can contain misleading information ( e.g. , "" mozambican man arrested for murder "" flips the meaning of the original ) or grammatical errors ( e.g. , verbless , as in "" alaska father who was too drunk to drive "" ) .",43,0,47
dataset/preprocessed/training-data/text_summarization/2,"Natural language generation ( NLG ) - based abstractive summarization also makes extensive use of structural information , including syntactic / semantic parse trees , discourse structures , and domainspecific templates built using a text planner or an OpenIE system .",44,0,41
dataset/preprocessed/training-data/text_summarization/2,"In particular , leverage OpenIE and dependency parsing to extract fact tuples from the source text and use those to improve the faithfulness of summaries .",45,0,26
dataset/preprocessed/training-data/text_summarization/2,"Different from the above approaches , this paper seeks to directly incorporate source - side syntactic structure in the copy mechanism of an abstractive sentence summarization system .",46,0,28
dataset/preprocessed/training-data/text_summarization/2,"It learns to recognize important source words and relations during training , while striving to preserve them in the summaries at test time to aid reproduction of factual details .",47,0,30
dataset/preprocessed/training-data/text_summarization/2,"Our intent of incorporating source syntax in summarization is different from that of neural machine translation ( NMT ) , in part because NMT does not handle the information loss from source to target .",48,0,35
dataset/preprocessed/training-data/text_summarization/2,"In contrast , a summarization system must selectively preserve source content to render concise and grammatical summaries .",49,0,18
dataset/preprocessed/training-data/text_summarization/4,Entity Commonsense Representation for Neural Abstractive Summarization,2,1,7
dataset/preprocessed/training-data/text_summarization/4,A major proportion of a text summary includes important entities found in the original text .,4,0,16
dataset/preprocessed/training-data/text_summarization/4,These entities buildup the topic of the summary .,5,0,9
dataset/preprocessed/training-data/text_summarization/4,"Moreover , they hold commonsense information once they are linked to a knowledge base .",6,0,15
dataset/preprocessed/training-data/text_summarization/4,"Based on these observations , this paper investigates the usage of linked entities to guide the decoder of a neural text summarizer to generate concise and better summaries .",7,0,29
dataset/preprocessed/training-data/text_summarization/4,"To this end , we leverage on an off - the - shelf entity linking system ( ELS ) to extract linked entities and propose Entity2Topic ( E2T ) , a module easily attachable to a sequence - to - sequence model that transforms a list of entities into a vector representation of the topic of the summary .",8,0,59
dataset/preprocessed/training-data/text_summarization/4,"Current available ELS's are still not sufficiently effective , possibly introducing unresolved ambiguities and irrelevant entities .",9,0,17
dataset/preprocessed/training-data/text_summarization/4,"We resolve the imperfections of the ELS by ( a ) encoding entities with selective dis ambiguation , and ( b ) pooling entity vectors using firm attention .",10,0,29
dataset/preprocessed/training-data/text_summarization/4,"By applying E2T to a simple sequenceto - sequence model with attention mechanism as base model , we see significant improvements of the performance in the Gigaword ( sentence to title ) and CNN ( long document to multi-sentence highlights ) summarization datasets by at least 2 ROUGE points .",11,0,50
dataset/preprocessed/training-data/text_summarization/4,Text summarization is a task to generate a shorter and concise version of a text while preserving the meaning of the original text .,13,1,24
dataset/preprocessed/training-data/text_summarization/4,The task can be divided into two subtask based on the approach : extractive and abstractive summarization .,14,0,18
dataset/preprocessed/training-data/text_summarization/4,Extractive summarization is a task to create summaries by pulling out snippets of text form the original text and combining them to form a summary .,15,0,26
dataset/preprocessed/training-data/text_summarization/4,Abstractive summarization asks to generate summaries from scratch without the restriction to use * Amplayo and Lim are co-first authors with equal contribution .,16,0,24
dataset/preprocessed/training-data/text_summarization/4,Names are arranged alphabetically .,17,0,5
dataset/preprocessed/training-data/text_summarization/4,the available words from the original text .,18,0,8
dataset/preprocessed/training-data/text_summarization/4,"Due to the limitations of extractive summarization on incoherent texts and unnatural methodology , the research trend has shifted towards abstractive summarization .",19,0,23
dataset/preprocessed/training-data/text_summarization/4,"Sequence - to - sequence models with attention mechanism have found great success in generating abstractive summaries , both from a single sentence and from along document with multiple sentences .",20,0,31
dataset/preprocessed/training-data/text_summarization/4,"However , when generating summaries , it is necessary to determine the main topic and to sift out unnecessary information that can be omitted .",21,0,25
dataset/preprocessed/training-data/text_summarization/4,"Sequenceto - sequence models have the tendency to include all the information , relevant or not , thatare found in the original text .",22,0,24
dataset/preprocessed/training-data/text_summarization/4,This may result to unconcise summaries that concentrates wrongly on irrelevant topics .,23,0,13
dataset/preprocessed/training-data/text_summarization/4,The problem is especially severe when summarizing longer texts .,24,0,10
dataset/preprocessed/training-data/text_summarization/4,"In this paper , we propose to use entities found in the original text to infer the summary topic , miti-gating the aforementioned problem .",25,0,25
dataset/preprocessed/training-data/text_summarization/4,"Specifically , we leverage on linked entities extracted by employing a readily available entity linking system .",26,0,17
dataset/preprocessed/training-data/text_summarization/4,The importance of using linked entities in summarization is intuitive and can be explained by looking at as an example .,27,0,21
dataset/preprocessed/training-data/text_summarization/4,"First ( O1 in the , aside from auxiliary words to construct a sentence , a summary is mainly composed of linked entities extracted from the original text .",28,0,29
dataset/preprocessed/training-data/text_summarization/4,"Second ( O2 ) , we can depict the main topic of the summary as a probability distribution of relevant entities from the list of entities .",29,0,27
dataset/preprocessed/training-data/text_summarization/4,"Finally ( O3 ) , we can leverage on entity commonsense learned from a separate large knowledge base such as Wikipedia .",30,0,22
dataset/preprocessed/training-data/text_summarization/4,"To this end , we present a method to effectively apply linked entities in sequence - tosequence models , called Entity2Topic ( E2T ) .",31,0,25
dataset/preprocessed/training-data/text_summarization/4,E2T is a module that can be easily attached to any sequence - to - sequence based summarization model .,32,0,20
dataset/preprocessed/training-data/text_summarization/4,"The module encodes the entities extracted from the original text by an entity linking system ( ELS ) , constructs a vector representing the topic of the summary to be generated , and informs the decoder about the constructed topic vector .",33,0,42
dataset/preprocessed/training-data/text_summarization/4,"Due to the imperfections of current ELS 's , the extracted linked entities maybe too ambiguous and coarse to be considered relevant to the summary .",34,0,26
dataset/preprocessed/training-data/text_summarization/4,We solve this issue by using entity encoders with selective disambiguation and by constructing topic vectors using firm attention .,35,0,20
dataset/preprocessed/training-data/text_summarization/4,"We experiment on two datasets , Gigaword and CNN , with varying lengths .",36,0,14
dataset/preprocessed/training-data/text_summarization/4,We show that applying our module to a sequence - to - sequence model with attention mechanism significantly increases its performance on both datasets .,37,0,25
dataset/preprocessed/training-data/text_summarization/4,"Moreover , when compared with the state - of - the - art models for each dataset , the model obtains a comparable performance on the Gigaword dataset where the texts are short , and outperforms all competing models on the CNN dataset where the texts are longer .",38,0,49
dataset/preprocessed/training-data/text_summarization/4,"Furthermore , we provide analysis on how our model effectively uses the extracted linked entities to produce concise and better summaries .",39,0,22
dataset/preprocessed/training-data/text_summarization/4,Usefulness of linked entities in summarization,40,0,6
dataset/preprocessed/training-data/text_summarization/4,"In the next subsections , we present detailed arguments with empirical and previously examined evidences on the observations and possible issues when using linked entities extracted by an entity linking system ( ELS ) for generating abstractive summaries .",41,0,39
dataset/preprocessed/training-data/text_summarization/4,"For this purpose , we use the development sets of the Gigaword dataset provided in and of the CNN dataset provided in as the experimental data for quantitative evidence and refer the readers to as the running example .",42,0,39
dataset/preprocessed/training-data/text_summarization/4,"As discussed in Section 1 , we find three observations that show the usefulness of linked entities for abstractive summarization .",44,0,21
dataset/preprocessed/training-data/text_summarization/4,"First , summaries are mainly composed of linked entities extracted from the original text .",45,0,15
dataset/preprocessed/training-data/text_summarization/4,"In the example , it can be seen that the summary contains four words that refer to different entities .",46,0,20
dataset/preprocessed/training-data/text_summarization/4,"In fact , all noun phrases in the summary mention at least one linked entity .",47,0,16
dataset/preprocessed/training-data/text_summarization/4,"In our experimental data , we extract linked entities from the original text and compare them to the noun phrases found in the summary .",48,0,25
dataset/preprocessed/training-data/text_summarization/4,"We report that 77.1 % and 75.1 % of the noun phrases on the Gigaword and CNN datasets , respectively , contain at least one linked entity , which confirms our observation .",49,0,33
dataset/preprocessed/training-data/text_summarization/13,Coarse-to-Fine Attention Models for Document Summarization,2,1,6
dataset/preprocessed/training-data/text_summarization/13,"Sequence - to - sequence models with attention have been successful for a variety of NLP problems , but their speed does not scale well for tasks with long source sequences such as document summarization .",4,0,36
dataset/preprocessed/training-data/text_summarization/13,"We propose a novel coarse - to - fine attention model that hierarchically reads a document , using coarse attention to select top - level chunks of text and fine attention to read the words of the chosen chunks .",5,0,40
dataset/preprocessed/training-data/text_summarization/13,"While the computation for training standard attention models scales linearly with source sequence length , our method scales with the number of top - level chunks and can handle much longer sequences .",6,0,33
dataset/preprocessed/training-data/text_summarization/13,"Empirically , we find that while coarse - tofine attention models lag behind state - of the - art baselines , our method achieves the desired behavior of sparsely attending to subsets of the document for generation .",7,0,38
dataset/preprocessed/training-data/text_summarization/13,"The sequence - to - sequence architecture of , also known as the encoder - decoder architecture , is now the gold standard for many NLP tasks , including machine translation , question answering , dialogue , caption generation , and in particular summarization .",9,0,45
dataset/preprocessed/training-data/text_summarization/13,A popular variant of sequence - to - sequence models are attention models .,10,0,14
dataset/preprocessed/training-data/text_summarization/13,"By keeping an encoded representation of each part of the input , we "" attend "" to the relevant part each time we produce an output from the decoder .",11,0,30
dataset/preprocessed/training-data/text_summarization/13,"In practice , this means computing attention weights for all encoder hidden states , then taking the weighted average as our new context vector .",12,0,25
dataset/preprocessed/training-data/text_summarization/13,"While successful , existing sequence - tosequence methods are computationally limited by the length of source and target sequences .",13,0,20
dataset/preprocessed/training-data/text_summarization/13,"For a problem such as document summarization , a source sequence of length N ( where N could potentially be very large ) requires O( N ) model computations to encode .",14,0,32
dataset/preprocessed/training-data/text_summarization/13,"However , it makes sense intuitively that not every word of the source will be necessary for generating a summary , and so we would like to reduce the amount of computation performed on the source .",15,0,37
dataset/preprocessed/training-data/text_summarization/13,"Therefore , in order to scale attention models for this problem , we aim to prune down the length of the source sequence in an intelligent way .",16,0,28
dataset/preprocessed/training-data/text_summarization/13,"Instead of naively attending to all the words of the source at once , our solution is to use a two - layer hierarchical attention .",17,0,26
dataset/preprocessed/training-data/text_summarization/13,"For document summarization , this means dividing the document into chunks of text , sparsely attending to one or a few chunks at a time using hard attention , then applying the usual full attention over those chunks - we call this method coarse - to - fine attention .",18,0,50
dataset/preprocessed/training-data/text_summarization/13,"Through experiments , we find that while coarse - to - fine attention does not perform as well as standard attention , it does show the desired behavior of sparsely reading the source sequence .",19,0,35
dataset/preprocessed/training-data/text_summarization/13,We structure the rest of the paper as follows .,20,0,10
dataset/preprocessed/training-data/text_summarization/13,"In Section 2 , we introduce related work on summarization and neural attention .",21,0,14
dataset/preprocessed/training-data/text_summarization/13,"In Section 3 , we review the encoder - decoder framework , and in Section 4 introduce our models .",22,0,20
dataset/preprocessed/training-data/text_summarization/13,"In Section 5 , we describe our experimental setup , and in Section 6 show results .",23,0,17
dataset/preprocessed/training-data/text_summarization/13,"Finally , we conclude in Section 7 .",24,0,8
dataset/preprocessed/training-data/text_summarization/13,"In summarization , neural attention models were first applied by to do headline generation , i.e. produce a title for a news article given only the first sentence .",26,0,29
dataset/preprocessed/training-data/text_summarization/13,"and apply attention models to summarize full documents , achieving stateof - the - art results on the CNN / Dailymail dataset .",27,0,23
dataset/preprocessed/training-data/text_summarization/13,"All of these models , however , suffer from the inherent complexity of attention over the full document .",28,0,19
dataset/preprocessed/training-data/text_summarization/13,"Indeed , report that a single model takes over 3 days to train .",29,0,14
dataset/preprocessed/training-data/text_summarization/13,Many techniques have been proposed in the literature to efficiently handle the problem of large inputs to deep neural networks .,30,0,21
dataset/preprocessed/training-data/text_summarization/13,"One particular framework is that of "" conditional computation "" , as coined by - the idea is to only compute a subset of a network 's units for a given input by gating different parts of the network .",31,0,40
dataset/preprocessed/training-data/text_summarization/13,"Several methods , some stochastic and some deterministic , have been explored in the vein of conditional computation .",32,0,19
dataset/preprocessed/training-data/text_summarization/13,"In this work , we will focus on stochastic methods , although deterministic methods are worth considering as future work .",33,0,21
dataset/preprocessed/training-data/text_summarization/13,"On the stochastic front , demonstrate the effectiveness of "" hard "" attention .",34,0,14
dataset/preprocessed/training-data/text_summarization/13,"While standard "" soft "" attention averages the representations of where the model attends to , hard attention discretely selects a single location .",35,0,24
dataset/preprocessed/training-data/text_summarization/13,"Hard attention has been successfully applied in various computer vision tasks , but so far has limited usage in NLP .",36,0,21
dataset/preprocessed/training-data/text_summarization/13,We will apply hard attention to the document summarization task by sparsifying our reading of the source text .,37,0,19
dataset/preprocessed/training-data/text_summarization/13,"We begin by describing the standard sequence - tosequence attention model , also known as encoderdecoder models .",39,0,18
dataset/preprocessed/training-data/text_summarization/13,"In the encoder - decoder architecture , an encoder recurrent neural network ( RNN ) reads the source sequence as input to produce the context , and a decoder RNN generates the output sequence using the context as input .",40,0,40
dataset/preprocessed/training-data/text_summarization/13,"Formally , suppose we have a vocabulary V .",41,0,9
dataset/preprocessed/training-data/text_summarization/13,"A given input sequence w 1 , . . . , w n ?",42,0,14
dataset/preprocessed/training-data/text_summarization/13,"V is transformed into a sequence of vectors x 1 , . . . , x n ?",43,0,18
dataset/preprocessed/training-data/text_summarization/13,Rd in through a word embedding matrix E ?,44,0,9
dataset/preprocessed/training-data/text_summarization/13,R | V|d in as x t = Ew t .,45,0,11
dataset/preprocessed/training-data/text_summarization/13,The encoder RNN is given by a parameterizable function f enc and a hidden state ht ?,46,0,17
dataset/preprocessed/training-data/text_summarization/13,"Rd hid at each time step t with ht = f enc ( x t , h t?1 ) .",47,0,20
dataset/preprocessed/training-data/text_summarization/13,"In our models , we use the long - short term memory ( LSTM ) network .",48,0,17
dataset/preprocessed/training-data/text_summarization/13,The decoder is another RNN f dec that generates output words y t ?,49,0,14
dataset/preprocessed/training-data/smile_recognition/0,Deep Learning For Smile Recognition,2,1,5
dataset/preprocessed/training-data/smile_recognition/0,"Inspired by recent successes of deep learning in computer vision , we propose a novel application of deep convolutional neural networks to facial expression recognition , in particular smile recognition .",4,1,31
dataset/preprocessed/training-data/smile_recognition/0,"A smile recognition test accuracy of 99. 45 % is achieved for the Denver Intensity of Spontaneous Facial Action ( DISFA ) database , significantly outperforming existing approaches based on hand - crafted features with accuracies ranging from 65 . 55 % to 79.67 % .",5,0,46
dataset/preprocessed/training-data/smile_recognition/0,"The novelty of this approach includes a comprehensive model selection of the architecture parameters , allowing to find an appropriate architecture for each expression such as smile .",6,0,28
dataset/preprocessed/training-data/smile_recognition/0,"This is feasible because all experiments were run on a Tesla K40c GPU , allowing a speedup of factor 10 over traditional computations on a CPU .",7,0,27
dataset/preprocessed/training-data/smile_recognition/0,"Neural networks are celebrating a comeback under the term "" deep learning "" for the last ten years by training many hidden layers allowing to selflearn complex feature hierarchies .",9,0,30
dataset/preprocessed/training-data/smile_recognition/0,"This makes them of particular interest for computer vision , in which feature description is a long - standing issue .",10,0,21
dataset/preprocessed/training-data/smile_recognition/0,"Many advances have been reported in this period , including new training methods and a paradigm shift of training from CPUs to GPUs .",11,0,24
dataset/preprocessed/training-data/smile_recognition/0,"As a result , those advances allow to train more reliable models much faster .",12,0,15
dataset/preprocessed/training-data/smile_recognition/0,This has for example resulted in breakthroughs 3 in signal processing .,13,0,12
dataset/preprocessed/training-data/smile_recognition/0,"Nonetheless , deep neural networks are not a magic bullet and successful training is still heavily based on experimentation .",14,0,20
dataset/preprocessed/training-data/smile_recognition/0,The Facial Action Coding System ( FACS ) 1 is a system to taxonomize any facial expression of a human being by their appearance on the face .,15,0,28
dataset/preprocessed/training-data/smile_recognition/0,"Action units describe muscles or muscle groups in the face , are set or un - set and the activation maybe on different intensity levels .",16,0,26
dataset/preprocessed/training-data/smile_recognition/0,State - of - the art approaches in this field mostly rely on hand - crafted features leaving a lot of potential for higher accuracies .,17,0,26
dataset/preprocessed/training-data/smile_recognition/0,"In contrast to other fields such as face or gesture recognition , only very few works on deep learning applied to facial expression recognition have been reported so far 2 in which the architecture parameters are fixed .",18,0,38
dataset/preprocessed/training-data/smile_recognition/0,We are not aware of publications in which the architecture of a deep neural network for facial expression recognition is subject to extensive model selection .,19,0,26
dataset/preprocessed/training-data/smile_recognition/0,This allows to learn appropriate architectures per action unit .,20,0,10
dataset/preprocessed/training-data/smile_recognition/0,Deep neural networks,21,0,3
dataset/preprocessed/training-data/smile_recognition/0,"Training neural networks is difficult , as their cost functions have many local minima .",22,0,15
dataset/preprocessed/training-data/smile_recognition/0,"The more hidden layers , the more difficult the training of a neural network .",23,0,15
dataset/preprocessed/training-data/smile_recognition/0,"Hence , training tends to converge to a local minimum , resulting in poor generalization of the network .",24,0,19
dataset/preprocessed/training-data/smile_recognition/0,"In order to overcome these issues , a variety of new concepts have been proposed in the literature , of which only a few can be named in this chapter .",25,0,31
dataset/preprocessed/training-data/smile_recognition/0,"Unsupervised pre-training methods , such as autoencoders 8 allow to initialize the weights well in order for backpropagation to quickly optimize them .",26,0,23
dataset/preprocessed/training-data/smile_recognition/0,The Rectified Linear Unit ( ReLU ) 7 and dropout 10 are new regularization methods .,27,0,16
dataset/preprocessed/training-data/smile_recognition/0,The new training methods and other new concepts can also lead to significant improvements of shallow neural networks with just a few hidden layers .,28,0,25
dataset/preprocessed/training-data/smile_recognition/0,Convolutional neural networks ( CNNs ) were initially proposed by LeCun 5 for the recognition of hand - written digits .,29,0,21
dataset/preprocessed/training-data/smile_recognition/0,"A CNN consists of two layers : a convolutional layer , followed by a subsampling layer .",30,0,17
dataset/preprocessed/training-data/smile_recognition/0,"Inspired by biological processes and exploiting the fact that nearby pixels are strongly correlated , CNNs are relatively insensitive to small translations or rotations of the image input .",31,0,29
dataset/preprocessed/training-data/smile_recognition/0,Training deep neural networks is slow due to the number of parameters in the model .,32,0,16
dataset/preprocessed/training-data/smile_recognition/0,"As the training can be described in a vectorized form , it is possible to massively parallelize it .",33,0,19
dataset/preprocessed/training-data/smile_recognition/0,GPUs have thousands of cores and are therefore an ideal candidate for the execution of the training of neural networks .,35,0,21
dataset/preprocessed/training-data/smile_recognition/0,Significant speedups of factor 10 or higher 9 have been reported .,36,0,12
dataset/preprocessed/training-data/smile_recognition/0,A difficulty is to write GPU code .,37,0,8
dataset/preprocessed/training-data/smile_recognition/0,"In the last few years , more abstract libraries have been released .",38,0,13
dataset/preprocessed/training-data/smile_recognition/0,"The Denver Intensity of Spontaneous Facial Action ( DISFA ) 6 database consists of 27 videos of 4844 frames each , with 130,788 images in total .",40,0,27
dataset/preprocessed/training-data/smile_recognition/0,"Action unit annotations are on different levels of intensity , which are ignored in the following experiments and action units are either set or unset .",41,0,26
dataset/preprocessed/training-data/smile_recognition/0,"DISFA was selected from a wider range of databases popular in the field of facial expression recognition because of the high number of smiles , i.e. action unit 12 .",42,0,30
dataset/preprocessed/training-data/smile_recognition/0,"In detail , 30,792 have this action unit set , 82,176 images have some action unit ( s ) set and 48,612 images have no action unit ( s ) set at all .",43,0,34
dataset/preprocessed/training-data/smile_recognition/0,contains a sample image of DISFA .,44,0,7
dataset/preprocessed/training-data/smile_recognition/0,In the original paper on DISFA 6 multi-class SVMs were trained for the different levels 0 - 5 of action unit intensity .,45,0,23
dataset/preprocessed/training-data/smile_recognition/0,Test accuracies for the individual levels and for the binary action unit recognition problem are reported for three different hand - crafted feature description techniques .,46,0,26
dataset/preprocessed/training-data/smile_recognition/0,"In those three cases , accuracies of 65.55 % , 72.94 % and 79. 67 % for smile recognition are reported .",47,0,22
dataset/preprocessed/training-data/smile_recognition/0,"In the following experiments , an aligned version of DISFA is used .",49,0,13
